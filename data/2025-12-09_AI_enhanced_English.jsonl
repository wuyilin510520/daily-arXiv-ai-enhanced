{"id": "2512.05969", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05969", "abs": "https://arxiv.org/abs/2512.05969", "authors": ["Hokin Deng"], "title": "Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices", "comment": "See $\\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and $\\href{https://github.com/hokindeng/VMEvalKit}{code}$", "summary": "We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the \"Task Pair\" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.", "AI": {"tldr": "Video generation models like Sora-2 can now reason with ~60% success on cognitive tasks (chess, Sudoku, Raven's Matrices) using a \"Task Pair\" paradigm and scalable evaluation framework.", "motivation": "To establish whether video generation models can perform reasoning tasks, and to create a scalable experimental paradigm for evaluating and improving their reasoning capabilities.", "method": "Developed a \"Task Pair\" experimental design and built VMEvalKit framework with 39 models, enabling automated evaluation of video models on reasoning tasks like chess, maze, Sudoku, mental rotation, and Raven's Matrices.", "result": "Leading models like Sora-2 achieve approximately 60% success rates on reasoning tasks, and automated evaluation strongly correlates with human judgment, validating the paradigm's reliability.", "conclusion": "Video generation models demonstrate reasoning capabilities, and the established paradigm enables scalable evaluation and future work like reinforcement learning for improving reasoning in video models."}}
{"id": "2512.05987", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05987", "abs": "https://arxiv.org/abs/2512.05987", "authors": ["Chenyue Yu", "Jianyu Yu"], "title": "Adaptive Dataset Quantization: A New Direction for Dataset Pruning", "comment": "Accepted by ICCPR 2025", "summary": "This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.", "AI": {"tldr": "A novel dataset quantization method reduces intra-sample redundancy in images for edge devices, using adaptive quantization allocation to maintain training performance while achieving significant compression.", "motivation": "Address storage and communication costs for large-scale datasets on resource-constrained edge devices by reducing intra-sample redundancy, unlike traditional methods that focus on inter-sample redundancy.", "method": "Uses linear symmetric quantization for initial range/scale per sample, then adaptive quantization allocation algorithm to distribute different quantization ratios across samples while maintaining constant total compression ratio.", "result": "Maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines on CIFAR-10, CIFAR-100, and ImageNet-1K.", "conclusion": "Proposed dataset quantization approach effectively reduces storage needs for edge devices while preserving essential features and training performance, offering a practical solution for resource-constrained environments."}}
{"id": "2512.05988", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05988", "abs": "https://arxiv.org/abs/2512.05988", "authors": ["Junho Kim", "Seongwon Lee"], "title": "VG3T: Visual Geometry Grounded Gaussian Transformer", "comment": null, "summary": "Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.", "AI": {"tldr": "VG3T is a multi-view feed-forward network that predicts 3D semantic occupancy via 3D Gaussians, achieving better performance with fewer primitives than previous methods.", "motivation": "Existing methods struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance in generating coherent 3D scene representations from multi-view images.", "method": "VG3T directly predicts semantically attributed 3D Gaussians in a joint, multi-view fashion, using Grid-Based Sampling and Positional Refinement to mitigate distance-dependent density bias in Gaussian initialization.", "result": "VG3T achieves 1.7%p improvement in mIoU while using 46% fewer primitives than previous state-of-the-art on the nuScenes benchmark.", "conclusion": "VG3T offers a unified paradigm for representing both geometry and semantics, overcoming fragmentation and inconsistency issues in view-by-view processing methods."}}
{"id": "2512.05991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.05991", "abs": "https://arxiv.org/abs/2512.05991", "authors": ["Chang Liu", "Tianjiao Jing", "Chengcheng Ma", "Xuanqi Zhou", "Zhengxuan Lian", "Qin Jin", "Hongliang Yuan", "Shi-Sheng Huang"], "title": "EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head", "comment": null, "summary": "Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.", "AI": {"tldr": "EmoDiffTalk: A novel 3D Gaussian Splatting talking head framework with emotion-aware diffusion for fine-grained facial animation and text-to-AU emotional editing.", "motivation": "Existing 3D talking head methods using 3D Gaussian Splatting lack fine-grained emotional expression manipulation and expansive dynamic emotional editing with multi-modal controls.", "method": "Proposes Emotion-aware Gaussian Diffusion with two components: 1) AU prompt Gaussian diffusion for fine-grained facial animation, and 2) accurate text-to-AU emotion controller for dynamic emotional editing using text input.", "result": "Superior performance on EmoTalk3D and RenderMe-360 datasets, demonstrating better emotional subtlety, lip-sync fidelity, and controllability compared to previous works.", "conclusion": "Establishes a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis, representing one of the first 3D Gaussian Splatting frameworks supporting continuous, multimodal emotional editing in AU-based expression space."}}
{"id": "2512.05993", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05993", "abs": "https://arxiv.org/abs/2512.05993", "authors": ["Ruchika Verma", "Shrishtee Kandoi", "Robina Afzal", "Shengjia Chen", "Jannes Jegminat", "Michael W. Karlovich", "Melissa Umphlett", "Timothy E. Richardson", "Kevin Clare", "Quazi Hossain", "Jorge Samanamud", "Phyllis L. Faust", "Elan D. Louis", "Ann C. McKee", "Thor D. Stein", "Jonathan D. Cherry", "Jesse Mez", "Anya C. McGoldrick", "Dalilah D. Quintana Mora", "Melissa J. Nirenberg", "Ruth H. Walker", "Yolfrankcis Mendez", "Susan Morgello", "Dennis W. Dickson", "Melissa E. Murray", "Carlos Cordon-Cardo", "Nadejda M. Tsankova", "Jamie M. Walker", "Diana K. Dangoor", "Stephanie McQuillan", "Emma L. Thorn", "Claudia De Sanctis", "Shuying Li", "Thomas J. Fuchs", "Kurt Farrell", "John F. Crary", "Gabriele Campanella"], "title": "Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology", "comment": null, "summary": "Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.", "AI": {"tldr": "NeuroFM is a specialized foundation model for neuropathology that outperforms general surgical pathology models on brain-specific tasks like neurodegenerative disease classification and segmentation.", "motivation": "Existing pathology foundation models are trained predominantly on surgical pathology data (non-nervous tissue), creating a domain mismatch for neuropathology which has unique cell types, cytoarchitecture, and disease-specific features like neurofibrillary tangles and amyloid plaques. This limits their ability to capture morphological patterns critical for neurodegenerative disease interpretation.", "method": "Developed NeuroFM, a domain-specialized foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies, rather than using general surgical pathology datasets.", "result": "NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks: mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification (including cerebellar essential tremor and spinocerebellar ataxia subtypes).", "conclusion": "Domain-specialized foundation models trained on brain tissue better capture neuropathology-specific features than general surgical pathology models. NeuroFM enables more accurate AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology."}}
{"id": "2512.05996", "categories": ["cs.CV", "cs.CY", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.05996", "abs": "https://arxiv.org/abs/2512.05996", "authors": ["Yi Liu", "Jingyu Song", "Vedanth Kallakuri", "Katherine A. Skinner"], "title": "FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting", "comment": "18 pages, under review", "summary": "Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.", "AI": {"tldr": "FishDetector-R1 is a unified MLLM-based framework for fish detection, segmentation, and counting using weak supervision, achieving significant performance gains on underwater fish imagery with improved cross-domain robustness.", "motivation": "Underwater fish imagery analysis is crucial for ecological monitoring but faces challenges due to visual degradation and expensive annotation requirements, necessitating more efficient and accurate solutions.", "method": "A unified MLLM-based framework with two key components: 1) a novel detect-to-count prompt enforcing spatially consistent detections and counts, and 2) Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm using sparse point labels.", "result": "On DeepFish dataset: 20% AP improvement, 10% mIoU improvement, 30% MAE reduction, 35% GAME reduction. Strong cross-domain generalization to other underwater datasets confirmed through ablation studies validating reward design effectiveness.", "conclusion": "FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision, offering substantial performance gains and robust generalization across underwater domains."}}
{"id": "2512.06003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06003", "abs": "https://arxiv.org/abs/2512.06003", "authors": ["Ramin Sharifi", "Pouya Shiri", "Amirali Baniasadi"], "title": "PrunedCaps: A Case For Primary Capsules Discrimination", "comment": null, "summary": "Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.", "AI": {"tldr": "Capsule Networks can be pruned by removing 95% of Primary Capsules, achieving up to 9.9x speedup and 95.36% reduction in FLOPs without accuracy loss on multiple datasets.", "motivation": "Capsule Networks have advantages over CNNs (better robustness to affine transformations, overlapping image detection) but are resource-inefficient due to high number of Primary Capsules, with slow and resource-hungry training/testing.", "method": "Investigates Primary Capsules pruning in CapsNets on MNIST, Fashion-MNIST, CIFAR-10, and SVHN datasets. Removes 95% of Capsules through pruning techniques.", "result": "Pruned CapsNet performs up to 9.90 times faster than conventional architecture with 95% capsule removal and no accuracy loss. Saves over 95.36% of floating-point operations in dynamic routing stage. Provides insights on why some datasets benefit more from pruning than others.", "conclusion": "Significant efficiency improvements can be achieved in Capsule Networks through Primary Capsules pruning, making them more practical for real-world applications while maintaining accuracy."}}
{"id": "2512.06006", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06006", "abs": "https://arxiv.org/abs/2512.06006", "authors": ["Xuefei", "Wang", "Kai A. Horstmann", "Ethan Lin", "Jonathan Chen", "Alexander R. Farhang", "Sophia Stiles", "Atharva Sehgal", "Jonathan Light", "David Van Valen", "Yisong Yue", "Jennifer J. Sun"], "title": "Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization", "comment": null, "summary": "Adapting production-level computer vision tools to bespoke scientific datasets is a critical \"last mile\" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.", "AI": {"tldr": "AI agents can automate adaptation of computer vision tools to scientific datasets, outperforming human experts with simpler architectures.", "motivation": "Adapting production-level computer vision tools to scientific datasets is a critical bottleneck. Current solutions (fine-tuning requiring large annotated datasets or manual code adaptation) are impractical for scientists who lack resources and time.", "method": "Introduce a systematic evaluation framework for agentic code optimization. Study three production-level biomedical imaging pipelines using AI agents to automate manual coding adaptation. Compare different agent architectures.", "result": "Simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Complex agent architectures are not universally beneficial. Framework is open-sourced and agent-generated functions were successfully deployed into a production pipeline.", "conclusion": "AI agents provide a practical solution for automating computer vision tool adaptation to scientific datasets, with simpler architectures often being more effective than complex ones. The approach demonstrates clear pathway for real-world impact."}}
{"id": "2512.06010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06010", "abs": "https://arxiv.org/abs/2512.06010", "authors": ["Thomas Massena", "Corentin Friedrich", "Franck Mamalet", "Mathieu Serrurier"], "title": "Fast and Flexible Robustness Certificates for Semantic Segmentation", "comment": null, "summary": "Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\\ell_2$ attacks of radius $\u03b5$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.", "AI": {"tldr": "Certifiably robust semantic segmentation networks with Lipschitz constraints that are efficiently trainable and provide real-time compatible certification 600\u00d7 faster than randomized smoothing.", "motivation": "Deep Neural Networks are vulnerable to adversarial perturbations, but most robust DL research focuses on classification tasks with few efficient certification procedures for semantic segmentation.", "method": "Introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable. Provide a novel framework that generalizes robustness certificates for semantic segmentation tasks using Lipschitz networks.", "result": "Achieve competitive pixel accuracy on challenging datasets like Cityscapes. Approach is around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Unlocks real-time compatible certifiably robust semantic segmentation for the first time.", "conclusion": "The proposed Lipschitz-constrained networks provide an efficient, flexible, and computationally efficient framework for certifiably robust semantic segmentation with real-time capabilities and comprehensive worst-case performance evaluation under adversarial attacks."}}
{"id": "2512.06012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06012", "abs": "https://arxiv.org/abs/2512.06012", "authors": ["Emmanuel Akeweje", "Conall Kirk", "Chi-Wai Chan", "Denis Dowling", "Mimi Zhang"], "title": "High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing", "comment": null, "summary": "Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.", "AI": {"tldr": "Automated ML framework for high-throughput powder morphology analysis in SLM using imaging and clustering to assess feedstock quality.", "motivation": "Conventional powder characterization methods for SLM are low-throughput and qualitative, failing to capture heterogeneity in industrial-scale batches needed for quality control.", "method": "Developed three ML clustering pipelines: autoencoder, shape-descriptor, and functional-data approaches. Used high-throughput imaging with shape extraction on ~126,000 powder images (0.5-102 \u03bcm diameter).", "result": "Fourier-descriptor + k-means pipeline performed best with lowest Davies-Bouldin index and highest Calinski-Harabasz score, achieving sub-millisecond runtime per particle on standard desktop.", "conclusion": "Unsupervised learning framework enables rapid automated powder morphology assessment, supporting shape evolution tracking across reuse cycles and real-time feedstock monitoring in SLM workflows."}}
{"id": "2512.06013", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06013", "abs": "https://arxiv.org/abs/2512.06013", "authors": ["Wenhao Li", "Chengwei Ma", "Weixin Mao"], "title": "VAT: Vision Action Transformer by Unlocking Full Representation of ViT", "comment": null, "summary": "In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.", "AI": {"tldr": "VAT (Vision Action Transformer) is a novel architecture that leverages the full feature hierarchy of Vision Transformers for robotic imitation learning, achieving state-of-the-art performance on manipulation tasks by progressively fusing perception and action across all transformer layers.", "motivation": "Current robot learning methods using Vision Transformers discard valuable information by only using the final layer's features, providing insufficient representation for robotic tasks. The authors argue that leveraging the complete feature hierarchy of ViTs is critical for better robotic policy learning.", "method": "VAT extends Vision Transformers by processing specialized action tokens with visual features across all transformer layers, enabling deep and progressive fusion of perception and action generation throughout the entire feature hierarchy rather than just at the final layer.", "result": "VAT achieves a 98.15% average success rate across four LIBERO benchmarks for simulated manipulation tasks, establishing a new state-of-the-art and outperforming prior methods like OpenVLA-OFT.", "conclusion": "VAT presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete \"representation trajectory\" of vision models to advance robotic policy learning, showing that using all transformer layers provides superior performance over using only final layer features."}}
{"id": "2512.06014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06014", "abs": "https://arxiv.org/abs/2512.06014", "authors": ["Jiho Shin", "Dominic Marshall", "Matthieu Komorowski"], "title": "Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets", "comment": null, "summary": "Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.", "AI": {"tldr": "Benchmark comparison of two chest X-ray foundation models (CXR-Foundation and MedImageInsight) on public datasets shows MedImageInsight performs slightly better, while CXR-Foundation has better cross-dataset stability.", "motivation": "While foundation models show strong performance in medical imaging, their comparative behavior across different datasets remains underexplored, creating a need for standardized evaluation and benchmarking.", "method": "Used unified preprocessing pipeline and fixed downstream classifiers to evaluate embeddings from both models on MIMIC-CR and NIH ChestX-ray14 datasets. Extracted embeddings from pre-trained encoders, trained LightGBM classifiers on multiple disease labels, and performed unsupervised clustering analysis.", "result": "MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited stronger cross-dataset stability. Unsupervised clustering revealed coherent disease-specific structure in MedImageInsight embeddings consistent with quantitative results.", "conclusion": "The study highlights the need for standardized evaluation of medical foundation models and establishes reproducible baselines for future multimodal and clinical integration research."}}
{"id": "2512.06020", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06020", "abs": "https://arxiv.org/abs/2512.06020", "authors": ["Wenyi Mo", "Tianyu Zhang", "Yalong Bai", "Ligong Han", "Ying Ba", "Dimitris N. Metaxas"], "title": "PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation", "comment": "Project Page: \\href{https://prefgen.github.io/}{\\texttt{https://prefgen.github.io}}", "summary": "Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.", "AI": {"tldr": "A multimodal framework using MLLMs to extract user preferences and inject them into diffusion models for personalized image generation.", "motivation": "Existing approaches fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals for preference-conditioned image generation.", "method": "Train MLLM with preference-oriented VQA, use two probing tasks (inter-user and intra-user discrimination) to isolate preference features, apply MMD-based alignment loss to bridge modality gap, and condition diffusion generator with resulting embeddings.", "result": "Method substantially outperforms strong baselines in both image quality and preference alignment, demonstrating effectiveness of representation extraction and alignment.", "conclusion": "The proposed multimodal framework effectively captures and injects user preferences into diffusion models for personalized image generation, achieving superior performance in preference alignment and image quality."}}
{"id": "2512.06024", "categories": ["cs.CV", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.06024", "abs": "https://arxiv.org/abs/2512.06024", "authors": ["Jiabin Liu", "Zihao Zhou", "Jialei Yan", "Anxin Guo", "Alvise Benetazzo", "Hui Li"], "title": "Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing", "comment": null, "summary": "Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.", "AI": {"tldr": "A neural network for 3D wave surface and velocity field reconstruction using attention-augmented pyramid architecture with physics constraints, achieving millimeter-level accuracy and fast dense reconstruction.", "motivation": "Need for precise 3D reconstruction of ocean wave surfaces and velocity fields to understand ocean physics, addressing high computational costs and persistent visual occlusions in long-term ocean observation.", "method": "Proposed wave free surface visual reconstruction neural network with attention-augmented pyramid architecture tailored to multi-scale, temporally continuous wave motions, using physics-based constraints for time-resolved 3D velocity field reconstruction from evolving free-surface boundaries.", "result": "Millimeter-level wave elevation prediction in central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, high-fidelity 3D reconstruction of nonlinear velocity fields, dense reconstruction of 2 million points in 1.35 seconds, outperforms conventional approaches and maintains strong generalization in occluded conditions.", "conclusion": "The proposed neural network enables efficient, accurate 3D wave reconstruction with strong generalization capabilities, overcoming computational and occlusion challenges in ocean wave observation through attention mechanisms and learned wave dynamics encoding."}}
{"id": "2512.06032", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06032", "abs": "https://arxiv.org/abs/2512.06032", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation", "comment": null, "summary": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.", "AI": {"tldr": "SAM2 and SAM3 represent fundamentally different segmentation paradigms: SAM2 uses spatial prompts for geometric/temporal segmentation, while SAM3 employs multimodal concept-driven segmentation with vision-language architecture for open-vocabulary reasoning.", "motivation": "To investigate the fundamental discontinuity between SAM2 and SAM3, explaining why expertise in prompt-based segmentation doesn't transfer to the new multimodal concept-driven paradigm, and to establish SAM3 as a new class of segmentation foundation model.", "method": "Structured analysis through five core components: conceptual break between prompt-based vs concept-based segmentation, architectural divergence, dataset/annotation differences, training/hyperparameter distinctions, and evaluation metrics/failure modes.", "result": "SAM3 represents a paradigm shift from geometric prompt-based segmentation to multimodal concept-driven segmentation with open-vocabulary reasoning, semantic grounding, and exemplar-based concept understanding, making it a fundamentally different class of model.", "conclusion": "SAM3 establishes a new class of segmentation foundation models and charts future directions for the emerging concept-driven segmentation era, representing a fundamental discontinuity from SAM2's prompt-based approach."}}
{"id": "2512.06058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06058", "abs": "https://arxiv.org/abs/2512.06058", "authors": ["Siming Yan"], "title": "Representation Learning for Point Cloud Understanding", "comment": "181 pages", "summary": "With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.", "AI": {"tldr": "This dissertation develops novel methods for 3D point cloud understanding by integrating 2D knowledge through supervised representation learning, self-supervised learning, and transfer learning from 2D to 3D domains.", "motivation": "With the increasing availability of 3D data from scanners, LiDAR, and RGB-D cameras, there's a need to better understand 3D environments for applications like autonomous driving, robotics, remote sensing, and medical treatment. The challenge is to leverage rich 2D knowledge to enhance 3D understanding without simply converting 2D data.", "method": "The dissertation focuses on three main approaches: 1) supervised representation learning for point cloud primitive segmentation, 2) self-supervised learning methods for 3D data, and 3) transfer learning from 2D to 3D domains by integrating pre-trained 2D models to support 3D network training.", "result": "Extensive experiments validate the effectiveness of the proposed methods, showing significant improvements in 3D understanding. The approach successfully advances point cloud representation learning by effectively integrating 2D knowledge without merely transforming 2D data.", "conclusion": "The dissertation demonstrates that integrating 2D knowledge through various learning paradigms can significantly enhance 3D point cloud understanding, offering promising potential for advancing applications in autonomous systems, robotics, and other fields requiring comprehensive environmental understanding."}}
{"id": "2512.06065", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06065", "abs": "https://arxiv.org/abs/2512.06065", "authors": ["Runjia Li", "Moayed Haji-Ali", "Ashkan Mirzaei", "Chaoyang Wang", "Arpit Sahni", "Ivan Skorokhodov", "Aliaksandr Siarohin", "Tomas Jakab", "Junlin Han", "Sergey Tulyakov", "Philip Torr", "Willi Menapace"], "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing", "comment": "Project page: https://snap-research.github.io/EgoEdit", "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit", "AI": {"tldr": "EgoEdit: A real-time instruction-guided video editing system for egocentric videos that addresses domain gaps in hand-object interactions and egomotion, with new datasets and benchmarks.", "motivation": "Existing AI video editors work well on third-person footage but struggle with egocentric videos due to rapid egomotion and frequent hand-object interactions, creating a significant domain gap. Offline editing pipelines also have high latency, limiting real-time interaction for AR applications.", "method": "1) Construct EgoEditData - a manually curated dataset for egocentric editing with rich hand-object interactions and explicit hand preservation. 2) Develop EgoEdit - an instruction-following egocentric video editor supporting real-time streaming inference on a single GPU. 3) Introduce EgoEditBench - an evaluation suite for instruction faithfulness, hand/interaction preservation, and temporal stability under egomotion.", "result": "EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks where existing methods struggle, while maintaining performance comparable to strongest baselines on general editing tasks.", "conclusion": "The paper presents a complete ecosystem for egocentric video editing that addresses unique challenges of egocentric views, enabling real-time instruction-guided editing for interactive AR applications. The datasets and benchmarks will be made public to advance research in this domain."}}
{"id": "2512.06080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06080", "abs": "https://arxiv.org/abs/2512.06080", "authors": ["Tzofi Klinghoffer", "Siddharth Somasundaram", "Xiaoyu Xiang", "Yuchen Fan", "Christian Richardt", "Akshat Dave", "Ramesh Raskar", "Rakesh Ranjan"], "title": "Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light", "comment": "SIGGRAPH Asia 2025. Project page: https://shoot-bounce-3d.github.io", "summary": "3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.", "AI": {"tldr": "Single-photon lidar captures multi-bounce light to reconstruct 3D scenes with occlusions and mirrors from single measurements using data-driven decomposition of multiplexed illumination.", "motivation": "3D scene reconstruction from single measurements is challenging with occlusions and specular materials like mirrors. Single-photon lidars can capture multi-bounce light containing valuable information about occluded geometry and material properties, but existing methods only work with sequential single-point illumination, not practical multiplexed illumination.", "method": "Data-driven approach using a large-scale simulated dataset of ~100k lidar transients for indoor scenes. Learn a prior on complex light transport to decompose measured two-bounce light into constituent contributions from each laser spot in multiplexed illumination scenarios.", "result": "Experimental demonstration of 3D geometry inference in scenes with occlusions and mirrors from a single measurement. Created first large-scale simulated dataset for this problem and released code/dataset publicly.", "conclusion": "Data-driven decomposition of multi-bounce light in single-photon lidar enables practical 3D reconstruction from multiplexed illumination, overcoming challenges of occlusions and specular reflections that are difficult to invert analytically."}}
{"id": "2512.06096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06096", "abs": "https://arxiv.org/abs/2512.06096", "authors": ["Karthik Mohan", "Sonam Singh", "Amit Arvind Kale"], "title": "BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving", "comment": null, "summary": "The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360\u00b0 BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.", "AI": {"tldr": "BeLLA connects 360\u00b0 BEV representations with LLMs for autonomous driving QA, outperforming existing methods on spatial reasoning tasks by up to +9.3%.", "motivation": "Existing VLMs/MLLMs in autonomous driving either use single-view encoders that miss spatial structure or aggregated multi-view features lacking unified spatial representation, making ego-centric direction, object relation, and contextual reasoning challenging.", "method": "BeLLA is an end-to-end architecture that connects unified 360\u00b0 Bird's Eye View (BEV) representations with a large language model for question answering in autonomous driving.", "result": "BeLLA consistently outperforms existing approaches on spatial reasoning questions (relative object positioning, behavioral understanding) achieving up to +9.3% absolute improvement on NuScenes-QA and DriveLM benchmarks, while performing competitively on other question categories.", "conclusion": "BeLLA demonstrates the effectiveness of connecting unified 360\u00b0 BEV representations with LLMs for handling diverse autonomous driving questions, particularly excelling at spatial reasoning tasks requiring understanding of ego-centric directions and object relations."}}
{"id": "2512.06103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06103", "abs": "https://arxiv.org/abs/2512.06103", "authors": ["Raghavendra Ramachandra", "Sushma Venkatesh"], "title": "SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection", "comment": "Accepted in IEEE T-BIOM", "summary": "Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \\textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\\,nm, 830\\,nm, 850\\,nm, 870\\,nm, and 980\\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.", "AI": {"tldr": "SpectraIrisPAD: A deep learning framework using multispectral imaging and Vision Transformer with spectral positional encoding for robust iris presentation attack detection, outperforming state-of-the-art methods.", "motivation": "Iris recognition systems are vulnerable to presentation attacks (spoofing), and while conventional systems use near-infrared imaging, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance attack detection generalizability.", "method": "Proposes SpectraIrisPAD framework using DINOv2 Vision Transformer backbone with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features for distinguishing genuine iris samples from spoofing artifacts.", "result": "Introduces new MSIrPAD dataset with 18,848 iris images across 5 NIR wavelengths (800-980nm) and 8 PAI categories. SpectraIrisPAD consistently outperforms state-of-the-art baselines across all metrics in unseen attack evaluation protocols, demonstrating superior robustness and generalizability.", "conclusion": "Multispectral imaging combined with advanced deep learning techniques (Vision Transformer with spectral positional encoding) provides an effective solution for robust iris presentation attack detection, significantly improving security of iris-based biometric systems against diverse spoofing attacks."}}
{"id": "2512.06105", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06105", "abs": "https://arxiv.org/abs/2512.06105", "authors": ["Junwen Zheng", "Xinran Xu", "Li Rong Wang", "Chang Cai", "Lucinda Siyun Tan", "Dingyuan Wang", "Hong Liang Tey", "Xiuyi Fan"], "title": "Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation", "comment": "AAAI-26-AIA", "summary": "Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.", "AI": {"tldr": "CEFM is a cross-modal explainable framework for melanoma diagnosis that uses contrastive learning to align clinical ABC criteria with visual features, generating interpretable textual explanations while maintaining high classification performance.", "motivation": "Deep learning models for melanoma classification achieve expert-level performance but suffer from opacity and lack of interpretability, creating barriers to clinical adoption as clinicians struggle to trust black-box decision-making processes.", "method": "CEFM leverages contrastive learning to map clinical ABC criteria (Asymmetry, Border, Color) into Vision Transformer embedding space using dual projection heads, aligning clinical semantics with visual features, then translates these aligned representations into structured textual explanations via natural language generation.", "result": "Achieves 92.79% accuracy and AUC of 0.961 on public datasets, with significant improvements across multiple interpretability metrics. Qualitative analysis shows learned embeddings align with clinicians' ABC rule application, bridging performance and clinical trust.", "conclusion": "CEFM effectively addresses the interpretability gap in melanoma diagnosis by creating transparent links between image data and clinical interpretation through cross-modal alignment, enabling both high-performance classification and clinical trust through explainable AI."}}
{"id": "2512.06158", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06158", "abs": "https://arxiv.org/abs/2512.06158", "authors": ["Su Sun", "Cheng Zhao", "Himangi Mittal", "Gaurav Mittal", "Rohith Kukkala", "Yingjie Victor Chen", "Mei Chen"], "title": "Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation", "comment": "15 pages, 11 figures", "summary": "Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \\emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \\emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \\emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.", "AI": {"tldr": "Track4DGen is a two-stage framework that integrates point tracking priors into multi-view video diffusion and 4D Gaussian Splatting to generate temporally consistent, text-editable 4D objects from sparse inputs.", "motivation": "Generating dynamic 4D objects from sparse inputs is challenging due to difficulties in preserving appearance and motion coherence across views and time while suppressing artifacts and temporal drift. The authors hypothesize that current methods suffer from view discrepancy because they rely on pixel- or latent-space video-diffusion losses that lack explicit temporally aware, feature-level tracking guidance.", "method": "Track4DGen uses a two-stage framework: 1) Stage One enforces dense feature-level point correspondences inside a multi-view video diffusion model using tracker-derived motion priors, producing temporally consistent features. 2) Stage Two reconstructs dynamic 4D Gaussian Splatting using a hybrid motion encoding that concatenates diffusion features (with tracking priors) with Hex-plane features, augmented with 4D Spherical Harmonics for higher-fidelity dynamics modeling.", "result": "Track4DGen surpasses baselines on both multi-view video generation and 4D generation benchmarks, producing temporally stable, text-editable 4D assets. The authors also curated Sketchfab28, a high-quality dataset for benchmarking object-centric 4D generation.", "conclusion": "Explicitly injecting tracker-derived motion priors into both multi-view video generation and 4D reconstruction stages enables superior 4D object generation with improved temporal consistency and cross-view coherence, addressing fundamental challenges in dynamic 4D generation from sparse inputs."}}
{"id": "2512.06171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06171", "abs": "https://arxiv.org/abs/2512.06171", "authors": ["Jessica Plassmann", "Nicolas Schuler", "Michael Schuth", "Georg von Freymann"], "title": "Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection", "comment": "11 pages, 4 figures", "summary": "Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.", "AI": {"tldr": "Automated workflow using deep learning generates defect annotations from shearography measurements, enabling weakly supervised training and reducing manual labeling effort for industrial defect detection.", "motivation": "Shearography is effective for detecting subsurface defects but lacks high-quality annotated datasets due to labor-intensive, subjective manual labeling, limiting industrial adoption.", "method": "Introduces an automated workflow that uses deep learning to generate defect annotations (segmentation and bounding-box labels) directly from shearography measurements.", "result": "Evaluation against expert-labeled data shows sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation.", "conclusion": "The automated annotation workflow addresses the dataset limitation in shearography, facilitating industrial adoption of robust defect detection through scalable, standardized dataset creation."}}
{"id": "2512.06174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06174", "abs": "https://arxiv.org/abs/2512.06174", "authors": ["Shilin Hu", "Jingyi Xu", "Akshat Dave", "Dimitris Samaras", "Hieu Le"], "title": "Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction", "comment": null, "summary": "Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.", "AI": {"tldr": "A novel shadow generation framework that integrates explicit physical modeling (geometry and illumination) with diffusion-based refinement to produce photorealistic, physically consistent shadows.", "motivation": "Current deep-learning-based shadow generation methods rarely incorporate explicit physical modeling of shadow formation, which follows the physics of light occlusion by objects. The paper aims to bridge this gap by embedding physical principles into neural networks for more accurate and realistic shadow generation.", "method": "1) Extract approximate 3D geometry (dense point maps) and predict dominant light direction from monocular RGB images. 2) Use physics-based modeling to compute initial shadow location and shape. 3) Integrate this physics-based estimate into a diffusion framework for realistic appearance refinement while maintaining geometric and illumination consistency.", "result": "The model, trained on DESOBAV2 dataset, produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting conditions.", "conclusion": "Integrating explicit physical modeling of geometry and illumination with deep learning diffusion frameworks enables superior shadow generation that combines physical accuracy with photorealistic appearance, addressing limitations of purely data-driven approaches."}}
{"id": "2512.06179", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06179", "abs": "https://arxiv.org/abs/2512.06179", "authors": ["Shilin Hu", "Jingyi Xu", "Sagnik Das", "Dimitris Samaras", "Hieu Le"], "title": "Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction", "comment": null, "summary": "Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.", "AI": {"tldr": "A framework for joint detection of cast and attached shadows using iterative geometry-illumination reasoning, with a new dataset and 33% BER improvement for attached shadows.", "motivation": "Existing shadow detection methods focus only on cast shadows, ignoring attached shadows which are crucial for 3D structure understanding. There are no dedicated datasets or models for attached shadow detection.", "method": "A closed-loop system with shadow detection module (predicts both shadow types) and light estimation module (infers light direction). Uses estimated light direction with surface normals to derive geometry-consistent partial map of self-occluded regions, which refines shadow predictions iteratively.", "result": "Experimental results show substantial improvement in attached shadow detection with at least 33% BER reduction, while maintaining strong performance for full and cast shadows.", "conclusion": "The proposed iterative geometry-illumination reasoning framework effectively addresses the gap in attached shadow detection and demonstrates significant performance improvements through mutual refinement of shadow segmentation and light estimation."}}
{"id": "2512.06185", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06185", "abs": "https://arxiv.org/abs/2512.06185", "authors": ["Ankit Gupta", "Christoph Adami", "Emily Dolson"], "title": "SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling", "comment": "10 pages with 8 figures, plus 13 pages and 16 figures of supplementary material", "summary": "Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the \"fooling images\" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.", "AI": {"tldr": "Modern DNNs remain vulnerable to high-confidence fooling attacks; transformers are most susceptible; SPOOF attack is efficient and persistent even with defense training.", "motivation": "Despite advances in deep learning, neural networks still exhibit overconfidence on non-natural images (fooling images). The paper revisits this vulnerability in modern architectures to understand if state-of-the-art models have overcome this fundamental weakness.", "method": "1) Re-implemented evolutionary fooling attacks (CPPN-based and direct-encoding) on modern convolutional and transformer classifiers. 2) Introduced SPOOF - a minimalist, consistent, and efficient black-box attack that generates fooling images with minimal pixel modifications. 3) Tested defense by retraining models with fooling images as an additional class.", "result": "1) High-confidence fooling persists in state-of-the-art networks. 2) Transformer-based ViT-B/16 is most susceptible, requiring fewer queries than convolutional models. 3) SPOOF generates unrecognizable fooling images with minimal compute. 4) Retraining defense provides only partial resistance - SPOOF still fools models with slightly higher query budgets.", "conclusion": "Modern deep classifiers remain fundamentally fragile to fooling attacks, with transformers being particularly vulnerable. Simple defenses like adding fooling images to training provide limited protection, highlighting persistent vulnerabilities in current DNN architectures."}}
{"id": "2512.06190", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.06190", "abs": "https://arxiv.org/abs/2512.06190", "authors": ["Shichen Li", "Ahmadreza Eslaminia", "Chenhui Shao"], "title": "Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying", "comment": null, "summary": "Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.", "AI": {"tldr": "Novel multi-modal method predicts food drying color trajectories with high accuracy using high-dimensional temporal color data and process parameters, achieving 90%+ error reduction over baselines.", "motivation": "Existing food drying color analysis uses low-dimensional features that can't capture complex dynamic color changes, and current models lack generalization to unseen drying conditions.", "method": "Multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters for accurate and data-efficient prediction.", "result": "Achieved RMSEs of 2.12 for cookie drying and 1.29 for apple drying under unseen conditions, reducing errors by over 90% compared to baseline models.", "conclusion": "The model demonstrates superior accuracy, robustness, and broad applicability for predicting color evolution in food drying processes."}}
{"id": "2512.06206", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06206", "abs": "https://arxiv.org/abs/2512.06206", "authors": ["Akis Linardos", "Sarthak Pati", "Ujjwal Baid", "Brandon Edwards", "Patrick Foley", "Kevin Ta", "Verena Chung", "Micah Sheller", "Muhammad Irfan Khan", "Mojtaba Jafaritadi", "Elina Kontio", "Suleiman Khan", "Leon M\u00e4chler", "Ivan Ezhov", "Suprosanna Shit", "Johannes C. Paetzold", "Gustav Grimberg", "Manuel A. Nickel", "David Naccache", "Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni", "Daewoon Kim", "Leonard L. Klausmann", "Prashant Shah", "Bjoern Menze", "Dimitrios Makris", "Spyridon Bakas"], "title": "The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning", "comment": "Published at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:033", "summary": "We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.", "AI": {"tldr": "The FeTS Challenge 2024 evaluated federated learning methods for glioma segmentation in MRI, with a PID-controller-based approach achieving top performance in both segmentation accuracy and communication efficiency.", "motivation": "To advance federated learning for medical imaging by evaluating new weight aggregation methods that improve robustness and efficiency in glioma sub-region segmentation across multi-institutional data while preserving privacy.", "method": "Standardized federated learning setup using multi-institutional BraTS dataset (1,251 training, 219 validation, 570 test cases). Six teams developed weight aggregation methods evaluated on segmentation performance (Dice Similarity Coefficient and 95th percentile Hausdorff Distance) and communication efficiency (convergence score).", "result": "PID-controller-based method achieved top ranking with mean DSC values of 0.733 (ET), 0.761 (TC), 0.751 (WT) and HD95 values of 33.922mm, 33.623mm, 32.309mm respectively, plus highest communication efficiency (convergence score 0.764), surpassing previous challenge methods.", "conclusion": "PID controllers are effective mechanisms for stabilizing and optimizing weight aggregation in federated learning for medical imaging, advancing the state of FL and demonstrating practical improvements over previous approaches."}}
{"id": "2512.06221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06221", "abs": "https://arxiv.org/abs/2512.06221", "authors": ["Alena Makarova"], "title": "Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study", "comment": "15 pages, 13 figures. Reproducibility study", "summary": "This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.", "AI": {"tldr": "Reproducibility study finds SVD+WDR image compression doesn't outperform JPEG2000 or WDR as originally claimed, highlighting implementation ambiguities.", "motivation": "To independently verify claims that combining SVD with WDR yields better compression than JPEG2000 and standalone WDR, and to examine reproducibility issues in the original paper.", "method": "Re-implemented the SVD+WDR method, filled in missing implementation details, replicated original experiments, and conducted additional tests on new images using PSNR and SSIM metrics.", "result": "Contrary to original claims, SVD+WDR generally doesn't surpass JPEG2000 or WDR in PSNR, and only partially improves SSIM relative to JPEG2000. Found significant ambiguities in original description affecting reproducibility.", "conclusion": "The study demonstrates reproducibility challenges in research, showing how implementation ambiguities can significantly impact performance claims, and calls for more detailed methodological descriptions."}}
{"id": "2512.06230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06230", "abs": "https://arxiv.org/abs/2512.06230", "authors": ["Pranav Balakrishnan", "Sidisha Barik", "Sean M. O'Rourke", "Benjamin M. Marlin"], "title": "GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking", "comment": null, "summary": "Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.", "AI": {"tldr": "A GPU-accelerated GLMB filter variant that handles multiple detections per object, improving parallel scalability for multi-target tracking in distributed sensor networks.", "motivation": "Standard labeled random finite set methods like GLMB filters are computationally expensive for multi-target tracking, especially with multiple hypotheses. The need arises for efficient tracking in distributed networks of machine learning-based virtual sensors where multiple detections per object occur.", "method": "Proposes a variant of the Generalized Labeled Multi-Bernoulli (GLMB) filter that allows multiple detections per object from the same sensor. This modification breaks inter-detection dependencies in filter updates, enabling improved parallel scalability and efficient GPU implementation.", "result": "The modified GLMB filter shows significantly improved parallel scalability. Preliminary analysis of a GPU-accelerated implementation demonstrates better runtime scalability with respect to number of objects and maximum retained hypotheses.", "conclusion": "The proposed GLMB filter variant enables efficient multi-target tracking in distributed sensor networks by breaking computational bottlenecks through parallelization, making it suitable for real-time applications with GPU acceleration."}}
{"id": "2512.06232", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06232", "abs": "https://arxiv.org/abs/2512.06232", "authors": ["Ellen Su", "Solim Legris", "Todd M. Gureckis", "Mengye Ren"], "title": "Opinion: Learning Intuitive Physics May Require More than Visual Data", "comment": null, "summary": "Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.", "AI": {"tldr": "Training on developmentally realistic video data (SAYCam) doesn't improve intuitive physics performance despite being more human-like, suggesting data distribution alone isn't sufficient for learning intuitive physics.", "motivation": "Humans learn intuitive physics from limited, developmentally appropriate visual experiences, while AI models trained on massive internet video data still fail at intuitive physics benchmarks. The paper investigates whether data distribution (rather than volume) is key to learning these principles.", "method": "Pretrained a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam dataset - a developmentally realistic, egocentric video dataset capturing three children's everyday visual experiences. This dataset represents only 0.01% of the data volume used by state-of-the-art models.", "result": "Training on the developmentally realistic dataset did NOT lead to significant performance improvements on the IntPhys2 benchmark. The model performed similarly to those trained on much larger but less developmentally appropriate datasets.", "conclusion": "Merely training on developmentally realistic data is insufficient for current architectures to learn intuitive physics representations. Varying visual data volume and distribution alone may not be enough to build systems with artificial intuitive physics - suggesting architectural or learning mechanism changes may be necessary."}}
{"id": "2512.06251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06251", "abs": "https://arxiv.org/abs/2512.06251", "authors": ["Fangzhou Lin", "Yuping Wang", "Yuliang Guo", "Zixun Huang", "Xinyu Huang", "Haichong Zhang", "Kazunori Yamada", "Zhengzhong Tu", "Liu Ren", "Ziming Zhang"], "title": "NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks", "comment": "12 pages, 7 figures", "summary": "Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.", "AI": {"tldr": "NexusFlow is a lightweight plug-and-play framework for partially supervised multi-task learning that uses invertible coupling layers to align latent feature distributions across structurally diverse tasks, enabling effective knowledge transfer even with incomplete annotations.", "motivation": "Existing PS-MTL approaches focus on homogeneous dense prediction tasks, leaving the realistic challenge of learning from structurally diverse tasks unexplored. There's a need for a framework that can handle both homogeneous and structurally different tasks with incomplete annotations.", "method": "NexusFlow introduces surrogate networks with invertible coupling layers to align latent feature distributions of tasks. The bijective coupling layers map features into a shared canonical space while preserving information, avoiding representational collapse and enabling alignment across structurally different tasks without reducing expressive capacity.", "result": "NexusFlow sets new SOTA on nuScenes for domain-partitioned autonomous driving (dense map reconstruction + sparse multi-object tracking). On NYUv2 with three homogeneous dense prediction tasks (segmentation, depth, surface normals), it yields consistent gains across all tasks, confirming broad applicability.", "conclusion": "NexusFlow is an effective, lightweight, plug-and-play framework for PS-MTL that works well with both structurally diverse and homogeneous tasks, demonstrating strong performance in challenging real-world scenarios like autonomous driving and general multi-task learning settings."}}
{"id": "2512.06255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06255", "abs": "https://arxiv.org/abs/2512.06255", "authors": ["Shijie Wang", "Xin Yu", "Yadan Luo", "Zijian Wang", "Pengfei Zhang", "Zi Huang"], "title": "Language-driven Fine-grained Retrieval", "comment": null, "summary": "Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer", "AI": {"tldr": "LaFG introduces a language-driven framework for fine-grained image retrieval that uses LLMs and VLMs to convert class names into attribute-level supervision, improving generalization to unseen categories.", "motivation": "Existing FGIR methods use one-hot labels from category names, which overlook rich semantic information and hinder modeling of cross-category details, limiting generalization to unseen categories.", "method": "LaFG converts class names into attribute-level supervision using LLMs to generate detailed descriptions, then uses VLMs to project them into vision-aligned space, clustering into dataset-wide attribute vocabulary. A global prompt template selects category-relevant attributes aggregated into linguistic prototypes that supervise the retrieval model.", "result": "The paper claims improved generalization to unseen categories by leveraging richer semantic information from category names through attribute-level supervision.", "conclusion": "LaFG demonstrates that converting class names into attribute-level supervision using LLMs and VLMs enables better modeling of cross-category details and improves generalization in fine-grained image retrieval."}}
{"id": "2512.06258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06258", "abs": "https://arxiv.org/abs/2512.06258", "authors": ["Chaoyang Wang", "Yangfan He", "Yiyang Zhou", "Yixuan Wang", "Jiaqi Liu", "Peng Xia", "Zhengzhong Tu", "Mohit Bansal", "Huaxiu Yao"], "title": "Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs", "comment": null, "summary": "We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.", "AI": {"tldr": "PSO is a two-stage post-training framework that addresses path selection bias in LVLMs by optimizing reasoning paths through policy optimization and preference alignment, improving reasoning accuracy by 7.4% on average.", "motivation": "LVLMs frequently arrive at correct answers through incorrect reasoning paths due to path selection bias in the reasoning search space, not lack of knowledge. The gap between Pass@K and Pass@1 shows failures stem from misreasoning rather than ignorance.", "method": "Two-stage PSO framework: 1) Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured reasoning; 2) Online preference optimization where models sample paths, self-evaluate, and align to preferred trajectories, with incorrect paths stored in Negative Replay Memory for continual refinement.", "result": "PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (7.4% average improvement), and yields more stable and consistent chains of thought across extensive experiments.", "conclusion": "PSO successfully addresses path selection bias in LVLMs, improving both reasoning performance and stability through systematic path optimization and continual learning from mistakes."}}
{"id": "2512.06269", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06269", "abs": "https://arxiv.org/abs/2512.06269", "authors": ["Quan Tran", "Tuan Dang"], "title": "TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting", "comment": "10 pages", "summary": "3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in \"floater\" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.", "AI": {"tldr": "3D Gaussian Splatting improved with multi-view triangulation to reduce floaters and improve geometry consistency, achieving state-of-the-art surface reconstruction results.", "motivation": "Current 3D Gaussian Splatting methods suffer from reconstruction inconsistencies, floater artifacts, and unstructured geometry due to being guided solely by photometric loss, which is an under-constrained process that prevents extraction of high-fidelity surfaces.", "method": "Introduces global geometry consistency enforcement through constrained multi-view triangulation. Uses various estimated views to achieve consensus on 3D representation, optimizing by penalizing deviation of rendered 3D points from robust consensus points that are re-triangulated from neighboring views in a self-supervised fashion.", "result": "Achieves state-of-the-art results across multiple datasets. On DTU dataset, attains mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods.", "conclusion": "The proposed method successfully addresses geometry inconsistencies in 3D Gaussian Splatting through multi-view triangulation constraints, enabling high-fidelity surface extraction and reducing artifacts. The approach will be open-sourced for community validation."}}
{"id": "2512.06275", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06275", "abs": "https://arxiv.org/abs/2512.06275", "authors": ["Kegang Wang", "Jiankai Tang", "Yuntao Wang", "Xin Liu", "Yuxuan Fan", "Jiatong Ji", "Yuanchun Shi", "Daniel McDuff"], "title": "FacePhys: State of the Heart Learning", "comment": null, "summary": "Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \\emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\\% to 99\\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.", "AI": {"tldr": "FacePhys is a memory-efficient remote photoplethysmography algorithm that achieves state-of-the-art performance with 49% error reduction while enabling real-time inference with minimal computational overhead.", "motivation": "Current camera-based vital sign monitoring (rPPG) faces practical limitations: computational constraints on front-end devices and accuracy degradation when transmitting compressed data. There's a need to resolve the trilemma of model scalability, cross-dataset generalization, and real-time operation.", "method": "FacePhys uses temporal-spatial state space duality and leverages a transferable heart state to capture subtle periodic variations across video frames. It maintains minimal computational overhead, enabling training on extended sequences and supporting low-latency inference.", "result": "Achieves 49% error reduction compared to existing methods, with real-time inference capability (3.6 MB memory footprint, 9.46 ms per-frame latency), surpassing existing methods by 83-99% in efficiency.", "conclusion": "FacePhys resolves the scalability-generalization-latency trilemma, enabling reliable real-time vital sign monitoring in practical deployments with minimal computational resources."}}
{"id": "2512.06276", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06276", "abs": "https://arxiv.org/abs/2512.06276", "authors": ["Tianyi Gao", "Hao Li", "Han Fang", "Xin Wei", "Xiaodong Dong", "Hongbo Sun", "Ye Yuan", "Zhongjiang He", "Jinglin Xu", "Jingmin Xin", "Hao Sun"], "title": "RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension", "comment": null, "summary": "Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.", "AI": {"tldr": "RefBench-PRO is a new REC benchmark that decomposes referring expressions into perception and reasoning dimensions with six sub-tasks, plus an RL-based learning scheme (Ref-R1) for improved localization.", "motivation": "Existing REC benchmarks lack interpretable scoring mechanisms and cannot reveal MLLM's grounding capabilities across different cognitive abilities. Current benchmarks primarily evaluate perceptual capabilities but don't assess reasoning skills.", "method": "1) Introduces RefBench-PRO benchmark with six sub-tasks (attribute, position, interaction, commonsense, relation, reject) across perception and reasoning dimensions. 2) Develops automated data-generation pipeline for diverse referring expressions. 3) Proposes Ref-R1, an RL-based learning scheme with Dynamic IoU-based GRPO for improved localization accuracy.", "result": "Extensive experiments show RefBench-PRO enables interpretable evaluation of MLLMs on REC, presenting greater challenges in both perception and reasoning. The benchmark reveals MLLM capabilities across different cognitive dimensions.", "conclusion": "RefBench-PRO addresses limitations of existing REC benchmarks by providing interpretable evaluation across perception and reasoning dimensions, establishing a stronger baseline for referring expression comprehension research."}}
{"id": "2512.06281", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06281", "abs": "https://arxiv.org/abs/2512.06281", "authors": ["Hengzhuang Li", "Xinsong Zhang", "Qiming Peng", "Bin Luo", "Han Hu", "Dengyang Jiang", "Han-Jia Ye", "Teng Zhang", "Hai Jin"], "title": "Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.", "AI": {"tldr": "LaVer is a training framework that addresses modality imbalance in MLLMs by using masked image modeling in the joint latent space to improve visual representation learning.", "motivation": "MLLMs suffer from modality imbalance where visual information is underutilized compared to text in deeper layers, causing degraded visual performance and hallucinations due to lack of direct visual supervisory signals during next-token-prediction training.", "method": "Proposes Latent Visual Reconstruction (LaVer) framework that uses masked image modeling in the joint latent semantic space of LLMs to provide direct visual activation signals, helping MLLMs learn more discriminative visual representations.", "result": "Extensive experiments across diverse benchmarks show superiority, especially in tasks requiring dense visual capabilities, with MLLMs exhibiting increased visual attention allocation indicating enhanced visual information utilization.", "conclusion": "LaVer effectively addresses modality imbalance in MLLMs by providing direct visual supervision through latent space reconstruction, leading to improved visual performance and reduced hallucinations."}}
{"id": "2512.06282", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.06282", "abs": "https://arxiv.org/abs/2512.06282", "authors": ["Lyn Chao-ling Chen", "Kuan-Wen Chen", "Yi-Ping Hung"], "title": "A Sleep Monitoring System Based on Audio, Video and Depth Information", "comment": "Accepted in the Computer Vision, Graphics and Image Processing (CVGIP 2013)", "summary": "For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.", "AI": {"tldr": "A noninvasive sleep monitoring system using event-based methods with depth, RGB, and microphone sensors to detect motion, lighting, and noise disturbances in home environments.", "motivation": "To enable quantitative evaluation of sleep disturbances through noninvasive monitoring in home contexts, addressing the need for objective measurement of sleep quality factors like movement, lighting changes, and noise.", "method": "Uses a device with infrared depth sensor, RGB camera, and four-microphone array in low-light conditions. Establishes separate background models: one in depth signals for movement magnitude measurement, another in color images for lighting effect measurement. Implements event detection algorithm to identify motion events, light-on/off events, and noise events from processed sensor data.", "result": "The system was tested in sleep conditions and experimental results validate the system's reliability for detecting and classifying sleep disturbance events.", "conclusion": "The developed event-based monitoring system provides a reliable, noninvasive approach for quantitative assessment of sleep disturbances in home environments using multimodal sensor data."}}
{"id": "2512.06290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06290", "abs": "https://arxiv.org/abs/2512.06290", "authors": ["Yiheng Huang", "Shuang She", "Zewei Wei", "Jianmin Lin", "Ming Yang", "Wenyin Liu"], "title": "StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification", "comment": "17 pages, 5 figures", "summary": "Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\\%$ to 95.54$\\%$, demonstrating the effectiveness and robustness of our approach.", "AI": {"tldr": "StrokeNet: A novel network for stroke classification using reference pair representations with spatial queries and feature interactions, achieving SOTA performance on handwritten datasets.", "motivation": "Stroke classification is challenging due to writing style variations, ambiguous content, and dynamic writing positions. Existing methods struggle to capture fine-grained semantic relationships between strokes, especially since stroke interactions are typically localized.", "method": "1. Encode strokes as reference pair representations (points + feature vectors) where reference points enable spatial queries and features mediate interaction modeling. 2. Dynamically select reference points for each stroke and sequence them. 3. Use Inline Sequence Attention (ISA) module to construct contextual features. 4. Employ Cross-Ellipse Query (CEQ) mechanism to cluster reference points and extract features across varying spatial scales. 5. Joint optimization framework with reference points regression for stroke categories and Auxiliary Branch (Aux-Branch) for adjacent stroke semantic transition modeling.", "result": "Achieves state-of-the-art performance on multiple public online handwritten datasets. On CASIA-onDo dataset, accuracy improves from 93.81% to 95.54%, demonstrating effectiveness and robustness.", "conclusion": "StrokeNet effectively addresses stroke classification challenges by modeling fine-grained semantic relationships through reference pair representations and spatial feature interactions, outperforming existing methods on benchmark datasets."}}
{"id": "2512.06306", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06306", "abs": "https://arxiv.org/abs/2512.06306", "authors": ["Haoxian Zhou", "Chuanzhi Xu", "Langyi Chen", "Haodong Chen", "Yuk Ying Chung", "Qiang Qu", "Xaoming Chen", "Weidong Cai"], "title": "Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation", "comment": null, "summary": "Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.", "AI": {"tldr": "Proposes a point cloud-based framework for human pose estimation from event cameras, using temporal slicing convolution and edge enhancement to improve performance without converting events to dense frames.", "motivation": "Existing methods convert event streams to dense frames, adding computation and sacrificing the high temporal resolution of event signals. The authors aim to exploit spatiotemporal properties directly from event streams using point cloud representation.", "method": "1) Event Temporal Slicing Convolution module captures short-term dependencies across event slices. 2) Event Slice Sequencing module for structured temporal modeling. 3) Edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse conditions.", "result": "Experiments on DHP19 dataset show consistent performance improvements across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.", "conclusion": "The proposed point cloud-based framework effectively exploits spatiotemporal properties of event streams for human pose estimation, avoiding the computational overhead and temporal resolution loss of frame-based approaches while improving performance."}}
{"id": "2512.06328", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06328", "abs": "https://arxiv.org/abs/2512.06328", "authors": ["Jiahao Li", "Yusheng Luo", "Yunzhong Lou", "Xiangdong Zhou"], "title": "ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models", "comment": "Accepted as an Oral presentation at AAAI 2026", "summary": "We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.", "AI": {"tldr": "ReCAD is a reinforcement learning framework that bootstraps pretrained large models to generate precise parametric CAD models from multimodal inputs, achieving state-of-the-art performance in text-to-CAD and image-to-CAD tasks.", "motivation": "Previous methods rely on supervised fine-tuning with limited editability and fail to exploit the strong generative priors of pretrained large models. There's a need for a framework that can generate precise parametric CAD models while maintaining editability and leveraging existing model capabilities.", "method": "1) Fine-tune vision-language models with rewritten CAD scripts into parameterized code for supervision; 2) Novel RL strategy using parameterized code as guidance to enhance reasoning; 3) Hierarchical primitive learning process with unified reward function for geometric accuracy and semantic fidelity.", "result": "State-of-the-art performance in both text-to-CAD and image-to-CAD tasks. For image-to-CAD: reduces mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), significantly outperforming baselines.", "conclusion": "ReCAD successfully bootstraps pretrained large models to generate precise parametric CAD models, enabling complex CAD operations from simple interfaces while maintaining editability and leveraging generative priors, setting new benchmarks in CAD generation tasks."}}
{"id": "2512.06330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06330", "abs": "https://arxiv.org/abs/2512.06330", "authors": ["Haoyu Zhang", "Junhan Luo", "Yugang Cao", "Siran Peng", "Jie Huang", "Liangjian-Deng"], "title": "S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening", "comment": null, "summary": "Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.", "AI": {"tldr": "S2WMamba is a pansharpening method that disentangles frequency information using 2D/1D wavelet transforms and performs lightweight cross-modal interaction via Mamba-based architecture to improve spatial-spectral fusion.", "motivation": "Traditional pansharpening methods often entangle spatial detail with spectral fidelity when jointly processing PAN and MS images, leading to suboptimal fusion results.", "method": "Uses 2D Haar DWT on PAN to localize spatial edges/textures, and channel-wise 1D Haar DWT on MS to separate spectral frequencies. Two parallel branches (Spectral and Spatial) exchange information through Mamba-based cross-modulation with linear complexity, followed by multi-scale dynamic gate fusion.", "result": "Outperforms recent baselines (FusionMamba, CANNet, U2Net, ARConv) on WV3, GF2, and QB datasets, improving PSNR by up to 0.23 dB and achieving HQNR 0.956 on full-resolution WV3.", "conclusion": "S2WMamba effectively disentangles frequency information for pansharpening through wavelet-based decomposition and efficient Mamba-based cross-modal interaction, achieving state-of-the-art performance with linear complexity."}}
{"id": "2512.06332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06332", "abs": "https://arxiv.org/abs/2512.06332", "authors": ["Jeffrey Gu", "Minkyu Jeon", "Ambri Ma", "Serena Yeung-Levy", "Ellen D. Zhong"], "title": "CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks", "comment": null, "summary": "Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.", "AI": {"tldr": "CryoHype: A transformer-based hypernetwork for cryo-EM reconstruction that handles compositional heterogeneity from mixtures of many distinct molecular species.", "motivation": "Cryo-EM has potential for high-throughput structure determination of many targets simultaneously, but existing methods focus on conformational heterogeneity within single/few structures and cannot resolve compositional heterogeneity from mixtures of many distinct molecular species.", "method": "Transformer-based hypernetwork that dynamically adjusts weights of an implicit neural representation for cryo-EM reconstruction.", "result": "Achieves state-of-the-art results on benchmark dataset with 100 structures, and scales to reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in fixed-pose setting.", "conclusion": "CryoHype successfully addresses the challenge of compositional heterogeneity in cryo-EM, enabling high-throughput structure determination of many molecular species simultaneously."}}
{"id": "2512.06344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06344", "abs": "https://arxiv.org/abs/2512.06344", "authors": ["Kaile Wang", "Lijun He", "Haisheng Fu", "Haixia Bi", "Fan Li"], "title": "Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate", "comment": null, "summary": "Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.", "AI": {"tldr": "MTGC is a multimodal-guided generative image compression framework that improves semantic consistency at ultra-low bitrates by integrating text captions, highly compressed images, and task-aware semantic pseudo-words.", "motivation": "Generative image compression shows good perceptual quality but suffers from semantic deviations and hallucinations at ultra-low bitrates (bpp < 0.05), limiting its reliability in 6G semantic communication scenarios.", "method": "MTGC integrates three guidance modalities: text captions for global semantics, highly compressed images for low-level visual info, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. Uses Task-Aware Semantic Compression Module (TASCM) to generate SPWs and Multimodal-Guided Diffusion Decoder (MGDD) with dual-path cooperative guidance mechanism.", "result": "Extensive experiments show MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on DIV2K dataset) while achieving gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.", "conclusion": "MTGC effectively addresses semantic deviation issues in generative image compression at ultra-low bitrates through multimodal guidance, making it suitable for bandwidth-constrained 6G semantic communication scenarios."}}
{"id": "2512.06345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06345", "abs": "https://arxiv.org/abs/2512.06345", "authors": ["Xiangshuai Song", "Jun-Jie Huang", "Tianrui Liu", "Ke Liang", "Chang Tang"], "title": "CLUENet: Cluster Attention Makes Neural Networks Have Eyes", "comment": "10 pages, 6 figures, 2026 Association for the Advancement of Artificial Intelligence", "summary": "Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.", "AI": {"tldr": "CLUENet is a transparent deep architecture for visual semantic understanding that combines clustering paradigms with attention mechanisms to achieve better accuracy, efficiency, and interpretability than existing methods.", "motivation": "Convolution- and attention-based models have rigid receptive fields and complex architectures that limit their ability to model irregular spatial patterns and hinder interpretability. Clustering paradigms offer interpretability but suffer from limited accuracy, low efficiency, and gradient vanishing during training.", "method": "CLUENet introduces three key innovations: (1) Global Soft Aggregation and Hard Assignment with Temperature-Scaled Cosine Attention and gated residual connections for enhanced local modeling, (2) inter-block Hard and Shared Feature Dispatching, and (3) an improved cluster pooling strategy.", "result": "Experiments on CIFAR-100 and Mini-ImageNet show that CLUENet outperforms existing clustering methods and mainstream visual models, achieving a compelling balance of accuracy, efficiency, and transparency.", "conclusion": "CLUENet successfully addresses the limitations of both traditional vision models and clustering paradigms, offering a transparent deep architecture that combines the interpretability of clustering with the performance of modern visual models."}}
{"id": "2512.06353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06353", "abs": "https://arxiv.org/abs/2512.06353", "authors": ["Kaicheng Yang", "Kaisen Yang", "Baiting Wu", "Xun Zhang", "Qianrui Yang", "Haotong Qin", "He Zhang", "Yulun Zhang"], "title": "TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search", "comment": "Code and Supplementary Material could be found at https://github.com/racoonykc/TreeQ", "summary": "Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ", "AI": {"tldr": "TreeQ is a unified framework for efficient mixed-precision quantization of Diffusion Transformers (DiTs), addressing search inefficiency, objective misalignment, and information bottlenecks to enable near-lossless 4-bit quantization.", "motivation": "DiTs outperform U-Nets for image generation but face deployment challenges due to high computational/memory demands. While MPQ has advanced U-Net quantization to sub-4bit, its application to DiTs remains limited and underexplored.", "method": "Three key components: 1) Tree Structured Search (TSS) for efficient O(n) space traversal using DiT's linear properties, 2) Environmental Noise Guidance (ENG) to unify PTQ and QAT objectives with single hyperparameter, 3) General Monarch Branch (GMB) structured sparse branch to prevent information loss in ultra-low-bit regimes.", "result": "State-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. First to achieve near-lossless 4-bit PTQ performance on DiT models.", "conclusion": "TreeQ provides a comprehensive solution for DiT quantization, addressing key challenges through efficient search, unified optimization, and information preservation, enabling practical deployment of high-performance DiT models."}}
{"id": "2512.06358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06358", "abs": "https://arxiv.org/abs/2512.06358", "authors": ["Mingjia Li", "Jin Hu", "Hainuo Wang", "Qiming Hu", "Jiarui Wang", "Xiaojie Guo"], "title": "Rectifying Latent Space for Generative Single-Image Reflection Removal", "comment": null, "summary": "Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.", "AI": {"tldr": "A latent diffusion model reframed for single-image reflection removal by aligning latent space with reflection physics, using task-specific text embeddings and depth-guided sampling.", "motivation": "Existing methods struggle with reflection removal due to inability to reason about composition of corrupted regions, leading to poor recovery and generalization in real-world scenarios.", "method": "Three synergistic components: 1) Reflection-equivariant VAE that aligns latent space with linear physics of reflection formation, 2) Learnable task-specific text embedding for precise guidance, 3) Depth-guided early-branching sampling strategy to harness generative stochasticity.", "result": "Achieves new state-of-the-art performance on multiple benchmarks and generalizes well to challenging real-world cases.", "conclusion": "Reframing latent diffusion models with physics-aligned latent space and specialized guidance enables effective single-image reflection removal with strong generalization."}}
{"id": "2512.06363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06363", "abs": "https://arxiv.org/abs/2512.06363", "authors": ["Jiabao Guo", "Yadian Wang", "Hui Ma", "Yuhao Fu", "Ju Jia", "Hui Liu", "Shengeng Tang", "Lechao Cheng", "Yunfeng Diao", "Ajian Liu"], "title": "Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection", "comment": null, "summary": "Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.", "AI": {"tldr": "SPL-UAD framework uses spoofing-aware prompt learning with parallel branches to separately optimize physical and digital attack detection, overcoming conflicting optimization in unified defense systems.", "motivation": "Real-world face recognition systems need protection against both physical presentation attacks (like masks or photos) and digital forgery attacks (like deepfakes). Current unified defense methods using CLIP with regularization suffer from conflicting optimization directions between detecting physical vs. digital attacks when using the same prompt spaces.", "method": "Proposes Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework that decouples optimization branches for physical and digital attacks in the prompt space. Features: 1) Learnable parallel prompt branch with adaptive Spoofing Context Prompt Generation for independent optimization control, 2) Cues-awareness Augmentation using dual-prompt mechanism to generate challenging sample mining tasks for enhanced robustness against unseen attacks.", "result": "Extensive experiments on large-scale UniAttackDataPlus dataset show significant performance improvements in unified attack detection tasks compared to existing approaches.", "conclusion": "SPL-UAD successfully addresses the optimization conflict problem in unified physical-digital defense by decoupling prompt spaces, enabling more effective protection against both types of biometric attacks while improving robustness to unseen attack types."}}
{"id": "2512.06368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06368", "abs": "https://arxiv.org/abs/2512.06368", "authors": ["Weitao Xiong", "Zhiyuan Yuan", "Jiahao Lu", "Chengfeng Zhao", "Peng Li", "Yuan Liu"], "title": "Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos", "comment": null, "summary": "Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.", "AI": {"tldr": "Human3R: A method for monocular dynamic human scene reconstruction using hybrid geometric priors (SMPL + monocular depth) to address geometric inconsistencies and resolution degradation.", "motivation": "Existing methods for monocular dynamic video reconstruction in human scenes suffer from geometric inconsistencies (distorted limb proportions, unnatural human-object fusion) and resolution degradation due to memory-constrained downsampling causing human boundary drift.", "method": "Proposes Human3R with hybrid geometric priors combining SMPL human body models with monocular depth estimation. Uses hierarchical pipeline with refinement components: processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. Integrates SMPL priors through Feature Fusion Module.", "result": "Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction compared to existing methods.", "conclusion": "The proposed Human3R method effectively addresses geometric inconsistencies and resolution degradation in monocular dynamic human scene reconstruction by incorporating structured human priors while preserving fine-grained details."}}
{"id": "2512.06373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06373", "abs": "https://arxiv.org/abs/2512.06373", "authors": ["Yuji Wang", "Wenlong Liu", "Jingxuan Niu", "Haoji Zhang", "Yansong Tang"], "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning", "comment": "The project page is [this url](https://github.com/VoyageWang/VG-Refiner)", "summary": "Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.", "AI": {"tldr": "VG-Refiner is a framework for tool-refined referring grounded reasoning that addresses unreliable tool outputs through a two-stage think-rethink mechanism and refinement rewards.", "motivation": "Existing tool-integrated visual reasoning (TiVR) paradigms focus on integrating visual tools but neglect handling unreliable/erroneous tool outputs, especially in referring and grounding tasks where inaccurate detection predictions mislead models into hallucinated reasoning.", "method": "Proposes VG-Refiner with: 1) Two-stage think-rethink mechanism for explicit analysis and response to tool feedback, 2) Refinement reward to encourage effective correction of poor tool results, 3) Small amount of task-specific data to enhance refinement capability.", "result": "Achieves significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.", "conclusion": "VG-Refiner addresses the critical limitation of handling unreliable tool outputs in TiVR, introducing the first framework for tool-refined referring grounded reasoning with effective refinement mechanisms and evaluation protocols."}}
{"id": "2512.06376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06376", "abs": "https://arxiv.org/abs/2512.06376", "authors": ["Xinhao Xiang", "Abhijeet Rastogi", "Jiawei Zhang"], "title": "Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework", "comment": null, "summary": "Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.", "AI": {"tldr": "AIGVs (AI-generated driving videos) show promise for autonomous driving but have quality issues; the paper introduces a diagnostic framework, failure taxonomy, benchmark dataset, and evaluator to filter AIGVs for safe use in AD pipelines.", "motivation": "Text-to-video models can generate driving scenes cheaply, but it's unclear if these AI-generated videos can reliably support autonomous driving model training and evaluation. The paper aims to systematically study this question.", "method": "1) Develop taxonomy of AIGV failure modes; 2) Build ADGV-Bench benchmark with human annotations; 3) Propose ADGVE evaluator combining static semantics, temporal cues, lane obedience, and VLM-guided reasoning; 4) Test impact of filtered vs. raw AIGVs on perception tasks.", "result": "Raw AIGVs degrade perception performance, but filtering with ADGVE improves both video quality metrics and downstream AD models, making AIGVs a beneficial complement to real-world data.", "conclusion": "AIGVs have both risks and promise for autonomous driving; the paper provides practical tools (diagnostic framework, benchmark, evaluator) for safely leveraging large-scale video generation in AD pipelines."}}
{"id": "2512.06377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06377", "abs": "https://arxiv.org/abs/2512.06377", "authors": ["Yi Huo", "Yun Ge"], "title": "VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System", "comment": null, "summary": "Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .", "AI": {"tldr": "Researchers add Dominance dimension to FER2013 dataset (creating VAD annotations) and improve prediction accuracy using orthogonal convolution in ResNet-based network.", "motivation": "Current FER datasets use limited emotion categories, while future affective computing needs more comprehensive VAD (Valence-Arousal-Dominance) metrics. Existing datasets like AffectNet lack the Dominance dimension.", "method": "1) Manually annotate Dominance dimension on FER2013 dataset to create VAD annotations; 2) Implement orthogonalized convolution in ResNet-based network to extract more diverse features and improve VAD prediction accuracy.", "result": "Dominance dimension can be measured but is more difficult to obtain than Valence and Arousal in both manual annotation and network prediction. Orthogonal convolution improves VAD prediction accuracy. New VAD-annotated dataset and code are publicly available.", "conclusion": "The research provides first Dominance annotations for FER dataset and proposes improved VAD prediction network using orthogonal convolution. The new dataset serves as benchmark for VAD emotion measurement, and the network provides baseline for VAD emotion prediction."}}
{"id": "2512.06379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06379", "abs": "https://arxiv.org/abs/2512.06379", "authors": ["Yi Huo", "Lei Zhang"], "title": "OCFER-Net: Recognizing Facial Expression in Online Learning System", "comment": null, "summary": "Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.", "AI": {"tldr": "OCFER-Net improves facial expression recognition by enforcing orthogonality on convolutional kernels via a regularizer, achieving better feature diversity and performance on FER-2013 dataset.", "motivation": "With the rise of online learning during COVID-19, emotion interaction becomes crucial for effective teaching. Facial Expression Recognition (FER) can help teachers understand students' emotional states, but existing methods don't fully exploit convolutional matrix orthogonality to improve feature diversity and expressiveness.", "method": "The paper proposes OCFER-Net, which enforces orthogonality on convolutional kernels using a regularizer. This approach extracts more diverse and expressive features by constraining the convolutional matrix to be orthogonal, improving the network's representation capabilities.", "result": "Experiments on the challenging FER-2013 dataset show superior performance over baselines by 1.087 (likely percentage points or accuracy improvement). The code is publicly available on GitHub.", "conclusion": "Enforcing orthogonality on convolutional kernels through regularization improves FER performance by enhancing feature diversity and expressiveness, making it valuable for emotion recognition in online learning environments."}}
{"id": "2512.06400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06400", "abs": "https://arxiv.org/abs/2512.06400", "authors": ["Jing Tao", "Yonghong Zong", "Banglei Guana", "Pengju Sun", "Taihang Lei", "Yang Shanga", "Qifeng Yu"], "title": "Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement", "comment": "The paper has been accepted and officially published by IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT", "summary": "In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.", "AI": {"tldr": "A region perception-based fusion framework for IR-VIS spectral fusion using multi-exposure and multi-modal imaging with SVE camera, achieving superior image clarity and performance in extreme conditions.", "motivation": "Existing methods for fusing infrared and visible spectra often compromise visible imagery quality, impacting measurement accuracy, especially under extreme conditions where accurate geometric fidelity of visible features and thermal radiation incorporation are crucial.", "method": "Proposes a region perception-based fusion framework combining multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. Features include: 1) region perception-based feature fusion for precise multi-modal registration, 2) adaptive fusion with contrast enhancement, 3) structural similarity compensation mechanism guided by regional saliency maps for optimal IR-VIS spectral integration, and 4) adaptation to single-exposure scenarios for robustness across conditions.", "result": "Experiments on synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, validated by both quantitative and visual evaluations.", "conclusion": "The proposed framework effectively addresses the challenge of fusing IR and VIS spectra while preserving geometric fidelity and incorporating thermal radiation, overcoming limitations of single-exposure methods in extreme environments."}}
{"id": "2512.06421", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06421", "abs": "https://arxiv.org/abs/2512.06421", "authors": ["Gengze Zhou", "Chongjian Ge", "Hao Tan", "Feng Liu", "Yicong Hong"], "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation", "comment": null, "summary": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.", "AI": {"tldr": "SAR (Self-Autoregressive Refinement) is a post-training method that addresses exposure bias in scale-wise autoregressive image generation models through staggered-scale rollouts and contrastive loss, improving quality with minimal computational overhead.", "motivation": "Scale-wise autoregressive models suffer from exposure bias that undermines generation quality, caused by train-test mismatch (models relying on imperfect predictions during inference) and imbalance in scale-wise learning difficulty.", "method": "Proposes Self-Autoregressive Refinement (SAR) with two components: 1) Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose models to their own intermediate predictions, aligning train-test patterns; 2) Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training.", "result": "Applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For example, SAR yields 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs).", "conclusion": "SAR is an efficient, scalable, and effective post-training method for visual autoregressive generation that addresses exposure bias through self-autoregressive refinement, making it a reliable enhancement for existing AR models."}}
{"id": "2512.06422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06422", "abs": "https://arxiv.org/abs/2512.06422", "authors": ["Chunwei Tian", "Jingyuan Xie", "Lingjun Li", "Wangmeng Zuo", "Yanning Zhang", "David Zhang"], "title": "A Perception CNN for Facial Expression Recognition", "comment": "in IEEE Transactions on Image Processing (2025)", "summary": "Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.", "AI": {"tldr": "PCNN is a perception CNN for facial expression recognition that uses five parallel networks to learn local facial features, employs multi-domain interaction to fuse local and global features, and uses a two-phase loss function for improved performance.", "motivation": "Standard CNNs may ignore the importance of facial segmentation in FER, failing to capture subtle changes in facial expressions effectively.", "method": "1) Five parallel networks learn local features from eyes, cheeks, and mouth; 2) Multi-domain interaction mechanism registers and fuses local sense organ features with global facial structural features; 3) Two-phase loss function restricts accuracy of sense information and reconstructed face images.", "result": "PCNN achieves superior results on multiple FER benchmarks including CK+, JAFFE, FER2013, FERPlus, RAF-DB, and Occlusion and Pose Variant Dataset.", "conclusion": "The proposed PCNN effectively addresses limitations of standard CNNs by incorporating facial segmentation and multi-domain feature fusion, leading to improved facial expression recognition performance across diverse datasets."}}
{"id": "2512.06424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06424", "abs": "https://arxiv.org/abs/2512.06424", "authors": ["Tianshan Zhang", "Zeyu Zhang", "Hao Tang"], "title": "DragMesh: Interactive 3D Generation Made Easy", "comment": null, "summary": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.", "AI": {"tldr": "DragMesh is a real-time interactive 3D articulation framework that decouples kinematic reasoning from motion generation, using a Kinematics Prediction Network for joint parameter inference and a Dual Quaternion VAE for plausible motion trajectory generation.", "motivation": "Current methods for articulated motion are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. There's a need for systems that can understand how objects move and respond to interactions in real-time while maintaining kinematic plausibility.", "method": "1) Decoupled kinematic reasoning: semantic intent reasoning for joint type + geometric regression for axis/origin using KPP-Net. 2) Dual Quaternion VAE (DQ-VAE) using dual quaternions for singularity-free rigid body motion representation. 3) FiLM conditioning injects joint priors at every layer of the Transformer decoder. 4) Numerically-stable cross-product loss ensures axis alignment.", "result": "Achieves real-time performance and enables plausible, generative articulation on novel objects without retraining. The framework maintains strict adherence to kinematics while being fast enough for interactive use.", "conclusion": "DragMesh offers a practical step toward generative 3D intelligence by providing a robust framework for real-time interactive 3D articulation that balances physical consistency with generative capabilities, enabling plausible motion on novel objects without retraining."}}
{"id": "2512.06426", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06426", "abs": "https://arxiv.org/abs/2512.06426", "authors": ["Nzakiese Mbongo", "Kailash A. Hambarde", "Hugo Proen\u00e7a"], "title": "When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition", "comment": "12 pages, 9 figures", "summary": "Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.", "AI": {"tldr": "Dual-path transformer framework using CLIP for gender recognition from extreme long-range imagery, combining visual and attribute-driven cues with spatial attention, evaluated on new U-DetAGReID dataset.", "motivation": "Gender recognition from extreme long-range imagery is challenging due to limited resolution, viewpoint variability, and loss of facial cues. Existing methods struggle with these constraints in unconstrained scenarios.", "method": "Dual-path transformer framework leveraging CLIP: (1) visual path with fine-tuned CLIP image encoder, (2) attribute-mediated path using soft-biometric prompts (hairstyle, clothing, accessories) aligned in CLIP space, plus spatial channel attention modules for discriminative localization.", "result": "Proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. New U-DetAGReID dataset created for evaluation.", "conclusion": "Language-guided dual-path learning offers principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios, with interpretable attribute localization and responsible abstention behavior."}}
{"id": "2512.06434", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06434", "abs": "https://arxiv.org/abs/2512.06434", "authors": ["Lucas R. Mareque", "Ricardo L. Armentano", "Leandro J. Cymberknop"], "title": "Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening", "comment": "8 pages, 2 figures, 3 tables", "summary": "Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.", "AI": {"tldr": "Deep learning models achieve sub-centimeter accuracy in estimating key anthropometric measurements from 2D body images, offering automated screening for cardiovascular risk assessment in athletes.", "motivation": "Traditional manual anthropometric measurements for preparticipation cardiovascular examination are labor-intensive, operator-dependent, and difficult to scale, creating a need for automated solutions to identify athletes at risk of sudden cardiac death.", "method": "Developed a fully automated deep-learning approach using VGG19, ResNet50, and DenseNet121 architectures with fully connected layers for regression, trained on 100,000 synthetic 2D images derived from 3D body meshes to estimate five key anthropometric measurements.", "result": "All models achieved sub-centimeter accuracy, with ResNet50 performing best (mean MAE of 0.668 cm across all measurements), demonstrating deep learning can deliver accurate anthropometric data at scale.", "conclusion": "Deep learning provides a practical automated tool for anthropometric measurement that can complement athlete screening protocols, with future work needed to validate models on real-world images for broader applicability."}}
{"id": "2512.06438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06438", "abs": "https://arxiv.org/abs/2512.06438", "authors": ["Ramazan Fazylov", "Sergey Zagoruyko", "Aleksandr Parkin", "Stamatis Lefkimmiatis", "Ivan Laptev"], "title": "AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars", "comment": null, "summary": "The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/", "AI": {"tldr": "AGORA extends 3D Gaussian Splatting with GAN framework to create animatable 3D human avatars that render at 250+ FPS on GPU and ~9 FPS on CPU, outperforming NeRF-based methods in expression accuracy.", "motivation": "Existing methods have limitations: NeRF-based approaches suffer from slow rendering and dynamic inconsistencies, while 3DGS methods are typically limited to static head generation without dynamic control. There's a need for high-fidelity, animatable 3D human avatars that can render in real-time for VR, telepresence, and entertainment applications.", "method": "AGORA extends 3D Gaussian Splatting within a generative adversarial network framework. It introduces a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals for identity-preserving, fine-grained expression control. Uses dual-discriminator training scheme leveraging synthetic renderings of parametric mesh to enforce expression fidelity.", "result": "Outperforms state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on single GPU. Notably achieves ~9 FPS under CPU-only inference - first demonstration of practical CPU-only animatable 3DGS avatar synthesis. Generates visually realistic and precisely controllable avatars.", "conclusion": "AGORA represents a significant step toward practical, high-performance digital humans by bridging the gap between high-fidelity animation and real-time rendering capabilities, making animatable 3D avatars more accessible for various applications."}}
{"id": "2512.06447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06447", "abs": "https://arxiv.org/abs/2512.06447", "authors": ["Jiuyi Chen", "Mingkui Tan", "Haifeng Lu", "Qiuna Xu", "Zhihua Wang", "Runhao Zeng", "Xiping Hu"], "title": "Towards Stable Cross-Domain Depression Recognition under Missing Modalities", "comment": null, "summary": "Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.", "AI": {"tldr": "A unified multimodal framework for stable cross-domain depression recognition using multimodal large language models that handles heterogeneous data and missing modalities.", "motivation": "Depression poses serious public health risks including suicide, requiring timely and scalable screening. Current multimodal depression detection methods lack unified frameworks for diverse scenarios and show limited stability to missing modalities common in real-world data.", "method": "Proposes SCD-MLLM with two key components: (1) Multi-Source Data Input Adapter (MDIA) using masking and task-specific prompts to transform heterogeneous inputs into uniform token sequences, and (2) Modality-Aware Adaptive Fusion Module (MAFM) that adaptively integrates audio and visual features via shared projection for resilience to missing modalities.", "result": "Outperforms state-of-the-art models and leading commercial LLMs (Gemini and GPT) across five heterogeneous depression datasets (CMDC, AVEC2014, DAIC-WOZ, DVlog, EATD) in both complete and partial modality settings, demonstrating superior cross-domain generalization and stability to missing modalities.", "conclusion": "SCD-MLLM provides a unified, generalizable framework for multimodal depression recognition that effectively handles diverse data sources and maintains stability with incomplete modality inputs, offering promising real-world applicability for scalable depression screening."}}
{"id": "2512.06485", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06485", "abs": "https://arxiv.org/abs/2512.06485", "authors": ["Kush Revankar", "Shreyas Deshpande", "Araham Sayeed", "Ansh Tandale", "Sarika Bobde"], "title": "Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction", "comment": null, "summary": "Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.", "AI": {"tldr": "Sanvaad is a lightweight multimodal accessibility framework for real-time two-way communication between deaf, visually impaired, and hearing users, using MediaPipe for sign recognition and speech processing for voice interfaces.", "motivation": "Current communication tools often only support one direction of interaction between deaf users, visually impaired users, and the general hearing population, creating barriers to inclusive communication.", "method": "The framework uses MediaPipe landmarks for efficient ISL recognition, voice-to-sign mapping with GIFs/alphabet visualizations, and screen-free voice interfaces with multilingual speech recognition, text summarization, and TTS. Built with Streamlit for cross-platform compatibility.", "result": "A lightweight multimodal framework that enables real-time two-way communication on edge devices without dedicated hardware, supporting both desktop and mobile environments.", "conclusion": "Sanvaad provides a practical, accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools in a unified framework."}}
{"id": "2512.06504", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06504", "abs": "https://arxiv.org/abs/2512.06504", "authors": ["Andrii Lysyi", "Anatoliy Sachenko", "Pavlo Radiuk", "Mykola Lysyi", "Oleksandr Melnychenko", "Diana Zahorodnia"], "title": "Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion", "comment": null, "summary": "The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.", "AI": {"tldr": "Intelligent multi-modal framework for automated PV inspection using thermal-RGB fusion, adaptive anomaly confirmation, and geospatial deduplication, achieving 90.3% mAP and 96% recall with 60-70% data reduction.", "motivation": "Address critical shortcomings of conventional PV inspection methods including thermal palette bias, data redundancy, and high communication bandwidth requirements to enhance plant safety and operational efficiency.", "method": "Synergistic architecture with palette-invariant thermal embedding fused with contrast-normalized RGB via gated mechanism, closed-loop adaptive re-acquisition controller using Rodrigues-based updates, and geospatial deduplication module using DBSCAN over haversine distance.", "result": "Achieved mAP@0.5 of 0.903 on PVF-10 benchmark (12-15% improvement over baselines), 96% recall in field validation, 15-20% reduction in duplicate-induced false positives, and 60-70% reduction in airborne data transmission.", "conclusion": "Establishes a powerful new paradigm for proactive PV inspection with validated system readiness, significantly improved detection performance, reduced false positives, and substantial bandwidth savings."}}
{"id": "2512.06521", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06521", "abs": "https://arxiv.org/abs/2512.06521", "authors": ["Jens Dede", "Anna F\u00f6rster"], "title": "ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images", "comment": "31 pages + appendix", "summary": "The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.\n  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.", "AI": {"tldr": "ShadowWolf is a unified AI framework that dynamically adapts wildlife monitoring models to changing environmental conditions, reducing labeling effort and improving real-world performance.", "motivation": "Increasing human-wildlife interactions due to habitat expansion require better wildlife monitoring. Traditional AI models struggle with environmental variability (landscape, weather, lighting, distance), limiting real-world robustness.", "method": "Proposes ShadowWolf - a unified framework integrating and optimizing AI training stages (image collection, labeling, model training). Features dynamic model retraining to adapt to changing conditions and application requirements, enabling on-site adaptation.", "result": "The framework reduces labeling efforts, allows for on-site model adaptation, and enhances accuracy and efficiency of wildlife monitoring systems.", "conclusion": "ShadowWolf's adaptive approach promotes more effective and scalable conservation efforts by addressing real-world environmental challenges in wildlife monitoring AI systems."}}
{"id": "2512.06530", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06530", "abs": "https://arxiv.org/abs/2512.06530", "authors": ["Mohammed Wattad", "Tamir Shor", "Alex Bronstein"], "title": "On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization", "comment": null, "summary": "Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.", "AI": {"tldr": "Learned k-space sampling patterns improve MRI reconstruction quality and show surprising transferability across imaging domains, with proposed stochastic perturbation method enhancing domain robustness.", "motivation": "Most existing learned k-space acquisition research focuses on single datasets/modalities with limited consideration of transferability across domains. The paper aims to demonstrate that learned sampling benefits can extend beyond training domains and improve generalization under domain shifts.", "method": "Two main contributions: 1) Systematic evaluation across datasets and acquisition paradigms showing learned sampling patterns improve generalization in cross-domain settings. 2) Novel method that enhances domain robustness by introducing acquisition uncertainty during training - stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions.", "result": "Learned k-space sampling patterns exhibit improved generalization under cross-domain settings, and the proposed stochastic perturbation method further enhances domain robustness by simulating scanner/imaging variability.", "conclusion": "K-space trajectory design should be treated not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction, with learned sampling patterns showing promising transferability across imaging domains."}}
{"id": "2512.06531", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06531", "abs": "https://arxiv.org/abs/2512.06531", "authors": ["Sayan Das", "Arghadip Biswas"], "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images", "comment": null, "summary": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.", "AI": {"tldr": "Proposed two novel deep learning architectures for brain tumor detection: SAETCN for tumor classification (99.38% accuracy) and SAS-Net for tumor segmentation (99.23% pixel accuracy).", "motivation": "Manual brain tumor detection from MRI scans is time-consuming and difficult due to increasing incidence rates and large data volumes. Existing AI models lack generalization and perform poorly on validation data.", "method": "Developed two novel deep learning architectures: (1) SAETCN (Self-Attention Enhancement Tumor Classification Network) for classifying glioma, meningioma, pituitary tumors, and non-tumor cases; (2) SAS-Net (Self-Attentive Segmentation Network) for accurate tumor segmentation.", "result": "SAETCN achieved 99.38% accuracy on validation dataset for tumor classification. SAS-Net achieved 99.23% overall pixel accuracy for tumor segmentation.", "conclusion": "The proposed architectures demonstrate high accuracy in both classification and segmentation tasks, offering promising CAD solutions for early brain tumor detection with improved generalization compared to existing models."}}
{"id": "2512.06560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06560", "abs": "https://arxiv.org/abs/2512.06560", "authors": ["Dalia Alzu'bi", "A. Ben Hamza"], "title": "Bridging spatial awareness and global context in medical image segmentation", "comment": null, "summary": "Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.", "AI": {"tldr": "U-CycleMLP is a lightweight U-shaped encoder-decoder network for medical image segmentation that uses position attention, dense atrous blocks, and channel CycleMLP blocks to capture both local and global context while maintaining computational efficiency.", "motivation": "Existing medical image segmentation models struggle to balance accuracy and efficiency, often failing to capture both local and global contextual information, leading to boundary pixel loss and segmentation errors.", "method": "U-shaped encoder-decoder architecture with: 1) encoder using position attention weight excitation blocks and dense atrous blocks for multiscale feature learning, 2) decoder with upsampling, dense atrous blocks, and feature fusion, 3) channel CycleMLP blocks along skip connections for enhanced feature integration with linear computational complexity.", "result": "Competitive performance across three benchmark datasets, achieving better segmentation accuracy than state-of-the-art methods, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities.", "conclusion": "U-CycleMLP effectively balances segmentation accuracy and computational efficiency by capturing both local and global contextual information, with ablation studies confirming the importance of its core architectural components."}}
{"id": "2512.06562", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06562", "abs": "https://arxiv.org/abs/2512.06562", "authors": ["Dung Thuy Nguyen", "Quang Nguyen", "Preston K. Robinette", "Eli Jiang", "Taylor T. Johnson", "Kevin Leach"], "title": "SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities", "comment": null, "summary": "Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.", "AI": {"tldr": "SUGAR is a scalable generative unlearning framework that removes specific human identities from 3D-aware generative models without retraining, using personalized surrogate latents to divert reconstructions to coherent alternatives while preserving model quality.", "motivation": "As 3D-aware generative models advance in creating realistic human identities, concerns arise about user consent and the need to remove specific individuals from model outputs. Current methods either project unwanted identities to unrealistic outputs or use static template faces, lacking scalability and quality preservation.", "method": "SUGAR learns a personalized surrogate latent for each identity to be removed, diverting reconstructions to visually coherent alternatives. It introduces a continual utility preservation objective to prevent degradation as more identities are forgotten, enabling simultaneous or sequential removal of multiple identities without full model retraining.", "result": "SUGAR achieves state-of-the-art performance in removing up to 200 identities, with up to 700% improvement in retention utility compared to existing baselines, while maintaining model quality and diversity.", "conclusion": "SUGAR provides an effective and scalable solution for generative unlearning that addresses privacy concerns in 3D-aware generative models, enabling identity removal while preserving overall model utility through personalized surrogate latents and continual utility preservation."}}
{"id": "2512.06565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06565", "abs": "https://arxiv.org/abs/2512.06565", "authors": ["Xiujin Liu"], "title": "GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation", "comment": "1 figures, 2 tables, 14pages", "summary": "We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.", "AI": {"tldr": "GNC-Pose is a learning-free monocular 6D object pose estimation method that uses rendering-based initialization, geometry-aware correspondence weighting, and GNC optimization to achieve competitive accuracy without training data.", "motivation": "The paper aims to provide a robust, learning-free solution for 6D object pose estimation that doesn't require learned features, training data, or category-specific priors, making it practical and simple to deploy.", "method": "Combines rendering-based initialization for coarse 2D-3D correspondences, introduces geometry-aware cluster-based weighting for robust confidence assignment based on 3D structural consistency, uses Graduated Non-Convexity (GNC) optimization to handle outliers, and includes final LM refinement for accuracy improvement.", "result": "Achieves competitive accuracy on the YCB Object and Model Set compared to both learning-based and learning-free methods, despite requiring no training data or learned features.", "conclusion": "GNC-Pose offers a simple, robust, and practical learning-free solution for 6D pose estimation that performs competitively without the need for training data or learned features."}}
{"id": "2512.06575", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06575", "abs": "https://arxiv.org/abs/2512.06575", "authors": ["Fariza Dahes"], "title": "Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules", "comment": "26 pages, 16 figures, 2 tables; proof of concept on mammography classification with compactness/separability modules and interactive dashboard; preprint submitted to arXiv cs.LG", "summary": "This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.", "AI": {"tldr": "Validation of a medical image classification framework (ConvNeXt Tiny with GAGM, SEVector, FSL) from Alzheimer MRI to mammography classification, showing GAGM and SEVector improve performance but FSL doesn't transfer well, with extensions including multi-metric evaluation and interactive dashboard.", "motivation": "To validate whether a recently proposed medical image classification framework (originally developed for Alzheimer MRI classification) can be effectively transposed to mammography classification, and to extend the framework with additional evaluation metrics and tools for clinical application.", "method": "Used a consolidated Kaggle dataset combining INbreast, MIAS, and DDSM mammography collections. Compared baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enhanced with GAGM (Global Average and Max Pooling fusion) and SEVector (lightweight channel attention) modules. Extended original framework with multi-metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis using Grad CAM, and developed an interactive dashboard for clinical exploration.", "result": "GAGM and SEVector modules effectively enhanced feature discriminability and reduced false negatives, especially for malignant cases. However, Feature Smoothing Loss (FSL) did not yield measurable improvements in mammography classification, suggesting its effectiveness depends on specific architectural and computational assumptions. The extended framework provides comprehensive evaluation and interpretability tools.", "conclusion": "The medical image classification framework is partially transferable to mammography - GAGM and SEVector work well but FSL doesn't. Future work should explore alternative approaches to improve intra-class compactness and inter-class separability, particularly for distinguishing malignant vs. benign cases in mammography classification."}}
{"id": "2512.06581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06581", "abs": "https://arxiv.org/abs/2512.06581", "authors": ["Yuhao Su", "Anwesa Choudhuri", "Zhongpai Gao", "Benjamin Planche", "Van Nguyen Nguyen", "Meng Zheng", "Yuhan Shen", "Arun Innanje", "Terrence Chen", "Ehsan Elhamifar", "Ziyan Wu"], "title": "MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding", "comment": null, "summary": "Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \\textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \\textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \\emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \\emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.", "AI": {"tldr": "MedVidBench is a large medical video benchmark with 531,850 video-instruction pairs across 8 sources. MedGRPO is a novel RL framework with cross-dataset reward normalization and medical LLM judge that improves medical video understanding beyond SFT baselines.", "motivation": "Large vision-language models struggle with medical video understanding due to challenges with spatial precision, temporal reasoning, and clinical semantics. There's a need for comprehensive benchmarks and effective training methods for medical video analysis.", "method": "1) Created MedVidBench benchmark with expert-guided prompting and dual-model validation. 2) Developed MedGRPO RL framework with cross-dataset reward normalization (maps median performance to common reward) and medical LLM judge (evaluates caption quality on 5 clinical dimensions).", "result": "SFT on MedVidBench outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks. MedGRPO further improves upon SFT baseline across grounding and captioning tasks, demonstrating effectiveness of balanced multi-dataset training.", "conclusion": "The work establishes foundational benchmark (MedVidBench) and robust training methodology (MedGRPO) for advancing vision-language models in medical domains, addressing key challenges in medical video understanding."}}
{"id": "2512.06598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06598", "abs": "https://arxiv.org/abs/2512.06598", "authors": ["Muhammad Adil", "Patrick J. Clemins", "Andrew W. Schroth", "Panagiotis D. Oikonomou", "Donna M. Rizzo", "Peter D. F. Isles", "Xiaohan Zhang", "Kareem I. Hannoun", "Scott Turnbull", "Noah B. Beckage", "Asim Zia", "Safwan Wshah"], "title": "From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain", "comment": "23 pages, 15 figures", "summary": "Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.", "AI": {"tldr": "A Transformer-BiLSTM model using satellite data predicts cyanobacterial harmful algal blooms up to 14 days ahead in Lake Champlain, achieving strong performance despite sparse data.", "motivation": "CyanoHABs threaten aquatic ecosystems and public health globally, with Lake Champlain particularly vulnerable due to nutrient enrichment and climate variability. Remote sensing offers scalable monitoring where in situ data is sparse.", "method": "Developed a remote sensing-only forecasting framework combining Transformers and BiLSTM. Used Cyanobacterial Index data from CyAN and temperature data from MODIS satellites. Applied two-stage preprocessing with forward fill, weighted temporal imputation, and smoothing to handle sparse data (30% missing CI, 90% missing temperature). Created features via equal frequency binning and temperature statistics extraction.", "result": "Transformer-BiLSTM achieved strong forecasting: F1 scores of 89.5% (1-day), 86.4% (2-day), 85.5% (3-day), and 78.9% with AUC 82.6% at 14-day horizon. Model captures complex spatiotemporal dynamics from sparse satellite data.", "conclusion": "The framework provides reliable early warning for CyanoHAB management by effectively handling sparse satellite data and capturing long-range dependencies and sequential dynamics in time series."}}
{"id": "2512.06612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06612", "abs": "https://arxiv.org/abs/2512.06612", "authors": ["Kazuya Nishimura", "Haruka Hirose", "Ryoma Bise", "Kaito Shiku", "Yasuhiro Kojima"], "title": "Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics", "comment": "Neurips 2025", "summary": "Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.", "AI": {"tldr": "STRank is a novel loss function for gene expression estimation from pathology images that focuses on learning relative expression patterns instead of absolute values, making it robust to noise and batch effects.", "motivation": "Current methods using point-wise loss functions struggle with accurately estimating absolute gene expression values due to stochastic noise and batch effects inherent in RNA sequencing techniques and cellular variability.", "method": "Proposes STRank loss function based on the assumption that relative expression patterns of genes remain consistent across experiments despite noise and batch effects affecting absolute values.", "result": "Experiments on synthetic and real datasets demonstrate the effectiveness of STRank in improving gene expression estimation from pathology images.", "conclusion": "Learning relative expression patterns rather than absolute values provides a more robust approach to gene expression estimation from pathology images, addressing challenges of noise and batch effects."}}
{"id": "2512.06613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06613", "abs": "https://arxiv.org/abs/2512.06613", "authors": ["Yueying Ke"], "title": "Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach", "comment": "10 pages, 6 figures, 2 tables, IEEE conference format. Submitted as course project", "summary": "Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.\n  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.\n  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).\n  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.", "AI": {"tldr": "Hierarchical CNN with cascaded heads for multi-level diatom classification outperforms flat models by improving accuracy at upper taxonomic levels and keeping errors taxonomically local.", "motivation": "Conventional diatom identification relies on expert taxonomists, and existing deep learning approaches treat classification as flat, predicting only one taxonomic rank. The authors investigate whether embedding taxonomic hierarchy into neural networks can improve both accuracy and error locality.", "method": "Introduce hierarchical convolutional network with five cascaded heads predicting class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Use filtered dataset of 1,456 diatom images covering 82 species.", "result": "Hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, 92.5% of misclassified species are correctly predicted at genus level (vs. 67.2% for flat baselines). Reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).", "conclusion": "Hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions."}}
{"id": "2512.06642", "categories": ["cs.CV", "astro-ph.CO", "astro-ph.IM", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06642", "abs": "https://arxiv.org/abs/2512.06642", "authors": ["Achmad Ardani Prasha", "Clavino Ourizqi Rachmadi", "Muhamad Fauzan Ibnu Syahlan", "Naufal Rahfi Anugerah", "Nanda Garin Raditya", "Putri Amelia", "Sabrina Laila Mutiara", "Hilman Syachr Ramadhan"], "title": "Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution", "comment": "21 pages, 7 figures, 3 table", "summary": "Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.", "AI": {"tldr": "MAE pretraining on simulated strong-lensing images improves classification of dark matter models and super-resolution reconstruction compared to training from scratch.", "motivation": "Strong gravitational lensing can reveal dark-matter substructure, but analyzing noisy, low-resolution images is challenging. Need generalizable representations for multiple analysis tasks.", "method": "Masked autoencoder (MAE) pretraining on simulated strong-lensing images from DeepLense benchmark, using Vision Transformer encoder with masked image modeling objective. Fine-tune separately for classification (dark matter models) and super-resolution tasks.", "result": "MAE pretraining with 90% mask ratio achieves: classification macro AUC 0.968 (vs 0.957 baseline), accuracy 88.65% (vs 82.46%). Super-resolution: PSNR ~33 dB, SSIM 0.961, modest improvement over scratch. Higher mask ratios improve classification but slightly degrade reconstruction.", "conclusion": "MAE pretraining on physics-rich simulations provides flexible, reusable encoder for multiple strong-lensing analysis tasks, outperforming training from scratch."}}
{"id": "2512.06657", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06657", "abs": "https://arxiv.org/abs/2512.06657", "authors": ["Qiyan Zhao", "Yue Yan", "Da-Han Wang"], "title": "TextMamba: Scene Text Detector with Mamba", "comment": null, "summary": "In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\\%, 89.2\\%, and 78.5\\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.", "AI": {"tldr": "A novel scene text detector using Mamba's selection mechanism with attention layers, Top_k algorithm for key information selection, dual-scale feed-forward network, and embedding pyramid enhancement, achieving SOTA performance on text detection benchmarks.", "motivation": "Transformer-based methods for scene text detection have limitations in cross-domain applications and long-range dependency modeling, often forgetting important information or focusing on irrelevant representations. Mamba's linear complexity selection mechanism offers better long-range dependency modeling.", "method": "Integrates Mamba's selection mechanism with attention layers, uses Top_k algorithm to explicitly select key information and reduce irrelevant interference, designs a dual-scale feed-forward network for high-dimensional hidden state interactions, and an embedding pyramid enhancement module for multi-scale feature fusion.", "result": "Achieves state-of-the-art or competitive performance with F-measures of 89.7% on CTW1500, 89.2% on TotalText, and 78.5% on ICDAR19ArT benchmarks.", "conclusion": "The proposed Mamba-based scene text detector effectively addresses limitations of Transformer-based methods by combining selection mechanisms with attention, achieving superior performance on various text detection benchmarks through improved long-range dependency modeling and information selection."}}
{"id": "2512.06662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06662", "abs": "https://arxiv.org/abs/2512.06662", "authors": ["Ruoyu Xue", "Hieu Le", "Jingyi Xu", "Sounak Mondal", "Abe Leite", "Gregory Zelinsky", "Minh Hoai", "Dimitris Samaras"], "title": "Personalized Image Descriptions from Attention Sequences", "comment": "10 pages, 4 figures", "summary": "People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.", "AI": {"tldr": "DEPER learns personalized subject embeddings combining linguistic style and viewing behavior to generate more human-aligned image descriptions, achieving 24% average improvement across four datasets.", "motivation": "Existing personalized image description models only focus on linguistic style, ignoring individual viewing patterns. People view images differently - focusing on different regions, objects, and details in varying orders - leading to substantial variability in descriptions that current models don't capture.", "method": "DEPER (DEscription-PERception persona encoder) learns subject embeddings that capture both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model for few-shot personalization without retraining.", "result": "Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions.", "conclusion": "Understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems. Personalized viewing behavior is a core factor that should be explicitly modeled in description generation."}}
{"id": "2512.06663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06663", "abs": "https://arxiv.org/abs/2512.06663", "authors": ["Yu Qi", "Yumeng Zhang", "Chenting Gong", "Xiao Tan", "Weiming Zhang", "Wei Zhang", "Jingdong Wang"], "title": "CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.", "AI": {"tldr": "CoT4Det improves LVLM perception performance by reformulating detection tasks into classification, counting, and grounding steps, boosting mAP from 19% to 33% on COCO2017.", "motivation": "Large Vision-Language Models excel at general vision-language tasks but perform poorly on perception-centric tasks like object detection, significantly underperforming task-specific models.", "method": "Chain-of-Thought for Detection (CoT4Det) reformulates perception tasks into three interpretable steps: classification (identifying objects), counting (determining quantities), and grounding (localizing objects).", "result": "CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val, achieves +2% improvement on RefCOCO series, and 19% improvement on Flickr30k entities, while maintaining general vision-language capabilities.", "conclusion": "The proposed three-step reasoning approach effectively bridges the gap between LVLM reasoning capabilities and perception tasks, offering significant performance improvements without compromising other abilities."}}
{"id": "2512.06673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06673", "abs": "https://arxiv.org/abs/2512.06673", "authors": ["Shida Gao", "Feng Xue", "Xiangfeng Wang", "Anlong Ming", "Teng Long", "Yihua Shao", "Haozhe Wang", "Zhaowen Lin", "Wei Wang", "Nicu Sebe"], "title": "1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning", "comment": null, "summary": "Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.", "AI": {"tldr": "DEViL is a Detector-Empowered Video LLM that integrates an open-vocabulary detector with a video LLM using reference-semantic tokens to improve spatio-temporal grounding while avoiding autoregressive spatial decoding issues.", "motivation": "Current MLLMs for spatio-temporal grounding use autoregressive spatial decoding (treating bounding boxes as text tokens), which leads to long output sequences, error accumulation over time, and progressive spatial drift across videos.", "method": "DEViL couples a Video LLM with an open-vocabulary detector via reference-semantic tokens (RST) that distill query semantics and serve as both control signals and replacements for detector text embeddings. Also introduces tube-mined temporal regularization (TTReg) to ensure temporally-consistent object queries.", "result": "DEViL achieves strong performance across fine-grained video understanding tasks, particularly STVG (Spatio-Temporal Video Grounding) and GroundedVQA.", "conclusion": "The proposed detector-empowered approach with reference-semantic tokens and temporal regularization effectively addresses limitations of autoregressive spatial decoding, enabling better spatio-temporal grounding and reasoning in videos."}}
{"id": "2512.06674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06674", "abs": "https://arxiv.org/abs/2512.06674", "authors": ["Songping Wang", "Rufan Qian", "Yueming Lyu", "Qinglong Liu", "Linzhuang Zou", "Jie Qin", "Songhua Liu", "Caifeng Shan"], "title": "RunawayEvil: Jailbreaking the Image-to-Video Generative Models", "comment": null, "summary": "Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a \"Strategy-Tactic-Action\" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.", "AI": {"tldr": "RunawayEvil is the first multimodal jailbreak framework for Image-to-Video models that uses a self-evolving \"Strategy-Tactic-Action\" paradigm to achieve state-of-the-art attack success rates on commercial I2V systems.", "motivation": "The security of multimodal systems like Image-to-Video generation models is critically underexplored, particularly their vulnerability to jailbreak attacks. There's a need to understand and analyze these vulnerabilities to build more robust video generation systems.", "method": "A \"Strategy-Tactic-Action\" paradigm with three core components: 1) Strategy-Aware Command Unit that self-evolves strategies using reinforcement learning and LLM-based exploration, 2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines, and 3) Tactical Action Unit that executes and evaluates multimodal coordinated attacks.", "result": "RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models like Open-Sora 2.0 and CogVideoX, outperforming existing methods by 58.5 to 79 percent on COCO2017 dataset.", "conclusion": "This work provides a critical tool for vulnerability analysis of I2V models, laying a foundation for developing more robust and secure video generation systems by exposing security weaknesses that need to be addressed."}}
{"id": "2512.06684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06684", "abs": "https://arxiv.org/abs/2512.06684", "authors": ["Yumeng He", "Zanwei Zhou", "Yekun Zheng", "Chen Liang", "Yunbo Wang", "Xiaokang Yang"], "title": "EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy", "comment": null, "summary": "Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.", "AI": {"tldr": "EMGauss: A Gaussian splatting-based framework for 3D reconstruction from 2D slices that treats slice progression as temporal evolution, overcoming limitations of isotropy-based methods in volume electron microscopy.", "motivation": "Volume electron microscopy (vEM) produces anisotropic volumes with limited axial resolution due to acquisition trade-offs. Existing deep learning methods fail for morphologically anisotropic structures because they rely on isotropy assumptions that don't hold for such biological structures.", "method": "Reframes slice-to-3D reconstruction as 3D dynamic scene rendering using Gaussian splatting, modeling axial slice progression as temporal evolution of 2D Gaussian point clouds. Incorporates Teacher-Student bootstrapping to use high-confidence predictions on unobserved slices as pseudo-supervisory signals for data-sparse regimes.", "result": "EMGauss substantially improves interpolation quality compared to diffusion- and GAN-based methods, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. It provides a generalizable solution beyond vEM to diverse imaging domains.", "conclusion": "EMGauss offers a novel framework for 3D reconstruction from planar 2D slices that circumvents limitations of isotropy-based approaches, providing superior performance and broader applicability across imaging domains."}}
{"id": "2512.06689", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.06689", "abs": "https://arxiv.org/abs/2512.06689", "authors": ["Jisoo Park", "Seonghak Lee", "Guisik Kim", "Taewoo Kim", "Junseok Kwon"], "title": "Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation", "comment": "Accepted to ASRU 2025", "summary": "Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.", "AI": {"tldr": "UniVoiceLite is a lightweight unsupervised audio-visual framework that unifies speech enhancement and speech separation using lip motion and facial identity cues, eliminating the need for paired noisy-clean data.", "motivation": "Real-world audio often contains both background noise and overlapping speakers, but traditional approaches treat speech enhancement and speech separation as separate tasks. Existing unified solutions use complex supervised models that limit scalability and generalization.", "method": "Proposes UniVoiceLite, an unsupervised audio-visual framework that uses lip motion and facial identity cues to guide speech extraction. Employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data.", "result": "Experimental results show UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization.", "conclusion": "UniVoiceLite provides a lightweight, unsupervised solution that effectively unifies speech enhancement and speech separation, addressing real-world audio challenges with improved scalability and generalization."}}
{"id": "2512.06726", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06726", "abs": "https://arxiv.org/abs/2512.06726", "authors": ["Shuo Li", "Jiajun Sun", "Zhihao Zhang", "Xiaoran Fan", "Senjie Jin", "Hui Li", "Yuming Yang", "Junjie Ye", "Lixing Shen", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "The Role of Entropy in Visual Grounding: Analysis and Optimization", "comment": null, "summary": "Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.", "AI": {"tldr": "ECVGPO is a new entropy control algorithm for visual grounding tasks that improves performance by better balancing exploration-exploitation trade-offs in multimodal LLMs.", "motivation": "While entropy control techniques have advanced reinforcement learning for multimodal LLMs, their role in perception-oriented tasks like visual grounding remains unexplored. The paper aims to understand entropy characteristics in visual grounding versus reasoning tasks and develop effective control strategies.", "method": "The authors analyze entropy characteristics in visual grounding tasks compared to reasoning tasks, then introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization) - an interpretable algorithm for effective entropy regulation that better balances exploration-exploitation trade-offs.", "result": "Experiments show ECVGPO achieves broad improvements across various benchmarks and models, demonstrating the effectiveness of their entropy control approach for visual grounding tasks.", "conclusion": "The paper successfully addresses the gap in understanding entropy control for perception-oriented tasks, introducing ECVGPO as an effective solution that improves visual grounding performance through better entropy regulation."}}
{"id": "2512.06736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06736", "abs": "https://arxiv.org/abs/2512.06736", "authors": ["Jiaxing Fan", "Jiaojiao Liu", "Wenkong Wang", "Yang Zhang", "Xin Ma", "Jichen Zhang"], "title": "Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data", "comment": null, "summary": "Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.", "AI": {"tldr": "GCN-LSTM-ATT network using skeleton data outperforms traditional ML methods in detecting compensatory movements in stroke patients during rehabilitation.", "motivation": "Stroke patients often develop compensatory movements during rehabilitation that hinder long-term recovery, making detection of these movements crucial for effective treatment.", "method": "Proposed Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) using skeleton data from Kinect camera, compared with SVM, KNN, and Random Forest on 16 stroke patients performing rehabilitation movements.", "result": "GCN-LSTM-ATT achieved 0.8580 accuracy, significantly outperforming traditional ML methods, with ablation studies confirming each component contributes to performance improvement.", "conclusion": "The model provides a more precise tool for detecting compensatory movements, potentially optimizing stroke rehabilitation training strategies."}}
{"id": "2512.06738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06738", "abs": "https://arxiv.org/abs/2512.06738", "authors": ["M Yashwanth", "Sampath Koti", "Arunabh Singh", "Shyam Marjit", "Anirban Chakraborty"], "title": "FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation", "comment": "Accepted to Winter Conference on Applications of Computer Vision (WACV) 2026, Round 1", "summary": "We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.", "AI": {"tldr": "FedSCAl is an FL framework for Federated source-Free Domain Adaptation that addresses client-drift in heterogeneous domains by aligning server-client predictions to improve pseudo-labeling accuracy.", "motivation": "The paper addresses the Federated source-Free Domain Adaptation (FFreeDA) problem where clients have unlabeled data with significant domain gaps, and only a pre-trained server model is available without access to source data during training. Existing SFDA methods struggle with client-drift in FL due to extreme data heterogeneity, leading to unreliable pseudo-labels.", "method": "FedSCAl introduces a Server-Client Alignment (SCAl) mechanism that regularizes client updates by aligning the predictions of client and server models. This alignment helps mitigate client-drift and improves pseudo-labeling accuracy in the federated setting.", "result": "Extensive experiments on benchmark vision datasets show that FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks, demonstrating improved performance in handling domain gaps and client heterogeneity.", "conclusion": "FedSCAl effectively addresses the challenges of FFreeDA by leveraging server-client alignment to reduce client-drift and enhance pseudo-label reliability, making it a robust solution for federated learning with source-free domain adaptation constraints."}}
{"id": "2512.06746", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06746", "abs": "https://arxiv.org/abs/2512.06746", "authors": ["Ruoxin Chen", "Jiahui Gao", "Kaiqing Lin", "Keyue Zhang", "Yandan Zhao", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "title": "Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection", "comment": null, "summary": "Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.", "AI": {"tldr": "The paper proposes AlignGemini, a two-branch detector for AI-generated image detection that aligns VLMs with semantic tasks and pixel-artifact experts with artifact detection, achieving +9.5% average accuracy gain.", "motivation": "Current VLMs used for AIGI detection require substantial resources and suffer from severe hallucinations. The core issue is task-model misalignment: semantics-oriented VLMs lack sensitivity to fine-grained pixel artifacts, while pixel-artifact detectors lack semantic awareness.", "method": "Formalizes AIGI detection as two complementary tasks: semantic consistency checking and pixel-artifact detection. Introduces Task-Model Alignment principle and implements AlignGemini - a two-branch detector with a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision.", "result": "On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy compared to existing methods, demonstrating the effectiveness of task-model alignment for generalizable AIGI detection.", "conclusion": "Task-model alignment is an effective path to generalizable AIGI detection. By separating semantic and pixel-artifact detection into specialized branches with orthogonal supervision, the approach overcomes systematic blind spots in current methods."}}
{"id": "2512.06750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06750", "abs": "https://arxiv.org/abs/2512.06750", "authors": ["Weiqi Li", "Xuanyu Zhang", "Bin Chen", "Jingfen Xie", "Yan Wang", "Kexin Zhang", "Junlin Li", "Li Zhang", "Jian Zhang", "Shijie Zhao"], "title": "UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement", "comment": null, "summary": "Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.", "AI": {"tldr": "UARE is the first unified vision-language model for image quality assessment, restoration, and enhancement that uses IQA to guide restoration through multi-task co-training.", "motivation": "Current approaches treat image quality assessment (IQA) and restoration as separate problems, despite their conceptual connection. Recent multimodal models show that better understanding can improve generation, motivating a unified model where IQA explicitly guides restoration.", "method": "Two-stage training framework: 1) Progressive easy-to-hard schedule from single-type distortions to mixed degradations, 2) Unified fine-tuning with interleaved text-image data aligning IQA signals with restoration objectives through multi-task co-training.", "result": "Extensive experiments across IQA, restoration, and enhancement tasks demonstrate UARE's effectiveness in leveraging IQA to boost restoration and enhancement performance.", "conclusion": "UARE successfully unifies IQA and restoration tasks, showing that explicit quality assessment guidance improves restoration outcomes, representing a valuable advancement in low-level vision."}}
{"id": "2512.06759", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06759", "abs": "https://arxiv.org/abs/2512.06759", "authors": ["Wenbo Lyu", "Yingjun Du", "Jinglin Zhao", "Xianton Zhen", "Ling Shao"], "title": "VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors", "comment": "12 pages,13figures", "summary": "Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench", "AI": {"tldr": "VisChainBench is a new benchmark for evaluating Large Vision-Language Models' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance.", "motivation": "Existing benchmarks focus on static/horizontal comparisons and rely heavily on language cues, overlooking progressive context-dependent reasoning and visual-to-visual inference challenges in multi-image, multi-turn scenarios.", "method": "Created VisChainBench with 1,457 tasks spanning over 20,000 images across three diverse domains (daily scenarios, engineering troubleshooting). Used multi-agent generation pipeline to ensure high visual diversity and controlled language bias.", "result": "A large-scale benchmark containing structured tasks that mimic real-world decision-making processes, with data and code available for download.", "conclusion": "VisChainBench addresses a critical gap in evaluating LVLMs' multi-step visual reasoning capabilities and provides a rigorous testbed for assessing context-dependent visual inference with minimal language guidance."}}
{"id": "2512.06763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06763", "abs": "https://arxiv.org/abs/2512.06763", "authors": ["Chengyang Yan", "Mitch Bryson", "Donald G. Dansereau"], "title": "JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms", "comment": null, "summary": "The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.", "AI": {"tldr": "Joint optimization of camera hardware and adaptive control algorithms for improved perception performance, using a hybrid framework that handles both continuous/discrete parameters and non-differentiable effects.", "motivation": "Current camera system designs focus on optimizing fixed parameters set at manufacturing, but many parameters (like exposure settings) require adaptive control at runtime. There's a need to jointly optimize both hardware and adaptive control algorithms for better perception task performance.", "method": "Proposes a unified optimization framework combining gradient-based and derivative-free methods. Introduces DF-Grad, a hybrid strategy that trains adaptive control networks using signals from derivative-free optimizers alongside unsupervised task-driven learning to handle non-differentiable effects like motion blur.", "result": "Outperforms baselines that optimize static and dynamic parameters separately, especially under challenging conditions like low light and fast motion. Shows improved perception performance through joint hardware and adaptive control optimization.", "conclusion": "Joint optimization of hardware parameters and adaptive control algorithms significantly improves perception performance and provides a unified approach to task-driven camera system design."}}
{"id": "2512.06769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06769", "abs": "https://arxiv.org/abs/2512.06769", "authors": ["Hang Yin", "Xiaomin He", "PeiWen Yuan", "Yiwei Li", "Jiayi Shi", "Wenxiao Fan", "Shaoxiong Feng", "Kan Li"], "title": "Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding", "comment": null, "summary": "Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\\text{MME}_{\\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.", "AI": {"tldr": "A plug-and-play method called SiTe (Stitch and Tell) addresses spatial hallucinations in vision-language models by creating stitched image-text pairs with structured spatial supervision, improving spatial understanding without harming general capabilities.", "motivation": "Vision-language models suffer from spatial hallucinations (incorrect descriptions of object positions) due to asymmetric properties between images and text. Existing approaches lack explicit spatial structure in training data.", "method": "SiTe constructs stitched image-text pairs by stitching images along spatial axes and generating spatially-aware captions or QA pairs based on the layout. It's annotation-free, plug-and-play, and doesn't require costly models or human involvement.", "result": "SiTe improves spatial understanding across three architectures (LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B, HALVA-7B) and eight benchmarks. It boosts MME_Position (+5.50%) and Spatial-MM (+4.19%) while maintaining or improving general benchmarks like COCO-QA (+1.02%) and MMBench (+4.76%).", "conclusion": "Explicitly injecting spatially-aware structure into training data effectively mitigates spatial hallucinations and improves spatial understanding while preserving general vision-language capabilities. SiTe offers a simple, annotation-free solution."}}
{"id": "2512.06774", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06774", "abs": "https://arxiv.org/abs/2512.06774", "authors": ["Longjie Zhao", "Ziming Hong", "Zhenyang Ren", "Runnan Chen", "Mingming Gong", "Tongliang Liu"], "title": "RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.", "AI": {"tldr": "RDSplat introduces a robust watermarking method for 3D Gaussian Splatting that survives diffusion-based editing by embedding watermarks in low-frequency Gaussians and using adversarial training.", "motivation": "Current 3DGS watermarking methods are vulnerable to diffusion-based editing which can erase embedded watermarks, creating an urgent need for watermarking techniques that are intrinsically resilient to such editing operations.", "method": "RDSplat uses a multi-domain framework that embeds watermarks into low-frequency Gaussians (which diffusion editing preserves) via coordinated covariance regularization and 2D filtering. It also employs adversarial training using Gaussian blur as a surrogate for diffusion editing to enhance robustness.", "result": "Comprehensive evaluations on three benchmark datasets show RDSplat maintains superior robustness under diffusion-based editing while preserving watermark invisibility, achieving state-of-the-art performance.", "conclusion": "RDSplat provides an effective solution for robust copyright protection of 3DGS assets against diffusion-based editing attacks, addressing a critical vulnerability in existing watermarking methods."}}
{"id": "2512.06783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06783", "abs": "https://arxiv.org/abs/2512.06783", "authors": ["Tobias Leuthold", "Michele Xiloyannis", "Yves Zimmermann"], "title": "Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos", "comment": "16 pages, 5 figures", "summary": "Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.", "AI": {"tldr": "Real-time post-processing algorithm that fuses BlazePose 2D/3D estimations with anatomical constraints using weighted optimization and Kalman filtering, achieving 10.2% reduction in 3D pose error and 16.6% reduction in angle errors.", "motivation": "Automated coaching applications for physical therapy and sports need accurate pose estimation, but current models like BlazePose lack anatomical constraints and could benefit from incorporating physical knowledge for more robust performance.", "method": "Real-time post-processing algorithm that fuses BlazePose 3D and 2D estimations using weighted optimization, penalizing deviations from expected bone lengths and biomechanical models. Bone lengths are refined using a Kalman filter with adaptive measurement trust.", "result": "10.2% reduction in 3D MPJPE and 16.6% decrease in errors of angles between body segments compared to BlazePose 3D estimation on the Physio2.2M dataset.", "conclusion": "The method provides robust, anatomically consistent pose estimation suitable for automated physiotherapy, healthcare, and sports coaching on consumer devices, running on backend with anonymized data only."}}
{"id": "2512.06793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06793", "abs": "https://arxiv.org/abs/2512.06793", "authors": ["Jiaxin Liu", "Gangwei Xu", "Xianqi Wang", "Chengliang Zhang", "Xin Yang"], "title": "Generalized Geometry Encoding Volume for Real-time Stereo Matching", "comment": "Accepted by AAAI 2026", "summary": "Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.", "AI": {"tldr": "GGEV is a real-time stereo matching network that achieves strong generalization by using depth-aware features and adaptive cost aggregation, outperforming existing real-time methods in zero-shot generalization.", "motivation": "Real-time stereo methods focus on in-domain performance but neglect generalization, while foundation models improve generalization but have high latency. There's a need for real-time methods with strong generalization for practical applications.", "method": "Proposes Generalized Geometry Encoding Volume (GGEV) with two key components: 1) extracting depth-aware features encoding domain-invariant structural priors, and 2) Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis.", "result": "GGEV surpasses all existing real-time methods in zero-shot generalization capability and achieves state-of-the-art performance on KITTI 2012, KITTI 2015, and ETH3D benchmarks.", "conclusion": "GGEV successfully addresses the trade-off between real-time performance and generalization in stereo matching, providing a lightweight yet effective solution for practical applications requiring both speed and robustness to unseen scenes."}}
{"id": "2512.06802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06802", "abs": "https://arxiv.org/abs/2512.06802", "authors": ["Yutong Wang", "Haiyu Zhang", "Tianfan Xue", "Yu Qiao", "Yaohui Wang", "Chang Xu", "Xinyuan Chen"], "title": "VDOT: Efficient Unified Video Creation via Optimal Transport Distillation", "comment": null, "summary": "The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.", "AI": {"tldr": "VDOT is an efficient unified video creation model that uses distribution matching distillation with optimal transport to achieve high-quality video generation in just 4 steps, outperforming models requiring 100 steps.", "motivation": "Existing video creation models are either limited to specific conditions or too slow for practical use due to complex inference processes, creating a need for efficient, unified video generation solutions.", "method": "Uses distribution matching distillation (DMD) with computational optimal transport instead of KL divergence to optimize discrepancy between real and fake score distributions, preventing gradient collapse. Integrates a discriminator for better video quality and develops automated video annotation pipeline and UVCBench evaluation benchmark.", "result": "VDOT with only 4 denoising steps outperforms or matches other baselines requiring 100 steps, demonstrating superior efficiency and quality in video generation.", "conclusion": "VDOT provides an efficient, unified solution for video creation that addresses limitations of existing models through optimal transport-based distillation and comprehensive training/evaluation infrastructure."}}
{"id": "2512.06810", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06810", "abs": "https://arxiv.org/abs/2512.06810", "authors": ["Yueqian Wang", "Songxiang Liu", "Disong Wang", "Nuo Xu", "Guanglu Wan", "Huishuai Zhang", "Dongyan Zhao"], "title": "MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning", "comment": null, "summary": "Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.", "AI": {"tldr": "MMDuet2: A proactive Video MLLM that autonomously decides when to respond during video streaming using text-to-text approach and multi-turn RL training, achieving SOTA on ProactiveVideoQA.", "motivation": "Existing Video MLLMs operate in turn-based manner where models only reply after user turns, but proactive interaction during video playback is crucial for real-time applications. Current methods face challenges with manual threshold tuning and precise reply time annotations.", "method": "Text-to-text approach where model autonomously decides to respond or remain silent at each turn based on dialogue history and visual context. Uses multi-turn RL training that encourages timely and accurate responses without requiring precise response time annotations. Trained on 52k videos with two dialogue types via SFT and RL.", "result": "MMDuet2 outperforms existing proactive Video MLLM baselines in both response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.", "conclusion": "The proposed proactive interaction approach with multi-turn RL training enables effective real-time video understanding without manual threshold tuning or precise time annotations, advancing Video MLLMs toward more natural human-AI interaction during video streaming."}}
{"id": "2512.06811", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.06811", "abs": "https://arxiv.org/abs/2512.06811", "authors": ["Xiang Lin", "Weixin Li", "Shu Guo", "Lihong Wang", "Di Huang"], "title": "RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models", "comment": "Accepted by AAAI 2026(Oral)", "summary": "Pre-trained Vision-Language Models (VLMs), \\textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.", "AI": {"tldr": "RMAdapter: A dual-branch adapter for VLMs with adaptation and reconstruction branches to balance task-specific knowledge and generalization in few-shot scenarios.", "motivation": "Current adapter-based approaches for Vision-Language Models (VLMs) like CLIP are underexplored and have performance gaps compared to prompt-based methods. There's a need to balance task-specific adaptation and generalization in few-shot fine-tuning scenarios.", "method": "Proposes Reconstruction-based Multimodal Adapter (RMAdapter) with dual-branch architecture: 1) Adaptation branch injects task-specific knowledge via parameter-efficient fine-tuning, 2) Reconstruction branch preserves general knowledge by reconstructing latent features back to original feature space. Uses local reconstruction loss, shared projection modules, and consistency constraint for minimal overhead.", "result": "RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics on three tasks: generalization to new categories, generalization to new target datasets, and domain generalization, without using data augmentation or duplicate prompt designs.", "conclusion": "RMAdapter effectively balances task-specific adaptation and generalization in VLMs through its dual-branch architecture, achieving superior performance in few-shot scenarios while remaining lightweight and computationally efficient."}}
{"id": "2512.06818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06818", "abs": "https://arxiv.org/abs/2512.06818", "authors": ["Jan Held", "Sanghyun Son", "Renaud Vandeghen", "Daniel Rebain", "Matheus Gadelha", "Yi Zhou", "Anthony Cioppa", "Ming C. Lin", "Marc Van Droogenbroeck", "Andrea Tagliasacchi"], "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes", "comment": null, "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.", "AI": {"tldr": "MeshSplatting bridges neural rendering and traditional 3D graphics by creating mesh-based representations from primitive-based splatting methods, enabling real-time rendering in standard game/AR/VR engines while improving quality and efficiency.", "motivation": "Primitive-based splatting methods (like 3D Gaussian Splatting) achieve real-time novel view synthesis but use point-based representations incompatible with mesh-based pipelines used in AR/VR and game engines, creating a gap between neural rendering and interactive 3D graphics.", "method": "MeshSplatting jointly optimizes geometry and appearance through differentiable rendering, enforces connectivity via restricted Delaunay triangulation, and refines surface consistency to create smooth, high-quality meshes.", "result": "On Mip-NeRF360, MeshSplatting achieves +0.69 dB PSNR improvement over state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory.", "conclusion": "MeshSplatting successfully bridges neural rendering and interactive 3D graphics by producing mesh-based representations that render efficiently in real-time 3D engines, enabling seamless real-time scene interaction."}}
{"id": "2512.06838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06838", "abs": "https://arxiv.org/abs/2512.06838", "authors": ["Jiahao Wang", "Zhongwei Jiang", "Wenchao Sun", "Jiaru Zhong", "Haibao Yu", "Yuner Zhang", "Chenyang Lu", "Chuang Zhang", "Lei He", "Shaobing Xu", "Jianqiang Wang"], "title": "SparseCoop: Cooperative Perception with Kinematic-Grounded Queries", "comment": "Accepted by AAAI 2026", "summary": "Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.", "AI": {"tldr": "SparseCoop is a fully sparse cooperative perception framework for 3D detection and tracking that eliminates BEV representations, using kinematic-grounded instance queries for precise alignment and efficient communication.", "motivation": "Current cooperative perception methods face quadratically-scaling communication costs from sharing dense BEV features, lack flexibility for precise alignment across asynchronous viewpoints, and sparse query-based methods suffer from inadequate geometric representations and training instability.", "method": "Proposes SparseCoop with three innovations: 1) kinematic-grounded instance query using explicit state vector with 3D geometry and velocity for spatio-temporal alignment, 2) coarse-to-fine aggregation module for robust fusion, and 3) cooperative instance denoising task to accelerate and stabilize training.", "result": "Achieves state-of-the-art performance on V2X-Seq and Griffin datasets with superior computational efficiency, low transmission cost, and strong robustness to communication latency.", "conclusion": "SparseCoop demonstrates that fully sparse cooperative perception without BEV representations can achieve high performance while addressing communication efficiency and alignment challenges in multi-vehicle perception systems."}}
{"id": "2512.06840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06840", "abs": "https://arxiv.org/abs/2512.06840", "authors": ["Satoshi Hashimoto", "Tatsuya Konishi", "Tomoya Kaichi", "Kazunori Matsumoto", "Mori Kurokawa"], "title": "CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles", "comment": "Accepted to WACV 2026", "summary": "Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the \"incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.", "AI": {"tldr": "CADE is the first approach combining continual learning with weakly-supervised video anomaly detection to handle domain shifts across multiple scenes, addressing forgetting and incompleteness issues through ensemble methods.", "motivation": "Existing weakly-supervised VAD methods focus on static datasets and neglect domain shifts across different scenes. When adapting to new domains, traditional training causes forgetting of previous scenes and performance degradation. There's a need for continual learning approaches in VAD to handle multi-scene datasets.", "method": "Proposes Continual Anomaly Detection with Ensembles (CADE) using Dual-Generator to address data imbalance and label uncertainty in WVAD. To combat forgetting-induced \"incompleteness\" (bias toward certain anomaly modes), employs Multi-Discriminator ensemble that captures missed anomalies from past scenes using multiple models.", "result": "Extensive experiments show CADE significantly outperforms existing VAD methods on common multi-scene VAD datasets including ShanghaiTech and Charlotte Anomaly datasets.", "conclusion": "CADE successfully bridges continual learning and weakly-supervised VAD, effectively handling domain shifts across multiple scenes while mitigating forgetting and incompleteness issues through ensemble techniques."}}
{"id": "2512.06845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06845", "abs": "https://arxiv.org/abs/2512.06845", "authors": ["Satoshi Hashimoto", "Hitoshi Nishimura", "Yanan Wang", "Mori Kurokawa"], "title": "Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.", "AI": {"tldr": "PA-VAD: A generation-driven video anomaly detection method that trains without real abnormal videos, using synthesized pseudo-abnormal videos paired with real normal videos, achieving state-of-the-art performance on standard benchmarks.", "motivation": "Real-world deployment of video anomaly detection is limited by the scarcity and high collection cost of real abnormal footage, making it impractical to obtain sufficient training data for supervised methods.", "method": "1) Synthesis: Select class-relevant initial images with CLIP, refine textual prompts with vision-language models for fidelity, then use video diffusion models to generate pseudo-abnormal videos. 2) Training: Mitigate excessive spatiotemporal magnitude in synthesized anomalies using a domain-aligned regularized module that combines domain alignment and memory usage-aware updates.", "result": "Achieves 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming UVAD state-of-the-art on UCF-Crime by +1.9%.", "conclusion": "High-accuracy anomaly detection can be achieved without collecting real anomalies, providing a practical and scalable path for real-world deployment by eliminating the need for costly abnormal video collection."}}
{"id": "2512.06849", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06849", "abs": "https://arxiv.org/abs/2512.06849", "authors": ["Matan Atad", "Alexander W. Marka", "Lisa Steinhelfer", "Anna Curto-Vilalta", "Yannik Leonhardt", "Sarah C. Foreman", "Anna-Sophia Walburga Dietrich", "Robert Graf", "Alexandra S. Gersing", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke", "Hendrik M\u00f6ller"], "title": "Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT", "comment": "In submission", "summary": "Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.", "AI": {"tldr": "A weakly supervised method for vertebral metastasis segmentation in CT using only vertebra-level labels (healthy/malignant) without lesion masks, combining diffusion autoencoder editing with hide-and-seek attribution to generate accurate lytic/blastic lesion segmentations.", "motivation": "Vertebral metastasis segmentation in CT is clinically important but difficult to scale due to scarce voxel-level annotations and the similarity between malignant lesions (lytic/blastic) and benign degenerative changes.", "method": "Combines a Diffusion Autoencoder (DAE) that produces classifier-guided healthy edits of vertebrae with pixel-wise difference maps for candidate lesion regions. Uses Hide-and-Seek Attribution: each candidate region is revealed while others are hidden, the edited image is projected back to the data manifold by DAE, and a latent-space classifier quantifies the isolated malignant contribution to determine true lesions.", "result": "Achieves strong performance on held-out radiologist annotations despite no mask supervision: blastic (F1: 0.91, Dice: 0.87) and lytic (F1: 0.85, Dice: 0.78), significantly exceeding baseline methods (F1: 0.79/0.67; Dice: 0.74/0.55).", "conclusion": "Vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT for vertebral metastasis."}}
{"id": "2512.06862", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06862", "abs": "https://arxiv.org/abs/2512.06862", "authors": ["Qiancheng Zheng", "Yunhang Shen", "Gen Luo", "Baiyang Song", "Xing Sun", "Xiaoshuai Sun", "Yiyi Zhou", "Rongrong Ji"], "title": "Omni-Referring Image Segmentation", "comment": null, "summary": "In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.", "AI": {"tldr": "Proposes OmniRIS - a novel task for highly generalized image segmentation that accepts both text instructions and visual reference prompts (masks, boxes, scribbles), enabling granular attribute referring and uncommon object grounding across various segmentation settings.", "motivation": "Existing segmentation tasks are unimodally conditioned (text-only or visual-only), limiting their generalization. There's a need for a more flexible approach that can leverage both text and visual modalities to handle diverse segmentation scenarios in practical applications.", "method": "1) Introduces OmniRIS task supporting omni-prompts (text + visual references). 2) Creates OmniRef dataset with 186,939 omni-prompts for 30,956 images. 3) Develops OmniSegNet baseline with omni-prompt encoding capabilities. 4) Establishes comprehensive evaluation system.", "result": "Extensive experiments validate OmniSegNet's capability to follow omni-modal instructions and demonstrate the superiority of OmniRIS for highly generalized image segmentation across various settings (one vs. many, many vs. many).", "conclusion": "OmniRIS represents a significant advancement in generalized image segmentation by supporting multimodal prompts, enabling both precise attribute-based referring and uncommon object grounding, with practical applications facilitated by the comprehensive OmniRef dataset and OmniSegNet baseline."}}
{"id": "2512.06864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06864", "abs": "https://arxiv.org/abs/2512.06864", "authors": ["Kaixuan Lu", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training", "comment": "Accepted to WACV 2026. arXiv admin note: substantial text overlap with arXiv:2508.19808", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\\text{AP}_{50}$ on YouTubeVIS-2019 $\\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.", "AI": {"tldr": "AutoQ-VIS is an unsupervised Video Instance Segmentation framework that uses quality-guided self-training to bridge the synthetic-to-real domain gap without human annotations, achieving state-of-the-art performance.", "motivation": "Video Instance Segmentation requires both pixel-level masks and temporal consistency labels, which are challenging to annotate. Existing unsupervised methods like VideoCutLER rely on synthetic data but suffer from the synthetic-to-real domain gap.", "method": "AutoQ-VIS establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos through quality-guided self-training.", "result": "Achieves 52.6 AP50 on YouTubeVIS-2019 val set, surpassing previous state-of-the-art VideoCutLER by 4.4% while requiring no human annotations.", "conclusion": "Demonstrates the viability of quality-aware self-training for unsupervised Video Instance Segmentation, effectively bridging the synthetic-to-real domain gap."}}
{"id": "2512.06865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06865", "abs": "https://arxiv.org/abs/2512.06865", "authors": ["Xiaosong Jia", "Chenhe Zhang", "Yule Jiang", "Songbur Wong", "Zhiyuan Zhang", "Chen Chen", "Shaofeng Zhang", "Xuanhe Zhou", "Xue Yang", "Junchi Yan", "Yu-Gang Jiang"], "title": "Spatial Retrieval Augmented Autonomous Driving", "comment": "Demo Page: https://spatialretrievalad.github.io/ with open sourced code, dataset, and checkpoints", "summary": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.\n  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.", "AI": {"tldr": "The paper proposes a spatial retrieval paradigm for autonomous driving that uses offline geographic images (like Google Maps) as additional input to enhance perception beyond onboard sensors, addressing limitations like occlusion and poor visibility.", "motivation": "Current autonomous driving systems rely on onboard sensors which have limitations: limited perception horizon, occlusion issues, and poor performance in extreme conditions like darkness and rain. Human drivers can recall road structure even under poor visibility, so the authors want to endow models with similar \"recall\" ability.", "method": "Proposes spatial retrieval paradigm using offline retrieved geographic images as additional input. These images are obtained from offline caches (Google Maps or stored datasets) without requiring additional sensors. Extended nuScenes dataset with Google Maps API images aligned with ego-vehicle trajectories.", "result": "Established baselines across five core AD tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show the extended modality enhances performance of certain tasks. Will open-source dataset curation code, data, and benchmarks.", "conclusion": "Spatial retrieval paradigm using offline geographic images is a plug-and-play extension that can enhance existing AD tasks by providing additional contextual information beyond real-time sensor data, addressing limitations of current perception systems."}}
{"id": "2512.06866", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06866", "abs": "https://arxiv.org/abs/2512.06866", "authors": ["Yulin Li", "Haokun Gui", "Ziyang Fan", "Junjie Wang", "Bin Kang", "Bin Chen", "Zhuotao Tian"], "title": "Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .", "AI": {"tldr": "DyToK is a training-free method that uses VLLMs' attention mechanisms to dynamically compress video tokens, achieving 4.3x faster inference while maintaining accuracy.", "motivation": "Existing VLLMs face efficiency bottlenecks from quadratic computational growth with long video sequences. Keyframe sampling methods add computational overhead and use suboptimal binary frame selection.", "method": "DyToK leverages VLLMs' inherent attention mechanisms to encode query-conditioned keyframe priors, enabling dynamic adjustment of per-frame token retention ratios to prioritize semantically rich frames while suppressing redundancies.", "result": "DyToK achieves state-of-the-art efficiency-accuracy tradeoffs, shows plug-and-play compatibility with existing compression methods, and attains 4.3x faster inference while preserving accuracy across multiple VLLMs.", "conclusion": "DyToK provides an effective training-free paradigm for dynamic token compression in VLLMs, addressing efficiency bottlenecks while maintaining video understanding capabilities."}}
{"id": "2512.06870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06870", "abs": "https://arxiv.org/abs/2512.06870", "authors": ["Wangkai Li", "Rui Sun", "Zhaoyang Li", "Tianzhu Zhang"], "title": "Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective", "comment": "Accepted by Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.", "AI": {"tldr": "ECOCSeg introduces error-correcting output codes for semantic segmentation to handle noisy pseudo-labels in UDA and SSL, improving stability and generalization through bit-level denoising.", "motivation": "Pseudo-label learning in semantic segmentation suffers from erroneous pseudo-labels that get amplified during training due to one-hot encoding limitations, especially in label-scarce scenarios like UDA and SSL.", "method": "Proposes ECOCSeg using error-correcting output codes (ECOC) to create fine-grained class encodings, introducing an ECOC-based classifier to disentangle classes into attributes and handle partial inaccurate bits, plus a bit-level label denoising mechanism.", "result": "ECOCSeg consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures and can be easily integrated with existing methods.", "conclusion": "ECOCSeg provides a novel perspective for segmentation models that addresses pseudo-label noise through ECOC encoding, offering improved stability, generalization, and higher-quality supervision for unlabeled images."}}
{"id": "2512.06877", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06877", "abs": "https://arxiv.org/abs/2512.06877", "authors": ["Mohammed Q. Alkhatib", "Ali Jamali", "Swalpa Kumar Roy"], "title": "SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification", "comment": "Accepted and presented in ICSPIS", "summary": "Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer", "AI": {"tldr": "Proposes lightweight convolutional mixer architecture for remote sensing scene classification, achieving good accuracy-efficiency balance on AID and EuroSAT benchmarks.", "motivation": "Remote sensing scene classification is crucial for Earth observation but remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions that reduce model generalization. Existing CNN and ViT models struggle with these challenges.", "method": "Lightweight architecture based on convolutional mixer paradigm that alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information with low parameters and computations.", "result": "Achieved 74.7% overall accuracy, 74.57% average accuracy, and 73.79 Kappa on AID dataset; 93.90% overall accuracy, 93.93% average accuracy, and 93.22 Kappa on EuroSAT dataset, demonstrating good balance between accuracy and efficiency compared to CNN- and transformer-based models.", "conclusion": "The proposed convolutional mixer approach provides an effective solution for remote sensing scene classification with good accuracy-efficiency trade-off, addressing generalization challenges while maintaining computational efficiency."}}
{"id": "2512.06882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06882", "abs": "https://arxiv.org/abs/2512.06882", "authors": ["Yu Zhu", "Naoya Chiba", "Koichi Hashimoto"], "title": "Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion", "comment": "Accepted to BMVC 2025 (Sheffield, UK, Nov 24-27, 2025). Supplementary video and poster available upon request", "summary": "Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.", "AI": {"tldr": "A hierarchical image-guided 3D segmentation framework that uses 2D foundation models (SAM + YOLO-World) to progressively refine segmentation from instance-level to part-level in complex industrial scenes.", "motivation": "Industrial environments have dense layouts with multi-scale objects where heavy occlusion weakens geometric boundaries and scale differences cause end-to-end models to fail at capturing both coarse and fine details. Existing methods either require costly 3D annotations or suffer from semantic inconsistencies across views.", "method": "Two-stage hierarchical approach: 1) Instance segmentation via top-view rendering with SAM masks prompted by YOLO-World, back-projected to 3D point cloud. 2) Part-level segmentation via multi-view rendering of each instance, applying same 2D segmentation at each view, followed by Bayesian updating fusion for cross-view semantic consistency.", "result": "Experiments on real-world factory data show effective handling of occlusion and structural complexity with consistently high per-class mIoU scores. Additional evaluations on public datasets confirm generalization ability, highlighting robustness, annotation efficiency, and adaptability to diverse 3D environments.", "conclusion": "The proposed hierarchical image-guided framework successfully addresses challenges in complex industrial 3D segmentation by leveraging 2D foundation models, achieving reliable segmentation without costly 3D annotations while maintaining semantic consistency across views."}}
{"id": "2512.06885", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06885", "abs": "https://arxiv.org/abs/2512.06885", "authors": ["Wancheng Feng", "Chen An", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Lukun Wang"], "title": "JoPano: Unified Panorama Generation via Joint Modeling", "comment": "Code: https://github.com/VIPL-GENUN/JoPano", "summary": "Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.", "AI": {"tldr": "JoPano is a unified DiT-based approach for panorama generation that addresses both text-to-panorama and view-to-panorama tasks using a Joint-Face Adapter and condition switching mechanism, achieving state-of-the-art performance with improved seam consistency.", "motivation": "Existing panorama generation methods face two major challenges: U-Net-based architectures limit visual quality, and treating text-to-panorama and view-to-panorama as independent tasks leads to modeling redundancy and inefficiency.", "method": "Proposes JoPano with three key components: 1) Joint-Face Adapter built on cubemap representation to transfer DiT capabilities to panorama domain, 2) Poisson Blending to reduce seam inconsistencies, and 3) Condition switching mechanism to unify both tasks in a single model.", "result": "Achieves state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics. Introduces Seam-SSIM and Seam-Sobel metrics for quantitative seam consistency evaluation. Generates high-quality panoramas for both tasks.", "conclusion": "JoPano successfully unifies text-to-panorama and view-to-panorama generation within a single DiT-based model, overcoming architectural limitations and task redundancy while improving visual quality and seam consistency."}}
{"id": "2512.06886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06886", "abs": "https://arxiv.org/abs/2512.06886", "authors": ["Wangkai Li", "Rui Sun", "Bohao Liao", "Zhaoyang Li", "Tianzhu Zhang"], "title": "Balanced Learning for Domain Adaptive Semantic Segmentation", "comment": "Accepted by International Conference on Machine Learning (ICML 2025)", "summary": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.", "AI": {"tldr": "BLDA is a balanced learning approach for unsupervised domain adaptation in semantic segmentation that addresses class imbalance by aligning logits distributions across classes without requiring prior knowledge of distribution shifts.", "motivation": "Self-training techniques in UDA for semantic segmentation struggle with class imbalance due to inherent distribution shifts between source and target domains, leading to biased learning where some classes are over-predicted while others are under-predicted.", "method": "BLDA identifies over/under-predicted classes by analyzing predicted logits distributions, introduces post-hoc alignment using shared anchor distributions, incorporates online logits correction into the loss function during self-training, and leverages cumulative density as domain-shared structural knowledge.", "result": "Extensive experiments on two standard UDA semantic segmentation benchmarks show BLDA consistently improves performance, particularly for under-predicted classes, when integrated into various existing methods.", "conclusion": "BLDA effectively addresses class imbalance in UDA semantic segmentation without requiring prior knowledge of distribution shifts, demonstrating robust performance improvements across different methods and benchmarks."}}
{"id": "2512.06888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06888", "abs": "https://arxiv.org/abs/2512.06888", "authors": ["Liyang Song", "Hardik Bishnoi", "Sai Kumar Reddy Manne", "Sarah Ostadabbas", "Briana J. Taylor", "Michael Wan"], "title": "Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation", "comment": null, "summary": "The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.", "AI": {"tldr": "Introduces AIR-400 dataset (400 videos) and first reproducible pipelines for vision-based infant respiration monitoring, addressing the lack of public infant respiration data and algorithms.", "motivation": "Contactless respiration monitoring for infants could enable early detection of breathing irregularities linked to neurodevelopmental issues and SIDS. However, while adult respiration monitoring has robust computer vision tools and datasets, there's only one small public infant respiration video dataset and no reproducible effective algorithms for infants.", "method": "Created AIR-400 dataset with 275 new annotated videos from 10 subjects (total 400 videos). Developed reproducible pipelines using infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs.", "result": "Established first reproducible benchmarks for state-of-the-art vision-based infant respiration estimation. Made dataset, code repository, and trained models publicly available.", "conclusion": "The work addresses critical gaps in infant respiration monitoring by providing the largest public annotated dataset (AIR-400) and first reproducible algorithms, enabling advances in early detection of breathing irregularities in infants."}}
{"id": "2512.06905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06905", "abs": "https://arxiv.org/abs/2512.06905", "authors": ["Zijian Zhou", "Shikun Liu", "Haozhe Liu", "Haonan Qiu", "Zhaochong An", "Weiming Ren", "Zhiheng Liu", "Xiaoke Huang", "Kam Woh Ng", "Tian Xie", "Xiao Han", "Yuren Cong", "Hang Li", "Chuyan Zhu", "Aditya Patel", "Tao Xiang", "Sen He"], "title": "Scaling Zero-Shot Reference-to-Video Generation", "comment": "Website: https://franciszzj.github.io/Saber/", "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.", "AI": {"tldr": "Saber is a scalable zero-shot framework for reference-to-video generation that eliminates the need for expensive reference image-video-text triplets by training only on video-text pairs.", "motivation": "Current R2V methods rely on expensive explicit reference image-video-text triplets that are difficult to scale, creating a bottleneck for practical applications.", "method": "Uses masked training strategy with tailored attention-based model design to learn identity-consistent and reference-aware representations, plus mask augmentation to reduce copy-paste artifacts.", "result": "Achieves superior performance on OpenS2V-Eval benchmark compared to methods trained with R2V data, with remarkable generalization across varying numbers of references.", "conclusion": "Saber provides a scalable zero-shot solution for R2V generation that bypasses data collection bottlenecks while maintaining high performance and generalization capabilities."}}
{"id": "2512.06921", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06921", "abs": "https://arxiv.org/abs/2512.06921", "authors": ["Ziyang Song", "Zelin Zang", "Xiaofan Ye", "Boqiang Xu", "Long Bai", "Jinlin Wu", "Hongliang Ren", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "title": "NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification", "comment": "Accepted by IEEE ICIA 2025", "summary": "Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.", "AI": {"tldr": "NeuroABench is the first multimodal benchmark for evaluating anatomical understanding in neurosurgical videos, revealing that current MLLMs perform significantly worse than human trainees in identifying anatomical structures.", "motivation": "Existing MLLM research focuses on surgical procedures and workflows but neglects anatomical comprehension, which is critical for surgeons to interpret, review, and learn from surgical videos. There's a need for standardized evaluation of anatomical understanding in the neurosurgical domain.", "method": "Created NeuroABench with 9 hours of annotated neurosurgical videos covering 89 procedures using a novel multimodal annotation pipeline with multiple review cycles. Evaluates identification of 68 clinical anatomical structures. Tested over 10 state-of-the-art MLLMs and compared with 4 neurosurgical trainees.", "result": "Best-performing MLLM achieved only 40.87% accuracy in anatomical identification. Neurosurgical trainees scored 28-56% accuracy with average of 46.5%. Best MLLM performed comparably to lowest-scoring student but significantly below human average.", "conclusion": "Current MLLMs show significant limitations in anatomical understanding despite progress. There's a substantial gap between machine and human-level performance in neurosurgical anatomical comprehension, highlighting the need for further research and improvement in this critical area."}}
{"id": "2512.06949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06949", "abs": "https://arxiv.org/abs/2512.06949", "authors": ["Shravan Venkatraman", "Muthu Subash Kavitha", "Joe Dhanith P R", "V Manikandarajan", "Jia Wu"], "title": "Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology", "comment": "19 pages, 5 figures, 2 tables", "summary": "Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\\% to 31.25\\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.", "AI": {"tldr": "NTRM introduces a graph neural network framework for histopathology image segmentation that models spatial and functional relationships between tissue types, outperforming CNN-based methods by 4.9-31.25% on skin cancer datasets.", "motivation": "Current CNN-based segmentation methods for histopathology images focus on visual texture but fail to model spatial context and inter-tissue relationships, particularly in regions with overlapping or morphologically similar tissues, limiting their ability to encode biological context.", "method": "NTRM augments CNNs with a tissue-level graph neural network that constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection to explicitly encode inter-tissue dependencies.", "result": "On the Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9% to 31.25% higher than the best-performing models among evaluated approaches.", "conclusion": "Relational modeling offers a principled path toward more context-aware and interpretable histological segmentation compared to local receptive-field architectures that lack tissue-level structural awareness, enabling structurally coherent predictions in boundary-dense zones."}}
{"id": "2512.06981", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06981", "abs": "https://arxiv.org/abs/2512.06981", "authors": ["Yuemin Wang", "Ian Stavness"], "title": "Selective Masking based Self-Supervised Learning for Image Semantic Segmentation", "comment": null, "summary": "This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.", "AI": {"tldr": "Proposes selective masking image reconstruction for self-supervised semantic segmentation pretraining, outperforming random masking and ImageNet pretraining on general and weed segmentation datasets.", "motivation": "Traditional random masking in self-supervised learning may not be optimal for semantic segmentation. The paper aims to improve pretraining efficiency and downstream segmentation accuracy, especially for resource-constrained scenarios and challenging classes.", "method": "Selective masking image reconstruction that iteratively masks patches with highest reconstruction loss, leveraging trained model's knowledge to focus on informative regions rather than random masking.", "result": "Outperforms random masking and supervised ImageNet pretraining by 2.9% on general datasets (Pascal VOC, Cityscapes) and 2.5% on weed segmentation datasets. Significantly improves accuracy for lowest-performing classes. Best results when using same dataset for pretraining and downstream tasks.", "conclusion": "Selective Masking Image Reconstruction provides effective self-supervised pretraining for semantic segmentation, especially beneficial for limited model capacity scenarios and improving performance on challenging classes."}}
{"id": "2512.07034", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07034", "abs": "https://arxiv.org/abs/2512.07034", "authors": ["Tuan-Anh Vu", "Hai Nguyen-Truong", "Ziqiang Zheng", "Binh-Son Hua", "Qing Guo", "Ivor Tsang", "Sai-Kit Yeung"], "title": "Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues", "comment": "Accepted to WACV 2026", "summary": "Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.", "AI": {"tldr": "TransCues is a transformer-based framework that enhances glass object segmentation by incorporating boundary and reflection features through specialized modules, achieving state-of-the-art performance across multiple benchmark datasets.", "motivation": "Glass objects are challenging to segment due to transparency and reflection properties. Existing methods fail to adequately capture both boundary and reflective-object features that humans use to perceive glass objects.", "method": "Proposes TransCues framework with Boundary Feature Enhancement and Reflection Feature Enhancement modules integrated into a pyramidal transformer encoder-decoder architecture for transparent object segmentation.", "result": "Significant performance improvements: +4.2% mIoU on Trans10K-v2, +5.6% on MSD, +10.1% on RGBD-Mirror, +13.1% on TROSD, and +8.3% on Stanford2D3D, outperforming state-of-the-art methods.", "conclusion": "Incorporating both boundary and reflection features in a mutually beneficial way effectively addresses glass object segmentation challenges, demonstrating the importance of these visual cues for transparent object perception."}}
{"id": "2512.07037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07037", "abs": "https://arxiv.org/abs/2512.07037", "authors": ["Josep M. Rocafort", "Shaolin Su", "Javier Vazquez-Corral", "Alexandra Gomez-Villa"], "title": "Evaluating and Preserving High-level Fidelity in Super-Resolution", "comment": null, "summary": "Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.", "AI": {"tldr": "The paper introduces a new evaluation criterion for image super-resolution models focusing on high-level semantic fidelity, creates the first annotated dataset for this purpose, and shows how foundation models can better measure and improve both semantic accuracy and perceptual quality.", "motivation": "Current SR models can hallucinate content changes while achieving high visual quality, but existing metrics don't measure these high-level semantic changes well. There's a need to evaluate SR models' reliability in preserving semantic fidelity as a complementary criterion to visual quality.", "method": "1) Constructed the first annotated dataset with fidelity scores from different SR models; 2) Evaluated SOTA SR models on high-level fidelity preservation; 3) Analyzed correlation between existing image quality metrics and fidelity; 4) Showed foundation models can better address high-level fidelity measurement; 5) Fine-tuned SR models using fidelity feedback to improve both semantic fidelity and perceptual quality.", "result": "Created first fidelity-annotated SR dataset, revealed limitations of current SR models in preserving semantic content, demonstrated foundation models outperform traditional metrics for fidelity measurement, and showed fidelity feedback can improve both semantic accuracy and visual quality in SR models.", "conclusion": "High-level semantic fidelity is an important complementary criterion for evaluating SR models. The proposed fidelity measurement approach using foundation models offers better assessment of semantic preservation, and incorporating fidelity feedback can optimize SR models to balance both perceptual quality and content accuracy."}}
{"id": "2512.07051", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07051", "abs": "https://arxiv.org/abs/2512.07051", "authors": ["Adnan Munir", "Shujaat Khan"], "title": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation", "comment": "11 pages, 7 figures", "summary": "Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.", "AI": {"tldr": "DAUNet: Lightweight UNet variant with Deformable V2 Convolutions and SimAM attention for medical image segmentation, achieving better performance with fewer parameters.", "motivation": "Medical image segmentation is crucial for automated diagnostic systems, but existing models often lack spatial adaptability and context-aware feature fusion while maintaining low complexity for real-time clinical deployment.", "method": "DAUNet integrates Deformable V2 Convolutions in the bottleneck for handling geometric variations, and Parameter-Free Attention (SimAM) modules in decoder and skip pathways for saliency-aware refinement, creating a lightweight UNet variant.", "result": "Outperforms state-of-the-art models on FH-PS-AoP (ultrasound) and FUMPE (CT) datasets in Dice score, HD95, and ASD metrics while maintaining superior parameter efficiency. Shows robustness to missing context and low-contrast regions.", "conclusion": "DAUNet's combination of deformable convolutions and attention mechanisms provides effective spatial adaptability and context-aware fusion suitable for real-time, resource-constrained clinical environments."}}
{"id": "2512.07052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07052", "abs": "https://arxiv.org/abs/2512.07052", "authors": ["Hoang-Nhat Tran", "Francesco Di Sario", "Gabriele Spadaro", "Giuseppe Valenzise", "Enzo Tartaglione"], "title": "RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting", "comment": null, "summary": "Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.", "AI": {"tldr": "A flexible compression scheme for 3D Gaussian Splatting that supports interpolation at any rate between predefined bounds without retraining.", "motivation": "3D Gaussian Splatting enables real-time photorealistic rendering but suffers from large memory requirements and costly training. Existing compression approaches operate at fixed rates, limiting adaptability to varying bandwidth and device constraints.", "method": "A flexible compression scheme that supports interpolation at any rate between predefined bounds. The method is computationally lightweight and requires no retraining for any rate.", "result": "The approach achieves efficient, high-quality compression while offering dynamic rate control, preserving rendering quality across a broad range of operating points.", "conclusion": "The proposed flexible compression scheme makes 3D Gaussian Splatting suitable for practical deployment in immersive applications with varying bandwidth and device constraints."}}
{"id": "2512.07062", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07062", "abs": "https://arxiv.org/abs/2512.07062", "authors": ["Changliang Xia", "Chengyou Jia", "Minnan Luo", "Zhuohang Dang", "Xin Shen", "Bowen Ping"], "title": "$\\mathrm{D}^{\\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction", "comment": null, "summary": "Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\\mathrm{D}^{\\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\\mathrm{D}^{\\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\\mathrm{D}^{\\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.", "AI": {"tldr": "D\u00b3-Predictor: A noise-free deterministic framework that reformulates pretrained diffusion models for dense prediction tasks by eliminating stochastic noise and aggregating timestep-dependent visual priors.", "motivation": "Diffusion models with strong visual priors are powerful for dense prediction but have a core limitation: stochastic noise in diffusion sampling is inherently misaligned with dense prediction's need for deterministic image-to-geometry mapping. This noise corrupts fine-grained spatial cues and pushes models toward timestep-specific noise objectives, destroying meaningful geometric structure mappings.", "method": "D\u00b3-Predictor reformulates pretrained diffusion models without stochastic noise. Instead of using noisy inputs, it views the diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, complete geometric prior. Task-specific supervision adapts this noise-free prior to dense prediction tasks.", "result": "Extensive experiments on various dense prediction tasks show D\u00b3-Predictor achieves competitive or state-of-the-art performance across diverse scenarios. It requires less than half the training data previously used and performs inference in a single step.", "conclusion": "D\u00b3-Predictor successfully addresses the stochastic noise limitation in diffusion models for dense prediction by creating a deterministic, noise-free framework that efficiently leverages aggregated visual priors while maintaining strong performance with reduced data requirements and single-step inference."}}
{"id": "2512.07065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07065", "abs": "https://arxiv.org/abs/2512.07065", "authors": ["Anil Chintapalli", "Peter Tenholder", "Henry Chen", "Arjun Rao"], "title": "Persistent Homology-Guided Frequency Filtering for Image Compression", "comment": "17 pages, 8 figures, code available at github.com/RMATH3/persistent-homology-compression", "summary": "Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.", "AI": {"tldr": "Using discrete Fourier transform with persistent homology to extract specific frequencies for image compression, achieving JPEG-comparable compression while improving noise robustness.", "motivation": "Address challenges in feature extraction from noisy image datasets to improve model reliability by developing a compression method that preserves meaningful topological features while handling noise.", "method": "Combine discrete Fourier transform with persistent homology analysis to identify and extract specific frequencies corresponding to important topological features, enabling image compression and reconstruction while differentiating meaningful data from noise.", "result": "Achieved compression levels comparable to JPEG across six different metrics, with potential to improve binary classification performance when augmenting Convolutional Neural Networks compared to traditional methods.", "conclusion": "Persistent homology-guided frequency filtration enhances image compression reliability under noisy conditions by preserving topological features while achieving effective compression."}}
{"id": "2512.07076", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07076", "abs": "https://arxiv.org/abs/2512.07076", "authors": ["Chen-Yang Wang", "Gepeng Ji", "Song Shao", "Ming-Ming Cheng", "Deng-Ping Fan"], "title": "Context-measure: Contextualizing Metric for Camouflage", "comment": "Technical Report", "summary": "Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.", "AI": {"tldr": "Proposes Context-measure, a new evaluation metric for camouflaged object segmentation that incorporates spatial context dependencies, outperforming existing context-independent metrics.", "motivation": "Current camouflage evaluation metrics overlook the critical context-dependent nature of camouflage, as they were originally designed for general/salient objects with uncorrelated spatial context assumptions.", "method": "Develops Context-measure based on a probabilistic pixel-aware correlation framework that incorporates spatial dependencies and pixel-wise camouflage quantification to better align with human perception.", "result": "Extensive experiments across three challenging camouflaged object segmentation datasets show Context-measure delivers more reliability than existing context-independent metrics.", "conclusion": "Context-measure provides a foundational evaluation benchmark for computer vision applications involving camouflaged patterns in agricultural, industrial, and medical scenarios."}}
{"id": "2512.07078", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07078", "abs": "https://arxiv.org/abs/2512.07078", "authors": ["Bo Gao", "Jingcheng Tong", "Xingsheng Chen", "Han Yu", "Zichen Li"], "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection", "comment": "16 pages", "summary": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.\n  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.\n  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.", "AI": {"tldr": "DFIR-DETR: A transformer-based detector using dynamic feature aggregation and frequency-domain processing for small object detection in UAV images and industrial inspection, achieving SOTA results with lightweight design.", "motivation": "Current transformer-based detectors struggle with small object detection due to feature degradation from downsampling, inability of spatial convolutions to capture long-range dependencies, and inefficient upsampling methods that inflate feature maps. These issues are critical for applications like UAV remote sensing and industrial defect detection where objects are small, features are sparse/weak, backgrounds are cluttered, and scales vary dramatically.", "method": "DFIR-DETR introduces three novel components: 1) DCFA module with dynamic K-sparse attention (reducing complexity from O(N\u00b2) to O(NK)) and spatial gated linear units for nonlinear modeling; 2) DFPN module with amplitude-normalized upsampling to prevent feature inflation and dual-path shuffle convolution to retain spatial details; 3) FIRC3 module operating in frequency domain for global receptive fields without efficiency loss.", "result": "Achieved state-of-the-art mAP50 scores of 92.9% on NEU-DET dataset and 51.6% on VisDrone dataset. The model remains lightweight with only 11.7M parameters and 41.2 GFLOPs, demonstrating strong generalization across different domains.", "conclusion": "DFIR-DETR effectively addresses key challenges in small object detection through dynamic feature aggregation and frequency-domain processing, achieving superior performance with efficient computation. The method generalizes well across different application domains and works effectively in resource-limited settings for cross-scene small object detection."}}
{"id": "2512.07107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07107", "abs": "https://arxiv.org/abs/2512.07107", "authors": ["Jaeyoon Lee", "Hojoon Jung", "Sungtae Hwang", "Jihyong Oh", "Jongwon Choi"], "title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision", "comment": "Project page: https://vilab-cau.github.io/COREA/", "summary": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.", "AI": {"tldr": "COREA is a unified framework that jointly learns relightable 3D Gaussians and SDF for accurate geometry and faithful relighting, addressing limitations of existing 3DGS methods through 3D-to-3D alignment and density control.", "motivation": "Existing 3D Gaussian Splatting methods have limitations: geometry is learned from 2D renderings leading to coarse surfaces and unreliable BRDF-lighting decomposition. There's a need for more accurate geometry reconstruction and stable physically-based rendering within a unified framework.", "method": "COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy where depth provides coarse alignment between 3D Gaussians and SDF, while depth gradients and normals refine fine-scale structure. A density-control mechanism stabilizes Gaussian growth for memory efficiency.", "result": "Experiments on standard benchmarks show COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and physically-based rendering compared to existing methods.", "conclusion": "COREA successfully addresses the limitations of current 3DGS approaches by enabling direct 3D geometric learning, resulting in more accurate geometry and stable BRDF-lighting decomposition within a unified framework."}}
{"id": "2512.07110", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07110", "abs": "https://arxiv.org/abs/2512.07110", "authors": ["Liangwei Jiang", "Jinluo Xie", "Yecheng Huang", "Hua Zhang", "Hongyu Yang", "Di Huang"], "title": "MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection", "comment": null, "summary": "Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \\textbf{representation} and \\textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \\emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "Proposes MSN, a two-stream network for copy-move forgery detection that improves representation via multi-directional CNN and localization via 2-D similarity matrix, with a new deep-synthesized forgery benchmark.", "motivation": "Copy-move forgery detection is increasingly challenging due to complex transformations and fine-tuned operations, especially with deep generative networks. Existing methods have limitations in representation and localization capabilities.", "method": "Multi-directional Similarity Network (MSN) with two streams: 1) Hierarchical multi-directional CNN encoding for better patch similarity measurement, 2) 2-D similarity matrix decoder that utilizes spatial information better than 1-D vector approaches.", "result": "State-of-the-art performance on CASIA CMFD, CoMoFoD benchmarks and a new deep-synthesized forgery database, demonstrating effectiveness in detecting both manual and deep network-based copy-move forgeries.", "conclusion": "MSN addresses key limitations in existing methods through improved representation and localization, and the new benchmark addresses the growing challenge of deep-synthesized copy-move forgeries."}}
{"id": "2512.07126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07126", "abs": "https://arxiv.org/abs/2512.07126", "authors": ["Shengjie Lu", "Zhibin Wan", "Jiejie Liu", "Quan Zhang", "Mingjie Sun"], "title": "Training-free Clothing Region of Interest Self-correction for Virtual Try-On", "comment": "16 pages, 8 figures", "summary": "VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.", "AI": {"tldr": "CSC-VTON improves virtual try-on by using energy constraints on attention maps to better preserve clothing details, and introduces a new evaluation metric (VTID) for better assessment.", "motivation": "Existing VTON methods fail to preserve target clothing details (patterns, textures, boundaries) in generated results, and current evaluation metrics focus only on image realism while ignoring alignment with target elements.", "method": "Proposes using an energy function to constrain attention maps during generation, focusing attention on clothing regions to preserve details. Also introduces Virtual Try-on Inception Distance (VTID) as a new evaluation metric.", "result": "Outperforms SOTA on VITON-HD and DressCode datasets by 1.4% (LPIPS), 2.3% (FID), 12.3% (KID), and 5.8% (VTID). Also improves downstream CC-Reid performance by 2.5%, 1.1%, and 1.6% on LTCC, PRCC, VC-Clothes datasets.", "conclusion": "The energy-constrained attention approach effectively preserves clothing details in VTON, and the new VTID metric provides more comprehensive evaluation. Generated data also benefits downstream tasks like clothing-change re-identification."}}
{"id": "2512.07128", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07128", "abs": "https://arxiv.org/abs/2512.07128", "authors": ["Chau Truong", "Hieu Ta Quang", "Dung D. Le"], "title": "MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP", "comment": null, "summary": "Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.", "AI": {"tldr": "MulCLIP improves vision-language alignment for long, detailed text by using multi-level alignment strategies without region proposals.", "motivation": "CLIP and similar models struggle with lengthy, detailed descriptions because they're trained on short captions. Existing region-proposal approaches help but add deployment costs.", "method": "MulCLIP uses end-to-end multi-level alignment: 1) global contrastive alignment with extended positional embeddings, 2) token reconstruction alignment for word-patch connections, and 3) subcaption-aggregated patch alignment that extracts context-rich patches for each subcaption.", "result": "The method consistently improves downstream performance across diverse benchmarks and shows better fine-grained capability than region-proposal approaches.", "conclusion": "MulCLIP's multi-scale alignment is key to better fine-grained understanding without region proposals, making it suitable for diverse real-world applications."}}
{"id": "2512.07135", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07135", "abs": "https://arxiv.org/abs/2512.07135", "authors": ["Zebin Xing", "Pengxuan Yang", "Linbo Wang", "Yichen Zhang", "Yiming Hu", "Yupeng Zheng", "Junli Wang", "Yinfeng Gao", "Guang Li", "Kun Ma", "Long Chen", "Zhongpu Xia", "Qichao Zhang", "Hangjun Ye", "Dongbin Zhao"], "title": "TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning", "comment": null, "summary": "Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.", "AI": {"tldr": "The paper proposes a scenario-aware trajectory prior using Mixture of Experts (MoE) and policy-driven refinement via Reinforcement Learning to improve autonomous driving planning performance.", "motivation": "Current end-to-end autonomous driving systems overlook two critical issues: 1) trajectory priors vary significantly across different driving scenarios, and 2) trajectory evaluation lacks policy-driven refinement due to limitations of one-stage supervised training.", "method": "1) Use Mixture of Experts (MoE) to apply different trajectory priors tailored to different scenarios. 2) Employ Reinforcement Learning to fine-tune the trajectory scoring mechanism. 3) Integrate models with different perception backbones to enhance perceptual features.", "result": "The integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place in the competition.", "conclusion": "The proposed approach effectively addresses scenario-specific trajectory prior adaptation and policy-driven refinement, leading to improved planning performance in autonomous driving systems."}}
{"id": "2512.07136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07136", "abs": "https://arxiv.org/abs/2512.07136", "authors": ["Siyang Jiang", "Mu Yuan", "Xiang Ji", "Bufang Yang", "Zeyu Liu", "Lilin Xu", "Yang Li", "Yuting He", "Liran Dong", "Wenrui Lu", "Zhenyu Yan", "Xiaofan Jiang", "Wei Gao", "Hongkai Chen", "Guoliang Xing"], "title": "A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning", "comment": null, "summary": "Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.", "AI": {"tldr": "CUHK-X is a large-scale multimodal dataset for human action recognition, understanding, and reasoning, addressing limitations of existing datasets that lack fine-grained textual descriptions for non-RGB modalities.", "motivation": "Existing multimodal HAR datasets provide only coarse data-label annotations, which are insufficient for fine-grained action understanding and reasoning tasks enabled by LLMs. Most LVLMs struggle with non-RGB modalities due to lack of large-scale data-caption resources.", "method": "Created CUHK-X dataset with 58,445 samples covering 40 actions by 30 participants across two indoor environments. Used prompt-based scene creation method leveraging LLMs to generate logically connected activity sequences, followed by human validation for caption consistency.", "result": "Achieved average accuracies of 76.52% for HAR, 40.76% for HAU, and 70.25% for HARn across three benchmarks with six evaluation tasks.", "conclusion": "CUHK-X enables the community to apply and develop data-intensive learning methods for robust multimodal human activity analysis, bridging the gap between multimodal sensing and LLM capabilities."}}
{"id": "2512.07141", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07141", "abs": "https://arxiv.org/abs/2512.07141", "authors": ["Fenghua Weng", "Chaochao Lu", "Xia Hu", "Wenqi Shao", "Wenjie Wang"], "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models", "comment": null, "summary": "As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.", "AI": {"tldr": "TRR (Think-Reflect-Revise) is a three-stage training framework that enhances safety alignment in Large Vision Language Models through policy-guided self-reflection, addressing vulnerabilities in single-pass safety reasoning.", "motivation": "Single-pass think-then-answer safety reasoning in LVLMs remains vulnerable to contextual/visual jailbreak attacks and may overlook explicit harmful content in their own outputs. The wasted signal from first-pass reasoning containing malicious content can be exploited through reflection for genuine self-correction.", "method": "Three-stage framework: 1) Build ReSafe dataset with 5,000 think-reflect-revise examples, 2) Fine-tune target model using ReSafe dataset to initialize reflective behavior, 3) Reinforce policy-guided reflection through reinforcement learning.", "result": "TRR substantially improves safety performance across safety-awareness benchmarks and jailbreak attacks, increasing safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B while preserving stable performance on general benchmarks like MMMU and MMStar.", "conclusion": "The think-reflect-revise paradigm effectively enhances LVLM safety alignment through self-reflection, demonstrating significant improvements in safety performance without compromising general capabilities."}}
{"id": "2512.07155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07155", "abs": "https://arxiv.org/abs/2512.07155", "authors": ["Dahyeon Kye", "Jeahun Sung", "MinKyu Jeon", "Jihyong Oh"], "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics", "comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/", "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.", "AI": {"tldr": "CHIMERA is a zero-shot diffusion-based framework for smooth image morphing using adaptive cache injection and semantic anchor prompting to handle large semantic disparities.", "motivation": "Diffusion models have strong generative capabilities but struggle with smooth, semantically consistent image morphing, often producing abrupt transitions or over-saturated appearances due to lack of adaptive structural and semantic alignments.", "method": "Formulates morphing as cached inversion-guided denoising with two key components: 1) Adaptive Cache Injection (ACI) that caches features from both inputs during DDIM inversion and re-injects them adaptively during denoising, and 2) Semantic Anchor Prompting (SAP) that uses a vision-language model to generate a shared anchor prompt as semantic bridge. Also introduces Global-Local Consistency Score (GLCS) as evaluation metric.", "result": "Extensive experiments and user studies show CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing new state-of-the-art in image morphing.", "conclusion": "CHIMERA successfully addresses the challenge of smooth image morphing with large semantic disparities through adaptive feature injection and semantic bridging, with code and project page to be publicly released."}}
{"id": "2512.07165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07165", "abs": "https://arxiv.org/abs/2512.07165", "authors": ["Muyu Xu", "Fangneng Zhan", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation", "comment": null, "summary": "Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.", "AI": {"tldr": "MuSASplat is a lightweight framework for sparse-view 3D Gaussian splatting that reduces computational costs while maintaining rendering quality through efficient fine-tuning and feature fusion.", "motivation": "Existing pose-free feed-forward methods for sparse-view 3D Gaussian splatting rely on full fine-tuning of large Vision Transformer backbones, which incurs substantial GPU costs and computational burden.", "method": "Introduces two key components: 1) A lightweight Multi-Scale Adapter for efficient fine-tuning of ViT-based architectures with minimal training parameters, and 2) A Feature Fusion Aggregator that integrates features across input views effectively while reducing memory usage and computational costs compared to memory banks.", "result": "Extensive experiments across diverse datasets show MuSASplat achieves state-of-the-art rendering quality while significantly reducing parameters and training resource requirements compared to existing methods.", "conclusion": "MuSASplat dramatically reduces computational burden for training pose-free feed-forward 3D Gaussian splatting models with little compromise in rendering quality, making it a more efficient and practical solution for sparse-view novel view synthesis."}}
{"id": "2512.07166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07166", "abs": "https://arxiv.org/abs/2512.07166", "authors": ["Siyuan Xu", "Yibing Liu", "Peilin Chen", "Yung-Hui Li", "Shiqi Wang", "Sam Kwong"], "title": "When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing", "comment": "9 pages,7figures", "summary": "Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.", "AI": {"tldr": "This paper addresses privacy leakage in MLLMs by focusing on restoring surrogate-driven protected data, introducing the SPPE dataset and a unified approach for privacy recovery while preserving MLLM editing fidelity.", "motivation": "Existing privacy protection methods for MLLMs effectively obscure private information but fail to evaluate the authenticity and recovery quality of user privacy. The paper aims to address the critical challenge of restoring surrogate-driven protected data in diverse MLLM scenarios.", "method": "1) Created the SPPE dataset with diverse privacy categories and user instructions, providing protected surrogates and their MLLM-edited versions. 2) Formulated privacy recovery as a guided generation task conditioned on complementary multimodal signals. 3) Introduced a unified approach that reconstructs private content while preserving MLLM-generated edits.", "result": "The approach generalizes well across diverse visual content and editing tasks on both SPPE and InstructPix2Pix datasets, achieving a strong balance between privacy protection and MLLM usability.", "conclusion": "The work successfully addresses the privacy recovery challenge in MLLMs by providing both a comprehensive dataset (SPPE) and a unified method that maintains privacy protection while ensuring MLLM functionality, bridging an important research gap in evaluating privacy recovery quality."}}
{"id": "2512.07170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07170", "abs": "https://arxiv.org/abs/2512.07170", "authors": ["Jiayang Li", "Chengjie Jiang", "Junjun Jiang", "Pengwei Liang", "Jiayi Ma", "Liqiang Nie"], "title": "Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach", "comment": null, "summary": "Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.", "AI": {"tldr": "DiTFuse is a unified diffusion-transformer framework for multimodal image fusion that uses natural language instructions for hierarchical control, trained without ground truth via multi-degradation masked modeling.", "motivation": "Existing fusion methods lack robustness, adaptability, and controllability; they're task-specific, can't incorporate user intent, and struggle with complex scenarios like low-light degradation. The absence of ground-truth fused images and small datasets make end-to-end training difficult.", "method": "DiTFuse uses a Diffusion-Transformer (DiT) framework that jointly encodes two images and natural-language instructions in shared latent space. Training employs multi-degradation masked-image modeling to learn cross-modal alignment, modality-invariant restoration, and task-aware feature selection without ground truth. A curated multi-granularity instruction dataset enables interactive fusion.", "result": "Superior quantitative and qualitative performance on IVIF, MFF, and MEF benchmarks with sharper textures and better semantic retention. The model supports multi-level user control, zero-shot generalization to other fusion scenarios, and instruction-conditioned segmentation.", "conclusion": "DiTFuse provides a unified, semantics-aware fusion framework that overcomes limitations of existing methods through instruction-driven control, enabling flexible adaptation to various fusion tasks without task-specific architectures."}}
{"id": "2512.07171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07171", "abs": "https://arxiv.org/abs/2512.07171", "authors": ["Shravan Venkatraman", "Rakesh Raj Madavan", "Pavan Kumar S", "Muthu Subash Kavitha"], "title": "TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration", "comment": "21 pages, 11 figures, 5 tables", "summary": "Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\\underline{t}$wo stage $\\underline{i}$nverse $\\underline{d}$egradation $\\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.", "AI": {"tldr": "TIDE is a two-stage underwater image restoration framework that explicitly models degradation characteristics through specialized prior decomposition and adaptive fusion.", "motivation": "Existing underwater image restoration methods apply uniform strategies across entire images, struggling with spatially varying co-occurring degradations that change with water conditions.", "method": "Two-stage inverse degradation estimation framework: 1) decomposes degradations into four factors (color distortion, haze, detail loss, noise), 2) designs restoration experts for each factor, 3) generates specialized hypotheses adaptively fused based on local degradation patterns, 4) progressive refinement stage corrects residual artifacts.", "result": "Achieves competitive performance on reference-based fidelity metrics and outperforms state-of-the-art methods on non-reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement across standard benchmarks and challenging turbid water conditions.", "conclusion": "TIDE effectively addresses spatially varying underwater degradations through explicit degradation modeling and targeted restoration, producing natural results even in highly degraded regions."}}
{"id": "2512.07186", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07186", "abs": "https://arxiv.org/abs/2512.07186", "authors": ["Zhuoming Liu", "Xiaofeng Gao", "Feiyang Niu", "Qiaozi Gao", "Liu Liu", "Robinson Piramuthu"], "title": "START: Spatial and Textual Learning for Chart Understanding", "comment": "WACV2026 Camera Ready", "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.", "AI": {"tldr": "START is a multimodal LLM approach for chart understanding that combines spatial layout learning with textual data representation through chart-element grounding and chart-to-code generation.", "motivation": "Charts combine structured visual layouts (spatial properties) with underlying data representations (textual properties), requiring both for precise chart reasoning. Existing methods struggle to handle this dual nature effectively.", "method": "Proposes START with two key components: (1) chart-element grounding to understand visual layout, and (2) chart-to-code generation to capture data details. Uses a novel data generation pipeline where MLLMs translate real chart images to executable code, then LLMs evolve the code to determine chart element positions.", "result": "START delivers consistent gains across model sizes and benchmarks over base models, surpassing prior state-of-the-art by a clear margin. Also introduces CS-Bench for evaluating chart spatial understanding.", "conclusion": "The approach successfully addresses the dual nature of chart understanding by combining spatial and textual learning, filling a critical gap in comprehensive chart understanding evaluation and achieving superior performance."}}
{"id": "2512.07190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07190", "abs": "https://arxiv.org/abs/2512.07190", "authors": ["Pengfei Gu", "Huimin Li", "Haoteng Tang", "Dongkuan", "Xu", "Erik Enriquez", "DongChul Kim", "Bin Fu", "Danny Z. Chen"], "title": "Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification", "comment": null, "summary": "Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.", "AI": {"tldr": "A new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them with vision backbones for improved medical image classification.", "motivation": "Current deep networks either focus on pixel-intensity features instead of fundamental anatomical structures, or capture only simple topological features via single-parameter persistence, missing complex anatomical patterns important for medical diagnosis.", "method": "1) Compute cubical persistence diagrams across multiple image resolutions/scales; 2) Develop a \"vineyard\" algorithm to consolidate PDs into a single stable diagram capturing signatures from global anatomy to local irregularities; 3) Design a cross-attention-based neural network to process consolidated PDs; 4) Fuse topological embeddings with CNN/Transformer feature maps in end-to-end architecture.", "result": "Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating enhanced model capacity to recognize complex anatomical structures.", "conclusion": "The comprehensive topological perspective provides robust and interpretable medical image classification by integrating multi-scale and multi-filtration topologies into vision backbones, capturing both global anatomy and subtle local irregularities indicative of disease."}}
{"id": "2512.07191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07191", "abs": "https://arxiv.org/abs/2512.07191", "authors": ["Wenqi Zhao", "Jiacheng Sang", "Fenghua Cheng", "Yonglu Shu", "Dong Li", "Xiaofeng Yang"], "title": "RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction", "comment": null, "summary": "Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.", "AI": {"tldr": "RefLSM: A novel variational level set model that integrates Retinex-inspired reflectance decomposition for robust medical image segmentation under non-uniform illumination and noise.", "motivation": "Medical image segmentation faces challenges from intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods struggle under severe non-uniform imaging conditions due to their dependence on approximate bias field estimations.", "method": "Proposes Reflectance-based Level Set Model (RefLSM) that decomposes images into reflectance and bias field components, then segments the illumination-invariant reflectance. Includes two innovations: 1) linear structural prior that guides smoothed reflectance gradients toward data-driven references, and 2) relaxed binary level-set enforced via convex relaxation and sign projection. Uses ADMM-based optimization for efficient solving.", "result": "Extensive experiments on multiple medical imaging datasets demonstrate RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.", "conclusion": "RefLSM effectively addresses limitations of traditional level set methods by explicitly integrating reflectance decomposition, providing a robust framework for medical image segmentation under challenging conditions like non-uniform illumination and noise."}}
{"id": "2512.07192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07192", "abs": "https://arxiv.org/abs/2512.07192", "authors": ["Niu Yi", "Xu Tianyi", "Ma Mingming", "Wang Xinkun"], "title": "HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression", "comment": "12 pages, 7 figures", "summary": "Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.", "AI": {"tldr": "HVQ-CGIC introduces a hyperprior-based entropy model for VQ-based generative image compression, achieving better rate-distortion performance and flexible rate control compared to existing methods.", "motivation": "Current VQ-based generative image compression methods use static global probability distributions for entropy estimation, which doesn't adapt to image content, leading to inefficient bitrate usage and poor rate control flexibility.", "method": "Introduces HVQ-CGIC framework with mathematical foundation for hyperprior in VQ indices entropy model, novel loss design for RD balance/control, and lightweight hyper-prior estimation network.", "result": "Achieves 61.3% fewer bits than Control-GIC, CDC and HiFiC for same LPIPS on Kodak dataset; significant RD performance advantage over SOTA generative compression methods.", "conclusion": "HVQ-CGIC has potential to become foundational component for VQGAN-based image compression, analogous to HyperPrior framework's role in neural image compression."}}
{"id": "2512.07197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07197", "abs": "https://arxiv.org/abs/2512.07197", "authors": ["Seokhyun Youn", "Soohyun Lee", "Geonho Kim", "Weeyoung Kwon", "Sung-Ho Bae", "Jihyong Oh"], "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting", "comment": "The first three authors contributed equally to this work. The last two authors are co-corresponding authors. Please visit our project page at https://cmlab-korea.github.io/Awesome-Efficient-GS/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.", "AI": {"tldr": "Survey paper providing first unified overview of efficient 3D/4D Gaussian Splatting techniques, categorizing methods into Parameter Compression and Restructuring Compression to address memory/computational challenges.", "motivation": "3D Gaussian Splatting enables real-time, high-fidelity 3D reconstruction but faces massive memory and computational demands, especially severe in 4D dynamic scenes. Need for efficient methods to reduce redundancy while preserving quality.", "method": "Systematic survey categorizing existing efficient 3D/4D Gaussian Splatting methods into two major directions: Parameter Compression and Restructuring Compression. Provides comprehensive summary of core ideas, methodological trends, datasets, evaluation metrics, and benchmark comparisons.", "result": "First unified overview of efficient Gaussian Splatting techniques, establishing systematic categorization framework for both 3D and 4D settings. Identifies current methodological trends and provides comparative analysis of existing approaches.", "conclusion": "Paper establishes foundation for efficient Gaussian Splatting research, discusses current limitations, and outlines promising directions toward scalable, compact, real-time Gaussian Splatting for both static and dynamic 3D scene representation."}}
{"id": "2512.07198", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07198", "abs": "https://arxiv.org/abs/2512.07198", "authors": ["Xiujie Song", "Qi Jia", "Shota Watanabe", "Xiaoyi Pang", "Ruijie Chen", "Mengyue Wu", "Kenny Q. Zhu"], "title": "Generating Storytelling Images with Rich Chains-of-Reasoning", "comment": null, "summary": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.", "AI": {"tldr": "The paper introduces Storytelling Image Generation, a task using AI to create images with rich narrative connections. They propose StorytellingPainter pipeline combining LLMs and T2I models, with evaluation framework and lightweight Mini-Storyteller models.", "motivation": "Storytelling images convey compelling narratives through rich visual connections, but are challenging to create manually and thus scarce. The paper aims to leverage generative AI to address this scarcity and enable diverse applications beyond illustration and cognitive screening.", "method": "Two-stage StorytellingPainter pipeline: 1) Uses LLMs for creative story reasoning, 2) Text-to-Image models for visual synthesis. Also develops evaluation framework with three evaluators: Semantic Complexity, KNN-based Diversity, and Story-Image Alignment. Creates Mini-Storyteller models via tailored training to bridge performance gap between open-source and proprietary LLMs.", "result": "Experimental results demonstrate feasibility and effectiveness of the approaches. The proposed pipeline successfully generates storytelling images, and the evaluation framework provides comprehensive assessment. Mini-Storyteller models show improved performance while being lightweight.", "conclusion": "The paper successfully introduces and addresses the Storytelling Image Generation task, providing a practical pipeline, evaluation framework, and lightweight models. This enables creation of semantically rich images with narrative connections that were previously challenging to produce."}}
{"id": "2512.07201", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07201", "abs": "https://arxiv.org/abs/2512.07201", "authors": ["Cheng Yu"], "title": "Understanding Diffusion Models via Code Execution", "comment": null, "summary": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.", "AI": {"tldr": "A minimal 300-line implementation tutorial explaining diffusion models from a code-execution perspective, bridging theory and practice.", "motivation": "To address the gap between complex mathematical formulations of diffusion models and practical implementations, providing clear guidance on how diffusion models actually operate in code.", "method": "Develop a concise implementation (~300 lines) that preserves essential components: forward diffusion, reverse sampling, noise-prediction network, and training loop while removing unnecessary engineering details.", "result": "Created a minimal working example that demonstrates diffusion models from a code-execution perspective, making the connection between theory and implementation clear.", "conclusion": "Provides researchers with an implementation-first understanding of diffusion models, bridging the gap between mathematical theory and practical code implementation."}}
{"id": "2512.07203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07203", "abs": "https://arxiv.org/abs/2512.07203", "authors": ["Xuhui Zheng", "Kang An", "Ziliang Wang", "Yuhang Wang", "Faqiang Qian", "Yichao Wu"], "title": "MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning", "comment": "7 pages, 1 figures", "summary": "Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.", "AI": {"tldr": "MMRPT introduces masked multimodal reinforcement pre-training that uses reinforcement learning to strengthen visual reasoning in MLLMs by rewarding visual grounding over caption imitation.", "motivation": "Current multimodal pre-training suffers from descriptive bias in image-caption pairs, causing models to rely on surface linguistic cues rather than grounded visual understanding.", "method": "Uses masked multimodal reinforcement pre-training: estimates visual dependency via attention, masks highly vision-dependent segments, then reconstructs them through vision-grounded reasoning guided by semantic-visual reward signals.", "result": "Shows consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning.", "conclusion": "Reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models."}}
{"id": "2512.07206", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07206", "abs": "https://arxiv.org/abs/2512.07206", "authors": ["Boyang Pan", "Zeyu Zhang", "Hongyu Meng", "Bin Cui", "Yingying Zhang", "Wenli Hou", "Junhao Li", "Langdi Zhong", "Xiaoxiao Chen", "Xiaoyu Xu", "Changjin Zuo", "Chao Cheng", "Nan-Jie Gong"], "title": "AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT", "comment": null, "summary": "Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.", "AI": {"tldr": "AutoLugano: A fully automated deep learning system for end-to-end lymphoma classification from FDG-PET/CT scans, performing lesion segmentation, anatomical localization, and automated Lugano staging.", "motivation": "To develop an automated system for lymphoma classification that can translate baseline FDG-PET/CT scans into complete Lugano staging, assisting in initial staging, treatment stratification, and clinical decision-making without manual intervention.", "method": "Three sequential modules: (1) Anatomy-Informed Lesion Segmentation using 3D nnU-Net on multi-channel inputs, (2) Atlas-based Anatomical Localization using TotalSegmentator to map lesions to 21 lymph node regions, and (3) Automated Lugano Staging translating spatial distribution into Lugano stages and therapeutic groups (Limited vs. Advanced Stage). Trained on autoPET dataset (n=1,007) and validated on independent cohort (n=67).", "result": "External validation showed overall accuracy of 88.31%, sensitivity 74.47%, specificity 94.21%, F1-score 80.80% for regional involvement detection. For therapeutic stratification (Limited vs. Advanced Stage), achieved accuracy of 85.07%, specificity 90.48%, sensitivity 82.61%, outperforming baseline models.", "conclusion": "AutoLugano represents the first fully automated, end-to-end pipeline that translates single baseline FDG-PET/CT scans into complete Lugano stages, demonstrating strong potential to assist in initial staging, treatment stratification, and clinical decision-making."}}
{"id": "2512.07211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07211", "abs": "https://arxiv.org/abs/2512.07211", "authors": ["Frederik Hagelskj\u00e6r", "Dimitrios Arapis", "Steffen Madsen", "Thorbj\u00f8rn Mosekj\u00e6r Iversen"], "title": "Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds", "comment": "8 pages, 8 figures, 5 tables, ICCR 2025", "summary": "Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.\n  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io", "AI": {"tldr": "First deep learning method for object pose uncertainty estimation using only 3D colorless data, validated in real-world bin picking with geometrically ambiguous objects.", "motivation": "Single pose estimates can't capture visual ambiguity, leading to unreliable robotic behavior. Existing methods rely on color information often unavailable in industrial settings.", "method": "Novel neural network-based approach for pose distribution estimation using only 3D colorless data, focusing on symmetries in reflection and revolution.", "result": "Validated in real-world bin picking scenario with objects of varying geometric ambiguity. Framework is extendable to full SE(3) pose distribution estimation.", "conclusion": "First deep learning approach for pose uncertainty estimation without RGB input, addressing industrial limitations where color information is often unavailable."}}
{"id": "2512.07215", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07215", "abs": "https://arxiv.org/abs/2512.07215", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation", "comment": null, "summary": "Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.", "AI": {"tldr": "Comparison of CLIP vs DINOv2 for 6D object pose estimation in hand-object grasping, showing CLIP excels in semantic understanding while DINOv2 provides better geometric features.", "motivation": "Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision with rich semantic and geometric representations, but their comparative strengths for 3D pose estimation in grasping scenarios need systematic evaluation.", "method": "Comprehensive visual comparison between CLIP-based and DINOv2-based approaches for 6D object pose estimation in hand-object grasping scenarios, evaluated through extensive experiments on benchmark datasets.", "result": "CLIP-based methods achieve better semantic consistency through language grounding, while DINOv2-based approaches demonstrate competitive performance with enhanced geometric precision. Both models show complementary strengths.", "conclusion": "The analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping applications based on whether semantic understanding or geometric precision is prioritized."}}
{"id": "2512.07228", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07228", "abs": "https://arxiv.org/abs/2512.07228", "authors": ["Hengyang Yao", "Lin Li", "Ke Sun", "Jianing Qiu", "Huiping Chen"], "title": "Towards Robust Protective Perturbation against DeepFake Face Swapping", "comment": null, "summary": "DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.", "AI": {"tldr": "EOLT framework learns optimal transformation distributions for DeepFake protection, achieving 26% higher robustness than state-of-the-art methods.", "motivation": "Current DeepFake protection methods using invisible perturbations are fragile against transformations like compression/resizing. Standard EOT with uniform sampling is suboptimal because protection robustness is highly sensitive to transformation choices.", "method": "Proposes Expectation Over Learned distribution of Transformation (EOLT) framework that treats transformation distribution as learnable. Uses policy network with reinforcement learning to prioritize critical transformations and generate instance-specific perturbations.", "result": "Achieves 26% higher average robustness and up to 30% gains on challenging transformation categories compared to state-of-the-art approaches.", "conclusion": "EOLT provides a more effective framework for DeepFake protection by learning optimal transformation distributions rather than using fixed uniform sampling, significantly improving robustness against various transformations."}}
{"id": "2512.07229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07229", "abs": "https://arxiv.org/abs/2512.07229", "authors": ["Fang Zhou", "Zhiqiang Chen", "Martin Pavlovski", "Yizhong Zhang"], "title": "ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery", "comment": "Accepted to the Main Track of the 28th European Conference on Artificial Intelligence (ECAI 2025). To appear in the proceedings published by IOS Press (DOI: 10.3233/FAIA413)", "summary": "Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.", "AI": {"tldr": "ReLKD is an end-to-end framework for Generalized Category Discovery that leverages implicit inter-class relations to improve novel class classification through a three-module architecture combining target-grained, coarse-grained, and distillation components.", "motivation": "Previous GCD approaches treat classes independently, ignoring important inter-class relationships that could enhance classification, especially for novel classes. Directly obtaining such relations is challenging in real-world scenarios where only known class labels are available.", "method": "ReLKD uses three modules: 1) target-grained module for discriminative representations, 2) coarse-grained module for capturing hierarchical class relations, and 3) distillation module that transfers knowledge from coarse-grained to target-grained module to refine representation learning.", "result": "Extensive experiments on four datasets demonstrate ReLKD's effectiveness, particularly in scenarios with limited labeled data. The method shows improved performance in classifying novel classes by exploiting implicit inter-class relations.", "conclusion": "ReLKD successfully addresses the challenge of leveraging implicit inter-class relations in GCD without requiring explicit relation annotations, providing an effective framework for novel class discovery especially when labeled data is scarce."}}
{"id": "2512.07230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07230", "abs": "https://arxiv.org/abs/2512.07230", "authors": ["Abhinav Raundhal", "Gaurav Behera", "P J Narayanan", "Ravi Kiran Sarvadevabhatla", "Makarand Tapaswi"], "title": "STRinGS: Selective Text Refinement in Gaussian Splatting", "comment": "Accepted to WACV 2026. Project Page, see https://STRinGS-official.github.io", "summary": "Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.", "AI": {"tldr": "STRinGS: A text-aware selective refinement framework for 3D Gaussian Splatting that preserves fine-grained text details by treating text and non-text regions separately during reconstruction.", "motivation": "3D representations like 3D Gaussian Splatting struggle to preserve fine-grained text details, which is critical since text conveys important contextual information in real-world scenes. Small errors in text reconstruction can lead to significant semantic loss.", "method": "A selective refinement framework that treats text and non-text regions separately. It refines text regions first, then merges them with non-text regions for full-scene optimization. Also introduces OCR Character Error Rate (CER) as a text readability measure and a curated dataset STRinGS-360.", "result": "STRinGS produces sharp, readable text even in challenging configurations, achieving 63.6% relative improvement over standard 3DGS at just 7K iterations. The method demonstrates superior text preservation capabilities.", "conclusion": "STRinGS and the accompanying STRinGS-360 dataset push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods that preserve semantic information."}}
{"id": "2512.07234", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07234", "abs": "https://arxiv.org/abs/2512.07234", "authors": ["Biao Chen", "Lin Zuo", "Mengmeng Jing", "Kunbin He", "Yuchen Wang"], "title": "Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models", "comment": null, "summary": "Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.", "AI": {"tldr": "Dropout Prompt Learning applies dropout to vision-language models by evaluating token significance across modalities and using residual entropy regularization to maintain semantic alignment while encouraging diverse representations.", "motivation": "To improve the robustness of vision-language models by applying dropout regularization in a more sophisticated way that considers both intra-modal context and inter-modal alignment, addressing challenges in low-shot learning, long-tail classification, and out-of-distribution generalization.", "method": "Proposes Dropout Prompt Learning with two key innovations: 1) Token-level dropout with flexible probabilities based on token significance considering both intra-modal context and inter-modal alignment, 2) Residual entropy regularization to maintain semantic alignment for knowledge transfer while encouraging diverse representations from dropout.", "result": "Experiments on 15 benchmarks show effectiveness in challenging scenarios. Notably outperforms regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in base-to-novel generalization.", "conclusion": "Dropout Prompt Learning effectively improves vision-language model robustness through sophisticated token-level dropout and residual entropy regularization, demonstrating strong performance across various challenging generalization tasks."}}
{"id": "2512.07237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07237", "abs": "https://arxiv.org/abs/2512.07237", "authors": ["Cheng Zhang", "Boying Li", "Meng Wei", "Yan-Pei Cao", "Camilo Cruz Gambardella", "Dinh Phung", "Jianfei Cai"], "title": "Unified Camera Positional Encoding for Controlled Video Generation", "comment": "Code: https://github.com/chengzhag/UCPE", "summary": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.", "AI": {"tldr": "UCPE introduces a unified camera positional encoding that handles full camera information (poses, intrinsics, distortions) for better camera-controllable video generation with Transformers.", "motivation": "Existing camera encoding methods rely on simplified pinhole assumptions, limiting generalization across diverse real-world camera intrinsics and lens distortions needed for 3D perception and video generation tasks.", "method": "Proposes Relative Ray Encoding for geometry-consistent representation of complete camera information, plus Absolute Orientation Encoding for pitch/roll control. Integrates into pretrained video Diffusion Transformers via lightweight spatial attention adapter (<1% trainable params).", "result": "Achieves state-of-the-art camera controllability and visual fidelity in camera-controlled text-to-video generation. Validated through extensive experiments on large video dataset covering diverse camera motions and lens types.", "conclusion": "UCPE demonstrates effectiveness as a general camera representation for Transformers, with potential applications across multi-view, video, and 3D tasks beyond video generation."}}
{"id": "2512.07241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07241", "abs": "https://arxiv.org/abs/2512.07241", "authors": ["Md. Srabon Chowdhury", "Syeda Fahmida Tanzim", "Sheekar Banerjee", "Ishtiak Al Mamoon", "AKM Muzahidul Islam"], "title": "Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture", "comment": null, "summary": "Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.", "AI": {"tldr": "A hybrid deep learning model combining SqueezeNet v1 and EfficientNet-B0 with handcrafted radiomic features achieves 98.93% accuracy for brain tumor classification from MRI, offering computational efficiency with clinical reliability.", "motivation": "Brain tumor diagnosis requires timely and accurate MRI interpretation, but manual tumor delineation is difficult, time-consuming, and prone to inter-observer errors. Current deep learning approaches need improvement in balancing computational efficiency with diagnostic accuracy.", "method": "Proposes a hybrid deep learning model combining SqueezeNet v1 (lightweight) and EfficientNet-B0 (high-performing), enhanced with handcrafted radiomic descriptors including HOG, LBP, Gabor filters, and Wavelet transforms. Trained on 7,023 MRI slices from the Nickparvar Brain Tumor MRI dataset across four classes: glioma, meningioma, pituitary tumor, and no tumor.", "result": "Achieved 98.93% testing accuracy, which improved to 99.08% with Test Time Augmentation (TTA). The model demonstrates excellent generalization with only 2.1 million parameters and less than 1.2 GFLOPs, offering a good balance between computational efficiency and diagnostic accuracy.", "conclusion": "The hybrid model achieves near-clinical reliability for automated MRI-based brain tumor classification, showing potential for clinical decision-support systems by combining computational efficiency with high diagnostic accuracy through the integration of deep learning and handcrafted radiomic features."}}
{"id": "2512.07245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07245", "abs": "https://arxiv.org/abs/2512.07245", "authors": ["Toshinori Yamauchi", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Zero-Shot Textual Explanations via Translating Decision-Critical Features", "comment": "11+6 pages, 8 figures, 4 tables", "summary": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.", "AI": {"tldr": "TEXTER generates textual explanations for image classifier decisions by isolating decision-critical features before alignment, producing more faithful explanations than existing methods.", "motivation": "Current zero-shot explanation methods produce generic descriptions of what's visible in images rather than explaining what specifically drives the classifier's prediction. Large vision-language models aren't designed for classifier-specific reasoning.", "method": "TEXTER identifies neurons contributing to predictions, emphasizes decision-critical features, maps them to CLIP feature space for text retrieval, and uses a sparse autoencoder to improve interpretability for Transformer architectures.", "result": "Extensive experiments show TEXTER generates more faithful and interpretable explanations than existing methods, better reflecting the model's actual reasoning process.", "conclusion": "By isolating decision-critical features before alignment, TEXTER overcomes limitations of existing methods and produces textual explanations that truly reflect classifier reasoning, with improved interpretability through sparse autoencoding."}}
{"id": "2512.07247", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07247", "abs": "https://arxiv.org/abs/2512.07247", "authors": ["Ziming Hong", "Tianyu Huang", "Runnan Chen", "Shanshan Ye", "Mingming Gong", "Bo Han", "Tongliang Liu"], "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing", "comment": "40 pages, 34 figures, 18 tables", "summary": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.", "AI": {"tldr": "AdLift is the first editing safeguard for 3D Gaussian Splatting that prevents unauthorized instruction-driven editing by lifting 2D adversarial perturbations into 3D Gaussian representations.", "motivation": "While diffusion-based 3DGS editing pipelines advance content creation, they expose 3D assets to risks of unauthorized editing and malicious tampering. Existing 2D adversarial perturbation methods face challenges when applied to 3DGS: they need view-generalizable protection and must balance invisibility with protection capability.", "method": "AdLift lifts strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguards using a tailored Lifted PGD. It alternates between gradient truncation during back-propagation from editing models and image-to-Gaussian fitting operations to optimize safeguard Gaussians across training views while maintaining perturbation constraints.", "result": "AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing methods, providing consistent adversarial-based protection performance across different viewpoints that generalizes to novel views.", "conclusion": "The proposed AdLift framework successfully addresses the challenges of view-generalizable protection and balancing invisibility with protection capability for 3DGS assets, establishing the first effective safeguard against unauthorized instruction-driven editing of 3D Gaussian Splatting content."}}
{"id": "2512.07251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07251", "abs": "https://arxiv.org/abs/2512.07251", "authors": ["Junqi Liu", "Zejun Wu", "Pedro R. A. S. Bassi", "Xinze Zhou", "Wenxuan Li", "Ibrahim E. Hamamci", "Sezgin Er", "Tianyu Lin", "Yi Luo", "Szymon P\u0142otka", "Bjoern Menze", "Daguang Xu", "Kai Ding", "Kang Wang", "Yang Yang", "Yucheng Tang", "Alan L. Yuille", "Zongwei Zhou"], "title": "See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement", "comment": null, "summary": "Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.", "AI": {"tldr": "SMILE is an anatomy-aware diffusion model for medical image enhancement that preserves organ structure and contrast dynamics while improving image quality and clinical utility.", "motivation": "Current medical image enhancement models often over-edit, distorting organs, creating false findings, and missing small tumors because they lack anatomical understanding and contrast dynamics knowledge.", "method": "SMILE uses three key innovations: (1) structure-aware supervision following true organ boundaries and contrast patterns, (2) registration-free learning with unaligned multi-phase CT scans, and (3) unified inference for fast, consistent enhancement across all contrast phases.", "result": "Across six external datasets, SMILE outperforms existing methods with 14.2% higher SSIM, 20.6% higher PSNR, 50% better FID, and improves cancer detection from non-contrast CT by up to 10% F1 score.", "conclusion": "SMILE provides anatomically accurate and diagnostically meaningful image enhancement by understanding organ shapes and contrast dynamics, enhancing only clinically relevant regions while preserving other areas."}}
{"id": "2512.07253", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07253", "abs": "https://arxiv.org/abs/2512.07253", "authors": ["Handing Xu", "Zhenguo Nie", "Tairan Peng", "Huimin Pan", "Xin-Jun Liu"], "title": "DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement", "comment": "18 pages, 8 figures, and 7 tables", "summary": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.", "AI": {"tldr": "A degradation-aware framework for real-time endoscopic video enhancement that uses contrastive learning to extract degradation representations and propagates them across frames for efficient, high-quality enhancement.", "motivation": "Endoscopic surgery depends on video quality, but videos suffer from illumination issues, scattering, occlusions, and motion blur that obscure anatomical details. Existing deep learning methods are too computationally heavy for real-time surgical use.", "method": "Proposes a degradation-aware framework that: 1) extracts degradation representations using contrastive learning, 2) fuses these representations with image features to guide a single-frame enhancement model, and 3) trains with cycle-consistency constraints between degraded and restored images for robustness.", "result": "The framework achieves superior balance between performance and efficiency compared to state-of-the-art methods, demonstrating effectiveness for real-time endoscopic video enhancement.", "conclusion": "Implicitly learning and propagating degradation representations offers a practical pathway for clinical applications of real-time endoscopic video enhancement."}}
{"id": "2512.07269", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07269", "abs": "https://arxiv.org/abs/2512.07269", "authors": ["Mike Diessner", "Yannick Tarant"], "title": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data", "comment": null, "summary": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.", "AI": {"tldr": "A graph generation pipeline using photogrammetry and deep learning to create virtual models of critical infrastructure from RGB and depth images, offering a cost-effective alternative to expensive laser scanning.", "motivation": "Current virtual models of critical infrastructure (water/energy plants) require expensive 3D laser scanning and specialist knowledge. There's a need for more accessible, cost-effective methods for creating digital twins.", "method": "Photogrammetry-based pipeline using stereo camera images (RGB + depth). Deep learning for object detection and instance segmentation, combined with user-defined heuristics/rules to infer object relations and generate graphs.", "result": "The method produces graphs close to ground truth for two hydraulic systems. It's flexible for specific applications and transparent enough for high-stakes decision-making in critical infrastructure.", "conclusion": "The photogrammetry-based graph generation pipeline provides a viable, cost-effective alternative to laser scanning for creating virtual models of critical infrastructure, with sufficient accuracy and transparency for practical applications."}}
{"id": "2512.07273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07273", "abs": "https://arxiv.org/abs/2512.07273", "authors": ["Zhi Rao", "Yucheng Zhou", "Benjia Zhou", "Yiqing Huang", "Sergio Escalera", "Jun Wan"], "title": "RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation", "comment": null, "summary": "Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.", "AI": {"tldr": "Proposes RVLF, a three-stage vision-language framework for gloss-free sign language translation that addresses inadequate sign representation and sentence-level semantic misalignment through semantic representation learning and GRPO-based reinforcement learning optimization.", "motivation": "Current gloss-free sign language translation suffers from two main problems: 1) inadequate sign representation that fails to capture nuanced visual cues, and 2) sentence-level semantic misalignment in LLM-based methods that limits translation quality.", "method": "Three-stage RVLF framework: 1) Builds a large vision-language model for sign language with semantic representation learning that fuses skeleton-based motion cues with DINOv2 visual features, followed by instruction tuning (SLT-SFT baseline). 2) Uses GRPO-based reinforcement learning optimization with reward function combining BLEU (translation fidelity) and ROUGE (sentence completeness) to fine-tune the model (SLT-GRPO).", "result": "Substantial improvements in BLEU-4 scores: +5.1 on CSL-Daily, +1.11 on PHOENIX-2014T, +1.4 on How2Sign, and +1.61 on OpenASL datasets. First work to incorporate GRPO into SLT, with experiments validating effectiveness in enhancing translation quality and semantic consistency.", "conclusion": "The proposed RVLF framework effectively addresses key challenges in gloss-free SLT through semantic representation learning and GRPO-based optimization, achieving significant performance improvements without requiring pre-training on external large-scale sign language datasets."}}
{"id": "2512.07275", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07275", "abs": "https://arxiv.org/abs/2512.07275", "authors": ["Siyu Wang", "Hua Wang", "Huiyu Li", "Fan Zhang"], "title": "Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation", "comment": "The paper has been accepted by BIBM 2025", "summary": "In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.", "AI": {"tldr": "Proposes a novel encoder-decoder network with multi-scale residual structures, MRCF module for cross-scale feature fusion, CMAM for dynamic attention, and EAB to address skip connection information loss, achieving state-of-the-art skin lesion segmentation performance.", "motivation": "Existing deep learning methods fail to effectively handle irregular lesion shapes and low contrast in skin lesion segmentation, which is crucial for early detection and accurate diagnosis of skin diseases.", "method": "Innovative encoder-decoder network with: 1) Multi-scale residual structures for rich feature extraction from different receptive fields; 2) MRCF module for cross-scale feature capture; 3) CMAM for dynamic attention across multiple contexts; 4) EAB to compensate for information loss in skip connections.", "result": "Extensive experiments on multiple skin lesion segmentation datasets show the proposed model significantly outperforms existing transformer and CNN-based models, demonstrating exceptional segmentation accuracy and robustness.", "conclusion": "The proposed architecture effectively addresses challenges in skin lesion segmentation through multi-scale feature extraction, cross-scale fusion, dynamic attention mechanisms, and improved skip connections, achieving superior performance for medical image analysis."}}
{"id": "2512.07276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07276", "abs": "https://arxiv.org/abs/2512.07276", "authors": ["Mai Tsujimoto", "Junjue Wang", "Weihao Xuan", "Naoto Yokoya"], "title": "Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery", "comment": "Accepted to WACV 2026. Camera-ready-based version with minor edits for readability (no change in the contents)", "summary": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.", "AI": {"tldr": "Geo3DVQA is a benchmark for evaluating vision-language models on 3D geospatial reasoning using only RGB remote sensing imagery, revealing current VLMs struggle with RGB-to-3D tasks but domain-specific fine-tuning significantly improves performance.", "motivation": "Current 3D geospatial analysis relies on expensive specialized sensors (LiDAR, multispectral) limiting global accessibility, and existing methods struggle with integrating multiple 3D cues, handling diverse queries, and providing interpretable reasoning.", "method": "Created Geo3DVQA benchmark with 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. Evaluated 10 state-of-the-art VLMs on realistic scenarios integrating elevation, sky view factors, and land cover patterns.", "result": "Current VLMs perform poorly on RGB-to-3D reasoning: GPT-4o (28.6%), Gemini-2.5-Flash (33.0%). Domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% accuracy (+24.8 points improvement), showing effectiveness of domain adaptation.", "conclusion": "Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis, revealing limitations of current VLMs while demonstrating the value of domain-specific adaptation for 3D reasoning from RGB imagery."}}
{"id": "2512.07302", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07302", "abs": "https://arxiv.org/abs/2512.07302", "authors": ["Mingning Guo", "Mengwei Wu", "Shaoxian Li", "Haifeng Li", "Chao Tao"], "title": "Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts", "comment": null, "summary": "Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.", "AI": {"tldr": "AerialVP is a novel agent framework that enhances task prompts for UAV image perception by extracting multi-dimensional auxiliary information, overcoming limitations of traditional VLM-based methods that struggle with UAV imagery challenges.", "motivation": "Traditional VLM-based image perception methods face limitations with UAV imagery due to challenges like target confusion, scale variations, and complex backgrounds. These issues arise from poor semantic alignment between simplistic task prompts and complex visual content in UAV images.", "method": "AerialVP is an agent framework that proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts through three stages: (1) analyzing task prompts to identify task type and enhancement needs, (2) selecting appropriate tools from a tool repository, and (3) generating enhanced task prompts based on analysis and selected tools.", "result": "Experimental results show that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs across the comprehensive AerialSense benchmark.", "conclusion": "AerialVP addresses the limitations of traditional VLM-based approaches for UAV image perception by enhancing task prompts with auxiliary information, providing a more effective framework for handling the unique challenges of UAV imagery."}}
{"id": "2512.07305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07305", "abs": "https://arxiv.org/abs/2512.07305", "authors": ["Tobias Abraham Haider"], "title": "Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset", "comment": null, "summary": "This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.", "AI": {"tldr": "Replication study confirms pretrained CNN (Google Inception-ResNet-v2) provides practical baseline for wildlife species identification from camera trap images, achieving 62% accuracy vs original 71%, but highlights need for species-specific adaptation.", "motivation": "To assess reproducibility and generalizability of Carl et al.'s approach using pretrained CNN for automated detection of European wild mammal species in camera trap images.", "method": "Reimplemented experiment from scratch using openly available resources and different dataset (900 images spanning 90 species), with minimal preprocessing, using Google Inception-ResNet-v2 model.", "result": "Obtained 62% overall classification accuracy (close to original 71%), with macro F1 score of 0.28 showing substantial per-class performance variation, confirming limitations when labels don't align with ImageNet classes.", "conclusion": "Pretrained CNNs provide practical baseline for wildlife species identification but require species-specific adaptation or transfer learning for consistent high-quality predictions."}}
{"id": "2512.07328", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07328", "abs": "https://arxiv.org/abs/2512.07328", "authors": ["Ziyang Mai", "Yu-Wing Tai"], "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation", "comment": null, "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.", "AI": {"tldr": "ContextAnyone is a context-aware diffusion framework for character-consistent video generation from text and a single reference image, addressing identity preservation beyond just facial features.", "motivation": "Existing text-to-video personalization methods focus mainly on facial identity but fail to preserve broader contextual cues like hairstyle, outfit, and body shape, which are critical for visual coherence across scenes.", "method": "A context-aware diffusion framework that jointly reconstructs reference images and generates new video frames using a DiT-based backbone with Emphasize-Attention module, dual-guidance loss, and Gap-RoPE positional embedding to separate reference and video tokens.", "result": "ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes.", "conclusion": "The proposed framework effectively addresses character identity consistency in video generation by preserving comprehensive contextual cues beyond facial features, enabling high-quality character-consistent video generation from minimal reference."}}
{"id": "2512.07331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07331", "abs": "https://arxiv.org/abs/2512.07331", "authors": ["Kanishk Awadhiya"], "title": "The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers", "comment": null, "summary": "Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a \"U-shaped\" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this \"Inductive Bottleneck\" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively \"learning\" a bottleneck to isolate semantic features.", "AI": {"tldr": "ViTs spontaneously develop a U-shaped entropy profile (compressing info in middle layers) that adapts to dataset complexity - deeper bottlenecks for more abstract semantic tasks.", "motivation": "Vision Transformers lack hierarchical biases of CNNs but still show U-shaped entropy patterns. The paper investigates whether this \"Inductive Bottleneck\" is an architectural artifact or data-dependent adaptation.", "method": "Analyzed layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets with varying compositional complexity (UC Merced, Tiny ImageNet, CIFAR-100). Examined correlation between bottleneck depth and semantic abstraction requirements.", "result": "Bottleneck depth strongly correlates with semantic abstraction needs. Texture-heavy datasets preserve high-rank representations throughout, while object-centric datasets drive networks to dampen high-frequency information in middle layers, creating bottlenecks to isolate semantic features.", "conclusion": "The \"Inductive Bottleneck\" in ViTs is not an architectural artifact but a data-dependent adaptation where networks learn to compress information in middle layers based on the semantic abstraction requirements of the task."}}
{"id": "2512.07338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07338", "abs": "https://arxiv.org/abs/2512.07338", "authors": ["Lu\u00eds Marnoto", "Alexandre Bernardino", "Bruno Martins"], "title": "Generalized Referring Expression Segmentation on Aerial Photos", "comment": "Submitted to IEEE J-STARS", "summary": "Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .", "AI": {"tldr": "Aerial-D is a new large-scale referring expression segmentation dataset for aerial imagery with 37K images and 1.5M expressions covering 260K annotated targets across 21 classes, created via an automated pipeline with LLM-enhanced expressions and historical imaging filters.", "motivation": "Referring expression segmentation in aerial imagery faces unique challenges due to varying spatial resolution, inconsistent color usage, small targets (few pixels), high object density, and partial occlusions. Existing datasets don't adequately address these aerial-specific difficulties.", "method": "Created Aerial-D dataset through a fully automatic pipeline combining systematic rule-based expression generation with LLM enhancement for linguistic variety and visual detail focus. Applied filters to simulate historic imaging conditions. Used RSRefSeg architecture for training models on Aerial-D combined with prior aerial datasets.", "result": "Models trained on Aerial-D combined with prior datasets achieve competitive performance on contemporary benchmarks while maintaining strong accuracy under monochrome, sepia, and grainy degradations typical of archival aerial photography.", "conclusion": "Aerial-D enables unified instance and semantic segmentation from text for both modern and historical aerial images, addressing the unique challenges of aerial imagery through a large-scale, diverse dataset with enhanced linguistic expressions and historical condition simulation."}}
{"id": "2512.07345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07345", "abs": "https://arxiv.org/abs/2512.07345", "authors": ["Shilong Jin", "Haoran Duan", "Litao Hua", "Wentao Huang", "Yuan Zhou"], "title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting", "comment": "15 pages, 8 figures, 5 tables, 2 algorithms, Accepted by AAAI 2026", "summary": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.", "AI": {"tldr": "TD-Attn is a novel framework that addresses multi-view inconsistency in 3D tasks by mitigating prior view bias in Text-to-Image diffusion models through 3D-aware attention guidance and hierarchical attention modulation.", "motivation": "Text-to-Image diffusion models have limitations due to prior view bias, which causes conflicting appearances between different views of an object. This bias makes subject-words activate prior view features regardless of target view conditions, leading to multi-view inconsistency in 3D tasks like generation and editing.", "method": "TD-Attn uses two key components: 1) 3D-Aware Attention Guidance Module (3D-AAG) constructs view-consistent 3D attention Gaussians for subject-words to enforce spatial consistency; 2) Hierarchical Attention Modulation Module (HAM) uses a Semantic Guidance Tree to direct a Semantic Response Profiler in localizing and modulating cross-attention layers responsive to view conditions.", "result": "Extensive experiments show TD-Attn significantly enhances multi-view consistency across 3D tasks and has potential to serve as a universal plugin for 3D generation and editing tasks.", "conclusion": "TD-Attn effectively addresses the prior view bias problem in T2I models, enabling more consistent and controllable 3D tasks without relying on extensive 3D training data, making it a versatile solution for multi-view consistency in 3D applications."}}
{"id": "2512.07348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07348", "abs": "https://arxiv.org/abs/2512.07348", "authors": ["Xinyu Wei", "Kangrui Cen", "Hongyang Wei", "Zhen Guo", "Bairui Li", "Zeqing Wang", "Jinrui Zhang", "Lei Zhang"], "title": "MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition", "comment": "Project Page: https://MICo-150K.github.io/", "summary": "In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.", "AI": {"tldr": "MICo-150K: A comprehensive dataset for Multi-Image Composition with 150K identity-consistent composite images, covering 7 tasks, plus a De&Re subset and MICo-Bench evaluation framework.", "motivation": "Multi-Image Composition (MICo) - synthesizing coherent images from multiple references - is challenging due to lack of high-quality training data. The paper aims to bridge this gap by creating systematic resources for MICo research.", "method": "1) Categorize MICo into 7 representative tasks; 2) Curate source images and construct diverse prompts; 3) Use proprietary models to synthesize composite images; 4) Human-in-the-loop filtering/refinement; 5) Create De&Re subset with real-world decomposition/recomposition; 6) Build MICo-Bench with 100 cases per task + 300 De&Re cases; 7) Introduce Weighted-Ref-VIEScore metric; 8) Fine-tune models on MICo-150K.", "result": "Created MICo-150K dataset with identity consistency, De&Re subset, and MICo-Bench. Fine-tuned models show MICo-150K effectively equips models without MICo capability and enhances existing skills. Qwen-MICo baseline matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs.", "conclusion": "The paper provides comprehensive resources (dataset, benchmark, baseline) for Multi-Image Composition research, addressing the data scarcity problem and enabling systematic evaluation and model development in this challenging domain."}}
{"id": "2512.07351", "categories": ["cs.CV", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.07351", "abs": "https://arxiv.org/abs/2512.07351", "authors": ["Sayeem Been Zaman", "Wasimul Karim", "Arefin Ittesafun Abian", "Reem E. Mohamed", "Md Rafiqul Islam", "Asif Karim", "Sami Azam"], "title": "DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection", "comment": null, "summary": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.", "AI": {"tldr": "DeepAgent: Multi-agent framework combining visual CNN and audio-visual inconsistency detection with Random Forest fusion for robust deepfake detection across multiple datasets.", "motivation": "Current deepfake detection methods that integrate audio and visual cues in single models are vulnerable to modality mismatches, noise, and manipulation. There's a need for more robust approaches that can handle diverse types of manipulations.", "method": "DeepAgent uses two complementary agents: Agent-1 (AlexNet-based CNN) detects visual manipulation symbols, while Agent-2 detects audio-visual inconsistencies using acoustic features, Whisper transcriptions, and EasyOCR frame reading. A Random Forest meta-classifier fuses their decisions.", "result": "Agent-1 achieved 94.35% accuracy on combined Celeb-DF and FakeAVCeleb datasets. Agent-2 and meta-classifier achieved 93.69% and 81.56% on FakeAVCeleb. Cross-dataset validation on DeepFakeTIMIT showed 97.49% accuracy for meta-classifier.", "conclusion": "Hierarchy-based fusion enhances robustness by mitigating individual modality weaknesses. Multi-agent approach effectively addresses diverse deepfake manipulations across different datasets."}}
{"id": "2512.07360", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07360", "abs": "https://arxiv.org/abs/2512.07360", "authors": ["Qiming Huang", "Hao Ai", "Jianbo Jiao"], "title": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "Accepted to WACV2026", "summary": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.", "AI": {"tldr": "Proposes structure-aware feature rectification using region adjacency graphs to refine CLIP features for better open-vocabulary semantic segmentation by enhancing local discrimination and reducing noisy predictions.", "motivation": "CLIP's contrastive training focuses on global semantic alignment, leading to suboptimal fine-grained region-text association and noisy predictions in local areas for open-vocabulary segmentation tasks.", "method": "Constructs region adjacency graph (RAG) based on low-level features (color, texture) to capture local structural relationships, then uses this to refine CLIP features by enhancing local discrimination.", "result": "Extensive experiments show the method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.", "conclusion": "Incorporating instance-specific structural priors through RAG-based feature rectification addresses CLIP's dispersed bias and improves open-vocabulary segmentation performance by enhancing local discrimination."}}
{"id": "2512.07379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07379", "abs": "https://arxiv.org/abs/2512.07379", "authors": ["Mahila Moghadami", "Mohammad Ali Keyvanrad", "Melika Sabaghian"], "title": "Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency", "comment": "22 pages, 16 figures", "summary": "This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.", "AI": {"tldr": "Enhanced SW-YOLO model for small object detection in aerial images achieves 61.2 mAP on VisDrone2019, significantly outperforming baseline YOLOv5L (35.5) and CZDet (58.36).", "motivation": "Small object detection in large-scale aerial images is crucial for critical applications, but current methods using image cropping and architectural modifications need improvement for better accuracy and robustness.", "method": "Enhanced SW-YOLO approach with refined sliding window cropping dimensions/overlap, architectural modifications including advanced feature extraction modules in neck, CBAM integration in backbone for spatial/channel information preservation, and new head design.", "result": "Proposed model achieves 61.2 mAP on VisDrone2019 dataset, significantly outperforming baseline YOLOv5L (35.5) and CZDet (58.36), demonstrating substantial accuracy improvement.", "conclusion": "The enhanced SW-YOLO framework with architectural modifications and refined cropping strategy provides a robust solution for small object detection in aerial imagery, achieving state-of-the-art performance on benchmark datasets."}}
{"id": "2512.07381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07381", "abs": "https://arxiv.org/abs/2512.07381", "authors": ["Shuohan Tao", "Boyao Zhou", "Hanzhang Tu", "Yuwang Wang", "Yebin Liu"], "title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects", "comment": null, "summary": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.", "AI": {"tldr": "Tessellation GS improves 3D Gaussian Splatting by anchoring 2D Gaussians to mesh faces with hierarchical neural features, enabling better dynamic scene reconstruction from single cameras.", "motivation": "3D Gaussian Splatting struggles with viewpoint extrapolation, overfitting, and poor generalization in sparse-view and dynamic scene reconstruction, especially with single camera setups.", "method": "Anchors 2D Gaussians to mesh faces, uses hierarchical neural features to infer Gaussian attributes, employs adaptive face subdivision guided by detail-aware loss, and leverages reconstruction foundation model priors to initialize Gaussian deformations.", "result": "Outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.", "conclusion": "Tessellation GS enables robust reconstruction of general dynamic objects from single static cameras, overcoming limitations of optimization-based methods for dynamic scene reconstruction."}}
{"id": "2512.07383", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07383", "abs": "https://arxiv.org/abs/2512.07383", "authors": ["Deepika SN Vemuri", "Gautham Bellamkonda", "Aditya Pola", "Vineeth N Balasubramanian"], "title": "LogicCBMs: Logic-Enhanced Concept-Based Learning", "comment": "18 pages, 19 figures, WACV 2026", "summary": "Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.", "AI": {"tldr": "LogicCBM enhances concept bottleneck models by replacing linear concept combinations with differentiable logic operations, improving accuracy, interpretability, and ability to capture inter-concept relations.", "motivation": "Current Concept Bottleneck Models (CBMs) are limited to linear combinations of concepts, which restricts their expressiveness. The authors want to go beyond simple weighted combinations to better capture complex relationships between concepts using logical operations while maintaining interpretability.", "method": "Proposes LogicCBM, which introduces a logic module that connects learned concepts from CBMs through differentiable logic operations. This module enables the model to use various logical operations (AND, OR, NOT, etc.) to combine concepts for final predictions while maintaining end-to-end learnability.", "result": "Empirical studies on well-known benchmarks and synthetic datasets show that LogicCBM models achieve better accuracy, perform effective interventions, and maintain high interpretability compared to traditional CBMs.", "conclusion": "Enhancing concept-based learning models with propositional logic through differentiable logic operations improves model expressivity, captures inter-concept relations better, and maintains interpretability while boosting accuracy."}}
{"id": "2512.07385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07385", "abs": "https://arxiv.org/abs/2512.07385", "authors": ["Chunhui Zhang", "Li Liu", "Zhipeng Zhang", "Yong Wang", "Hao Wen", "Xi Zhou", "Shiming Ge", "Yanfeng Wang"], "title": "How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline", "comment": "https://github.com/983632847/Awesome-Multimodal-Object-Tracking", "summary": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.", "AI": {"tldr": "Proposes UAV-Anti-UAV tracking task where a pursuer UAV tracks an adversarial UAV, introduces a million-scale dataset, and presents MambaSTS baseline method with integrated spatial-temporal-semantic learning.", "motivation": "Current Anti-UAV research focuses on fixed ground cameras (RGB/IR/RGB-IR), but lacks tracking from moving UAV platforms. There's a gap in handling dual-dynamic disturbances from both capturing platform and target motion.", "method": "Proposes MambaSTS baseline method using Mamba and Transformer models: Mamba for global semantic features, Transformer for spatial features, with temporal token propagation for video-level long-term context via state space model's long-sequence modeling capabilities.", "result": "Created UAV-Anti-UAV dataset with 1,810 videos, manual annotations (bounding boxes, language prompts, 15 tracking attributes). Experimental evaluation of 50 modern deep tracking algorithms shows significant room for improvement in this domain.", "conclusion": "UAV-Anti-UAV is a challenging new tracking task with dual-dynamic disturbances. The proposed dataset and MambaSTS baseline provide foundation for future research, with current methods showing substantial improvement potential."}}
{"id": "2512.07391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07391", "abs": "https://arxiv.org/abs/2512.07391", "authors": ["\u0110or\u0111e Nedeljkovi\u0107"], "title": "GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring", "comment": null, "summary": "Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.", "AI": {"tldr": "GlimmerNet is an ultra-lightweight CNN that achieves strong global perception without expensive self-attention, using grouped dilated convolutions and efficient feature fusion to set new accuracy-efficiency trade-offs for UAV vision tasks.", "motivation": "While CNNs are efficient for edge/mobile vision, recent Vision Transformers add global context but introduce computational overhead. The paper aims to achieve strong global perception without expensive components for resource-constrained UAV platforms.", "method": "GlimmerNet uses Grouped Dilated Depthwise Convolutions (GDBlocks) that partition channels into groups with different dilation rates for multi-scale feature extraction at no parameter cost. An Aggregator module fuses features using grouped pointwise convolution to lower parameter overhead.", "result": "With only 31K parameters and 29% fewer FLOPs than recent baselines, GlimmerNet achieves state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset.", "conclusion": "GlimmerNet establishes a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms, demonstrating that strong global perception can be achieved without computationally expensive components."}}
{"id": "2512.07394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07394", "abs": "https://arxiv.org/abs/2512.07394", "authors": ["Zhifan Zhu", "Siddhant Bansal", "Shashank Tripathi", "Dima Damen"], "title": "Reconstructing Objects along Hand Interaction Timelines in Egocentric Video", "comment": "webpage: https://zhifanzhu.github.io/objects-along-hit", "summary": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.", "AI": {"tldr": "ROHIT introduces reconstructing objects along hand interaction timelines using pose constraints and propagation for better 3D reconstruction from egocentric videos without 3D ground truth.", "motivation": "Current object reconstruction methods lack temporal modeling of hand-object interactions, especially during stable grasps where objects undergo pose changes while being held. There's a need to reconstruct objects along their interaction timeline without requiring 3D ground truth annotations.", "method": "Defines Hand Interaction Timeline (HIT) with static\u2192contact\u2192grip\u2192release\u2192static phases. Proposes Constrained Optimisation and Propagation (COP) framework that models pose constraints over HIT and propagates object poses along the timeline. Focuses on stable grasps where hand maintains constant contact.", "result": "Evaluated on HOT3D (1.2K clips) and EPIC-Kitchens (2.4K clips, 390 objects, 9 categories, 141 environments). COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% using 2D projection error metrics without 3D ground truth.", "conclusion": "ROHIT enables effective object reconstruction along hand interaction timelines using temporal pose constraints and propagation, demonstrating significant improvements in reconstruction quality for egocentric videos of daily interactions."}}
{"id": "2512.07410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07410", "abs": "https://arxiv.org/abs/2512.07410", "authors": ["Bin Li", "Ruichi Zhang", "Han Liang", "Jingyan Zhang", "Juze Zhang", "Xin Chen", "Lan Xu", "Jingyi Yu", "Jingya Wang"], "title": "InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs", "comment": "Project page: https://binlee26.github.io/InterAgent-Page", "summary": "Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.", "AI": {"tldr": "InterAgent: First end-to-end framework for text-driven physics-based multi-agent humanoid control using autoregressive diffusion transformer with multi-stream blocks and interaction graph representation.", "motivation": "Existing methods focus on single-agent scenarios and lack physically plausible interplay for multi-agent interactions. Need to bridge the gap for realistic humanoid social behaviors.", "method": "Autoregressive diffusion transformer with multi-stream blocks decouples proprioception, exteroception, and action. Novel interaction graph exteroception representation captures joint-to-joint spatial dependencies. Sparse edge-based attention mechanism dynamically prunes redundant connections and emphasizes critical inter-agent relations.", "result": "Consistently outperforms multiple strong baselines, achieving state-of-the-art performance. Enables coherent, physically plausible, and semantically faithful multi-agent behaviors from text prompts only.", "conclusion": "InterAgent successfully bridges the gap for physics-based multi-agent humanoid control, demonstrating superior performance in generating realistic social behaviors from text descriptions."}}
{"id": "2512.07415", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07415", "abs": "https://arxiv.org/abs/2512.07415", "authors": ["Gabriele Galatolo", "Mirco Nanni"], "title": "Data-driven Exploration of Mobility Interaction Patterns", "comment": null, "summary": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.", "AI": {"tldr": "A data mining approach to discover mobility interaction patterns from real data, applied to cars and pedestrians, to improve human dynamics simulation models.", "motivation": "Existing human dynamics modeling approaches use preconceived behavioral models, but there's a need for data-driven methods that can discover actual interaction patterns from real mobility data to better capture how individuals influence each other's movements.", "method": "A data mining approach that searches mobility events for evidence of mutual interactions between individuals, identifies complex persistent patterns and time-evolving configurations of events, and analyzes these patterns to understand mobility interaction mechanics.", "result": "The methodology was applied to two real case studies (cars and pedestrians) with full experimental evaluation including performance analysis, parameter sensitivity assessment, and interpretation of sample results.", "conclusion": "The data-driven approach can provide new insights into mobility interaction mechanics that can potentially improve existing simulation models for applications like crowd simulation and emergency management."}}
{"id": "2512.07426", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07426", "abs": "https://arxiv.org/abs/2512.07426", "authors": ["Karel Moens", "Matthew B. Blaschko", "Tinne Tuytelaars", "Bart Diricx", "Jonas De Vylder", "Mustafa Yousif"], "title": "When normalization hallucinates: unseen risks in AI-powered whole slide image processing", "comment": "4 pages, accepted for oral presentation at SPIE Medical Imaging, 2026", "summary": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.", "AI": {"tldr": "Deep learning-based WSI normalization methods can introduce realistic-looking hallucinations that mask diagnostic features and threaten downstream analysis, requiring new detection methods and stricter validation.", "motivation": "Current deep learning WSI normalization models tend to produce average-looking outputs that may mask important diagnostic features and can introduce undetectable hallucinations, posing serious risks to clinical analysis that are often overlooked in evaluation.", "method": "Proposed a novel image comparison measure to automatically detect hallucinations in normalized outputs, then systematically evaluated several well-cited normalization methods retrained on real-world clinical data using this measure.", "result": "Found concerning frequency of hallucinations when models are retrained on real-world clinical data, revealing significant inconsistencies and failures not captured by conventional metrics, demonstrating the real and underappreciated risk of hallucinations.", "conclusion": "Highlights the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment to address the serious threat posed by hallucinated content in normalized WSIs."}}
{"id": "2512.07469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07469", "abs": "https://arxiv.org/abs/2512.07469", "authors": ["Xiangpeng Yang", "Ji Xie", "Yiyuan Yang", "Yan Huang", "Min Xu", "Qiang Wu"], "title": "Unified Video Editing with Temporal Reasoner", "comment": "Project Page: https://videocof.github.io/", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "AI": {"tldr": "VideoCoF introduces a Chain-of-Frames approach for precise mask-free video editing by predicting reasoning tokens before generating target videos, achieving state-of-the-art performance with minimal data.", "motivation": "Existing video editing methods face a trade-off: expert models need task-specific masks, while unified models lack spatial cues, resulting in weak instruction-to-region mapping and imprecise localization.", "method": "Proposes VideoCoF with Chain-of-Frames reasoning that enforces \"see, reason, then edit\" procedure. The model first predicts reasoning tokens (edit-region latents) before generating target video tokens, plus RoPE alignment for motion consistency and length extrapolation.", "result": "Achieves state-of-the-art performance on VideoCoF-Bench with only 50k video pairs, demonstrating precise instruction-to-region alignment and fine-grained editing without user-provided masks.", "conclusion": "VideoCoF resolves the trade-off between precision and unification in video editing through explicit reasoning tokens, enabling mask-free precise editing with minimal data requirements."}}
{"id": "2512.07480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07480", "abs": "https://arxiv.org/abs/2512.07480", "authors": ["Naifu Xue", "Zhaoyang Jia", "Jiahao Li", "Bin Li", "Zihan Zheng", "Yuan Zhang", "Yan Lu"], "title": "Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance", "comment": null, "summary": "While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.", "AI": {"tldr": "S2VC is a single-step diffusion-based video codec that achieves state-of-the-art perceptual quality at low bitrates with 52.73% bitrate savings over prior methods, using efficient single-step generation instead of heavy sampling.", "motivation": "Traditional and neural video codecs struggle with perceptual quality at low bitrates. Some NVCs use perceptual/adversarial objectives but still have artifacts, while others use diffusion models with heavy sampling complexity. There's a need for efficient, high-quality video compression.", "method": "S2VC integrates conditional coding framework with efficient single-step diffusion generator. It introduces Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features (replacing text captions), and Temporal Consistency Guidance in the diffusion U-Net to enforce temporal coherence across frames.", "result": "Extensive experiments show S2VC delivers state-of-the-art perceptual quality with average 52.73% bitrate saving over prior perceptual methods, demonstrating efficient high-quality video compression.", "conclusion": "S2VC shows the promise of single-step diffusion for efficient, high-quality video compression, overcoming limitations of both traditional codecs and complex diffusion models while maintaining temporal consistency."}}
{"id": "2512.07498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07498", "abs": "https://arxiv.org/abs/2512.07498", "authors": ["Chih-Chung Hsu", "Shao-Ning Chen", "Chia-Ming Lee", "Yi-Fang Wang", "Yi-Shiuan Chou"], "title": "Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior", "comment": "16 pages (including appendix)", "summary": "Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.", "AI": {"tldr": "LR-GCN: A Laplacian-regularized graph convolutional network for robust DeepFake detection that handles noisy, shuffled, or corrupted face sequences using adaptive sparse graphs and spectral analysis.", "motivation": "Real-world DeepFake detection faces challenges from compression artifacts, occlusions, and adversarial attacks that destabilize face detection, leading to invalid or misdetected faces. Most existing detectors assume clean, temporally consistent facial sequences, which rarely holds in practice.", "method": "Proposes LR-GCN with Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into adaptive sparse graphs based on semantic affinities. Uses dual-level sparsity on graph structure and node features to suppress invalid faces. Introduces Graph Laplacian Spectral Prior as high-pass operator to highlight forgery artifacts, followed by low-pass GCN aggregation for a spectral band-pass mechanism.", "result": "Achieves state-of-the-art performance on FF++, Celeb-DFv2, and DFDC datasets. Shows significantly improved robustness under severe disruptions including missing faces, occlusions, and adversarial perturbations.", "conclusion": "LR-GCN provides a robust solution for real-world DeepFake detection by handling noisy and unordered face sequences through adaptive graph construction and spectral analysis, overcoming limitations of traditional temporal-based methods."}}
{"id": "2512.07500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07500", "abs": "https://arxiv.org/abs/2512.07500", "authors": ["Penghui Liu", "Jiangshan Wang", "Yutong Shen", "Shanhui Mo", "Chenyang Qi", "Yue Ma"], "title": "MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer", "comment": null, "summary": "Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.", "AI": {"tldr": "MultiMotion is a novel framework that enables precise multi-object video motion transfer using Diffusion Transformers by disentangling motion features with mask-aware attention and efficient sampling.", "motivation": "Current Diffusion Transformer architectures struggle with multi-object video motion transfer due to motion entanglement and lack of object-level control, limiting their ability to handle complex scenes with multiple moving entities.", "method": "The framework introduces Mask-aware Attention Motion Flow (AMF) that uses SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline, plus RectPC, a high-order predictor-corrector solver for efficient sampling.", "result": "MultiMotion achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects while maintaining DiT's high quality and scalability, and the authors created the first benchmark dataset for DiT-based multi-object motion transfer.", "conclusion": "The proposed MultiMotion framework successfully overcomes the limitations of existing DiT architectures for multi-object motion transfer, providing a unified solution with explicit object-level control and efficient sampling for complex video generation tasks."}}
{"id": "2512.07503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07503", "abs": "https://arxiv.org/abs/2512.07503", "authors": ["Yao Teng", "Zhihuan Jiang", "Han Shi", "Xian Liu", "Xuefei Ning", "Guohao Dai", "Yu Wang", "Zhenguo Li", "Xihui Liu"], "title": "SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation", "comment": null, "summary": "Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\\times$ to $3\\times$ inference latency reduction and $2\\times$ to $7\\times$ step compression, while preserving visual quality with no observable degradation.", "AI": {"tldr": "SJD++ accelerates autoregressive text-to-image generation by 2-7\u00d7 using probabilistic parallel decoding with multi-token prediction and token reuse, maintaining visual quality.", "motivation": "Large autoregressive models produce high-quality images but are slow due to requiring hundreds to thousands of sequential forward passes for next-token prediction during inference.", "method": "Speculative Jacobi Decoding++ (SJD++) combines Jacobi decoding's iterative multi-token prediction with speculative sampling's probabilistic drafting-and-verification, plus reuses high-confidence draft tokens after verification instead of resampling all.", "result": "SJD++ achieves 2\u00d7 to 3\u00d7 inference latency reduction and 2\u00d7 to 7\u00d7 step compression across several autoregressive text-to-image models, with no observable visual quality degradation.", "conclusion": "SJD++ is an effective training-free acceleration method for autoregressive image generation that significantly reduces inference time while preserving output quality."}}
{"id": "2512.07504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07504", "abs": "https://arxiv.org/abs/2512.07504", "authors": ["Ryota Okumura", "Kaede Shiohara", "Toshihiko Yamasaki"], "title": "ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points", "comment": "Accepted to WACV 2026, 8 pages, supplementary included. Dataset and code: https://github.com/RyotaOkumura/ControlVP", "summary": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .", "AI": {"tldr": "ControlVP is a user-guided framework that corrects vanishing point inconsistencies in text-to-image generated scenes, improving geometric realism while maintaining visual quality.", "motivation": "Current text-to-image models like Stable Diffusion often produce geometric inconsistencies, particularly vanishing point errors where parallel lines don't converge correctly, leading to structurally implausible scenes that undermine spatial realism, especially in architectural images.", "method": "Extends pre-trained diffusion models by incorporating structural guidance from building contours and introduces geometric constraints that explicitly align image edges with perspective cues to correct vanishing point inconsistencies.", "result": "The method enhances global geometric consistency while maintaining visual fidelity comparable to baseline models, making it particularly valuable for applications requiring accurate spatial structure like image-to-3D reconstruction.", "conclusion": "ControlVP provides an effective user-guided solution for correcting vanishing point inconsistencies in generated images, improving structural realism without compromising visual quality, with potential applications in 3D reconstruction and architectural visualization."}}
{"id": "2512.07514", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07514", "abs": "https://arxiv.org/abs/2512.07514", "authors": ["Junkai Lin", "Hang Long", "Huipeng Guo", "Jielei Zhang", "JiaYi Yang", "Tianle Guo", "Yang Yang", "Jianwen Li", "Wenxiao Zhang", "Matthias Nie\u00dfner", "Wei Yang"], "title": "MeshRipple: Structured Autoregressive Generation of Artist-Meshes", "comment": null, "summary": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.", "AI": {"tldr": "MeshRipple generates 3D meshes by expanding outward from a frontier like a ripple, using frontier-aware tokenization, expansive prediction, and sparse-attention global memory to maintain topological coherence and long-range dependencies.", "motivation": "Current autoregressive mesh generators serialize faces into sequences and use sliding-window inference due to memory limits, but this breaks long-range geometric dependencies, resulting in holes and fragmented components in generated meshes.", "method": "MeshRipple introduces three key innovations: 1) frontier-aware BFS tokenization that aligns generation order with surface topology, 2) expansive prediction strategy for coherent connected surface growth, and 3) sparse-attention global memory providing effectively unbounded receptive field to resolve long-range topological dependencies.", "result": "MeshRipple outperforms strong recent baselines, generating meshes with high surface fidelity and topological completeness by maintaining coherent, connected surface growth and resolving long-range dependencies.", "conclusion": "MeshRipple's integrated design addresses the critical limitation of broken long-range dependencies in autoregressive mesh generation, enabling generation of topologically complete meshes with high surface fidelity through frontier-aware expansion and global memory."}}
{"id": "2512.07527", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.07527", "abs": "https://arxiv.org/abs/2512.07527", "authors": ["Fei Yu", "Yu Liu", "Luyang Tang", "Mingchao Sun", "Zengye Ge", "Rui Bu", "Yuchao Jin", "Haisen Zhao", "He Sun", "Yangyan Li", "Mu Xu", "Wenzheng Chen", "Baoquan Chen"], "title": "From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images", "comment": null, "summary": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.\n  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.\n  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\\,\\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.", "AI": {"tldr": "Proposes a method for city-scale 3D reconstruction from sparse satellite imagery by modeling geometry as 2.5D height maps and enhancing textures with generative networks to synthesize realistic ground-level views.", "motivation": "City-scale 3D reconstruction from satellite imagery faces extreme viewpoint extrapolation challenges (nearly 90\u00b0 viewpoint gaps). Current methods like NeRF and 3DGS fail due to severely foreshortened facades and flawed textures in sparse orbital images.", "method": "1) Models city geometry as 2.5D height map using Z-monotonic signed distance field (SDF) for stable optimization from sparse satellite views. 2) Paints mesh appearance via differentiable rendering and trains generative texture restoration network to enhance degraded satellite inputs with high-frequency details.", "result": "Demonstrates scalability and robustness on large-scale urban reconstruction, reconstructing 4km\u00b2 regions from few satellite images. Achieves state-of-the-art performance in synthesizing photorealistic ground views, producing high-fidelity, application-ready assets.", "conclusion": "The method successfully addresses extreme viewpoint extrapolation in city-scale 3D reconstruction from satellite imagery, producing visually compelling models suitable for urban planning and simulation applications."}}
{"id": "2512.07564", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07564", "abs": "https://arxiv.org/abs/2512.07564", "authors": ["Kassoum Sanogo", "Renzo Ardiccioni"], "title": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models", "comment": "24 pages, 3 figures, 2 tables. Training-free self-correction framework for vision-language models. Code and implementation details will be released at: https://github.com/kassoumsanogo1/self-correcting-vlm-re-Attention.git", "summary": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.", "AI": {"tldr": "Training-free self-correction framework reduces hallucinations in vision-language models by iteratively refining responses through uncertainty-guided visual re-attention without gradient updates.", "motivation": "Vision-language models frequently generate hallucinated content (plausible but incorrect claims about image content), which undermines their reliability for trustworthy multimodal systems.", "method": "Proposes a training-free framework that combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. The method enables VLMs to iteratively refine responses through uncertainty-guided visual re-attention, operating entirely with frozen, pretrained VLMs without gradient updates.", "result": "Reduces hallucination rates by 9.8 percentage points compared to baseline on POPE and MMHAL BENCH benchmarks using Qwen2.5-VL-7B. Improves object existence accuracy by 4.7 points on adversarial splits. Qualitative analysis confirms uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails.", "conclusion": "The training-free self-correction framework effectively reduces hallucinations in VLMs through uncertainty-guided visual re-attention, demonstrating improved reliability without requiring model retraining. The approach shows promise for developing more trustworthy multimodal systems and will be extended to diverse architectures in future work."}}
{"id": "2512.07568", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.07568", "abs": "https://arxiv.org/abs/2512.07568", "authors": ["Xuecheng Li", "Weikuan Jia", "Alisher Kurbonaliev", "Qurbonaliev Alisher", "Khudzhamkulov Rustam", "Ismoilov Shuhratjon", "Eshmatov Javhariddin", "Yuanjie Zheng"], "title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation", "comment": null, "summary": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na\u00efve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.", "AI": {"tldr": "DSRSD-Net disentangles modality-specific and modality-shared information through residual decomposition and semantic decorrelation to address modality dominance and spurious correlations in multimodal learning.", "motivation": "Multimodal representations suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. High-variance modalities overshadow weaker but semantically important signals, while naive fusion entangles modality-shared and modality-specific factors uncontrollably.", "method": "Proposes Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net) with: (1) dual-stream representation learning separating intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) residual semantic alignment head mapping shared factors into common space using contrastive and regression objectives; (3) decorrelation and orthogonality loss regularizing covariance structure and enforcing orthogonality between shared and private streams.", "result": "Experimental results on two large-scale educational benchmarks demonstrate DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.", "conclusion": "DSRSD-Net effectively addresses modality dominance and spurious correlations through explicit disentanglement of modality-specific and modality-shared information with residual decomposition and semantic decorrelation constraints, improving both prediction performance and interpretability."}}
{"id": "2512.07580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07580", "abs": "https://arxiv.org/abs/2512.07580", "authors": ["Yahong Wang", "Juncheng Wu", "Zhangkai Ni", "Longzhen Yang", "Yihang Liu", "Chengmei Yang", "Ying Wen", "Xianfeng Tang", "Hui Liu", "Yuyin Zhou", "Lianghua He"], "title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs", "comment": null, "summary": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.", "AI": {"tldr": "Vision LLMs waste computation on redundant visual tokens in deep layers; random pruning works as well as complex methods beyond the \"information horizon\" where tokens lose salience.", "motivation": "VLLMs use hundreds of visual tokens, causing high computational costs. Existing token pruning methods fail in deep layers, performing no better than random pruning, suggesting visual tokens become redundant as network depth increases.", "method": "Propose measuring token information content by output probability change upon removal. Analyze information across layers to identify \"information horizon\" where tokens become redundant. Use random pruning in deep layers and integrate it with existing methods like DivPrune.", "result": "Three key findings: 1) Visual token information becomes uniform and vanishes at intermediate layers (information horizon); 2) Horizon depth varies by task (deeper for OCR than VQA); 3) Horizon correlates with model capacity (stronger models use deeper tokens). Random pruning in deep layers works effectively.", "conclusion": "Simple random pruning in deep layers balances performance and efficiency. Combined with existing methods, achieves SOTA: 96.9% of Qwen-2.5-VL-7B performance with 50% token pruning. Visual tokens become redundant beyond information horizon, making complex pruning unnecessary."}}
{"id": "2512.07584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07584", "abs": "https://arxiv.org/abs/2512.07584", "authors": ["Meituan LongCat Team", "Hanghang Ma", "Haoxian Tan", "Jiale Huang", "Junqiang Wu", "Jun-Yan He", "Lishuai Gao", "Songlin Xiao", "Xiaoming Wei", "Xiaoqi Ma", "Xunliang Cai", "Yayong Guan", "Jie Hu"], "title": "LongCat-Image Technical Report", "comment": null, "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.", "AI": {"tldr": "LongCat-Image is a new open-source bilingual (Chinese-English) image generation model that excels at multilingual text rendering, photorealism, efficiency, and comes with a comprehensive open-source ecosystem.", "motivation": "To address challenges in multilingual text rendering (especially Chinese characters), photorealism, deployment efficiency, and developer accessibility that current leading models face.", "method": "Uses rigorous data curation across pre-training, mid-training, and SFT stages, plus curated reward models during RL phase. Features a compact 6B parameter diffusion model (vs typical 20B+ MoE architectures).", "result": "Achieves SOTA in text rendering and photorealism, sets new industry standard for Chinese character rendering (including complex/rare characters), achieves remarkable efficiency with minimal VRAM usage, and excels in image editing with superior consistency.", "conclusion": "LongCat-Image establishes a comprehensive open-source ecosystem with multiple model versions and full training toolchain, aiming to empower developers and researchers in visual content creation."}}
{"id": "2512.07590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07590", "abs": "https://arxiv.org/abs/2512.07590", "authors": ["Kaili Qi", "Zhongyi Huang", "Wenli Yang"], "title": "Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation", "comment": null, "summary": "To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.", "AI": {"tldr": "Robust VM_TUNet integrates variational PDEs with deep learning for noisy image segmentation, combining physical priors and edge detection in a hybrid framework with frequency preprocessing and stable local computation modules.", "motivation": "Addressing the challenge of segmenting noisy images with blurred or fragmented boundaries by combining the interpretability and boundary-smoothing advantages of variational PDEs with the strong representational ability of deep neural networks.", "method": "Proposes robust VM_TUNet, a hybrid framework integrating variational methods with deep learning. Incorporates physical priors, edge detector, and mean curvature term into modified Cahn-Hilliard equation. Architecture has two modules: F module for frequency domain preprocessing to alleviate poor local minima, and T module for accurate stable local computations with stability estimate.", "result": "Extensive experiments on three benchmark datasets show the method achieves balanced trade-off between performance and computational efficiency, yielding competitive quantitative results and improved visual quality compared to pure CNN-based models, while achieving performance close to transformer-based methods with reasonable computational expense.", "conclusion": "The robust VM_TUNet framework successfully combines variational PDEs with deep learning for noisy image segmentation, achieving good performance-efficiency balance and demonstrating advantages over pure CNN approaches while maintaining computational practicality."}}
{"id": "2512.07596", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07596", "abs": "https://arxiv.org/abs/2512.07596", "authors": ["Wenzhen Dong", "Jieming Yu", "Yiming Huang", "Hongqiu Wang", "Lei Zhu", "Albert C. S. Chung", "Hongliang Ren", "Long Bai"], "title": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery", "comment": "Technical Report", "summary": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.", "AI": {"tldr": "SAM 3 shows improved zero-shot segmentation and 3D capabilities over previous versions, with strong performance in surgical image/video segmentation but limited language prompt effectiveness in medical domains.", "motivation": "To evaluate the enhanced capabilities of SAM 3 in robot-assisted surgery, particularly its zero-shot segmentation with various prompts (point, bounding box, language) and 3D reconstruction abilities for surgical applications.", "method": "Empirical evaluation benchmarking SAM 3's zero-shot segmentation with point/box prompts, exploring language prompt segmentation, and investigating 3D reconstruction from 2D surgical images. Comprehensive testing on MICCAI EndoVis 2017/2018 benchmarks and zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF datasets.", "result": "SAM 3 shows clear improvements over SAM and SAM 2 in image/video segmentation under spatial prompts. Language prompts show potential but perform suboptimally in surgical domains. Strong monocular depth estimation and realistic 3D instrument reconstruction demonstrated, but limitations remain in complex, highly dynamic surgical scenes.", "conclusion": "SAM 3 advances surgical segmentation with enhanced zero-shot capabilities and promising 3D reconstruction, but requires domain-specific training for language prompts and further improvements for complex dynamic surgical environments."}}
{"id": "2512.07599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07599", "abs": "https://arxiv.org/abs/2512.07599", "authors": ["Hanshi Wang", "Zijian Cai", "Jin Gao", "Yiwei Zhang", "Weiming Hu", "Ke Wang", "Zhipeng Zhang"], "title": "Online Segment Any 3D Thing as Instance Tracking", "comment": "NeurIPS 2025, Code is at https://github.com/AutoLab-SAI-SJTU/AutoSeg3D", "summary": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.", "AI": {"tldr": "AutoSeg3D reformulates online 3D segmentation as an instance tracking problem using object queries for temporal information propagation, achieving state-of-the-art results on multiple datasets.", "motivation": "Current query-based 3D segmentation methods lack temporal understanding, which is crucial for embodied agents operating in dynamic environments. Viewpoint variations in robotics lead to partial object visibility across frames, requiring holistic object understanding beyond instantaneous views.", "method": "Reconceptualizes 3D segmentation as instance tracking using object queries for temporal propagation: long-term instance association maintains feature/identity coherence, short-term instance update enriches observations. Introduces spatial consistency learning to mitigate VFM fragmentation and enhance temporal learning.", "result": "Achieves new SOTA, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets. Temporal information exchange through sparse queries enhances spatial comprehension without computational burden of dense interactions.", "conclusion": "AutoSeg3D successfully integrates temporal understanding into 3D segmentation through instance tracking, enabling embodied agents to develop holistic object understanding despite partial visibility, while maintaining computational efficiency through sparse query-based temporal propagation."}}
{"id": "2512.07606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07606", "abs": "https://arxiv.org/abs/2512.07606", "authors": ["Jingna Qiu", "Frauke Wilm", "Mathias \u00d6ttl", "Jonas Utz", "Maja Schlereth", "Moritz Schillinger", "Marc Aubreville", "Katharina Breininger"], "title": "Decomposition Sampling for Efficient Region Annotations in Active Learning", "comment": null, "summary": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.", "AI": {"tldr": "DECOMP is an active learning method for dense prediction tasks that decomposes images into class-specific components using pseudo-labels and samples regions from each class, improving annotation efficiency and minority-class performance.", "motivation": "Dense prediction tasks in medical imaging require costly region-level annotations, but existing active learning methods for representative region selection have high computational costs, choose irrelevant regions, and rely too heavily on uncertainty sampling.", "method": "DECOMP decomposes images into class-specific components using pseudo-labels, then samples regions from each class. Class-wise predictive confidence guides sampling to ensure difficult classes receive more annotations, enhancing diversity and focusing on minority classes.", "result": "DECOMP consistently outperforms baseline methods across ROI classification, 2-D segmentation, and 3-D segmentation tasks by better sampling minority-class regions and improving performance on challenging classes.", "conclusion": "DECOMP addresses limitations of existing active learning methods for dense prediction by providing an efficient, diverse sampling strategy that focuses on difficult classes, making medical image annotation more efficient and effective."}}
{"id": "2512.07628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07628", "abs": "https://arxiv.org/abs/2512.07628", "authors": ["Zhiqi Li", "Wenhuan Li", "Tengfei Wang", "Zhenwei Wang", "Junta Wu", "Haoyuan Wang", "Yunhan Yang", "Zehuan Huang", "Yang Li", "Peidong Liu", "Chunchao Guo"], "title": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation", "comment": null, "summary": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA", "AI": {"tldr": "MoCA is a compositional 3D generative model that enables efficient, fine-grained 3D asset creation with scalable components using importance-based routing and compression techniques.", "motivation": "Existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components, limiting their practical application for complex compositional generation.", "method": "MoCA introduces two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserves contextual priors of unselected components while reducing computational complexity.", "result": "Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks, demonstrating efficient and scalable performance.", "conclusion": "MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components, addressing the computational bottleneck of previous methods while maintaining generation quality."}}
{"id": "2512.07651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07651", "abs": "https://arxiv.org/abs/2512.07651", "authors": ["Yuanye Liu", "Hanxiao Zhang", "Nannan Shi", "Yuxin Shi", "Arif Mahmood", "Murtaza Taj", "Xiahai Zhuang"], "title": "Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method", "comment": null, "summary": "Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.", "AI": {"tldr": "The paper introduces LiQA dataset for liver fibrosis staging, part of CARE 2024 challenge, with 440 multi-phase MRI scans, and presents a top-performing method combining semi-supervised learning with multi-view consensus and CAM regularization.", "motivation": "Liver fibrosis is a major global health issue requiring accurate staging for clinical management. Current methods need to handle real-world complexities like domain shifts, missing modalities, and spatial misalignment in clinical settings.", "method": "1) Created LiQA dataset with 440 patients' multi-phase, multi-center MRI scans. 2) Top-performing method integrates semi-supervised learning with external data for robust segmentation, and uses multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging.", "result": "The baseline evaluation demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings for liver fibrosis staging.", "conclusion": "The LiQA dataset provides a valuable benchmark for liver fibrosis algorithms, and the proposed methodology shows promising results in handling real-world clinical complexities through multi-source data integration and anatomical constraints."}}
{"id": "2512.07652", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07652", "abs": "https://arxiv.org/abs/2512.07652", "authors": ["Hamad Almazrouei", "Mariam Al Nasseri", "Maha Alzaabi"], "title": "An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research", "comment": null, "summary": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.", "AI": {"tldr": "AI-powered AUV system combines YOLOv12 Nano, ResNet50, PCA, K-Means++, and GPT-4o Mini for automated underwater object detection, clustering, and report generation, achieving 0.512 mAP@0.5 on 55K+ marine images.", "motivation": "Traditional sea exploration faces challenges from extreme conditions, limited visibility, high costs, and vast unexplored regions, requiring automated solutions to reduce human risk and improve efficiency.", "method": "Integrated system using YOLOv12 Nano for real-time object detection, ResNet50 CNN for feature extraction, PCA for dimensionality reduction (preserving 98% variance), K-Means++ for clustering marine objects, and GPT-4o Mini LLM for generating structured reports.", "result": "System achieved mAP@0.5 of 0.512, precision of 0.535, recall of 0.438 on DeepFish and OzFish datasets (55K+ images). PCA reduced features while preserving variance, K-Means++ successfully clustered objects, and LLM generated insightful location-based summaries.", "conclusion": "The integrated AI-AUV system reduces human diving risks, increases mission efficiency, and enhances underwater data analysis speed and depth, enabling more effective scientific research in challenging marine environments."}}
{"id": "2512.07661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07661", "abs": "https://arxiv.org/abs/2512.07661", "authors": ["Shiaho Li", "Naisheng Ye", "Tianyu Li", "Kashyap Chitta", "Tuo An", "Peng Su", "Boyang Wang", "Haiou Liu", "Chen Lv", "Hongyang Li"], "title": "Optimization-Guided Diffusion for Interactive Scene Generation", "comment": null, "summary": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.", "AI": {"tldr": "OMEGA is an optimization-guided framework that improves diffusion-based multi-agent scene generation by enforcing physical and social constraints, enabling realistic safety-critical scenario creation for autonomous vehicle testing.", "motivation": "Safety-critical driving events are rare in real datasets but essential for evaluating autonomous vehicles. Existing data-driven scene generation models often lack controllability or produce unrealistic samples that violate physical/social constraints.", "method": "OMEGA uses constrained optimization to re-anchor each reverse diffusion step, steering generation toward physically plausible and behaviorally coherent trajectories. For adversarial scenarios, it formulates ego-attacker interactions as game-theoretic optimization in distribution space to approximate Nash equilibria.", "result": "OMEGA improves generation realism, consistency, and controllability: increases valid scenes from 32.35% to 72.27% for free exploration, and from 11% to 80% for controllability-focused generation. Generates 5\u00d7 more near-collision frames with time-to-collision under 3 seconds while maintaining scene realism.", "conclusion": "OMEGA provides an effective training-free framework for generating realistic, controllable multi-agent driving scenes with safety-critical events, addressing key limitations in existing scene generation methods for autonomous vehicle evaluation."}}
{"id": "2512.07668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07668", "abs": "https://arxiv.org/abs/2512.07668", "authors": ["Ronan John", "Aditya Kesari", "Vincenzo DiMatteo", "Kristin Dana"], "title": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset", "comment": null, "summary": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .", "AI": {"tldr": "EgoCampus dataset captures pedestrian eye gaze during outdoor campus navigation using Meta's Project Aria glasses, enabling gaze prediction modeling with EgoCampusNet.", "motivation": "To address the gap in predicting human visual attention during real-world outdoor navigation, as existing egocentric datasets focus on indoor tasks or lack eye gaze information.", "method": "Collected EgoCampus dataset using Meta's Project Aria glasses with eye tracking, RGB cameras, inertial sensors, and GPS from 80+ pedestrians across 25 outdoor campus paths. Developed EgoCampusNet for gaze prediction during navigation.", "result": "Created a diverse gaze-annotated video dataset spanning 6 km of outdoor campus paths, providing rich data from human perspective for studying real-world attention and gaze prediction.", "conclusion": "The EgoCampus dataset and EgoCampusNet method offer valuable resources for understanding visual attention during outdoor pedestrian navigation and advancing gaze prediction models for real-world applications."}}
{"id": "2512.07674", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07674", "abs": "https://arxiv.org/abs/2512.07674", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations", "comment": null, "summary": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.", "AI": {"tldr": "DIST-CLIP is a novel MRI harmonization framework that disentangles anatomical content from image contrast using CLIP guidance, enabling flexible style transfer with either target images or DICOM metadata to address real-world clinical data heterogeneity.", "motivation": "Deep learning for medical imaging faces clinical generalization challenges due to data heterogeneity, especially in MRI where scanner differences, acquisition protocols, and sequence parameters cause domain shifts. Existing harmonization methods are insufficient - image-based approaches need target images, while text-guided methods use simplistic labels or limited datasets, failing to capture real-world clinical variability.", "method": "DIST-CLIP disentangles anatomical content from image contrast, extracting contrast representations using pre-trained CLIP encoders. These contrast embeddings are integrated into anatomical content via a novel Adaptive Style Transfer module. The framework flexibly uses either target images or DICOM metadata for guidance.", "result": "DIST-CLIP showed significant improvements over state-of-the-art methods in both style translation fidelity and anatomical preservation when trained and evaluated on diverse real-world clinical datasets.", "conclusion": "DIST-CLIP offers a flexible solution for MRI style transfer and standardization, addressing real-world clinical heterogeneity by enabling guidance from either images or metadata, with code and weights to be publicly available."}}
{"id": "2512.07698", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07698", "abs": "https://arxiv.org/abs/2512.07698", "authors": ["Arslan Artykov", "Corentin Sautier", "Vincent Lepetit"], "title": "sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only", "comment": null, "summary": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/", "AI": {"tldr": "First data-driven method to jointly predict part segmentation and joint parameters from monocular video using synthetic training, enabling real-world generalization.", "motivation": "Understanding articulated objects is crucial for robotics and digital twins, but existing methods rely on multi-view systems, scanning, or static cameras, lacking practical solutions for monocular video.", "method": "Data-driven approach trained solely on synthetic data that jointly predicts part segmentation and joint parameters from monocular video captured with freely moving camera.", "result": "Strong generalization to real-world objects from synthetic training, operates on casually recorded video, suitable for real-time applications in dynamic environments.", "conclusion": "Presents scalable and practical solution for articulated object understanding from monocular video, enabling applications in robotics and digital twin creation."}}
{"id": "2512.07702", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07702", "abs": "https://arxiv.org/abs/2512.07702", "authors": ["Sangha Park", "Eunji Kim", "Yeongtak Oh", "Jooyoung Choi", "Sungroh Yoon"], "title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment", "comment": "WACV 2026", "summary": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.", "AI": {"tldr": "NPC is an automated pipeline that improves text-to-image alignment by identifying and applying negative prompts to suppress unintended content in diffusion models.", "motivation": "Despite progress in text-to-image generation, achieving precise text-image alignment remains challenging, especially for prompts with rich compositional structure or imaginative elements.", "method": "NPC analyzes cross-attention patterns to understand why both targeted and untargeted negative prompts work, then uses a verifier-captioner-proposer framework to generate candidate negative prompts and ranks them with a salient text-space score for selection.", "result": "NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench.", "conclusion": "NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models by guiding what not to generate."}}
{"id": "2512.07703", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07703", "abs": "https://arxiv.org/abs/2512.07703", "authors": ["Leo Fillioux", "Enzo Ferrante", "Paul-Henry Courn\u00e8de", "Maria Vakalopoulou", "Stergios Christodoulidis"], "title": "PVeRA: Probabilistic Vector-Based Random Matrix Adaptation", "comment": null, "summary": "Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.", "AI": {"tldr": "PVeRA is a probabilistic version of the VeRA adapter that modifies low-rank matrices probabilistically to handle input ambiguities and enable different sampling configurations during training/testing, outperforming VeRA and other adapters on VTAB-1k benchmark.", "motivation": "Large foundation models require vast datasets and computational resources for training/finetuning, which are scarce and costly. Parameter-efficient adaptation methods like adapters offer computationally efficient solutions by adding small trainable modules to frozen backbones.", "method": "Proposes PVeRA, a probabilistic version of the VeRA adapter that modifies VeRA's low-rank matrices in a probabilistic manner. This allows handling inherent input ambiguities and enables different sampling configurations during training and testing phases.", "result": "Comprehensive evaluation on VTAB-1k benchmark with seven adapters shows PVeRA outperforms VeRA and other adapter methods.", "conclusion": "PVeRA provides an effective probabilistic adaptation approach that improves upon VeRA by better handling input ambiguities through probabilistic modeling, offering superior performance while maintaining parameter efficiency."}}
{"id": "2512.07712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07712", "abs": "https://arxiv.org/abs/2512.07712", "authors": ["Sayak Dutta", "Harish Katti", "Shashikant Verma", "Shanmuganathan Raman"], "title": "UnCageNet: Tracking and Pose Estimation of Caged Animal", "comment": "9 pages, 2 figures, 2 tables. Accepted to the Indian Conference on Computer Vision, Graphics, and Image Processing (ICVGIP 2025), Mandi, India", "summary": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.", "AI": {"tldr": "A three-stage preprocessing pipeline improves animal tracking/pose estimation by removing cage occlusions through segmentation, inpainting, and evaluation.", "motivation": "Existing animal tracking and pose estimation systems (like STEP and ViTPose) suffer significant performance degradation when processing images/videos with cage structures and systematic occlusions.", "method": "Three-stage pipeline: (1) cage segmentation using Gabor-enhanced ResNet-UNet with 72 directional kernels, (2) cage inpainting using CRFill for content-aware reconstruction, (3) evaluation of pose estimation/tracking on uncaged frames.", "result": "Removing cage occlusions enables pose estimation and tracking performance comparable to environments without occlusions, with significant improvements in keypoint detection accuracy and trajectory consistency.", "conclusion": "The proposed preprocessing pipeline effectively addresses cage occlusion problems in animal tracking systems, restoring performance to levels similar to occlusion-free environments."}}
{"id": "2512.07720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07720", "abs": "https://arxiv.org/abs/2512.07720", "authors": ["Fan Yang", "Heyuan Li", "Peihao Li", "Weihao Yuan", "Lingteng Qiu", "Chaoyue Song", "Cheng Chen", "Yisheng He", "Shifeng Zhang", "Xiaoguang Han", "Steven Hoi", "Guosheng Lin"], "title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation", "comment": null, "summary": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa", "AI": {"tldr": "A novel framework that combines 3D reconstruction models with video diffusion models to generate high-fidelity upper-body 3D avatars from single images, addressing limitations of both approaches.", "motivation": "Current 3D avatar generation methods produce stable structures but suffer from blurry textures and stiff motion, while video models create photorealistic results but have unstable body structures and identity drift. There's a need to combine the strengths of both approaches for high-quality avatar generation.", "method": "The framework uses a 3D reconstruction model to provide robust structural and appearance priors, which then guides a real-time autoregressive video diffusion model for rendering. This combines geometric stability with generative capabilities to synthesize high-frequency details and fluid dynamics.", "result": "The approach significantly reduces artifacts, achieves substantial improvements in visual quality over leading methods, and enables real-time synthesis of photorealistic details and fluid dynamics while preventing structural inconsistencies.", "conclusion": "By uniting 3D reconstruction stability with video generation capabilities, the method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion, providing a robust solution for real-time applications like gaming and VR."}}
{"id": "2512.07729", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07729", "abs": "https://arxiv.org/abs/2512.07729", "authors": ["Aidas Aglinskas", "Stefano Anzellotti"], "title": "Improving action classification with brain-inspired deep networks", "comment": null, "summary": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.", "AI": {"tldr": "Brain-inspired DNNs with separate body and scene processing streams improve action recognition and better match human performance patterns compared to standard DNNs.", "motivation": "To understand whether humans are better at extracting action information from both body and background compared to DNNs, and whether brain-inspired architectures with domain-specific streams can improve DNN performance and make it more human-like.", "method": "1) Tested DNNs and humans on HAA500 dataset with three stimulus versions: full (body+background), body-only, and background-only. 2) Implemented novel brain-inspired architecture with separate streams for body and scene processing. 3) Compared performance patterns across stimulus types between humans, standard DNNs, and brain-inspired DNNs.", "result": "Standard DNNs performed well on full and background-only stimuli but at chance-level on body-only stimuli. Humans performed accurately on all three stimulus types, with better performance on body-only than background-only. Brain-inspired architecture improved overall action recognition and produced accuracy patterns more closely matching human performance.", "conclusion": "Brain-inspired domain-specific architectures with separate body and scene processing streams can improve DNN action recognition performance and make it more human-like, suggesting that incorporating biological principles can enhance AI systems."}}
{"id": "2512.07730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07730", "abs": "https://arxiv.org/abs/2512.07730", "authors": ["Sangha Park", "Seungryong Yoo", "Jisoo Mok", "Sungroh Yoon"], "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination", "comment": "WACV 2026", "summary": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.", "AI": {"tldr": "SAVE is a training-free framework that reduces object hallucination in MLLMs by steering models along sparse autoencoder features that capture visual understanding, achieving state-of-the-art performance on hallucination benchmarks.", "motivation": "Multimodal Large Language Models (MLLMs) suffer from object hallucination due to language priors and visual information loss, which undermines their reliability in grounded visual understanding tasks.", "method": "SAVE uses sparse autoencoder (SAE) latent features to identify visual understanding features via a binary object-presence QA probe, then steers the model along these features to reinforce grounded visual perception and reduce hallucination.", "result": "SAVE outperforms state-of-the-art training-free methods, achieving 10%p improvement on CHAIR_S and consistent gains on POPE and MMHal-Bench across multiple models and layers, demonstrating robustness and generalizability.", "conclusion": "Steering MLLMs along SAE-derived visual understanding features effectively mitigates hallucination by suppressing uncertain object tokens and increasing attention to image tokens, offering a simple yet powerful training-free solution."}}
{"id": "2512.07733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07733", "abs": "https://arxiv.org/abs/2512.07733", "authors": ["Meng Cao", "Xingyu Li", "Xue Liu", "Ian Reid", "Xiaodan Liang"], "title": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "comment": null, "summary": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.", "AI": {"tldr": "SpatialDreamer is a reinforcement learning framework that enables MLLMs to perform complex spatial reasoning through active exploration and mental simulation, using a novel Geometric Policy Optimization method for long-horizon tasks.", "motivation": "Current Multi-modal Large Language Models (MLLMs) have limited performance on complex spatial reasoning tasks requiring mental simulation. They rely on passive observation rather than active mental imagery processes, creating a gap in human-like spatial reasoning capabilities.", "method": "Proposes SpatialDreamer, a reinforcement learning framework with closed-loop active exploration, visual imagination via a world model, and evidence-grounded reasoning. Introduces Geometric Policy Optimization (GeoPO) with tree-structured sampling and step-level reward estimation using geometric consistency constraints to address fine-grained reward supervision in long-horizon reasoning tasks.", "result": "Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, showing significant advancement in active spatial mental simulation for MLLMs.", "conclusion": "SpatialDreamer represents a critical advancement in enabling human-like active spatial mental simulation for MLLMs, bridging the gap between passive observation and active mental imagery processes in spatial reasoning tasks."}}
{"id": "2512.07738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07738", "abs": "https://arxiv.org/abs/2512.07738", "authors": ["Dengjia Zhang", "Charles Weng", "Katherine Guerrerio", "Yi Lu", "Kenton Murray", "Alexander Martin", "Reno Kriz", "Benjamin Van Durme"], "title": "HLTCOE Evaluation Team at TREC 2025: VQA Track", "comment": "7 pages, 1 figure", "summary": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.", "AI": {"tldr": "The paper presents a listwise learning framework for video question answering that reranks candidate answers using a novel Masked Pointer Cross-Entropy Loss with Rank Weights to improve semantic precision and ranking consistency.", "motivation": "To address challenges in video question answering, particularly improving semantic precision and ranking consistency in answer generation, especially for questions requiring temporal reasoning and semantic disambiguation.", "method": "A two-stage approach: 1) Base multimodal model generates multiple candidate answers for video-question pairs, 2) Reranking using a model trained with novel Masked Pointer Cross-Entropy Loss with Rank Weights that integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction.", "result": "Experiments show consistent gains in accuracy and ranking stability, with particular improvements for temporal reasoning and semantic disambiguation questions.", "conclusion": "The proposed listwise learning framework successfully bridges generative modeling with discriminative ranking to produce coherent, fine-grained answer lists with improved semantic precision and ranking consistency in video question answering."}}
{"id": "2512.07745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07745", "abs": "https://arxiv.org/abs/2512.07745", "authors": ["Jialv Zou", "Shaoyu Chen", "Bencheng Liao", "Zhiyu Zheng", "Yuehao Song", "Lefei Zhang", "Qian Zhang", "Wenyu Liu", "Xinggang Wang"], "title": "DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving", "comment": null, "summary": "Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2", "AI": {"tldr": "DiffusionDriveV2 improves autonomous driving diffusion models by using reinforcement learning to enhance trajectory quality while maintaining diversity, solving the mode collapse vs. quality dilemma.", "motivation": "Existing diffusion models for autonomous driving suffer from mode collapse, generating conservative behaviors. While DiffusionDrive uses anchors for diversity, it lacks constraints, creating a trade-off between diversity and consistent high quality.", "method": "Uses reinforcement learning with scale-adaptive multiplicative noise for exploration, intra-anchor GRPO for advantage estimation within anchors, and inter-anchor truncated GRPO for global perspective across different driving intentions.", "result": "Achieves 91.2 PDMS on NAVSIM v1 and 85.5 EPDMS on NAVSIM v2 with ResNet-34 backbone, setting new records. Resolves diversity vs. quality dilemma for truncated diffusion models.", "conclusion": "DiffusionDriveV2 successfully enhances autonomous driving diffusion models by combining RL constraints with multimodal generation, achieving superior performance while maintaining behavioral diversity."}}
{"id": "2512.07747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07747", "abs": "https://arxiv.org/abs/2512.07747", "authors": ["Shihao Zhao", "Yitong Chen", "Zeyinzi Jiang", "Bojia Zi", "Shaozhe Hao", "Yu Liu", "Chaojie Mao", "Kwan-Yee K. Wong"], "title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation", "comment": null, "summary": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.", "AI": {"tldr": "Unison is a low-cost multimodal AI that automatically understands user intentions and performs diverse understanding/generation tasks with minimal training (500k samples, 50 GPU hours).", "motivation": "Existing multimodal approaches either require massive resources (auto-regressive transformers) or suffer from limited task coverage/poor quality (two-stage methods). Both lack automatic intention parsing and require manual parameter configuration.", "method": "Adopts a two-stage scheme connecting pre-trained understanding and generative models with alignment fine-tuning. Preserves pre-trained model capabilities while adding automatic intention parsing to determine task types and extract required meta-information.", "result": "With only 500k training samples and 50 GPU hours, Unison accurately identifies tasks, extracts parameters automatically, and achieves superior performance across text/image/video understanding and diverse generation tasks.", "conclusion": "Unison demonstrates that high-quality multimodal understanding and generation can be achieved with minimal training cost while enabling full automation through intelligent intention parsing, making advanced multimodal AI accessible to ordinary researchers."}}
{"id": "2512.07756", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07756", "abs": "https://arxiv.org/abs/2512.07756", "authors": ["Mayank Anand", "Ujair Alam", "Surya Prakash", "Priya Shukla", "Gora Chand Nandi", "Domenec Puig"], "title": "UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction", "comment": null, "summary": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.", "AI": {"tldr": "UltrasODM is a dual-stream framework that assists sonographers during ultrasound acquisition by providing calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts to reduce reconstruction errors.", "motivation": "Clinical ultrasound acquisition is highly operator-dependent, with rapid probe motion and brightness fluctuations causing reconstruction errors that reduce trust and clinical utility.", "method": "UltrasODM integrates: (1) contrastive ranking module for grouping frames by motion similarity, (2) optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (3) Human-in-the-Loop layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps.", "result": "Evaluated on clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs.", "conclusion": "UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows by emphasizing transparency and clinician feedback through unobtrusive alerts and corrective action suggestions."}}
{"id": "2512.07760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07760", "abs": "https://arxiv.org/abs/2512.07760", "authors": ["Menglin Wang", "Xiaojin Gong", "Jiachen Li", "Genlin Ji"], "title": "Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification", "comment": "Accepted to AAAI 2026", "summary": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.", "AI": {"tldr": "This paper proposes a novel unsupervised visible-infrared person re-identification method that addresses cross-modality learning through bias-mitigated global association and modality-invariant representation learning, achieving state-of-the-art performance.", "motivation": "Existing unsupervised VI-ReID methods using optimal transport for cross-modality association are prone to propagating local cluster errors and overlook global instance-level relations. The significant modality gap between visible and infrared images makes reliable cross-modality association challenging.", "method": "1) Modality-aware Jaccard distance to mitigate distance bias caused by modality discrepancy for reliable cross-modality associations through global clustering. 2) A 'split-and-contrast' strategy to obtain modality-specific global prototypes and align them under global association guidance for modality-invariant representation learning.", "result": "The method achieves state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin.", "conclusion": "The proposed approach effectively addresses cross-modality learning in unsupervised VI-ReID through bias-mitigated global association and modality-invariant representation learning, demonstrating superior performance despite conceptual simplicity."}}
{"id": "2512.07776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07776", "abs": "https://arxiv.org/abs/2512.07776", "authors": ["Maximilian Schall", "Felix Leonard Kn\u00f6fel", "Noah Elias K\u00f6nig", "Jan Jonas Kubeler", "Maximilian von Klinski", "Joan Wilhelm Linnemann", "Xiaoshi Liu", "Iven Jelle Schlegelmilch", "Ole Woyciniuk", "Alexandra Schild", "Dante Wasmuht", "Magdalena Bermejo Espinet", "German Illera Basas", "Gerard de Melo"], "title": "GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring", "comment": "Accepted at WACV 2026", "summary": "Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, \"in-the-wild\" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species", "AI": {"tldr": "GorillaWatch: An end-to-end pipeline with novel datasets and methods for automated gorilla re-identification from camera trap footage using multi-frame self-supervised pretraining and attention-based verification.", "motivation": "Current manual re-identification of critically endangered western lowland gorillas from camera trap footage is labor-intensive and inefficient. The lack of large-scale \"in-the-wild\" video datasets prevents automation of this crucial conservation monitoring task.", "method": "1) Created three novel datasets: Gorilla-SPAC-Wild (largest wild primate video dataset), Gorilla-Berlin-Zoo (cross-domain generalization), Gorilla-SPAC-MoT (multi-object tracking). 2) Developed GorillaWatch end-to-end pipeline integrating detection, tracking, and re-ID. 3) Introduced multi-frame self-supervised pretraining using tracklet consistency. 4) Used differentiable AttnLRP adaptation to verify model focuses on biometric traits. 5) Integrated spatiotemporal constraints into clustering for unsupervised population counting.", "result": "Large-scale image backbones outperform specialized video architectures for feature aggregation. The pipeline enables scalable, non-invasive monitoring with scientific validation that models rely on discriminative biometric features rather than background correlations.", "conclusion": "GorillaWatch provides a comprehensive solution for automated gorilla monitoring, addressing dataset limitations and ensuring scientific validity. Public release of code and datasets facilitates scalable conservation efforts for endangered species."}}
{"id": "2512.07778", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07778", "abs": "https://arxiv.org/abs/2512.07778", "authors": ["Sen Ye", "Jianning Pei", "Mengde Xu", "Shuyang Gu", "Chunyu Wang", "Liwei Wang", "Han Hu"], "title": "Distribution Matching Variational AutoEncoder", "comment": null, "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", "AI": {"tldr": "DMVAE introduces distribution-matching constraints to align encoder latents with arbitrary reference distributions, enabling systematic exploration of optimal latent distributions for image generation.", "motivation": "Existing visual generative models compress images into latent spaces without explicitly shaping their distributions, making it unclear which distributions are optimal for modeling. Current approaches like VAEs and foundation model aligned encoders implicitly constrain latent spaces without explicit distribution shaping.", "method": "DMVAE explicitly aligns the encoder's latent distribution with arbitrary reference distributions via distribution matching constraints, generalizing beyond conventional VAE Gaussian priors. This enables alignment with distributions from self-supervised features, diffusion noise, or other prior distributions.", "result": "SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, achieving gFID=3.2 on ImageNet with only 64 training epochs. Distribution-level alignment proves more effective than fixed priors.", "conclusion": "Choosing suitable latent distribution structures through distribution-level alignment, rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis."}}
{"id": "2512.07802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07802", "abs": "https://arxiv.org/abs/2512.07802", "authors": ["Zhaochong An", "Menglin Jia", "Haonan Qiu", "Zijian Zhou", "Xiaoke Huang", "Zhiheng Liu", "Weiming Ren", "Kumara Kahatapitiya", "Ding Liu", "Sen He", "Chenyang Zhang", "Tao Xiang", "Fanny Yang", "Serge Belongie", "Tian Xie"], "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "comment": "Project Page: https://zhaochongan.github.io/projects/OneStory", "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "AI": {"tldr": "OneStory is a novel multi-shot video generation method that enables consistent long-form narrative storytelling by reformulating the task as next-shot generation with global cross-shot context modeling.", "motivation": "Existing multi-shot video generation methods struggle with modeling long-range cross-shot context due to limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. There's a need for better global context modeling for consistent and scalable narrative generation.", "method": "Reformulates multi-shot video generation as a next-shot generation task using autoregressive shot synthesis. Introduces two key modules: 1) Frame Selection module that constructs semantically-relevant global memory from informative frames of prior shots, and 2) Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. Uses pretrained image-to-video models for strong visual conditioning.", "result": "OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings. The method enables controllable and immersive long-form video storytelling, outperforming existing approaches.", "conclusion": "OneStory successfully addresses the challenge of global cross-shot context modeling in multi-shot video generation, enabling consistent and scalable narrative generation through its novel next-shot paradigm and compact context modeling approach, making it suitable for real-world storytelling applications."}}
{"id": "2512.07806", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07806", "abs": "https://arxiv.org/abs/2512.07806", "authors": ["Gyeongjin Kang", "Seungkwon Yang", "Seungtae Nam", "Younggeun Lee", "Jungwoo Kim", "Eunbyung Park"], "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader", "comment": "Project page: see https://gynjn.github.io/MVP/", "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.", "AI": {"tldr": "MVP is a scalable multi-view transformer architecture that reconstructs large 3D scenes from many images in one pass using dual hierarchies for efficiency and detail.", "motivation": "To enable fast, efficient reconstruction of large 3D scenes from many images (tens to hundreds) in a single forward pass, addressing scalability and computational challenges in multi-view 3D reconstruction.", "method": "Multi-view Pyramid Transformer with two core design principles: 1) local-to-global inter-view hierarchy (local views \u2192 groups \u2192 full scene), and 2) fine-to-coarse intra-view hierarchy (detailed spatial representations \u2192 compact information-dense tokens). Combined with 3D Gaussian Splatting as 3D representation.", "result": "Achieves state-of-the-art generalizable reconstruction quality across diverse datasets while maintaining high efficiency and scalability across various view configurations.", "conclusion": "MVP's dual hierarchy design enables both computational efficiency and representational richness, allowing fast reconstruction of large, complex scenes from many images in a single forward pass."}}
{"id": "2512.07807", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.07807", "abs": "https://arxiv.org/abs/2512.07807", "authors": ["Shai Krakovsky", "Gal Fiebelman", "Sagie Benaim", "Hadar Averbuch-Elor"], "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes", "comment": "Accepted to SIGGRAPH Asia 2025. Project webpage: https://tau-vailab.github.io/Lang3D-XL", "summary": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.", "AI": {"tldr": "A novel approach for embedding language fields in 3D representations using low-dimensional semantic bottleneck features and a multi-resolution hash encoder, addressing efficiency and semantic misalignment issues in large-scale scene understanding.", "motivation": "Embedding language fields in 3D representations enables richer semantic understanding of spatial environments, linking geometry with descriptive meaning for intuitive human-computer interaction, scene querying/editing, and improved tasks like scene retrieval and navigation. However, existing feature distillation approaches struggle with massive Internet data due to semantic feature misalignment and inefficiency in memory/runtime.", "method": "1) Introduces extremely low-dimensional semantic bottleneck features within 3D Gaussian representation, processed through rendering and a multi-resolution feature-based hash encoder for efficiency. 2) Proposes an Attenuated Downsampler module and several regularizations to address semantic misalignment of ground truth 2D features.", "result": "The method is evaluated on the in-the-wild HolyScenes dataset and demonstrates superior performance and efficiency compared to existing approaches.", "conclusion": "The proposed approach effectively addresses the challenges of semantic feature misalignment and inefficiency in learning language fields from massive Internet data, enabling more effective 3D semantic understanding for large-scale scenes."}}
{"id": "2512.07821", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07821", "abs": "https://arxiv.org/abs/2512.07821", "authors": ["Shaoheng Fang", "Hanwen Jiang", "Yunpeng Bai", "Niloy J. Mitra", "Qixing Huang"], "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling", "comment": null, "summary": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.", "AI": {"tldr": "WorldReel is a 4D video generator that produces spatio-temporally consistent videos with explicit 4D scene representations (pointmaps, camera trajectory, dense flow), achieving superior geometric consistency and motion coherence compared to existing methods.", "motivation": "Current video generators achieve photorealism but lack 3D consistency, producing fundamentally inconsistent scenes. There's a need for video generation that maintains coherent geometry and appearance over time, especially with dynamic content and camera movement.", "method": "WorldReel jointly generates RGB frames with explicit 4D scene representations including pointmaps, camera trajectory, and dense flow mapping. It's trained using a blend of synthetic data (providing precise 4D supervision for geometry, motion, and camera) and real videos (contributing visual diversity and realism).", "result": "WorldReel sets new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts compared to competing methods.", "conclusion": "WorldReel advances video generation toward 4D-consistent world modeling, enabling agents to render, interact, and reason about scenes through a single stable spatiotemporal representation, bridging the gap between photorealism and 3D consistency."}}
{"id": "2512.07826", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07826", "abs": "https://arxiv.org/abs/2512.07826", "authors": ["Haoyang He", "Jie Wang", "Jiangning Zhang", "Zhucun Xue", "Xingyuan Bu", "Qiangpeng Yang", "Shilei Wen", "Lei Xie"], "title": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing", "comment": "38 pages", "summary": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.", "AI": {"tldr": "OpenVE-3M is a large-scale, high-quality dataset for instruction-based video editing with 8 edit types, plus OpenVE-Bench benchmark and OpenVE-Edit model achieving SOTA results.", "motivation": "Large-scale, high-quality datasets for instruction-based video editing are scarce compared to image editing datasets, creating a gap in the field.", "method": "Created OpenVE-3M dataset with two categories: spatially-aligned edits (6 types) and non-spatially-aligned edits (2 types) using a meticulously designed data pipeline with quality filtering. Built OpenVE-Bench benchmark with 431 video-edit pairs and 3 human-aligned metrics. Trained OpenVE-Edit, a 5B parameter model on the dataset.", "result": "OpenVE-3M surpasses existing datasets in scale, diversity, instruction length, and quality. OpenVE-Edit model sets new SOTA on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline.", "conclusion": "The work addresses the scarcity of video editing datasets by providing OpenVE-3M dataset, OpenVE-Bench benchmark, and OpenVE-Edit model that demonstrates remarkable efficiency and effectiveness."}}
{"id": "2512.07829", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07829", "abs": "https://arxiv.org/abs/2512.07829", "authors": ["Yuan Gao", "Chen Chen", "Tianrong Chen", "Jiatao Gu"], "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation", "comment": null, "summary": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.", "AI": {"tldr": "FAE (Feature Auto-Encoder) adapts pre-trained visual representations into low-dimensional latents suitable for generation using minimal architecture (single attention layer), achieving state-of-the-art image generation quality.", "motivation": "There's a fundamental mismatch between understanding-oriented visual representations and generation-friendly latent spaces. Pre-trained encoders need high-dimensional features for diverse hypotheses, while generative models need low-dimensional latents that preserve noise. Existing approaches require complex architectures to bridge this gap.", "method": "FAE uses a simple framework with two separate deep decoders: one reconstructs the original feature space, and another uses those reconstructed features for image generation. It can work with various self-supervised encoders (DINO, SigLIP) and plug into diffusion models or normalizing flows.", "result": "Achieves near state-of-the-art performance: FID of 1.29 (with CFG) and 1.48 (without CFG) on ImageNet 256x256 at 800 epochs. Also shows fast learning with FID of 1.70 (CFG) and 2.08 (no CFG) at just 80 epochs.", "conclusion": "FAE provides a simple yet effective solution to adapt pre-trained visual representations for generation tasks, achieving strong performance with minimal architectural complexity across different generative model families."}}
{"id": "2512.07831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07831", "abs": "https://arxiv.org/abs/2512.07831", "authors": ["Jiehui Huang", "Yuechen Zhang", "Xu He", "Yuan Gao", "Zhi Cen", "Bin Xia", "Yan Zhou", "Xin Tao", "Pengfei Wan", "Jiaya Jia"], "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation", "comment": "Project Website https://jackailab.github.io/Projects/UnityVideo", "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo", "AI": {"tldr": "UnityVideo is a unified framework for world-aware video generation that jointly learns across multiple modalities and training paradigms to improve cross-modal understanding and video quality.", "motivation": "Current video generation models are limited by single-modality conditioning, which constrains holistic world understanding due to insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation.", "method": "Introduces two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. Also contributes a large-scale unified dataset with 1.3M samples.", "result": "UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. It achieves superior video quality, consistency, and improved alignment with physical world constraints.", "conclusion": "UnityVideo addresses limitations of single-modality video generation by enabling joint learning across multiple modalities, resulting in better world understanding and video synthesis capabilities."}}
{"id": "2512.07833", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07833", "abs": "https://arxiv.org/abs/2512.07833", "authors": ["Thao Nguyen", "Sicheng Mo", "Krishna Kumar Singh", "Yilin Wang", "Jing Shi", "Nicholas Kolkin", "Eli Shechtman", "Yong Jae Lee", "Yuheng Li"], "title": "Relational Visual Similarity", "comment": "Project page, data, and code: https://thaoshibe.github.io/relsim", "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "AI": {"tldr": "This paper introduces a new approach to measure relational similarity between images, going beyond traditional attribute-based similarity metrics to capture the underlying relational logic that humans perceive.", "motivation": "Current visual similarity metrics (LPIPS, CLIP, DINO) focus only on perceptual attribute similarity and fail to capture the rich relational similarities that humans perceive. Humans can see relational similarities (like Earth-peach analogy) that go beyond surface attributes, but existing models cannot.", "method": "1) Formulate relational image similarity as a measurable problem. 2) Curate a 114k image-caption dataset with anonymized captions describing relational logic rather than surface content. 3) Finetune a Vision-Language model on this dataset to measure relational similarity between images.", "result": "Developed the first model that can measure relational similarity between images, connecting images by their underlying relational structure rather than visible appearance. Showed that existing image similarity models fail to capture relational similarity.", "conclusion": "Relational similarity has important real-world applications, but existing visual computing models have a critical gap in failing to capture it. The proposed approach represents a first step toward bridging this gap by modeling relational structure in images."}}
{"id": "2512.07834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07834", "abs": "https://arxiv.org/abs/2512.07834", "authors": ["Yi-Chuan Huang", "Jiewen Chan", "Hao-Jen Chien", "Yu-Lun Liu"], "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "comment": "Project page: https://yichuanh.github.io/Voxify-3D/", "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "AI": {"tldr": "Voxify3D is a differentiable two-stage framework that generates voxel art from 3D meshes using orthographic pixel art supervision, patch-based CLIP alignment, and palette-constrained Gumbel-Softmax quantization.", "motivation": "Automated voxel art generation from 3D meshes is challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve pixel-precise, palette-constrained aesthetics.", "method": "A differentiable two-stage framework with three key components: (1) orthographic pixel art supervision for precise voxel-pixel alignment, (2) patch-based CLIP alignment for semantic preservation across discretization levels, and (3) palette-constrained Gumbel-Softmax quantization for differentiable optimization over discrete color spaces.", "result": "Superior performance with 37.12 CLIP-IQA score and 77.90% user preference across diverse characters, supporting controllable abstraction with 2-8 colors and 20x-50x resolutions.", "conclusion": "Voxify3D successfully addresses fundamental challenges in voxel art generation through synergistic integration of orthographic supervision, semantic preservation, and differentiable discrete optimization, enabling high-quality, controllable voxel art generation from 3D meshes."}}
