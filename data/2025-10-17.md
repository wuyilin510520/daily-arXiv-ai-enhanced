<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 100]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: MultiFoodChat is a zero-shot food recognition framework using multi-agent dialogue between vision-language and large language models for collaborative reasoning without training or annotations.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of supervised models that require large labeled datasets and have poor generalization to unseen food categories.

Method: Dialogue-driven multi-agent framework with Object Perception Token (OPT) for visual attributes and Interactive Reasoning Agent (IRA) for contextual interpretation through visual-textual dialogues.

Result: Achieves superior recognition accuracy and interpretability on multiple public food datasets compared to existing unsupervised and few-shot methods.

Conclusion: Shows potential as a new paradigm for intelligent food quality inspection and analysis through flexible, human-like understanding of complex food scenes.

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [2] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: A system for segmenting dark endometrial implants in laparoscopic surgery videos to assist gynecologic physicians in identifying endometriosis.


<details>
  <summary>Details</summary>
Motivation: Endometriosis is difficult to identify due to its varied visual appearance in different locations, making it challenging for non-specialists. The system aims to help gynecologic physicians by providing automated detection assistance.

Method: The system is trained to segment dark endometrial implants and can analyze laparoscopic surgery videos, annotating identified regions with multi-colored overlays and providing a detection summary for improved video browsing.

Result: The system successfully identifies and segments dark endometrial implant regions in laparoscopic videos, providing visual annotations and detection summaries.

Conclusion: The developed system offers valuable assistance to gynecologic physicians by automating the detection of dark endometrial implants in surgery videos, potentially improving diagnostic accuracy and efficiency.

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [3] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: This paper proposes combining traditional vision models (YOLO) with Vision Language Models (VLMs) like LLaVA, ChatGPT, and Gemini to improve remote sensing image analysis, particularly for aircraft detection and scene understanding in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional vision models in remote sensing require extensive labeled data and struggle with contextual understanding in complex environments, while VLMs remain underexplored for remote sensing applications despite their potential for integrating visual and textual data.

Method: Integration of YOLO with VLMs (LLaVA, ChatGPT, Gemini) for enhanced image analysis, evaluated on both labeled and unlabeled remote sensing data, including degraded image scenarios common in remote sensing.

Result: Achieved 48.46% average MAE improvement in aircraft detection and counting accuracy across models, particularly in challenging conditions, and 6.17% improvement in CLIPScore for comprehensive remote sensing image understanding.

Conclusion: The combination of traditional vision models and VLMs enables more advanced and efficient remote sensing image analysis, especially beneficial for few-shot learning scenarios.

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [4] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: An AI system using deep learning was developed to detect cribriform morphology in prostate cancer biopsies, showing superior performance to expert pathologists in validation studies.


<details>
  <summary>Details</summary>
Motivation: Cribriform morphology in prostate cancer indicates poor prognosis but is underreported and has high interobserver variability among pathologists, creating a need for standardized detection methods.

Method: Used EfficientNetV2-S encoder with multiple instance learning for whole-slide classification, trained on 640 biopsies from 430 patients across three cohorts, with internal and external validation on independent datasets.

Result: Model achieved AUC 0.97 (internal) and 0.90 (external), with Cohen's kappa 0.81 and 0.55 respectively. Outperformed all nine expert pathologists in inter-rater analysis (kappa 0.66 vs 0.35-0.62 range).

Conclusion: The AI model demonstrates pathologist-level performance for cribriform morphology detection, potentially enhancing diagnostic reliability, standardizing reporting, and improving treatment decisions.

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [5] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: NAPPure is an adversarial purification framework that handles non-additive perturbations like blur, occlusion, and distortion, unlike existing methods that only work with additive perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial purification methods are ineffective against non-additive perturbations (blur, occlusion, distortion) since they're designed for additive perturbations, but non-additive perturbations are common in real-world scenarios.

Method: Proposed NAPPure framework that establishes the generation process of adversarial images and disentangles clean images from perturbation parameters through likelihood maximization.

Result: Experiments on GTSRB and CIFAR-10 datasets show NAPPure significantly boosts model robustness against non-additive perturbations.

Conclusion: NAPPure effectively extends adversarial purification to handle non-additive perturbations, providing better real-world robustness for image classification models.

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [6] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: Vgent is a graph-based retrieval-reasoning-augmented generation framework that enhances large video language models for long video understanding by preserving temporal dependencies and reducing irrelevant information.


<details>
  <summary>Details</summary>
Motivation: Long video understanding faces challenges due to intensive video tokens exceeding context windows and difficulty retaining long-term sequential information. Existing RAG methods for videos suffer from disrupted temporal dependencies and irrelevant information inclusion.

Method: Proposes Vgent with two innovations: (1) represents videos as structured graphs preserving semantic relationships across clips, (2) introduces intermediate reasoning step with structured verification to reduce retrieval noise and aggregate relevant information across clips.

Result: Achieved 3.0%-5.4% improvement over base models on MLVU benchmark and outperformed state-of-the-art video RAG methods by 8.6% across three long-video understanding benchmarks.

Conclusion: Vgent effectively addresses long video understanding challenges by combining graph-based retrieval with structured reasoning, significantly improving LVLM performance on long video tasks.

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [7] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: TPL is a prototype-based framework that creates compact 1D representations from video embeddings to synchronize videos with nonlinear temporal misalignment, especially effective for generative AI videos.


<details>
  <summary>Details</summary>
Motivation: Traditional video synchronization works well for simultaneous multi-camera captures but fails for videos from different scenes or generative AI videos due to diverse subjects, backgrounds, and nonlinear temporal misalignment.

Method: Temporal Prototype Learning (TPL) constructs shared compact 1D representations from high-dimensional embeddings using pretrained models, learns unified prototype sequences to anchor key action phases, and avoids exhaustive pairwise matching.

Result: TPL improves synchronization accuracy, efficiency, and robustness across diverse datasets, including fine-grained frame retrieval and phase classification tasks, and is the first approach to handle synchronization for multiple generative AI videos.

Conclusion: TPL provides an effective framework for robust video synchronization across diverse scenarios, including the challenging case of generative AI videos, through prototype-based temporal alignment.

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [8] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: A zero-shot pipeline for creating hyperrealistic 3D avatars from unstructured phone images using generative canonicalization and transformer-based models trained on high-fidelity Gaussian splatting avatars.


<details>
  <summary>Details</summary>
Motivation: Existing methods have limitations: single-view approaches suffer from geometric inconsistencies and hallucinations, while synthetic data-trained models fail to capture high-frequency details like skin wrinkles and fine hair, limiting realism.

Method: Two key contributions: (1) generative canonicalization module that processes multiple unstructured views into standardized representation, and (2) transformer-based model trained on large-scale dataset of high-fidelity Gaussian splatting avatars from dome captures of real people.

Result: The "Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars with compelling realism and robust identity preservation from unstructured photos.

Conclusion: The method successfully addresses limitations of existing approaches by combining generative canonicalization with training on real human captures, achieving hyperrealistic and identity-preserving 3D avatars from few unstructured images.

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [9] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: cubic is a Python library that accelerates bioimage analysis by providing GPU-accelerated alternatives to SciPy and scikit-image APIs, enabling faster processing of 2D and 3D biological images while maintaining compatibility with existing workflows.


<details>
  <summary>Details</summary>
Motivation: Modern microscopy generates large 2D/3D datasets, but existing bioimage analysis tools lack scalability, GPU acceleration, proper APIs, and integration with modern scientific computing workflows.

Method: cubic augments SciPy and scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM. It uses device-agnostic API that automatically dispatches operations to GPU when data are on device, otherwise executes on CPU.

Result: Benchmarking shows substantial speedups for individual operations and existing deconvolution/segmentation pipelines while maintaining algorithmic fidelity. Enables GPU acceleration of preprocessing, segmentation, and feature extraction for 2D/3D data.

Conclusion: cubic establishes a foundation for scalable, reproducible bioimage analysis that integrates with Python scientific computing ecosystem, supporting both interactive exploration and automated high-throughput workflows.

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [10] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: A framework for video diffusion models that achieves multi-view character consistency and 3D camera control through a novel data pipeline using 4D Gaussian Splatting and video relighting.


<details>
  <summary>Details</summary>
Motivation: To enable better integration of video generation into virtual production by providing strong multi-view identity preservation, precise camera control, and lighting adaptability.

Method: Train character consistency using volumetric capture performances re-rendered with diverse camera trajectories via 4DGS, lighting variability from video relighting model, and fine-tune state-of-the-art video diffusion models on this data.

Result: Improved video quality, higher personalization accuracy, enhanced camera control and lighting adaptability, with support for multi-subject generation, scene customization, and motion/spatial layout control.

Conclusion: The framework advances the integration of video generation into virtual production by providing comprehensive capabilities for character consistency, camera control, and lighting adaptation.

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [11] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: Proposes joint modeling of Big Five and HEXACO personality traits from multimodal human behavior for improved personality recognition.


<details>
  <summary>Details</summary>
Motivation: Previous studies focused only on Big Five traits, but HEXACO can evaluate important traits like Honesty-Humility related to aggression and social dominance. The relationships between Big Five and HEXACO in machine learning models haven't been clarified.

Method: Joint optimization method for recognizing both Big Five and HEXACO personality traits from multimodal human behavior data.

Result: Experiments using self-introduction video dataset demonstrate effective recognition of both Big Five and HEXACO traits.

Conclusion: The proposed joint modeling approach successfully recognizes both personality frameworks, improving multimodal human behavior awareness.

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [12] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: Proposes a lightweight method for AI-generated image detection using bit-plane-based noise extraction and maximum gradient patch selection, achieving 98.9% accuracy with millisecond-level processing.


<details>
  <summary>Details</summary>
Motivation: Current methods for detecting AI-generated images have high computational costs and fail to capture intrinsic noise features in raw images, making efficient detection challenging.

Method: Uses bit-plane-based image processing to extract noise patterns, applies image normalization strategies, designs maximum gradient patch selection to amplify noise signals, and proposes lightweight classification heads.

Result: Achieves 98.9% average accuracy on GenImage benchmark, with 11.9% improvement over existing methods. Shows excellent cross-generator generalization (98.2% GAN→Diffusion, 99.2% Diffusion→GAN) and runs nearly 100x faster than existing methods.

Conclusion: The proposed method effectively detects AI-generated images by leveraging noise patterns through bit-plane processing, offering high accuracy, strong generalization, and significantly faster processing compared to existing approaches.

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [13] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: PIA is a novel multimodal audio-visual framework that detects deepfakes by analyzing phoneme-temporal relationships, dynamic face motion, and facial identity inconsistencies across multiple modalities.


<details>
  <summary>Details</summary>
Motivation: Traditional deepfake detection methods relying on manual phoneme-viseme alignment thresholds, frame-level consistency checks, or unimodal strategies fail to detect modern deepfakes generated by advanced generative models like GANs, diffusion models, and neural rendering techniques.

Method: PIA integrates phoneme sequences, lip geometry data, and advanced facial identity embeddings to analyze inconsistencies across language, dynamic face motion, and facial identification cues in a multimodal approach.

Result: The integrated method significantly improves detection of subtle deepfake alterations by identifying temporal discrepancies and inconsistencies that traditional detectors overlook.

Conclusion: The proposed Phoneme-Temporal and Identity-Dynamic Analysis (PIA) framework effectively addresses limitations of conventional deepfake detection methods by leveraging multimodal audio-visual analysis.

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [14] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: Proposes Event Interval Modulation (EIM) for event-based optical camera communication, achieving 28 kbps over 10m and 8.4 kbps over 50m - setting a new benchmark for bit rate.


<details>
  <summary>Details</summary>
Motivation: Existing event-based OCC systems use conventional modulation schemes that don't fully exploit the unique characteristics of event-based vision sensors (EVS), which offer high-speed, low-latency communication but are limited by current modulation methods.

Method: Developed Event Interval Modulation (EIM) scheme that modulates information using intervals between events. Tuned EVS parameters for optimal frequency response, determined maximum modulation order experimentally, and conducted transmission experiments.

Result: Achieved successful transmission at 28 kbps over 10 meters and 8.4 kbps over 50 meters in indoor environment, setting new benchmark for bit rate in event-based OCC systems.

Conclusion: EIM effectively exploits EVS characteristics, significantly improving transmission speed and demonstrating practical long-range communication capabilities for event-based OCC systems.

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [15] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

TL;DR: MACE enables efficient localization and high-quality rendering in large-scale scenes using mixed expert networks and auxiliary-loss-free load balancing.


<details>
  <summary>Details</summary>
Motivation: Scene Coordinate Regression methods struggle with large-scale scenes due to single network capacity limitations, requiring more efficient approaches.

Method: Proposes Mixed Expert-based Accelerated Coordinate Encoding (MACE) with gating network for sub-network selection and ALF-LB strategy for load balancing.

Result: Achieves significant cost reduction while maintaining higher precision, with high-quality rendering results after only 10 minutes of training on Cambridge test set.

Conclusion: MACE provides an efficient solution for large-scale scene applications by combining mixed expert networks with optimized load balancing strategies.

Abstract: Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [16] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: IPRO is a reinforcement learning-based video diffusion framework that enhances identity preservation in image-to-video generation by optimizing models using face identity scoring without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing I2V models struggle with maintaining identity consistency when human faces occupy small portions of images and undergo significant expression/movement changes, which is critical since humans are sensitive to identity variations.

Method: Proposes Identity-Preserving Reward-guided Optimization (IPRO) that uses face identity scorer for direct model tuning, backpropagates reward through last sampling steps for richer gradients, employs facial scoring with ground-truth videos as feature pools, and incorporates KL-divergence regularization.

Result: Extensive experiments on Wan 2.2 I2V model and in-house I2V model demonstrate the method's effectiveness in preserving identity.

Conclusion: IPRO provides a novel and effective approach for enhancing identity preservation in human-centric video generation without requiring architectural modifications.

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [17] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Identity-GRPO is a human feedback-driven optimization pipeline that improves multi-human identity preservation in video generation, achieving up to 18.9% improvement in human consistency metrics over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Advanced video generation methods like VACE and Phantom struggle with maintaining consistent identities across multiple human characters in dynamic interactions, which is critical for realistic video generation.

Method: Proposed Identity-GRPO pipeline includes: 1) training a video reward model on large-scale preference dataset with human-annotated and synthetic distortion data focused on human consistency, 2) employing GRPO variant tailored for multi-human consistency optimization.

Result: Identity-GRPO achieves up to 18.9% improvement in human consistency metrics over baseline methods and greatly enhances both VACE and Phantom video generation models.

Conclusion: The method offers actionable insights for aligning reinforcement learning with personalized video generation and effectively addresses multi-human identity preservation challenges.

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [18] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: Proposes MatchAttention for cross-view matching with dynamic relative position matching and BilinearSoftmax for continuous attention sampling, achieving state-of-the-art stereo matching with high efficiency.


<details>
  <summary>Details</summary>
Motivation: High-resolution image matching is challenging due to quadratic complexity and lack of explicit matching constraints in existing cross-attention mechanisms.

Method: MatchAttention dynamically matches relative positions with BilinearSoftmax for continuous attention sampling. Uses hierarchical MatchDecoder with gated cross-MatchAttention and consistency-constrained loss to handle occlusions.

Result: MatchStereo-B ranked 1st on Middlebury benchmark, processes KITTI-resolution in 29ms. MatchStereo-T handles 4K UHD in 0.1s with 3GB GPU memory. SOTA on KITTI 2012/2015, ETH3D, and Spring flow datasets.

Conclusion: Enables real-time, high-resolution, high-accuracy cross-view matching with low computational complexity through efficient attention mechanisms.

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [19] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: Robust demodulation scheme for optical camera communication using event-based vision sensor with OOK, toggle demodulation, and digital PLL, achieving BER < 10^-3 at 200m-60kbps and 400m-30kbps outdoors.


<details>
  <summary>Details</summary>
Motivation: To develop a robust demodulation method for optical camera communication systems that can maintain reliable performance over long distances in outdoor environments.

Method: Combines On-Off Keying (OOK) modulation with toggle demodulation and a digital phase-locked loop using an event-based vision sensor.

Result: Achieved BER < 10^-3 at 200m with 60kbps and 400m with 30kbps in outdoor experiments, representing the first report of such performance.

Conclusion: The proposed demodulation scheme successfully enables reliable long-distance optical camera communication in outdoor settings with significant performance improvements.

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [20] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: GauSSmart is a hybrid method that combines 2D foundational models with 3D Gaussian Splatting to improve scene reconstruction by addressing sparse coverage and fine detail preservation issues.


<details>
  <summary>Details</summary>
Motivation: Gaussian Splatting struggles with capturing fine details and maintaining realism in sparse regions due to limitations of sparse 3D training data, creating a need for enhanced reconstruction methods.

Method: Integrates 2D computer vision techniques (convex filtering, semantic feature supervision from DINO) with 3D Gaussian Splatting, using 2D segmentation priors and feature embeddings to guide Gaussian densification and refinement.

Result: Consistently outperforms existing Gaussian Splatting across three datasets in most evaluated scenes, demonstrating improved coverage in underrepresented areas and better structural detail preservation.

Conclusion: Hybrid 2D-3D approaches show significant potential, proving that thoughtful combination of 2D foundational models with 3D reconstruction can overcome limitations inherent in either approach alone.

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [21] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

TL;DR: A causal-inference-based framework using front-door principle to address domain shift in histopathology by leveraging semantic features and mitigating confounders, achieving up to 7% improvement on CAMELYON17 and private datasets.


<details>
  <summary>Details</summary>
Motivation: Domain shift in histopathology caused by differences in acquisition processes or data sources challenges deep learning model generalization. Existing methods focus on statistical correlations but overlook causal relationships.

Method: Proposes a causal-inference framework using front-door principle with transformation strategies that incorporate mediators and observed tissue slides to leverage semantic features while mitigating confounder impact.

Result: Validated on CAMELYON17 and private histopathology datasets, achieving up to 7% improvement in performance across unseen domains and outperforming existing baselines.

Conclusion: Causal inference shows strong potential as a powerful tool for addressing domain shift in histopathology image analysis, demonstrating consistent performance gains across different datasets.

Abstract: Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [22] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: Training-free tri-layer contrastive decoding with watermarking to reduce hallucinations in Large Vision-Language Models by selecting mature/amateur layers, identifying pivot layers via watermark questions, and applying contrastive decoding.


<details>
  <summary>Details</summary>
Motivation: LVLMs often hallucinate by relying too heavily on single modalities or memorizing training data without proper visual grounding, despite showing promising multimodal performance.

Method: Three-step approach: (1) select mature and amateur layers from decoding layers, (2) identify pivot layer using watermark-related questions to assess visual grounding, (3) apply tri-layer contrastive decoding for final output generation.

Result: Achieves state-of-the-art performance on POPE, MME and AMBER benchmarks in reducing hallucinations and generating more visually grounded responses.

Conclusion: The proposed training-free method effectively reduces hallucinations in LVLMs through tri-layer contrastive decoding with watermarking, improving visual grounding without requiring additional training.

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [23] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

TL;DR: MID-StyleGAN is a new framework that generates synthetic ocular images to address data scarcity in iris presentation attack detection, combining diffusion models and GANs to produce realistic multi-domain data that significantly improves PAD system performance.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of datasets for training and evaluating iris presentation attack detection (PAD) techniques due to difficulties in constructing and imaging presentation attacks (PAs) like artificial eyes, printed eye images, or cosmetic contact lenses.

Method: MID-StyleGAN combines diffusion models and GANs in a multi-domain architecture that enables translation between bonafide ocular images and different PA domains, using an adaptive loss function tailored for ocular data to maintain domain consistency.

Result: MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data significantly enhances PAD system performance - on LivDet2020 dataset, true detect rate at 1% false detect rate improved from 93.41% to 98.72%.

Conclusion: MID-StyleGAN provides a scalable solution to the data scarcity problem in iris and ocular biometrics by generating realistic synthetic data that substantially improves presentation attack detection performance.

Abstract: An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [24] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: VaCo enhances MLLMs by integrating vision-centric information through visual discriminative alignment with multiple vision foundation models, improving visual comprehension capabilities.


<details>
  <summary>Details</summary>
Motivation: Mainstream MLLMs focus only on next-token prediction of text, neglecting critical vision-centric information needed for analytical abilities.

Method: Introduces VaCo with Modular Task Queries (MTQs), Visual Alignment Layers (VALs), and Token Gateway Mask (TGM) to coordinate multiple vision foundation models and activate specific visual signals.

Result: Extensive experiments show VaCo significantly improves performance of different MLLMs on various benchmarks.

Conclusion: VaCo demonstrates superior capabilities in visual comprehension by unifying optimization of both textual and visual outputs in MLLMs.

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [25] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: A self-supervised RGB-D registration method using cycle-consistent keypoints and a novel pose block with GRU and transformation synchronization, achieving state-of-the-art performance on ScanNet and 3DMatch datasets.


<details>
  <summary>Details</summary>
Motivation: To leverage the abundance of unlabeled RGB-D data from consumer depth cameras for geometric scene reasoning, moving beyond traditional geometric and feature-based similarity approaches.

Method: Uses cycle-consistent keypoints for spatial coherence constraints and introduces a pose block combining GRU recurrent unit with transformation synchronization to blend historical and multi-view data.

Result: Outperforms previous self-supervised registration methods on ScanNet and 3DMatch datasets, even surpassing some older supervised methods. Integration into existing methods demonstrates component effectiveness.

Conclusion: The proposed approach successfully utilizes unlabeled RGB-D data through novel keypoint consistency and pose synchronization techniques, achieving superior registration performance in self-supervised settings.

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [26] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: SPR is a Spatial Preference Rewarding approach that enhances MLLMs' fine-grained spatial understanding by rewarding detailed responses with precise object localization over vague ones.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack fine-grained spatial perception abilities like detailed region descriptions and accurate object localization, and often fail to meet user requirements for spatial understanding due to focusing on pre-annotated data without direct supervision of actual responses.

Method: SPR introduces semantic and localization scores to evaluate MLLM-generated descriptions, refines descriptions for better localization accuracy, and pairs best-scored refinements with lowest-scored initial descriptions for direct preference optimization.

Result: Extensive experiments on standard referring and grounding benchmarks show that SPR effectively improves MLLM spatial understanding capabilities with minimal training overhead.

Conclusion: SPR successfully enhances MLLMs' fine-grained spatial capabilities through preference-based optimization, achieving better alignment with visual input while maintaining efficiency.

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [27] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: DOS improves multi-object image generation by modifying CLIP text embeddings to address object neglect and mixing issues.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with prompts containing multiple objects, often resulting in object neglect or mixing, particularly in scenarios with similar shapes, textures, background biases, or many objects.

Method: DOS modifies three types of CLIP text embeddings before passing them to text-to-image models to better separate objects in multi-object scenarios.

Result: DOS consistently improves multi-object generation success rates, reduces object mixing, and significantly outperforms four competing methods in human evaluations (26.24%-43.04% more votes across four benchmarks).

Conclusion: DOS is a practical and effective solution for improving multi-object image generation in text-to-image models.

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [28] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: Proposed DRBD-Mamba, an efficient 3D brain tumor segmentation model using dual-resolution bi-directional Mamba with space-filling curves and gated fusion, achieving improved performance and 15x efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Address computational overhead of Mamba-based models and lack of robustness evaluation across diverse BraTS data partitions for reliable brain tumor segmentation.

Method: Dual-resolution bi-directional Mamba with space-filling curve for 3D-to-1D mapping, gated fusion module for forward/reverse context integration, and quantization block for feature discretization.

Result: Achieved Dice improvements: 0.10% whole tumor, 1.75% tumor core, 0.93% enhancing tumor on test set; 0.86% tumor core and 1.45% enhancing tumor gains on proposed five folds with 15x efficiency improvement.

Conclusion: DRBD-Mamba provides efficient, robust brain tumor segmentation with competitive accuracy and significant computational advantages over existing approaches.

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [29] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: BoardVision is a framework for detecting assembly-level motherboard defects like missing screws and loose wiring, using ensemble methods to balance precision and recall, with robustness evaluation and a deployable GUI tool.


<details>
  <summary>Details</summary>
Motivation: Assembly-level inspection of full motherboards is underexplored compared to bare-board or trace-level defects, despite being critical for reliability in high-volume electronics manufacturing.

Method: Benchmarks YOLOv7 and Faster R-CNN on motherboard dataset, proposes Confidence-Temporal Voting ensemble to balance precision/recall, evaluates robustness under perturbations, and develops GUI inspection tool.

Result: YOLOv7 excels in precision but underperforms in recall, while Faster R-CNN shows the reverse; ensemble method balances both metrics; framework demonstrates stability challenges under realistic perturbations.

Conclusion: Computer vision techniques can effectively transition from benchmark results to practical quality assurance for assembly-level motherboard manufacturing through balanced detection approaches and deployable tools.

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [30] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: DCMIL is a dual-curriculum contrastive multi-instance learning model that efficiently processes whole slide images for cancer prognosis without dense annotations, outperforming standard methods across 12 cancer types.


<details>
  <summary>Details</summary>
Motivation: Computational pathology faces challenges with gigapixel-size inputs and scarce manual annotations, while current methods overlook fine-grained information across multi-magnification WSIs and tumor microenvironment variations.

Method: Proposed an easy-to-hard progressive representation learning model (DCMIL) using dual-curriculum contrastive multi-instance learning to transform gigapixel WSIs directly into outcome predictions without dense annotations.

Result: Extensive experiments on 12 cancer types (5,954 patients, 12.54 million tiles) show DCMIL outperforms standard WSI-based prognostic models, identifies fine-grained prognosis-salient regions, provides robust uncertainty estimation, and captures morphological differences between normal and tumor tissues.

Conclusion: DCMIL demonstrates superior performance in cancer prognosis, offers biological insights through morphological analysis, and has publicly available code for broader adoption in computational pathology.

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [31] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: This paper proposes a neural video compression framework that unifies intra and inter coding, addressing limitations in existing NVC schemes like disocclusion handling and error propagation.


<details>
  <summary>Details</summary>
Motivation: Existing neural video compression schemes have limitations in handling disocclusion, new content, and interframe error propagation. The authors aim to eliminate these issues by borrowing concepts from classic video coding.

Method: The framework uses a single model that adaptively performs intra/inter coding for every frame, with simultaneous two-frame compression to exploit both forward and backward interframe redundancy.

Result: The scheme outperforms DCVC-RT by 10.7% BD-rate reduction on average, provides more stable bitrate and quality per frame, while maintaining real-time encoding/decoding performance.

Conclusion: The proposed unified intra/inter coding framework effectively addresses key limitations in neural video compression and achieves superior compression efficiency compared to state-of-the-art methods.

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [32] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: A universal adversarial attack for video object detection that uses nuclear norm regularization to create structured perturbations concentrated in the background, optimized with an adaptive exponentiated gradient method.


<details>
  <summary>Details</summary>
Motivation: Video-based object detection is crucial for safety-critical applications, but deep learning models are vulnerable to adversarial attacks, especially universal perturbations that can fool detectors across multiple frames.

Method: Proposes a minimally distorted universal adversarial attack using nuclear norm regularization to concentrate perturbations in the background, optimized with an adaptive optimistic exponentiated gradient method for better scalability and convergence.

Result: The proposed attack outperforms both low-rank projected gradient descent and Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.

Conclusion: The method provides an effective and stealthy universal adversarial attack for video object detection, with publicly available code and data.

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [33] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: Unsupervised deep generative models (autoencoders, VAEs, GANs, diffusion models) can detect brain anomalies by learning from healthy data only, achieving good performance for focal lesions and producing interpretable pseudo-healthy reconstructions.


<details>
  <summary>Details</summary>
Motivation: Supervised methods require large annotated datasets and are limited to known pathologies, while unsupervised generative models can work with healthy data only and detect various anomalies without prior annotation.

Method: PRISMA-guided scoping review of 49 studies (2018-2025) covering autoencoders, variational autoencoders, generative adversarial networks, and denoising diffusion models applied to brain MRI and CT for detecting tumors, stroke, multiple sclerosis, and small vessel disease.

Result: Generative models achieved encouraging performance for large focal lesions and showed progress in detecting subtle abnormalities. They can produce interpretable pseudo-healthy reconstructions, which is valuable when annotated data are scarce.

Conclusion: Generative models offer compelling direction for anomaly detection, enabling semi-supervised learning, biomarker discovery, and cross-disease mapping. Future work should focus on anatomy-aware modeling, foundation models, appropriate metrics, and clinical validation.

Abstract: Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [34] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

TL;DR: Proposes MIR-L, a compression strategy for multi-task image restoration models using iterative pruning to find sparse subnetworks that maintain performance with only 10% of parameters.


<details>
  <summary>Details</summary>
Motivation: Multi-task image restoration models are computationally inefficient due to high parameter counts, despite their ability to handle multiple degradation types simultaneously.

Method: Uses iterative pruning strategy that removes low-magnitude weights across multiple rounds while resetting remaining weights to original initialization, discovering "winning tickets" at high sparsity levels.

Result: Experimental evaluation on deraining, dehazing, and denoising tasks shows MIR-L retains only 10% of trainable parameters while maintaining high image restoration performance.

Conclusion: The proposed compression strategy effectively reduces computational overhead of multi-task image restoration models while preserving or even enhancing their performance.

Abstract: Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [35] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: The paper presents a method for detecting grazing activity using Sentinel-2 satellite imagery and CNN-LSTM models, achieving 77% F1 score and enabling more efficient conservation compliance inspections.


<details>
  <summary>Details</summary>
Motivation: Grazing monitoring is important for both agriculture and biodiversity conservation, but scalable monitoring methods are limited. Current inspection methods are resource-intensive and inefficient.

Method: Used Sentinel-2 L2A time series (April-October) with polygon-defined field boundaries. Trained an ensemble of CNN-LSTM models on multi-temporal reflectance features for binary classification (grazed/not grazed).

Result: Achieved 77% average F1 score across five validation splits, with 90% recall on grazed pastures. When prioritizing inspections based on model predictions, achieved 17.2x more confirmed non-grazing sites than random inspection with only 4% site visits.

Conclusion: Coarse-resolution, freely available satellite data can reliably guide inspection resources for conservation compliance monitoring. The approach significantly improves inspection efficiency for land-use compliance.

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [36] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

TL;DR: Vision Mamba is introduced as a backbone for 3D porous media permeability prediction, offering linear scaling with image resolution vs ViT's quadratic scaling, and fewer parameters than CNNs for improved efficiency.


<details>
  <summary>Details</summary>
Motivation: To leverage Vision Mamba's computational advantages over Vision Transformers (linear vs quadratic scaling) and parameter efficiency over CNNs for permeability prediction in 3D porous media.

Method: Used Vision Mamba as backbone network, compared performance with ViT and CNN models, conducted ablation study to analyze component effects on accuracy.

Result: Demonstrated practical advantages of Vision Mamba over ViTs and CNNs in permeability prediction of 3D porous media.

Conclusion: Vision Mamba has potential to be integrated into large vision models as an alternative to ViTs, with source code made available for reproducibility and further research.

Abstract: Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [37] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: SurgScan is an AI-powered framework using YOLOv8 for real-time defect detection in surgical instruments, achieving 99.3% accuracy with 4.2-5.8 ms inference speeds, trained on 102,876 images across 11 instrument types and 5 defect categories.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of surgical instruments is prone to human error and inconsistency, posing serious risks to sterility, mechanical integrity, and patient safety by increasing surgical complication risks.

Method: Uses YOLOv8 architecture trained on a high-resolution dataset of 102,876 images covering 11 instrument types and 5 major defect categories, with contrast-enhanced preprocessing to improve detection performance.

Result: Achieves 99.3% accuracy with real-time inference speeds of 4.2-5.8 ms per image, outperforming state-of-the-art CNN architectures. Statistical analysis confirms contrast-enhanced preprocessing significantly improves defect detection.

Conclusion: SurgScan provides a scalable, cost-effective AI solution for automated quality control that reduces reliance on manual inspection while ensuring compliance with ISO 13485 and FDA standards, enhancing defect detection in medical manufacturing.

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [38] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: The paper proposes a noise projector that refines initial noise in text-to-image generation to improve text-image alignment by addressing training-inference mismatch, without modifying the Stable Diffusion model.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-to-image generation suffer from misalignment between generated images and prompts due to training-inference mismatch - training uses prompt-conditioned noises while inference uses prompt-agnostic Gaussian noise.

Method: A noise projector is trained to map initial noise to prompt-aware counterparts using VLM feedback distilled into a reward model, optimized via quasi-direct preference optimization.

Result: Extensive experiments show that prompt-aware noise projection improves text-image alignment across diverse prompts with minimal inference cost.

Conclusion: The proposed noise projector effectively addresses training-inference mismatch in text-to-image generation, improving alignment without modifying the underlying SD model and with low computational overhead.

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [39] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR-VL is a state-of-the-art, resource-efficient vision-language model for document parsing that integrates a dynamic resolution visual encoder with a language model, supporting 109 languages and excelling at recognizing complex elements while maintaining minimal resource consumption.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and powerful document parsing model that can handle multiple languages and complex document elements while being practical for real-world deployment with fast inference speeds.

Method: Uses PaddleOCR-VL-0.9B, a compact vision-language model combining a NaViT-style dynamic resolution visual encoder with ERNIE-4.5-0.3B language model for accurate element recognition.

Result: Achieves SOTA performance on both page-level document parsing and element-level recognition, significantly outperforms existing solutions, shows strong competitiveness against top-tier VLMs, and delivers fast inference speeds.

Conclusion: PaddleOCR-VL is highly suitable for practical deployment in real-world scenarios due to its superior performance, efficiency, and multilingual capabilities.

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [40] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: DentVFM is the first vision foundation model for dentistry that uses self-supervised learning on 1.6M multi-modal radiographic images, outperforming existing methods in generalization, label efficiency, and cross-modality diagnostics.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current dental AI systems including single-modality focus, task-specific design, and reliance on costly labeled data that hinder generalization across diverse clinical scenarios.

Method: Developed DentVFM family of vision foundation models using Vision Transformer architecture with 2D/3D variants, trained via self-supervised learning on DentVista dataset (1.6M multi-modal radiographic images), and evaluated using DentBench comprehensive benchmark.

Result: DentVFM demonstrates robust generalization across dental tasks (disease diagnosis, treatment analysis, biomarker identification, landmark detection), significantly outperforms supervised/self-supervised/weakly supervised baselines, and provides more reliable cross-modality diagnostics than experienced dentists.

Conclusion: DentVFM sets a new paradigm for dental AI with superior scalability, adaptability, and label efficiency to improve intelligent dental healthcare and address global oral healthcare gaps.

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [41] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: PL-SE-ADA is a domain harmonization framework for brain MR images that disentangles domain-invariant and domain-specific features while providing interpretability through visualization of both components.


<details>
  <summary>Details</summary>
Motivation: Medical images show domain shifts across imaging sites that degrade ML performance. Existing domain harmonization methods lack interpretability, which is essential for medical applications.

Method: Uses two encoders to extract domain-invariant (z_u) and domain-specific (z_d) features, a decoder for reconstruction, and a domain predictor. Employs adversarial training and reconstructs input by summing reconstructions from both z_u and z_d.

Result: Achieves equal or better performance than prior methods in image reconstruction, disease classification, and domain recognition. Enables visualization of domain-independent brain features and domain-specific components.

Conclusion: PL-SE-ADA provides effective domain harmonization with high interpretability, making it suitable for medical applications where understanding model decisions is crucial.

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [42] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: VisualSplit is a framework that decomposes images into classical visual descriptors (edge, color, intensity) and uses them as complementary components for modern learning, enabling interpretable visual understanding and attribute control.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between opaque deep learning representations and interpretable classical visual descriptors, exploring whether modern learning can benefit from classical visual cues.

Method: A reconstruction-driven pre-training scheme that explicitly decomposes images into decoupled classical descriptors, treating each as independent but complementary components of visual knowledge.

Result: The method learns to capture the essence of each visual descriptor while preserving interpretability, and facilitates effective attribute control in advanced visual tasks like image generation and editing.

Conclusion: VisualSplit demonstrates the effectiveness of integrating classical visual descriptors into modern learning approaches for improved interpretability and attribute control in visual understanding tasks.

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [43] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: FMA is a model-agnostic multi-step alignment approach that learns a cross-modal velocity field for more precise feature alignment in challenging datasets where modalities are highly entangled.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods perform one-step adjustment which is insufficient for complex datasets where features from different modalities are highly entangled, requiring more sophisticated alignment approaches.

Method: Proposes Flow Matching Alignment (FMA) with three key components: fixed coupling strategy for category correspondence, noise augmentation for data scarcity, and early-stopping solver for efficiency and accuracy.

Result: FMA consistently yields significant performance gains across various benchmarks and backbones, particularly on challenging datasets, demonstrating superior alignment capabilities compared to one-step PEFT methods.

Conclusion: Multi-step adjustment through FMA provides more precise and robust cross-modal alignment than traditional one-step PEFT methods, especially for complex datasets with highly entangled multimodal features.

Abstract: Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [44] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

TL;DR: A training-free method called Scene De-Contextualization (SDeC) addresses identity shift in text-to-image generation by suppressing the natural correlation between subject and scene context, enabling consistent subject generation across diverse scenes without requiring prior knowledge of all target scenes.


<details>
  <summary>Details</summary>
Motivation: Current T2I generation methods struggle with identity preservation across different scenes due to identity shift, and existing solutions rely on unrealistic assumptions of knowing all target scenes in advance.

Method: SDeC identifies and suppresses latent scene-ID correlation in prompt embeddings by quantifying SVD directional stability to adaptively re-weight eigenvalues, implementing an inversion process of T2I's built-in scene contextualization.

Result: Experiments show SDeC significantly enhances identity preservation while maintaining scene diversity, working effectively with per-scene use without requiring prior access to all target scenes.

Conclusion: SDeC provides a flexible, general solution for real-world T2I applications where prior knowledge of target scenes is unavailable or varies over time, effectively addressing the fundamental issue of scene contextualization.

Abstract: Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


### [45] [Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video](https://arxiv.org/abs/2510.14560)
*Yulin Zhang,Cheng Shi,Yang Wang,Sibei Yang*

Main category: cs.CV

TL;DR: The paper introduces a proactive AI assistant that can understand, anticipate, and respond to events in ego-streaming video input, featuring proactive coherence, just-in-time responsiveness, and synchronized efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop AI that functions in human-like settings by moving beyond passive observation to actively understand, anticipate, and respond to unfolding events in real-time.

Method: Proposes a comprehensive pipeline including: (1) a data engine, (2) multi-stage training strategy, and (3) proactive dynamic compression technique, evaluated using the novel ESTP-Bench framework and ESTP-F1 metric.

Result: The proposed model effectively addresses the three key properties while outperforming multiple baselines across diverse online and offline benchmarks.

Conclusion: The approach successfully enables AI assistants to proactively answer diverse, evolving questions at opportune moments while maintaining synchronized perception and reasoning in ego-streaming video scenarios.

Abstract: Envision an AI capable of functioning in human-like settings, moving beyond
mere observation to actively understand, anticipate, and proactively respond to
unfolding events. Towards this vision, we focus on the innovative task where,
given ego-streaming video input, an assistant proactively answers diverse,
evolving questions at the opportune moment, while maintaining synchronized
perception and reasoning. This task embodies three key properties: (1)
Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized
Efficiency. To evaluate and address these properties, we first introduce
ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a
novel framework designed for their rigorous assessment. Secondly, we propose a
comprehensive technical pipeline to enable models to tackle this challenging
task. This pipeline comprises: (1) a data engine, (2) a multi-stage training
strategy, and (3) a proactive dynamic compression technique. Our proposed model
effectively addresses these critical properties while outperforming multiple
baselines across diverse online and offline benchmarks. Project
Page:https://zhangyl4.github.io/publications/eyes-wide-open/

</details>


### [46] [BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU](https://arxiv.org/abs/2510.14564)
*Junyi Wu,Jiaming Xu,Jinhao Li,Yongkang Zhou,Jiayi Pan,Xingyang Li,Guohao Dai*

Main category: cs.CV

TL;DR: BalanceGS is an algorithm-system co-designed approach that optimizes 3D Gaussian Splatting training by addressing inefficiencies in density allocation, computation workload, and memory access, achieving 1.44× speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Traditional 3DGS suffers from three critical inefficiencies: skewed density allocation during Gaussian densification, imbalanced computation workload during Gaussian projection, and fragmented memory access during color splatting.

Method: Three-level optimization: (1) Algorithm-level: heuristic workload-sensitive Gaussian density control to balance point distributions; (2) System-level: similarity-based Gaussian sampling and merging for adaptive workload distribution; (3) Mapping-level: reordering-based memory access mapping strategy for efficient RGB storage.

Result: Achieves 1.44× training speedup on NVIDIA A100 GPU compared to standard 3DGS, with negligible quality degradation.

Conclusion: BalanceGS effectively addresses the inefficiencies in traditional 3DGS training through algorithm-system co-design, significantly improving training efficiency while maintaining reconstruction quality.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

</details>


### [47] [CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification](https://arxiv.org/abs/2510.14576)
*Dongwook Lee,Sol Han,Jinwhan Kim*

Main category: cs.CV

TL;DR: CALM-Net is a curvature-aware multi-branch neural network for vehicle re-identification using LiDAR point clouds, achieving 1.97% accuracy improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning discriminative and complementary features from 3D point clouds to distinguish between vehicles for re-identification tasks.

Method: Uses multi-branch architecture integrating edge convolution, point attention, and curvature embedding to characterize local surface variation in point clouds.

Result: Achieves mean re-identification accuracy improvement of approximately 1.97% points compared with the strongest baseline on nuScenes dataset.

Conclusion: Curvature information integration and multi-branch feature learning are effective for LiDAR point cloud-based vehicle re-identification.

Abstract: This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based
multi-branch neural network for vehicle re-identification. The proposed model
addresses the challenge of learning discriminative and complementary features
from three-dimensional point clouds to distinguish between vehicles. CALM-Net
employs a multi-branch architecture that integrates edge convolution, point
attention, and a curvature embedding that characterizes local surface variation
in point clouds. By combining these mechanisms, the model learns richer
geometric and contextual features that are well suited for the
re-identification task. Experimental evaluation on the large-scale nuScenes
dataset demonstrates that CALM-Net achieves a mean re-identification accuracy
improvement of approximately 1.97\% points compared with the strongest baseline
in our study. The results confirms the effectiveness of incorporating curvature
information into deep learning architectures and highlight the benefit of
multi-branch feature learning for LiDAR point cloud-based vehicle
re-identification.

</details>


### [48] [Talking Points: Describing and Localizing Pixels](https://arxiv.org/abs/2510.14583)
*Matan Rusanovsky,Shimon Malnick,Shai Avidan*

Main category: cs.CV

TL;DR: A novel framework for pixel-level keypoint grounding using natural language, consisting of a Point Descriptor that generates contextual keypoint descriptions and a Point Localizer that regresses pixel coordinates from these descriptions.


<details>
  <summary>Details</summary>
Motivation: Vision-language models currently lack pixel-precise keypoint comprehension through natural language, being limited to object-level or region-level grounding.

Method: Two-component framework: Point Descriptor generates free-form, coarse-to-fine keypoint descriptions, and Point Localizer regresses pixel coordinates. Trained on LlamaPointInPart dataset (20K+ image-keypoint-description triplets) using GRPO optimization with frozen Point Localizer as reward model.

Result: Superior performance compared to baseline models on the LlamaPointInPart dataset, demonstrating effective pixel-level keypoint grounding through natural language.

Conclusion: The bidirectional framework enables both keypoint-guided image understanding and language-guided precise localization, with potential for future applications in cross-modal understanding.

Abstract: Vision-language models have achieved remarkable success in cross-modal
understanding. Yet, these models remain limited to object-level or region-level
grounding, lacking the capability for pixel-precise keypoint comprehension
through natural language. We introduce a novel framework for pixel level
grounding. The framework consists of two complementary components: a Point
Descriptor that generates rich, contextual descriptions of individual
keypoints, and a Point Localizer that regresses precise pixel coordinates from
these descriptions. Unlike prior work that relies on templated prompts or
keypoint names, our approach produces free-form, coarse-to-fine descriptions
that situate keypoints within their visual context. Since there is no available
dataset to train such a system, we introduce LlamaPointInPart, a carefully
curated dataset of 20K+ image-keypoint-description triplets synthesized from
multiple vision-language models, capturing multi-scale information from
scene-level context to visual features around the keypoint. For cross-category
generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the
frozen Point Localizer as a reward model to produce descriptions that maximize
localization accuracy. To evaluate our results we establish a new evaluation
protocol. Instead of comparing the text description produced by our method to
the ground truth, we use the localizer to determine how close is the predicted
point generated to the ground truth point. Experiments demonstrate superior
performance compared to baseline models on LlamaPointInPart.The bidirectional
nature of our framework should enable future applications in both
keypoint-guided image understanding and language-guided precise localization.
Our code and dataset are publicly available at
https://github.com/matanr/Talking_Points.

</details>


### [49] [STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding](https://arxiv.org/abs/2510.14588)
*Zhifei Chen,Tianshuo Xu,Leyi Wu,Luozhou Wang,Dongyu Yan,Zihan You,Wenting Luo,Guo Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: STANCE is an image-to-video framework that improves object motion coherence using Instance Cues for dense motion control and Dense RoPE to preserve motion guidance in token space, with joint RGB+auxiliary prediction for better temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Current video generation struggles with coherent object motion and interactions due to insufficient motion guidance from sparse human inputs and optimization conflicts between appearance and motion.

Method: Uses Instance Cues to create dense 2.5D motion fields from sparse user hints, and Dense RoPE to maintain motion guidance in token space. Employs joint RGB+auxiliary map prediction to separate structure from appearance.

Result: Improved temporal coherence and motion consistency without requiring per-frame trajectory scripts.

Conclusion: STANCE effectively addresses motion guidance bottlenecks in video generation through simple but effective components that separate motion control from appearance optimization.

Abstract: Video generation has recently made striking visual progress, but maintaining
coherent object motion and interactions remains difficult. We trace two
practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
often collapse to too few effective tokens after encoding, weakening guidance;
and (ii) optimizing for appearance and motion in a single head can favor
texture over temporal consistency. We present STANCE, an image-to-video
framework that addresses both issues with two simple components. First, we
introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
user-editable hints into a dense 2.5D (camera-relative) motion field by
averaging per-instance flow and augmenting with monocular depth over the
instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
remaining easy to use. Second, we preserve the salience of these cues in token
space with Dense RoPE, which tags a small set of motion tokens (anchored on the
first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
\(+\) auxiliary-map prediction (segmentation or depth), our model anchors
structure while RGB handles appearance, stabilizing optimization and improving
temporal coherence without requiring per-frame trajectory scripts.

</details>


### [50] [Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers](https://arxiv.org/abs/2510.14594)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: A hierarchical re-classification system that refines high-level taxonomic labels to species-level identification using SpeciesNet predictions combined with CLIP embeddings and metric learning, achieving 96.5% accuracy on re-classified detections.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art animal classification models like SpeciesNet use conservative rollup strategies, resulting in many animals labeled at high taxonomic levels rather than specific species, limiting the usefulness of species-level identification.

Method: Five-stage pipeline combining SpeciesNet EfficientNetV2-M predictions with CLIP embeddings and metric learning: high-confidence acceptance, bird override, centroid building, triplet-loss metric learning, and adaptive cosine-distance scoring.

Result: Evaluated on LILA BC Desert Lion Conservation dataset (4,018 images, 15,031 detections). Recovered 761 bird detections from "blank" and "animal" labels, re-classified 456 detections with 96.5% accuracy, achieving species-level identification for 64.9% of cases.

Conclusion: The hierarchical re-classification system successfully refines high-level taxonomic labels to species-level identification with high accuracy, significantly improving species-level identification capabilities in animal classification systems.

Abstract: State-of-the-art animal classification models like SpeciesNet provide
predictions across thousands of species but use conservative rollup strategies,
resulting in many animals labeled at high taxonomic levels rather than species.
We present a hierarchical re-classification system for the Animal Detect
platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP
embeddings and metric learning to refine high-level taxonomic labels toward
species-level identification. Our five-stage pipeline (high-confidence
acceptance, bird override, centroid building, triplet-loss metric learning, and
adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC
Desert Lion Conservation dataset (4,018 images, 15,031 detections). After
recovering 761 bird detections from "blank" and "animal" labels, we re-classify
456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving
species-level identification for 64.9 percent

</details>


### [51] [Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering](https://arxiv.org/abs/2510.14596)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: Zero-shot wildlife image organization using self-supervised vision transformers achieves high accuracy for species clustering and similarity ordering, enabling efficient camera trap data analysis.


<details>
  <summary>Details</summary>
Motivation: Camera traps generate millions of wildlife images, but many datasets contain species not covered by existing classifiers, requiring zero-shot approaches for organizing unlabeled imagery.

Method: Compare unsupervised clustering methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor) with dimensionality reduction (PCA, UMAP), and demonstrate continuous 1D similarity ordering via t-SNE projection.

Result: DINOv2 with UMAP and GMM achieves 88.6% accuracy (macro-F1 = 0.874), while 1D sorting reaches 88.2% coherence for mammals/birds and 95.2% for fish across 1,500 images.

Conclusion: Continuous similarity ordering deployed in production enables rapid exploratory analysis and accelerates manual annotation workflows for biodiversity monitoring.

Abstract: Camera traps generate millions of wildlife images, yet many datasets contain
species that are absent from existing classifiers. This work evaluates
zero-shot approaches for organizing unlabeled wildlife imagery using
self-supervised vision transformers, developed and tested within the Animal
Detect platform for camera trap analysis. We compare unsupervised clustering
methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)
combined with dimensionality reduction techniques (PCA, UMAP), and we
demonstrate continuous 1D similarity ordering via t-SNE projection. On a
5-species test set with ground truth labels used only for evaluation, DINOv2
with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D
sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent
for fish across 1,500 images. Based on these findings, we deployed continuous
similarity ordering in production, enabling rapid exploratory analysis and
accelerating manual annotation workflows for biodiversity monitoring.

</details>


### [52] [Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering](https://arxiv.org/abs/2510.14605)
*Yuyang Hong,Jiaqi Gu,Qi Yang,Lubin Fan,Yue Wu,Ying Wang,Kun Ding,Shiming Xiang,Jieping Ye*

Main category: cs.CV

TL;DR: Wiki-PRF is a three-stage method for KB-VQA that improves multimodal knowledge retrieval through visual tool invocation, multimodal retrieval, and relevance filtering, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented generation methods struggle with multimodal query quality and relevance of retrieved results in knowledge-based visual question answering tasks.

Method: Three-stage approach: Processing (visual tools for multimodal info extraction), Retrieval (multimodal knowledge retrieval), and Filtering (relevance filtering). Uses VLM trained with reinforcement learning using answer accuracy and format consistency as rewards.

Result: Achieved significant improvements of 36.0 and 42.8 on E-VQA and InfoSeek datasets respectively, demonstrating state-of-the-art performance in answer quality.

Conclusion: Wiki-PRF effectively addresses KB-VQA challenges through its three-stage framework and reinforcement learning training, significantly improving multimodal knowledge retrieval and answer quality.

Abstract: Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF

</details>


### [53] [Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding](https://arxiv.org/abs/2510.14617)
*Ning Ding,Keisuke Fujii,Toru Tamaki*

Main category: cs.CV

TL;DR: Shot2Tactic-Caption is a dual-branch framework for badminton video captioning that generates both shot-level action descriptions and tactic-level temporal execution narratives, using a novel shot-wise prompt-guided mechanism to handle interrupted and resumed tactics.


<details>
  <summary>Details</summary>
Motivation: Tactical understanding in badminton requires interpreting both individual actions and how tactics dynamically unfold over time, but existing methods lack the ability to capture temporal tactical execution patterns.

Method: Dual-branch design with visual encoder, spatio-temporal Transformer encoder, and Transformer decoder; introduces Tactic Unit Detector and shot-wise prompt-guided mechanism using predicted tactic type/state as cross-attention prompts.

Result: Framework effectively generates both shot and tactic captions; ResNet50-based spatio-temporal encoder performs best; shot-wise prompt structuring produces more coherent and accurate tactic descriptions.

Conclusion: The proposed Shot2Tactic-Caption framework successfully addresses multi-scale tactical understanding in badminton, enabling comprehensive analysis of both individual actions and temporal tactical execution patterns.

Abstract: Tactical understanding in badminton involves interpreting not only individual
actions but also how tactics are dynamically executed over time. In this paper,
we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and
temporal multi-scale video captioning in badminton, capable of generating
shot-level captions that describe individual actions and tactic-level captions
that capture how these actions unfold over time within a tactical execution. We
also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning
dataset containing 5,494 shot captions and 544 tactic captions.
Shot2Tactic-Caption adopts a dual-branch design, with both branches including a
visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based
decoder to generate shot and tactic captions. To support tactic captioning, we
additionally introduce a Tactic Unit Detector that identifies valid tactic
units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic
captioning, we further incorporate a shot-wise prompt-guided mechanism, where
the predicted tactic type and state are embedded as prompts and injected into
the decoder via cross-attention. The shot-wise prompt-guided mechanism enables
our system not only to describe successfully executed tactics but also to
capture tactical executions that are temporarily interrupted and later resumed.
Experimental results demonstrate the effectiveness of our framework in
generating both shot and tactic captions. Ablation studies show that the
ResNet50-based spatio-temporal encoder outperforms other variants, and that
shot-wise prompt structuring leads to more coherent and accurate tactic
captioning.

</details>


### [54] [Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference](https://arxiv.org/abs/2510.14624)
*Natan Bagrov,Eugene Khvedchenia,Borys Tymchenko,Shay Aharon,Lior Kadoch,Tomer Keren,Ofri Masad,Yonatan Geifman,Ran Zilberstein,Tuomas Rintamaki,Matthieu Le,Andrew Tao*

Main category: cs.CV

TL;DR: EVS is a plug-and-play method that reduces video token redundancy by pruning static patches across frames, enabling faster inference and longer video sequences without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Vision-language models face scalability issues with long videos due to quadratic processing costs and token budget limitations, causing context constraints and latency problems.

Method: Efficient Video Sampling identifies and removes temporally static patches (unchanged spatial regions across consecutive frames) while preserving positional identity, requiring no retraining.

Result: EVS reduces token count by up to 4x faster time-to-first-token with minimal accuracy loss, and when combined with uptraining, maintains full performance under aggressive pruning.

Conclusion: EVS enables scalable video-language understanding by improving efficiency-accuracy trade-offs without sacrificing quality, unlocking practical long-video processing capabilities.

Abstract: Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.

</details>


### [55] [Adapting Self-Supervised Representations as a Latent Space for Efficient Generation](https://arxiv.org/abs/2510.14630)
*Ming Gui,Johannes Schusterbauer,Timy Phan,Felix Krause,Josh Susskind,Miguel Angel Bautista,Björn Ommer*

Main category: cs.CV

TL;DR: RepTok is a generative modeling framework that uses a single continuous latent token from self-supervised vision transformers for efficient image generation and reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address spatial redundancies in 2D latent spaces and reduce training costs while maintaining competitive generation performance.

Method: Fine-tunes semantic token embedding from pre-trained SSL encoder, pairs with generative decoder using flow matching objective, adds cosine-similarity loss to preserve SSL space geometry.

Result: Achieves competitive results on class-conditional ImageNet generation and text-to-image synthesis on MS-COCO with limited training budgets.

Conclusion: Fine-tuned SSL representations can serve as compact and effective latent spaces for efficient generative modeling.

Abstract: We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.

</details>


### [56] [SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation](https://arxiv.org/abs/2510.14634)
*Jihyun Yu,Yoojin Oh,Wonho Bae,Mingyu Kim,Junhyug Noh*

Main category: cs.CV

TL;DR: SteeringTTA is a test-time adaptation method that uses Feynman-Kac steering to guide diffusion-based input adaptation for classification without model updates or source data, achieving improved robustness on ImageNet-C.


<details>
  <summary>Details</summary>
Motivation: Existing input-only diffusion-based TTA methods rely on gradient guidance, which limits exploration and generalization across different distortion types in test-time adaptation scenarios.

Method: Proposes SteeringTTA framework that adapts Feynman-Kac steering to guide diffusion-based input adaptation using rewards driven by pseudo-labels. Maintains multiple particle trajectories steered by cumulative top-K probabilities and entropy schedule to balance exploration and confidence.

Result: On ImageNet-C, SteeringTTA consistently outperforms baseline methods without requiring any model updates or source data.

Conclusion: SteeringTTA provides an effective inference-only framework for test-time adaptation that improves robustness to distribution shifts through guided diffusion-based input adaptation.

Abstract: Test-time adaptation (TTA) aims to correct performance degradation of deep
models under distribution shifts by updating models or inputs using unlabeled
test data. Input-only diffusion-based TTA methods improve robustness for
classification to corruptions but rely on gradient guidance, limiting
exploration and generalization across distortion types. We propose SteeringTTA,
an inference-only framework that adapts Feynman-Kac steering to guide
diffusion-based input adaptation for classification with rewards driven by
pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by
a combination of cumulative top-K probabilities and an entropy schedule, to
balance exploration and confidence. On ImageNet-C, SteeringTTA consistently
outperforms the baseline without any model updates or source data.

</details>


### [57] [In-Context Learning with Unpaired Clips for Instruction-based Video Editing](https://arxiv.org/abs/2510.14648)
*Xinyao Liao,Xianfang Zeng,Ziye Song,Zhoujie Fu,Gang Yu,Guosheng Lin*

Main category: cs.CV

TL;DR: A low-cost pretraining strategy for instruction-based video editing that uses in-context learning from unpaired video clips, achieving superior performance with minimal paired data.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of high cost and complexity in constructing large-scale paired video editing datasets for instruction-based video editing.

Method: Pretrain a foundation video generation model using in-context learning from ~1M unpaired video clips, then fine-tune with <150k curated editing pairs. Built upon HunyuanVideoT2V.

Result: Achieves 12% improvement in instruction following and 15% improvement in editing quality compared to existing methods, demonstrating superior instruction alignment and visual fidelity.

Conclusion: The proposed low-cost pretraining strategy effectively endows video generation models with general editing capabilities and can be efficiently refined with minimal paired data, advancing instruction-based video editing.

Abstract: Despite the rapid progress of instruction-based image editing, its extension
to video remains underexplored, primarily due to the prohibitive cost and
complexity of constructing large-scale paired video editing datasets. To
address this challenge, we introduce a low-cost pretraining strategy for
instruction-based video editing that leverages in-context learning from
unpaired video clips. We show that pretraining a foundation video generation
model with this strategy endows it with general editing capabilities, such as
adding, replacing, or deleting operations, according to input editing
instructions. The pretrained model can then be efficiently refined with a small
amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our
framework first pretrains on approximately 1M real video clips to learn basic
editing concepts, and subsequently fine-tunes on fewer than 150k curated
editing pairs to extend more editing tasks and improve the editing quality.
Comparative experiments show that our method surpasses existing
instruction-based video editing approaches in both instruction alignment and
visual fidelity, achieving a 12\% improvement in editing instruction following
and a 15\% improvement in editing quality.

</details>


### [58] [Decorrelation Speeds Up Vision Transformers](https://arxiv.org/abs/2510.14657)
*Kieran Carrigg,Rob van Gastel,Melda Yeghaian,Sander Dalm,Faysal Boughorbel,Marcel van Gerven*

Main category: cs.CV

TL;DR: DBP-MAE integrates Decorrelated Backpropagation into MAE pre-training to accelerate convergence, reducing training time by 21.1% and carbon emissions by 21.4% while improving downstream segmentation performance by 1.1 mIoU points.


<details>
  <summary>Details</summary>
Motivation: MAE pre-training of vision transformers provides strong performance but has high computational costs that make it impractical for time- and resource-constrained industrial settings.

Method: Integrate Decorrelated Backpropagation (DBP) into MAE pre-training, selectively applying it to the encoder to iteratively reduce input correlations at each layer and accelerate convergence.

Result: DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4%, and improves segmentation mIoU by 1.1 points on ImageNet-1K pre-training with ADE20K fine-tuning. Similar gains observed on proprietary industrial data.

Conclusion: DBP can effectively reduce training time and energy use while improving downstream performance for large-scale ViT pre-training, making it applicable in real-world industrial scenarios.

Abstract: Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields
strong performance in low-label regimes but comes with substantial
computational costs, making it impractical in time- and resource-constrained
industrial settings. We address this by integrating Decorrelated
Backpropagation (DBP) into MAE pre-training, an optimization method that
iteratively reduces input correlations at each layer to accelerate convergence.
Applied selectively to the encoder, DBP achieves faster pre-training without
loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE
reduces wall-clock time to baseline performance by 21.1%, lowers carbon
emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe
similar gains when pre-training and fine-tuning on proprietary industrial data,
confirming the method's applicability in real-world scenarios. These results
demonstrate that DBP can reduce training time and energy use while improving
downstream performance for large-scale ViT pre-training.

</details>


### [59] [EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)](https://arxiv.org/abs/2510.14661)
*Weikang Yu,Vincent Nwazelibe,Xianping Ma,Xiaokang Zhang,Richard Gloaguen,Xiao Xiang Zhu,Pedram Ghamisi*

Main category: cs.CV

TL;DR: EuroMineNet is the first comprehensive multitemporal benchmark for mining footprint mapping using Sentinel-2 imagery, covering 133 EU mining sites from 2015-2024 with expert-verified annotations.


<details>
  <summary>Details</summary>
Motivation: Mining causes environmental degradation but existing datasets lack temporal depth and geographic scope for consistent long-term monitoring of mining-induced land surface changes.

Method: Created EuroMineNet with annual Sentinel-2 observations and expert-verified annotations across 133 EU mining sites, supporting two tasks: multitemporal footprint mapping with CA-TIoU metric and cross-temporal change detection.

Result: Benchmarked 20 deep learning models showing GeoAI methods effectively identify long-term environmental changes but struggle with short-term dynamics important for timely mitigation.

Conclusion: EuroMineNet advances temporally consistent mining monitoring for sustainable land-use management and environmental resilience, contributing to GeoAI applications for social and environmental good.

Abstract: Mining activities are essential for industrial and economic development, but
remain a leading source of environmental degradation, contributing to
deforestation, soil erosion, and water contamination. Sustainable resource
management and environmental governance require consistent, long-term
monitoring of mining-induced land surface changes, yet existing datasets are
often limited in temporal depth or geographic scope. To address this gap, we
present EuroMineNet, the first comprehensive multitemporal benchmark for mining
footprint mapping and monitoring based on Sentinel-2 multispectral imagery.
Spanning 133 mining sites across the European Union, EuroMineNet provides
annual observations and expert-verified annotations from 2015 to 2024, enabling
GeoAI-based models to analyze environmental dynamics at a continental scale. It
supports two sustainability-driven tasks: (1) multitemporal mining footprint
mapping for consistent annual land-use delineation, evaluated with a novel
Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change
detection to capture both gradual and abrupt surface transformations.
Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI
methods effectively identify long-term environmental changes, challenges remain
in detecting short-term dynamics critical for timely mitigation. By advancing
temporally consistent and explainable mining monitoring, EuroMineNet
contributes to sustainable land-use management, environmental resilience, and
the broader goal of applying GeoAI for social and environmental good. We
release the codes and datasets by aligning with FAIR and the open science
paradigm at https://github.com/EricYu97/EuroMineNet.

</details>


### [60] [WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging](https://arxiv.org/abs/2510.14668)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Sami Azam,Asif Karim,Jemima Beissbarth,Amanda Leach*

Main category: cs.CV

TL;DR: WeCKD introduces a chain-based knowledge distillation framework where models learn progressively from predecessors in a weakly-supervised manner, achieving significant performance gains with minimal data.


<details>
  <summary>Details</summary>
Motivation: Traditional KD suffers from knowledge degradation, inefficient supervision, and reliance on strong teachers or large datasets, limiting effectiveness in real-world limited-data scenarios.

Method: Progressive distillation chain where each model learns from its predecessor and refines knowledge before passing forward, trained on only a fraction of the dataset with minimal supervision.

Result: Achieves up to +23% accuracy gain over single backbone models, matches or surpasses supervised methods on otoscopic imaging datasets, and generalizes well to other medical imaging modalities.

Conclusion: WeCKD provides an effective solution for knowledge transfer in limited-data scenarios, demonstrating strong potential for real-world medical imaging applications.

Abstract: Knowledge distillation (KD) has traditionally relied on a static
teacher-student framework, where a large, well-trained teacher transfers
knowledge to a single student model. However, these approaches often suffer
from knowledge degradation, inefficient supervision, and reliance on either a
very strong teacher model or large labeled datasets, which limits their
effectiveness in real-world, limited-data scenarios. To address these, we
present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that
redefines knowledge transfer through a structured sequence of interconnected
models. Unlike conventional KD, it forms a progressive distillation chain,
where each model not only learns from its predecessor but also refines the
knowledge before passing it forward. This structured knowledge transfer further
enhances feature learning, reduces data dependency, and mitigates the
limitations of one-step KD. Each model in the distillation chain is trained on
only a fraction of the dataset and demonstrates that effective learning can be
achieved with minimal supervision. Extensive evaluations across four otoscopic
imaging datasets demonstrate that it not only matches but in many cases
surpasses the performance of existing supervised methods. Experimental results
on two other datasets further underscore its generalization across diverse
medical imaging modalities, including microscopic and magnetic resonance
imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of
up to +23% over a single backbone trained on the same limited data, which
highlights its potential for real-world adoption.

</details>


### [61] [VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning](https://arxiv.org/abs/2510.14672)
*Jinglei Zhang,Yuanfan Guo,Rolandos Alexandros Potamias,Jiankang Deng,Hang Xu,Chao Ma*

Main category: cs.CV

TL;DR: VTimeCoT is a training-free framework that enhances video temporal grounding and reasoning in MLLMs using progress bar tools and visuotemporal chain-of-thought reasoning.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs have deficiencies in video temporal grounding and reasoning, limiting their effectiveness for real-world video understanding systems.

Method: Uses two visual tools: plug-and-play progress bar integration and high-efficiency highlighting, plus visuotemporal CoT that integrates cross-modality reasoning across video and text.

Result: Significant performance improvements on Qwen2VL-7B and GPT4o baselines for video temporal grounding and reasoning-based question answering.

Conclusion: The framework achieves compositional and interpretable reasoning process, enhancing video understanding capabilities.

Abstract: In recent years, video question answering based on multimodal large language
models (MLLM) has garnered considerable attention, due to the benefits from the
substantial advancements in LLMs. However, these models have a notable
deficiency in the domains of video temporal grounding and reasoning, posing
challenges to the development of effective real-world video understanding
systems. Inspired by how humans use video players to interact with the progress
bar for video comprehension, we introduce VTimeCoT, a simple yet effective
training-free framework, designed for high-performance video grounding and
reasoning. The proposed framework incorporates two novel visual tools of the
progress bar: a plug-and-play progress bar integration tool and a
high-efficiency highlighting tool. In addition, to address the limitations of
conventional text-based chain-of-thought (CoT) approaches, we introduce a
visuotemporal CoT process that integrates cross-modality reasoning across both
video and text. Our approach demonstrates significant performance improvements
on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and
reasoning-based question answering. Finally, we showcase that the proposed
framework achieves a compositional and interpretable reasoning process. Project
page: https://vtimecot.github.io

</details>


### [62] [Leveraging Learned Image Prior for 3D Gaussian Compression](https://arxiv.org/abs/2510.14705)
*Seungjoo Shin,Jaesik Park,Sunghyun Cho*

Main category: cs.CV

TL;DR: A novel 3DGS compression framework that uses learned image priors to restore quality degradation from compression, achieving superior rate-distortion performance with less storage.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS compression methods lack learned priors, limiting further advances in rate-distortion trade-off despite considerable storage reduction.

Method: Uses a restoration network to model compression artifacts in image space between degraded and original Gaussians, with coarse rendering residuals as side information to enhance performance.

Result: Superior rate-distortion performance and better rendering quality than state-of-the-art 3DGS compression methods while requiring substantially less storage.

Conclusion: The framework effectively leverages learned image priors to refine compressed Gaussians, creating highly compact representations with enhanced rendering performance, and is compatible with existing Gaussian compression methods.

Abstract: Compression techniques for 3D Gaussian Splatting (3DGS) have recently
achieved considerable success in minimizing storage overhead for 3D Gaussians
while preserving high rendering quality. Despite the impressive storage
reduction, the lack of learned priors restricts further advances in the
rate-distortion trade-off for 3DGS compression tasks. To address this, we
introduce a novel 3DGS compression framework that leverages the powerful
representational capacity of learned image priors to recover
compression-induced quality degradation. Built upon initially compressed
Gaussians, our restoration network effectively models the compression artifacts
in the image space between degraded and original Gaussians. To enhance the
rate-distortion performance, we provide coarse rendering residuals into the
restoration network as side information. By leveraging the supervision of
restored images, the compressed Gaussians are refined, resulting in a highly
compact representation with enhanced rendering performance. Our framework is
designed to be compatible with existing Gaussian compression methods, making it
broadly applicable across different baselines. Extensive experiments validate
the effectiveness of our framework, demonstrating superior rate-distortion
performance and outperforming the rendering quality of state-of-the-art 3DGS
compression methods while requiring substantially less storage.

</details>


### [63] [Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery](https://arxiv.org/abs/2510.14709)
*Caleb Robinson,Kimberly T. Goetz,Christin B. Khan,Meredith Sackett,Kathleen Leonard,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: Semi-automated whale detection in satellite imagery using statistical anomaly detection to flag spatial outliers, reducing expert inspection area by up to 99.8% while maintaining high recall rates.


<details>
  <summary>Details</summary>
Motivation: Traditional whale monitoring methods are expensive and difficult to scale, while automated detection faces challenges due to lack of annotated data, image variability, and pipeline costs.

Method: Statistical anomaly detection method that flags spatial outliers ('interesting points') paired with web-based labeling interface for expert annotation, without requiring labeled training data.

Result: Achieved recalls of 90.3% to 96.4% on benchmark scenes, reducing expert inspection area from over 1,000 sq km to less than 2 sq km (up to 99.8% reduction).

Conclusion: The approach offers a scalable first step toward machine-assisted marine mammal monitoring from space and has been open sourced.

Abstract: Effective monitoring of whale populations is critical for conservation, but
traditional survey methods are expensive and difficult to scale. While prior
work has shown that whales can be identified in very high-resolution (VHR)
satellite imagery, large-scale automated detection remains challenging due to a
lack of annotated imagery, variability in image quality and environmental
conditions, and the cost of building robust machine learning pipelines over
massive remote sensing archives. We present a semi-automated approach for
surfacing possible whale detections in VHR imagery using a statistical anomaly
detection method that flags spatial outliers, i.e. "interesting points". We
pair this detector with a web-based labeling interface designed to enable
experts to quickly annotate the interesting points. We evaluate our system on
three benchmark scenes with known whale annotations and achieve recalls of
90.3% to 96.4%, while reducing the area requiring expert inspection by up to
99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method
does not rely on labeled training data and offers a scalable first step toward
future machine-assisted marine mammal monitoring from space. We have open
sourced this pipeline at https://github.com/microsoft/whales.

</details>


### [64] [Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models](https://arxiv.org/abs/2510.14713)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: This paper presents the first systematic evaluation of deep video camera movement classification models on historical archival footage, achieving 80.25% accuracy with Video Swin Transformer on WWII footage.


<details>
  <summary>Details</summary>
Motivation: Camera movement conveys essential spatial and narrative information in videos, but existing CMC methods' generalization to historical footage remains unexplored despite their good performance on modern datasets.

Method: Systematic evaluation of five standard video classification models on the HISTORIAN dataset containing expert-annotated World War II footage, with analysis of model design and label definition differences.

Result: Video Swin Transformer achieved the best performance with 80.25% accuracy, showing strong convergence despite limited training data on historical footage.

Conclusion: The findings highlight challenges and potential for adapting existing models to low-quality video, motivating future work combining diverse input modalities and temporal architectures for historical footage analysis.

Abstract: Camera movement conveys spatial and narrative information essential for
understanding video content. While recent camera movement classification (CMC)
methods perform well on modern datasets, their generalization to historical
footage remains unexplored. This paper presents the first systematic evaluation
of deep video CMC models on archival film material. We summarize representative
methods and datasets, highlighting differences in model design and label
definitions. Five standard video classification models are assessed on the
HISTORIAN dataset, which includes expert-annotated World War II footage. The
best-performing model, Video Swin Transformer, achieves 80.25% accuracy,
showing strong convergence despite limited training data. Our findings
highlight the challenges and potential of adapting existing models to
low-quality video and motivate future work combining diverse input modalities
and temporal architectures.

</details>


### [65] [Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection](https://arxiv.org/abs/2510.14726)
*Dingzhou Xie,Rushi Lan,Cheng Pang,Enhao Ning,Jiahao Zeng,Wei Zheng*

Main category: cs.CV

TL;DR: Proposes CFSAM, a cross-layer feature self-attention module that models multi-scale dependencies for object detection, achieving significant performance improvements on PASCAL VOC and COCO datasets.


<details>
  <summary>Details</summary>
Motivation: Existing attention mechanisms focus on single or dual-layer features, missing rich inter-layer dependencies across multi-scale representations needed for detecting objects with large scale variations.

Method: CFSAM consists of three components: convolutional local feature extractor, Transformer-based global modeling unit for cross-layer interactions, and feature fusion mechanism to enhance original representations. Integrated into SSD300 framework.

Result: Achieved 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO (vs. 43.1% baseline), outperforming existing attention modules. Also accelerates convergence without substantial computational overhead.

Conclusion: Explicit cross-layer attention modeling is crucial for advancing multi-scale object detection, as demonstrated by CFSAM's significant performance improvements.

Abstract: Recent object detection methods have made remarkable progress by leveraging
attention mechanisms to improve feature discriminability. However, most
existing approaches are confined to refining single-layer or fusing dual-layer
features, overlooking the rich inter-layer dependencies across multi-scale
representations. This limits their ability to capture comprehensive contextual
information essential for detecting objects with large scale variations. In
this paper, we propose a novel Cross-Layer Feature Self-Attention Module
(CFSAM), which holistically models both local and global dependencies within
multi-scale feature maps. CFSAM consists of three key components: a
convolutional local feature extractor, a Transformer-based global modeling unit
that efficiently captures cross-layer interactions, and a feature fusion
mechanism to restore and enhance the original representations. When integrated
into the SSD300 framework, CFSAM significantly boosts detection performance,
achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO
(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the
module accelerates convergence during training without introducing substantial
computational overhead. Our work highlights the importance of explicit
cross-layer attention modeling in advancing multi-scale object detection.

</details>


### [66] [Free-Grained Hierarchical Recognition](https://arxiv.org/abs/2510.14737)
*Seulki Park,Zilin Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: ImageNet-F benchmark enables hierarchical image classification with mixed-granularity supervision, simulating real-world annotation behavior using CLIP to measure semantic ambiguity.


<details>
  <summary>Details</summary>
Motivation: Real-world image annotations vary in granularity due to factors like image quality and annotator expertise, but existing methods assume complete fine-grained labels.

Method: Proposed free-grain learning with heterogeneous supervision, enhanced by pseudo-attributes from vision-language models and semi-supervised learning for visual guidance.

Result: Developed methods and strong baselines substantially improve performance under mixed supervision conditions.

Conclusion: The ImageNet-F benchmark and proposed methods advance hierarchical classification under realistic constraints of varying annotation granularity.

Abstract: Hierarchical image classification predicts labels across a semantic taxonomy,
but existing methods typically assume complete, fine-grained annotations, an
assumption rarely met in practice. Real-world supervision varies in
granularity, influenced by image quality, annotator expertise, and task
demands; a distant bird may be labeled Bird, while a close-up reveals Bald
eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet
and structured into cognitively inspired basic, subordinate, and fine-grained
levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,
mixed-granularity labels reflecting human annotation behavior. We propose
free-grain learning, with heterogeneous supervision across instances. We
develop methods that enhance semantic guidance via pseudo-attributes from
vision-language models and visual guidance via semi-supervised learning. These,
along with strong baselines, substantially improve performance under mixed
supervision. Together, our benchmark and methods advance hierarchical
classification under real-world constraints.

</details>


### [67] [DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models](https://arxiv.org/abs/2510.14741)
*Simone Carnemolla,Matteo Pennisi,Sarinda Samarasinghe,Giovanni Bellitto,Simone Palazzo,Daniela Giordano,Mubarak Shah,Concetto Spampinato*

Main category: cs.CV

TL;DR: DEXTER is a data-free framework that uses diffusion models and large language models to generate global, textual explanations of visual classifiers without needing training data or ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: To build transparent and trustworthy AI systems by understanding and explaining machine learning model behavior, addressing the need for model interpretability without data access.

Method: Optimizes text prompts to synthesize class-conditional images that strongly activate target classifiers, then uses these synthetic samples to generate natural language reports describing class-specific decision patterns and biases.

Result: Outperforms existing approaches in global model explanation and class-level bias reporting across ImageNet, Waterbirds, CelebA, and FairFaces datasets, producing accurate and interpretable outputs as confirmed by user studies.

Conclusion: DEXTER provides an effective framework for generating natural language explanations of visual classifiers' decision processes without data dependencies, enabling comprehensive model transparency and bias analysis.

Abstract: Understanding and explaining the behavior of machine learning models is
essential for building transparent and trustworthy AI systems. We introduce
DEXTER, a data-free framework that employs diffusion models and large language
models to generate global, textual explanations of visual classifiers. DEXTER
operates by optimizing text prompts to synthesize class-conditional images that
strongly activate a target classifier. These synthetic samples are then used to
elicit detailed natural language reports that describe class-specific decision
patterns and biases. Unlike prior work, DEXTER enables natural language
explanation about a classifier's decision process without access to training
data or ground-truth labels. We demonstrate DEXTER's flexibility across three
tasks-activation maximization, slice discovery and debiasing, and bias
explanation-each illustrating its ability to uncover the internal mechanisms of
visual classifiers. Quantitative and qualitative evaluations, including a user
study, show that DEXTER produces accurate, interpretable outputs. Experiments
on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms
existing approaches in global model explanation and class-level bias reporting.
Code is available at https://github.com/perceivelab/dexter.

</details>


### [68] [LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement](https://arxiv.org/abs/2510.14753)
*Xu Wu,Zhihui Lai,Xianxu Hou,Jie Zhou,Ya-nan Zhang,Linlin Shen*

Main category: cs.CV

TL;DR: LightQANet introduces quantized and adaptive feature learning for low-light image enhancement, using a Light Quantization Module for structured light factor extraction and a Light-Aware Prompt Module for dynamic adaptation to varying illumination conditions.


<details>
  <summary>Details</summary>
Motivation: Existing low-light image enhancement methods fail to extract reliable feature representations due to severely degraded pixel-level information under low-light conditions, leading to poor texture restoration, color inconsistency, and artifacts.

Method: Proposes LightQANet with two key components: 1) Light Quantization Module (LQM) for explicit extraction and quantification of illumination-related factors, and 2) Light-Aware Prompt Module (LAPM) that encodes illumination priors into learnable prompts to dynamically guide feature learning.

Result: Extensive experiments on multiple low-light datasets demonstrate state-of-the-art performance, delivering superior qualitative and quantitative results across various challenging lighting scenarios.

Conclusion: The proposed LightQANet framework successfully addresses the challenges of low-light image enhancement by combining static modeling of illumination factors with dynamic adaptation to varying lighting conditions, achieving robust and consistent image quality.

Abstract: Low-light image enhancement (LLIE) aims to improve illumination while
preserving high-quality color and texture. However, existing methods often fail
to extract reliable feature representations due to severely degraded
pixel-level information under low-light conditions, resulting in poor texture
restoration, color inconsistency, and artifact. To address these challenges, we
propose LightQANet, a novel framework that introduces quantized and adaptive
feature learning for low-light enhancement, aiming to achieve consistent and
robust image quality across diverse lighting conditions. From the static
modeling perspective, we design a Light Quantization Module (LQM) to explicitly
extract and quantify illumination-related factors from image features. By
enforcing structured light factor learning, LQM enhances the extraction of
light-invariant representations and mitigates feature inconsistency across
varying illumination levels. From the dynamic adaptation perspective, we
introduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors
into learnable prompts to dynamically guide the feature learning process. LAPM
enables the model to flexibly adapt to complex and continuously changing
lighting conditions, further improving image enhancement. Extensive experiments
on multiple low-light datasets demonstrate that our method achieves
state-of-the-art performance, delivering superior qualitative and quantitative
results across various challenging lighting scenarios.

</details>


### [69] [Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality](https://arxiv.org/abs/2510.14765)
*Giuseppe Lorenzo Catalano,Agata Marta Soccini*

Main category: cs.CV

TL;DR: This paper proposes an unconditional diffusion model for reconstructing Martian terrain heightmaps with missing data, outperforming traditional interpolation methods in accuracy and perceptual quality.


<details>
  <summary>Details</summary>
Motivation: Space exploration relies on accurate 3D planetary terrain representations, but Martian heightmaps often contain missing values due to acquisition constraints. Current interpolation methods fail to preserve geometric coherence, and conditional deep learning approaches used on Earth cannot be applied to Mars due to limited data.

Method: The authors developed an unconditional diffusion model trained on 12,000 augmented Martian heightmaps from NASA's HiRISE survey. They used a non-homogeneous rescaling strategy to capture multi-scale terrain features before resizing to 128x128 resolution.

Result: The method was evaluated on 1000 samples against established techniques (Inverse Distance Weighting, kriging, Navier-Stokes). Results show consistent outperformance: 4-15% improvement in RMSE for reconstruction accuracy and 29-81% improvement in LPIPS for perceptual similarity compared to original data.

Conclusion: The proposed unconditional diffusion model successfully reconstructs Martian terrain with missing data, providing more accurate and perceptually similar results than traditional void-filling methods, making it valuable for space exploration applications.

Abstract: Space exploration increasingly relies on Virtual Reality for several tasks,
such as mission planning, multidisciplinary scientific analysis, and astronaut
training. A key factor for the reliability of the simulations is having
accurate 3D representations of planetary terrains. Extraterrestrial heightmaps
derived from satellite imagery often contain missing values due to acquisition
and transmission constraints. Mars is among the most studied planets beyond
Earth, and its extensive terrain datasets make the Martian surface
reconstruction a valuable task, although many areas remain unmapped. Deep
learning algorithms can support void-filling tasks; however, whereas Earth's
comprehensive datasets enables the use of conditional methods, such approaches
cannot be applied to Mars. Current approaches rely on simpler interpolation
techniques which, however, often fail to preserve geometric coherence. In this
work, we propose a method for reconstructing the surface of Mars based on an
unconditional diffusion model. Training was conducted on an augmented dataset
of 12000 Martian heightmaps derived from NASA's HiRISE survey. A
non-homogeneous rescaling strategy captures terrain features across multiple
scales before resizing to a fixed 128x128 model resolution. We compared our
method against established void-filling and inpainting techniques, including
Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an
evaluation set of 1000 samples. Results show that our approach consistently
outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)
and perceptual similarity (29-81% on LPIPS) with the original data.

</details>


### [70] [MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks](https://arxiv.org/abs/2510.14770)
*Zhang Nengbo,Hann Woei Ho,Ye Zhou*

Main category: cs.CV

TL;DR: A visual communication framework for MAV swarms using motion-based signaling inspired by honeybee waggle dance, employing event cameras and SNNs for energy-efficient information transmission.


<details>
  <summary>Details</summary>
Motivation: Conventional radio-based MAV communication suffers from spectrum congestion, jamming, and high power consumption, requiring alternative methods for reliable swarm communication in constrained environments.

Method: MAVs convey information through deliberate flight patterns using four motion primitives as control symbols. Event cameras capture these motions, and an event frame-based segmentation model with lightweight SNN decodes the signals through an integrated algorithm.

Result: Experimental results show accurate decoding of motion sequences with low power consumption, validating the framework's effectiveness for MAV communication.

Conclusion: The proposed visual communication framework offers an energy-efficient alternative to radio-based methods for MAV swarms in constrained environments, demonstrating reliable performance with minimal power requirements.

Abstract: Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in
environments, where conventional radio-based methods suffer from spectrum
congestion, jamming, and high power consumption. Inspired by the waggle dance
of honeybees, which efficiently communicate the location of food sources
without sound or contact, we propose a novel visual communication framework for
MAV swarms using motion-based signaling. In this framework, MAVs convey
information, such as heading and distance, through deliberate flight patterns,
which are passively captured by event cameras and interpreted using a
predefined visual codebook of four motion primitives: vertical (up/down),
horizontal (left/right), left-to-up-to-right, and left-to-down-to-right,
representing control symbols (``start'', ``end'', ``1'', ``0''). To decode
these signals, we design an event frame-based segmentation model and a
lightweight Spiking Neural Network (SNN) for action recognition. An integrated
decoding algorithm then combines segmentation and classification to robustly
interpret MAV motion sequences. Experimental results validate the framework's
effectiveness, which demonstrates accurate decoding and low power consumption,
and highlights its potential as an energy-efficient alternative for MAV
communication in constrained environments.

</details>


### [71] [CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection](https://arxiv.org/abs/2510.14792)
*Hojun Choi,Youngsun Lim,Jaeyo Shin,Hyunjung Shim*

Main category: cs.CV

TL;DR: CoT-PL introduces chain-of-thought reasoning into pseudo-labeling for open-vocabulary object detection, decomposing object understanding into three steps and using contrastive background learning to improve robustness in crowded/occluded scenes.


<details>
  <summary>Details</summary>
Motivation: Current OVD methods rely on direct image-text matching but fail to handle semantically complex scenes with crowding or occlusion, lacking intermediate reasoning steps needed for robust interpretation.

Method: Uses structured visual chain-of-thought reasoning with three steps: region perception, category recognition via zero-shot reasoning, and background grounding. Introduces contrastive background learning (CBL) that uses background cues as negatives to disentangle object and background features.

Result: Achieves 103.4% and 168.4% relative improvements in novel-class pseudo-label quality for crowded and occluded scenes respectively. Sets new SOTA with +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes.

Conclusion: CoT reasoning combined with contrastive background learning creates an integrated pipeline that significantly improves pseudo-labeling robustness and performance in challenging open-vocabulary detection scenarios.

Abstract: Open-vocabulary object detection (OVD) seeks to recognize and localize object
categories beyond those seen during training. Recent approaches typically
leverage vision-language models (VLMs) to generate pseudo-labels using
image-text alignment, allowing detectors to generalize to unseen classes
without explicit supervision. However, these methods depend heavily on direct
image-text matching, neglecting the intermediate reasoning steps essential for
interpreting semantically complex scenes. This results in limited robustness
when confronted with crowded or occluded visual contexts. In this paper, we
introduce CoT-PL, a new framework that employs structured visual
chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL
decomposes object understanding into three interpretable steps: (1) region
perception even for unseen objects, (2) category recognition via zero-shot
reasoning, and (3) background grounding to separate semantically complex
objects. Crucially, the third step naturally motivates our contrastive
background learning (CBL) that uses the pre-computed background cues as
negatives to promote feature disentanglement between objects and background. In
this way, CoT reasoning and CBL form an integrated pipeline tailored to robust
pseudo-labeling in crowded or occluded scenes. Notably, in these two settings,
our novel-class pseudo-label quality achieves relative improvements of 103.4%
and 168.4% over the best prior, respectively. Our extensive experiments
demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9
mask AP on LVIS for novel classes, setting a new state of the art.

</details>


### [72] [Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images](https://arxiv.org/abs/2510.14800)
*Usama Sajjad,Abdul Rehman Akbar,Ziyu Su,Deborah Knight,Wendy L. Frankel,Metin N. Gurcan,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: PRISM is an interpretable AI model for colorectal cancer prognosis that captures continuous morphological variability in tumor evolution, achieving superior 5-year overall survival prediction compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current foundation models in computational pathology overlook organ-specific morphological patterns that influence tumor behavior and patient outcomes. CRC remains a major global health burden with high mortality rates.

Method: Developed PRISM model trained on 8.74 million histological images from 424 stage III CRC patients, incorporating continuous variability spectrum within distinct morphologies to reflect incremental evolutionary processes.

Result: PRISM achieved AUC=0.70 for 5-year OS, outperforming CRC-specific methods by 15% and AI foundation models by ~23% accuracy. Showed sex-agnostic robustness and stable performance across subgroups with minimal treatment regimen differences.

Conclusion: PRISM provides superior prognostic performance by capturing phenotypic diversity through continuous morphological variability, offering an interpretable AI approach that reflects the incremental nature of malignant transformation.

Abstract: Colorectal cancer (CRC) remains the third most prevalent malignancy globally,
with approximately 154,000 new cases and 54,000 projected deaths anticipated
for 2025. The recent advancement of foundation models in computational
pathology has been largely propelled by task agnostic methodologies that can
overlook organ-specific crucial morphological patterns that represent distinct
biological processes that can fundamentally influence tumor behavior,
therapeutic response, and patient outcomes. The aim of this study is to develop
a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated
Spatial Morphology), that incorporates a continuous variability spectrum within
each distinct morphology to characterize phenotypic diversity and reflecting
the principle that malignant transformation occurs through incremental
evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained
on 8.74 million histological images extracted from surgical resection specimens
of 424 patients with stage III CRC. PRISM achieved superior prognostic
performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;
HR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific
methods by 15% and AI foundation models by ~23% accuracy. It showed
sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable
performance across clinicopathological subgroups, with minimal accuracy
fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,
replicating the Alliance cohort finding of no survival difference between
treatments.

</details>


### [73] [Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks](https://arxiv.org/abs/2510.14803)
*Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Szymon Płotka,Jieneng Chen,Qi Chen,Zheren Zhu,Jakub Prządo,Ibrahim E. Hamacı,Sezgin Er,Yuhan Wang,Ashwin Kumar,Bjoern Menze,Jarosław B. Ćwikła,Yuyin Zhou,Akshay S. Chaudhari,Curtis P. Langlotz,Sergio Decherchi,Andrea Cavalli,Kang Wang,Yang Yang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: R-Super trains AI to segment tumors using medical reports instead of costly manual tumor masks, achieving comparable performance while enabling segmentation of previously unsupported tumor types.


<details>
  <summary>Details</summary>
Motivation: Manual tumor mask creation for AI training is expensive and time-consuming, while medical reports describing tumors are abundant and underutilized in clinical practice.

Method: R-Super trains AI models to segment tumors by matching their descriptions in medical reports, using 101,654 reports to scale training without extensive manual mask creation.

Result: Models trained with reports achieved performance comparable to those trained with 723 masks. Combining reports and masks improved sensitivity by +13% and specificity by +8%, surpassing radiologists in detecting 5 of 7 tumor types.

Conclusion: R-Super challenges the necessity of labor-intensive tumor mask creation and establishes a scalable path for early tumor detection across diverse tumor types, including previously unsupported organs.

Abstract: Early tumor detection save lives. Each year, more than 300 million computed
tomography (CT) scans are performed worldwide, offering a vast opportunity for
effective cancer screening. However, detecting small or early-stage tumors on
these CT scans remains challenging, even for experts. Artificial intelligence
(AI) models can assist by highlighting suspicious regions, but training such
models typically requires extensive tumor masks--detailed, voxel-wise outlines
of tumors manually drawn by radiologists. Drawing these masks is costly,
requiring years of effort and millions of dollars. In contrast, nearly every CT
scan in clinical practice is already accompanied by medical reports describing
the tumor's size, number, appearance, and sometimes, pathology
results--information that is rich, abundant, and often underutilized for AI
training. We introduce R-Super, which trains AI to segment tumors that match
their descriptions in medical reports. This approach scales AI training with
large collections of readily available medical reports, substantially reducing
the need for manually drawn tumor masks. When trained on 101,654 reports, AI
models achieved performance comparable to those trained on 723 masks. Combining
reports and masks further improved sensitivity by +13% and specificity by +8%,
surpassing radiologists in detecting five of the seven tumor types. Notably,
R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,
bladder, uterus, and esophagus, for which no public masks or AI models
previously existed. This study challenges the long-held belief that
large-scale, labor-intensive tumor mask creation is indispensable, establishing
a scalable and accessible path toward early detection across diverse tumor
types.
  We plan to release our trained models, code, and dataset at
https://github.com/MrGiovanni/R-Super

</details>


### [74] [Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning](https://arxiv.org/abs/2510.14819)
*Ji Cao,Yu Wang,Tongya Zheng,Zujie Ren,Canghong Jin,Gang Chen,Mingli Song*

Main category: cs.CV

TL;DR: PRTraj is a novel trajectory representation learning framework that integrates environment perception and route choice modeling to create more effective trajectory embeddings for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory representation learning methods treat trajectories as isolated sequences without considering external environmental factors and internal route choice behaviors that shape trajectory formation.

Method: PRTraj uses an Environment Perception Module to capture multi-granularity environmental semantics from POI distributions, and a Route Choice Encoder that models road segment transitions as decision sequences to capture route choice behavior.

Result: Extensive experiments on 3 real-world datasets across 5 downstream tasks show PRTraj's effectiveness, generalizability, and strong data efficiency with robust performance in few-shot scenarios.

Conclusion: PRTraj successfully bridges the gap in trajectory representation learning by incorporating both environmental context and route choice behavior, demonstrating superior performance across multiple tasks and datasets.

Abstract: Trajectory Representation Learning (TRL) aims to encode raw trajectories into
low-dimensional vectors, which can then be leveraged in various downstream
tasks, including travel time estimation, location prediction, and trajectory
similarity analysis. However, existing TRL methods suffer from a key oversight:
treating trajectories as isolated spatio-temporal sequences, without
considering the external environment and internal route choice behavior that
govern their formation. To bridge this gap, we propose a novel framework that
unifies comprehensive environment \textbf{P}erception and explicit
\textbf{R}oute choice modeling for effective \textbf{Traj}ectory representation
learning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces an
Environment Perception Module to enhance the road network by capturing
multi-granularity environmental semantics from surrounding POI distributions.
Building on this environment-aware backbone, a Route Choice Encoder then
captures the route choice behavior inherent in each trajectory by modeling its
constituent road segment transitions as a sequence of decisions. These
route-choice-aware representations are finally aggregated to form the global
trajectory embedding. Extensive experiments on 3 real-world datasets across 5
downstream tasks validate the effectiveness and generalizability of PRTraj.
Moreover, PRTraj demonstrates strong data efficiency, maintaining robust
performance under few-shot scenarios. Our code is available at:
https://anonymous.4open.science/r/PRTraj.

</details>


### [75] [FraQAT: Quantization Aware Training with Fractional bits](https://arxiv.org/abs/2510.14823)
*Luca Morreale,Alberto Gil C. P. Ramos,Malcolm Chadwick,Mehid Noroozi,Ruchika Chavhan,Abhinav Mehrotra,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: Proposes fractional bits quantization (SHORT) to progressively reduce model precision from 32 to 4 bits while maintaining generation quality, enabling deployment of large generative models on smartphones.


<details>
  <summary>Details</summary>
Motivation: Large generative models cannot be deployed on smartphones due to memory and computation constraints. Aggressive quantization addresses efficiency but degrades model quality.

Method: Fractional bits quantization approach that progressively reduces precision from 32 to 4 bits per parameter, exploiting fractional bits during optimization to preserve quality.

Result: SHORT yields improved quality on various diffusion models (SD3.5-Medium, Sana, PixArt, FLUX.1-schnell) with 4-7% lower FiD than standard QAT. Successfully deployed Sana on Samsung S25U with Snapdragon 8 Elite HTP.

Conclusion: Fractional bits quantization enables efficient deployment of large generative models on mobile devices while maintaining high generation quality.

Abstract: State-of-the-art (SOTA) generative models have demonstrated impressive
capabilities in image synthesis or text generation, often with a large capacity
model. However, these large models cannot be deployed on smartphones due to the
limited availability of on-board memory and computations. Quantization methods
lower the precision of the model parameters, allowing for efficient
computations, \eg, in \INT{8}. Although aggressive quantization addresses
efficiency and memory constraints, preserving the quality of the model remains
a challenge. To retain quality in previous aggressive quantization, we propose
a new fractional bits quantization (\short) approach. The novelty is a simple
yet effective idea: we progressively reduce the model's precision from 32 to 4
bits per parameter, and exploit the fractional bits during optimization to
maintain high generation quality. We show that the \short{} yields improved
quality on a variety of diffusion models, including SD3.5-Medium, Sana,
\pixart, and FLUX.1-schnell, while achieving $4-7\%$ lower FiD than standard
QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the
Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).

</details>


### [76] [Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data](https://arxiv.org/abs/2510.14831)
*Qi Chen,Xinze Zhou,Chen Liu,Hao Chen,Wenxuan Li,Zekun Jiang,Ziyan Huang,Yuxuan Zhao,Dexin Yu,Junjun He,Yefeng Zheng,Ling Shao,Alan Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: Synthetic data can reduce the need for large annotated medical datasets by achieving the same tumor segmentation performance with fewer real scans. AbdomenAtlas 2.0 provides 10,135 CT scans with 15,130 tumor instances across six organs, significantly outperforming existing datasets.


<details>
  <summary>Details</summary>
Motivation: AI for tumor segmentation is limited by the lack of large, voxel-wise annotated datasets which are hard to create and require medical experts. The authors found that AI performance stopped improving after 1,500 scans in their proprietary dataset.

Method: Created AbdomenAtlas 2.0 - a dataset of 10,135 CT scans with 15,130 tumor instances manually annotated in six organs (pancreas, liver, kidney, colon, esophagus, uterus) and 5,893 control scans, annotated by 23 expert radiologists.

Result: With synthetic data, they reached the same performance using only 500 real scans instead of 1,500. AbdomenAtlas 2.0 achieves +7% DSC gain on in-distribution tests and +16% on out-of-distribution tests compared to public datasets.

Conclusion: Synthetic data can steepen data scaling laws, enabling more efficient model training than real data alone. AbdomenAtlas 2.0 provides a strong foundation for training AI to segment tumors in six organs based on lessons from their proprietary dataset.

Abstract: AI for tumor segmentation is limited by the lack of large, voxel-wise
annotated datasets, which are hard to create and require medical experts. In
our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found
that AI performance stopped improving after 1,500 scans. With synthetic data,
we reached the same performance using only 500 real scans. This finding
suggests that synthetic data can steepen data scaling laws, enabling more
efficient model training than real data alone. Motivated by these lessons, we
created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130
tumor instances per-voxel manually annotated in six organs (pancreas, liver,
kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23
expert radiologists, it is several orders of magnitude larger than existing
public tumor datasets. While we continue expanding the dataset, the current
version of AbdomenAtlas 2.0 already provides a strong foundation--based on
lessons from the JHH dataset--for training AI to segment tumors in six organs.
It achieves notable improvements over public datasets, with a +7% DSC gain on
in-distribution tests and +16% on out-of-distribution tests.

</details>


### [77] [QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models](https://arxiv.org/abs/2510.14836)
*Yixuan Li,Yuhui Chen,Mingcai Zhou,Haoran Li*

Main category: cs.CV

TL;DR: QDepth-VLA enhances VLA models with depth prediction to improve spatial reasoning for manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models lack 3D structure understanding needed for precise control in manipulation tasks.

Method: Augments VLA models with auxiliary depth prediction using a dedicated depth expert that predicts quantized depth tokens from VQ-VAE encoder.

Result: Demonstrates strong spatial reasoning and competitive performance on simulation benchmarks and real-world manipulation tasks.

Conclusion: QDepth-VLA effectively improves spatial perception and reasoning in VLA models through depth-aware representations.

Abstract: Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)
models to accomplish fine-grained manipulation tasks. However, existing
approaches often lack the ability to understand and reason over the essential
3D structures necessary for precise control. To address this limitation, we
propose QDepth-VLA, a general framework that augments VLA models with an
auxiliary depth prediction task. A dedicated depth expert is designed to
predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,
enabling the model to learn depth-aware representations that capture critical
geometric cues. Experimental results on the simulation benchmarks and
real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning
and competitive performance on manipulation tasks.

</details>


### [78] [ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints](https://arxiv.org/abs/2510.14847)
*Meiqi Wu,Jiashu Zhu,Xiaokun Feng,Chubin Chen,Chen Zhu,Bingze Song,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: ImagerySearch is a prompt-guided adaptive test-time search strategy that dynamically adjusts inference search space and reward function for better video generation in imaginative scenarios with long-distance semantic relationships.


<details>
  <summary>Details</summary>
Motivation: Current video generation models perform well in realistic scenarios but struggle with imaginative prompts involving rarely co-occurring concepts and long-distance semantic relationships outside training distributions.

Method: Proposes ImagerySearch - a prompt-guided adaptive test-time search strategy that dynamically adjusts both inference search space and reward function based on semantic relationships in the prompt. Also introduces LDT-Bench benchmark for long-distance semantic prompts.

Result: Extensive experiments show ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench across diverse prompt types.

Conclusion: ImagerySearch effectively addresses the challenge of imaginative video generation by adapting search strategies to prompt semantics, with LDT-Bench providing a dedicated evaluation framework for future research in this direction.

Abstract: Video generation models have achieved remarkable progress, particularly
excelling in realistic scenarios; however, their performance degrades notably
in imaginative scenarios. These prompts often involve rarely co-occurring
concepts with long-distance semantic relationships, falling outside training
distributions. Existing methods typically apply test-time scaling for improving
video quality, but their fixed search spaces and static reward designs limit
adaptability to imaginative scenarios. To fill this gap, we propose
ImagerySearch, a prompt-guided adaptive test-time search strategy that
dynamically adjusts both the inference search space and reward function
according to semantic relationships in the prompt. This enables more coherent
and visually plausible videos in challenging imaginative settings. To evaluate
progress in this direction, we introduce LDT-Bench, the first dedicated
benchmark for long-distance semantic prompts, consisting of 2,839 diverse
concept pairs and an automated protocol for assessing creative generation
capabilities. Extensive experiments show that ImagerySearch consistently
outperforms strong video generation baselines and existing test-time scaling
approaches on LDT-Bench, and achieves competitive improvements on VBench,
demonstrating its effectiveness across diverse prompt types. We will release
LDT-Bench and code to facilitate future research on imaginative video
generation.

</details>


### [79] [A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation](https://arxiv.org/abs/2510.14855)
*Harsha Kotla,Arun Kumar Rajasekaran,Hannah Rana*

Main category: cs.CV

TL;DR: A deep learning framework that classifies skin lesions and quantifies ABCD features (Asymmetry, Border irregularity, Color variation, Diameter) while simulating evolution over time for the E feature, providing interpretable ML diagnoses linked to clinical criteria.


<details>
  <summary>Details</summary>
Motivation: Early melanoma detection is crucial for survival rates, but current deep learning methods treat skin lesion analysis as black boxes without explaining human-interpretable features like the ABCDE criteria used by dermatologists.

Method: Proposed framework classifies skin lesions and quantifies scores for each ABCD feature, simulates feature evolution over time for the E aspect, and visualizes ABCD feature trajectories in latent space as lesions evolve from benign to malignant.

Result: Achieved 89% classification accuracy with melanoma AUC of 0.96; feature evaluation performed well for asymmetry, color variation, and diameter prediction, though border irregularity remained more challenging to model.

Conclusion: The framework enables doctors to link ML diagnoses to clinically relevant ABCDE criteria, improving understanding of skin cancer progression and providing interpretable deep learning for dermatology.

Abstract: Early detection of melanoma has grown to be essential because it
significantly improves survival rates, but automated analysis of skin lesions
still remains challenging. ABCDE, which stands for Asymmetry, Border
irregularity, Color variation, Diameter, and Evolving, is a well-known
classification method for skin lesions, but most deep learning mechanisms treat
it as a black box, as most of the human interpretable features are not
explained. In this work, we propose a deep learning framework that both
classifies skin lesions into categories and also quantifies scores for each
ABCD feature. It simulates the evolution of these features over time in order
to represent the E aspect, opening more windows for future exploration. The A,
B, C, and D values are quantified particularly within this work. Moreover, this
framework also visualizes ABCD feature trajectories in latent space as skin
lesions evolve from benign nevuses to malignant melanoma. The experiments are
conducted using the HAM10000 dataset that contains around ten thousand images
of skin lesions of varying stages. In summary, the classification worked with
an accuracy of around 89 percent, with melanoma AUC being 0.96, while the
feature evaluation performed well in predicting asymmetry, color variation, and
diameter, though border irregularity remains more difficult to model. Overall,
this work provides a deep learning framework that will allow doctors to link ML
diagnoses to clinically relevant criteria, thus improving our understanding of
skin cancer progression.

</details>


### [80] [Multi-modal video data-pipelines for machine learning with minimal human supervision](https://arxiv.org/abs/2510.14862)
*Mihai-Cristian Pîrvu,Marius Leordeanu*

Main category: cs.CV

TL;DR: This paper presents a method to combine multiple visual modalities using minimal human supervision, leveraging pre-trained experts and procedural combinations on raw videos through an autonomous data-pipeline. The approach uses PHG-MAE for multi-modal learning and achieves competitive results with a highly efficient model (<1M parameters) compared to much larger models (~300M parameters).


<details>
  <summary>Details</summary>
Motivation: The real world is inherently multi-modal, but traditional ML models are unimodal and recent trends focus on bi-modality. To truly understand the world, the authors aim to integrate all independent modalities using minimal human supervision.

Method: The method uses pre-trained experts and procedural combinations between them on raw videos through a fully autonomous data-pipeline. It employs PHG-MAE, a model designed for multi-modal data, which is efficiently distilled into a low-parameter model (<1M parameters).

Result: The distilled model (<1M parameters) achieves competitive results compared to models with ~300M parameters. The approach is successfully deployed for real-time semantic segmentation from handheld devices or webcams on commodity hardware, and also supports other models like DPT for near real-time depth estimation.

Conclusion: The work demonstrates that efficient multi-modal integration is feasible with minimal human supervision, achieving competitive performance with significantly reduced model size, enabling practical deployment on commodity hardware for real-time applications.

Abstract: The real-world is inherently multi-modal at its core. Our tools observe and
take snapshots of it, in digital form, such as videos or sounds, however much
of it is lost. Similarly for actions and information passing between humans,
languages are used as a written form of communication. Traditionally, Machine
Learning models have been unimodal (i.e. rgb -> semantic or text ->
sentiment_class). Recent trends go towards bi-modality, where images and text
are learned together, however, in order to truly understand the world, we need
to integrate all these independent modalities. In this work we try to combine
as many visual modalities as we can using little to no human supervision. In
order to do this, we use pre-trained experts and procedural combinations
between them on top of raw videos using a fully autonomous data-pipeline, which
we also open-source. We then make use of PHG-MAE, a model specifically designed
to leverage multi-modal data. We show that this model which was efficiently
distilled into a low-parameter (<1M) can have competitive results compared to
models of ~300M parameters. We deploy this model and analyze the use-case of
real-time semantic segmentation from handheld devices or webcams on commodity
hardware. Finally, we deploy other off-the-shelf models using the same
framework, such as DPT for near real-time depth estimation.

</details>


### [81] [Benchmarking Multimodal Large Language Models for Face Recognition](https://arxiv.org/abs/2510.14866)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: This paper presents a systematic benchmark evaluating multimodal large language models (MLLMs) for face recognition, comparing them against specialized face recognition models on standard datasets.


<details>
  <summary>Details</summary>
Motivation: While MLLMs have shown strong performance in vision-language tasks, their potential in face recognition remains underexplored, particularly for open-source models that need evaluation against existing specialized models.

Method: The authors conducted systematic benchmarking of state-of-the-art MLLMs on multiple face recognition datasets including LFW, CALFW, CPLFW, CFP, AgeDB and RFW using zero-shot applications.

Result: Experimental results show that MLLMs capture rich semantic cues useful for face-related tasks, but they lag behind specialized face recognition models in high-precision recognition scenarios.

Conclusion: The benchmark provides a foundation for advancing MLLM-based face recognition and offers insights for designing next-generation models with higher accuracy and generalization.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable performance
across diverse vision-and-language tasks. However, their potential in face
recognition remains underexplored. In particular, the performance of
open-source MLLMs needs to be evaluated and compared with existing face
recognition models on standard benchmarks with similar protocol. In this work,
we present a systematic benchmark of state-of-the-art MLLMs for face
recognition on several face recognition datasets, including LFW, CALFW, CPLFW,
CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich
semantic cues useful for face-related tasks, they lag behind specialized models
in high-precision recognition scenarios in zero-shot applications. This
benchmark provides a foundation for advancing MLLM-based face recognition,
offering insights for the design of next-generation models with higher accuracy
and generalization. The source code of our benchmark is publicly available in
the project page.

</details>


### [82] [TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions](https://arxiv.org/abs/2510.14874)
*Guangyi Han,Wei Zhai,Yuhang Yang,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Introduces Free-Form HOI Generation for diverse hand-object interactions beyond grasping, using WildO2 dataset and TOUCH framework with multi-level diffusion model and contact modeling.


<details>
  <summary>Details</summary>
Motivation: Existing HOI generation is limited to fixed grasping patterns with strong inductive bias, failing to capture the diversity of daily hand-object interactions like pushing, poking, and rotating.

Method: Constructed WildO2 dataset with 4.4k unique interactions across 92 intents and 610 object categories. Proposed TOUCH framework with three-stage multi-level diffusion model using explicit contact modeling and physical constraint refinement.

Result: Comprehensive experiments demonstrate ability to generate controllable, diverse, and physically plausible hand interactions representative of daily activities.

Conclusion: The method successfully extends HOI from grasping to free-form interactions, enabling fine-grained semantic control for versatile hand poses beyond traditional grasping priors.

Abstract: Hand-object interaction (HOI) is fundamental for humans to express intent.
Existing HOI generation research is predominantly confined to fixed grasping
patterns, where control is tied to physical priors such as force closure or
generic intent instructions, even when expressed through elaborate language.
Such an overly general conditioning imposes a strong inductive bias for stable
grasps, thus failing to capture the diversity of daily HOI. To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating. To support this task, we construct WildO2, an in-the-wild
diverse 3D HOI dataset, which includes diverse HOI derived from internet
videos. Specifically, it contains 4.4k unique interactions across 92 intents
and 610 object categories, each with detailed semantic annotations. Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors. This process leverages
explicit contact modeling for conditioning and is subsequently refined with
contact consistency and physical constraints to ensure realism. Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.
The project page is $\href{https://guangyid.github.io/hoi123touch}{here}$.

</details>


### [83] [BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data](https://arxiv.org/abs/2510.14876)
*Roni Goldshmidt,Hamish Scott,Lorenzo Niccolini,Shizhan Zhu,Daniel Moura,Orly Zvitia*

Main category: cs.CV

TL;DR: BADAS is a family of collision prediction models that distinguishes ego-vehicle threats from random accidents, trained on Nexar's real-world dashcam collision dataset with ego-centric evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing collision prediction methods fail to distinguish between ego-vehicle threats and random accidents not involving the ego vehicle, leading to excessive false alerts in real-world deployment.

Method: BADAS uses a V-JEPA2 backbone trained end-to-end, with two variants: BADAS-Open (trained on 1.5k public videos) and BADAS1.0 (trained on 40k proprietary videos). The approach includes re-annotating benchmarks for ego involvement, adding consensus alert-time labels, and synthesizing negatives.

Result: BADAS achieves state-of-the-art AP/AUC across DAD, DADA-2000, DoTA, and Nexar datasets, outperforms forward-collision ADAS baseline, and produces more realistic time-to-accident estimates.

Conclusion: The authors release BADAS-Open model weights, code, and re-annotations of evaluation datasets to promote ego-centric collision prediction research.

Abstract: Existing collision prediction methods often fail to distinguish between
ego-vehicle threats and random accidents not involving the ego vehicle, leading
to excessive false alerts in real-world deployment. We present BADAS, a family
of collision prediction models trained on Nexar's real-world dashcam collision
dataset -- the first benchmark designed explicitly for ego-centric evaluation.
We re-annotate major benchmarks to identify ego involvement, add consensus
alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC
and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and
comes in two variants: BADAS-Open (trained on our 1.5k public videos) and
BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and
Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a
forward-collision ADAS baseline while producing more realistic time-to-accident
estimates. We release our BADAS-Open model weights and code, along with
re-annotations of all evaluation datasets to promote ego-centric collision
prediction research.

</details>


### [84] [ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention](https://arxiv.org/abs/2510.14882)
*Keli Liu,Zhendong Wang,Wengang Zhou,Shaodong Xu,Ruixiao Dong,Houqiang Li*

Main category: cs.CV

TL;DR: ScaleWeaver enables controllable text-to-image generation in visual autoregressive models through parameter-efficient fine-tuning with a novel Reference Attention module that reduces computational cost while maintaining precise control.


<details>
  <summary>Details</summary>
Motivation: Control mechanisms have been well-explored for diffusion models, but enabling precise and flexible control within the visual autoregressive (VAR) paradigm remains underexplored despite VAR models' recent advances in generation fidelity and efficiency.

Method: ScaleWeaver uses parameter-efficient fine-tuning with an improved MMDiT block containing a Reference Attention module that efficiently incorporates conditional information by discarding unnecessary image→condition attention, emphasizing parameter reuse, and using zero-initialized linear projection to preserve base model capabilities.

Result: Extensive experiments show ScaleWeaver delivers high-quality generation with precise control while achieving superior efficiency over diffusion-based methods.

Conclusion: ScaleWeaver provides a practical and effective solution for controllable text-to-image generation within the visual autoregressive paradigm, balancing quality, control precision, and computational efficiency.

Abstract: Text-to-image generation with visual autoregressive~(VAR) models has recently
achieved impressive advances in generation fidelity and inference efficiency.
While control mechanisms have been explored for diffusion models, enabling
precise and flexible control within VAR paradigm remains underexplored. To
bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel
framework designed to achieve high-fidelity, controllable generation upon
advanced VAR models through parameter-efficient fine-tuning. The core module in
ScaleWeaver is the improved MMDiT block with the proposed Reference Attention
module, which efficiently and effectively incorporates conditional information.
Different from MM Attention, the proposed Reference Attention module discards
the unnecessary attention from image$\rightarrow$condition, reducing
computational cost while stabilizing control injection. Besides, it
strategically emphasizes parameter reuse, leveraging the capability of the VAR
backbone itself with a few introduced parameters to process control
information, and equipping a zero-initialized linear projection to ensure that
control signals are incorporated effectively without disrupting the generative
capability of the base model. Extensive experiments show that ScaleWeaver
delivers high-quality generation and precise control while attaining superior
efficiency over diffusion-based methods, making ScaleWeaver a practical and
effective solution for controllable text-to-image generation within the visual
autoregressive paradigm. Code and models will be released.

</details>


### [85] [You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction](https://arxiv.org/abs/2510.14885)
*Logan Lawrence,Oindrila Saha,Megan Wei,Chen Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: The paper proposes nlg2choice, a two-stage method for evaluating MLLMs on fine-grained visual classification tasks with hundreds to thousands of choices, addressing challenges in free-form response evaluation and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods struggle with free-form responses from auto-regressive MLLMs, especially in fine-grained visual classification where choice counts are high (hundreds to thousands) and choices are highly related. Current approaches don't handle multiple choice questions beyond 5-way options well and face computational challenges in retrieval-based problems.

Method: nlg2choice uses a two-stage approach: first asking the MLLM an open-ended question with minimal constraints, then using text-only constrained decoding to predict the most likely choice. For retrieval settings, they compute probability of constrained responses with early stopping to improve throughput.

Result: The method shows improvement over seven fine-grained visual datasets in both classification and retrieval tasks, and maintains performance across various natural language task implementations used by LLM users.

Conclusion: nlg2choice provides an effective solution for evaluating MLLMs on fine-grained visual classification with large choice sets, addressing both evaluation challenges and computational efficiency concerns in retrieval-based settings.

Abstract: Despite the renewed interest in zero-shot visual classification due to the
rise of Multimodal Large Language Models (MLLMs), the problem of evaluating
free-form responses of auto-regressive models remains a persistent challenge.
Most existing works focus on language-only tasks or don't consider Multiple
Choice Questions (MCQs) beyond 5-way options, both of which are critical
capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where
choice counts are in the hundreds to thousands and the choices are highly
related. Furthermore, in this highly multi-way MCQ setting it is not clear how
to extend LLM choice extraction to retrieval-based problems, where computing
probabilities over the choice set is computationally costly. In this work we
investigate nlg2choice, a simple two-stage method which first asks the MLLM an
open-ended question for the task with minimal constraints, then uses text-only
constrained decoding to predict the most likely choice. In retrieval settings,
we compute the probability of the constrained response taking that choice with
an early stopping method to significantly improve throughput. Our results show
improvement over a suite of seven fine-grained visual datasets when evaluating
in terms of classification and retrieval, and show that this performance holds
over the various ways that users of LLMs can implement tasks in natural
language.

</details>


### [86] [Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection](https://arxiv.org/abs/2510.14896)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: A novel semi-supervised video anomaly detection framework using Multimodal Large Language Models to analyze object activities and interactions over time, providing explainability and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised VAD methods struggle with complex anomalies involving object interactions and lack explainability.

Method: Query MLLMs with visual inputs of object pairs at different moments to generate textual descriptions of activities and interactions from nominal videos, then compare test videos to these descriptions.

Result: Effectively detects complex interaction-based anomalies and achieves state-of-the-art performance on datasets without interaction anomalies.

Conclusion: The proposed MLLM-based framework provides inherent explainability, can be combined with traditional VAD methods, and demonstrates superior performance across various anomaly types.

Abstract: Existing semi-supervised video anomaly detection (VAD) methods often struggle
with detecting complex anomalies involving object interactions and generally
lack explainability. To overcome these limitations, we propose a novel VAD
framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous
MLLM-based approaches that make direct anomaly judgments at the frame level,
our method focuses on extracting and interpreting object activity and
interactions over time. By querying an MLLM with visual inputs of object pairs
at different moments, we generate textual descriptions of the activity and
interactions from nominal videos. These textual descriptions serve as a
high-level representation of the activity and interactions of objects in a
video. They are used to detect anomalies during test time by comparing them to
textual descriptions found in nominal training videos. Our approach inherently
provides explainability and can be combined with many traditional VAD methods
to further enhance their interpretability. Extensive experiments on benchmark
datasets demonstrate that our method not only detects complex interaction-based
anomalies effectively but also achieves state-of-the-art performance on
datasets without interaction anomalies.

</details>


### [87] [MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos](https://arxiv.org/abs/2510.14904)
*Gabriel Fiastre,Antoine Yang,Cordelia Schmid*

Main category: cs.CV

TL;DR: The paper proposes MaskCaptioner, an end-to-end model for Dense Video Object Captioning that jointly detects, tracks, segments, and captions object trajectories using synthetic captions generated from a VLM on extended datasets.


<details>
  <summary>Details</summary>
Motivation: Previous approaches use disjoint training strategies due to the complexity and high cost of manual annotation for DVOC, leading to suboptimal performance.

Method: Generate synthetic captions for spatio-temporally localized entities using a state-of-the-art VLM, extend LVIS and LV-VIS datasets with these captions (creating LVISCap and LV-VISCap), and train MaskCaptioner end-to-end on these datasets.

Result: MaskCaptioner achieves state-of-the-art DVOC results on three benchmarks: VidSTG, VLN, and BenSMOT after pretraining on LVISCap and LV-VISCap.

Conclusion: The proposed approach effectively addresses the annotation cost problem in DVOC through synthetic caption generation and enables end-to-end training, achieving superior performance on multiple benchmarks.

Abstract: Dense Video Object Captioning (DVOC) is the task of jointly detecting,
tracking, and captioning object trajectories in a video, requiring the ability
to understand spatio-temporal details and describe them in natural language.
Due to the complexity of the task and the high cost associated with manual
annotation, previous approaches resort to disjoint training strategies,
potentially leading to suboptimal performance. To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets
with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an
end-to-end model capable of jointly detecting, segmenting, tracking and
captioning object trajectories. Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are
available at https://www.gabriel.fiastre.fr/maskcaptioner/.

</details>


### [88] [3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation](https://arxiv.org/abs/2510.14945)
*JoungBin Lee,Jaewoo Jung,Jisang Han,Takuya Narihira,Kazumi Fukuda,Junyoung Seo,Sunghwan Hong,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: 3DScenePrompt is a framework for generating video chunks with precise camera control and scene consistency using dual spatio-temporal conditioning and a 3D scene memory that separates static geometry from dynamic elements.


<details>
  <summary>Details</summary>
Motivation: Existing methods conditioned on single images or short clips struggle with maintaining scene consistency and precise camera control when generating longer video sequences, especially when dealing with dynamic elements that should evolve naturally.

Method: Uses dual spatio-temporal conditioning: temporal conditioning on adjacent frames for motion continuity, and spatial conditioning through a 3D scene memory that represents static geometry extracted from input video using dynamic SLAM with dynamic masking. The static scene can be projected to any target viewpoint to provide geometrically consistent warped views as 3D spatial prompts.

Result: Significantly outperforms existing methods in scene consistency, camera controllability, and generation quality while maintaining computational efficiency and motion realism.

Conclusion: The framework successfully enables long-range spatial coherence and precise camera control in video generation by separating static scene geometry from dynamic elements, allowing dynamic regions to evolve naturally from temporal context.

Abstract: We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency. Unlike methods conditioned on a single image or a
short clip, we employ dual spatio-temporal conditioning that reformulates
context-view referencing across the input video. Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency. However, when generating beyond temporal
boundaries, directly using spatially adjacent frames would incorrectly preserve
dynamic elements from the past. We address this by introducing a 3D scene
memory that represents exclusively the static geometry extracted from the
entire input video. To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements. The static scene representation can then
be projected to any target viewpoint, providing geometrically consistent warped
views that serve as strong 3D spatial prompts while allowing dynamic regions to
evolve naturally from temporal context. This enables our model to maintain
long-range spatial coherence and precise camera control without sacrificing
computational efficiency or motion realism. Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality. Project page :
https://cvlab-kaist.github.io/3DScenePrompt/

</details>


### [89] [OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression](https://arxiv.org/abs/2510.14954)
*Zhe Li,Weihao Yuan,Weichao Shen,Siyu Zhu,Zilong Dong,Chang Xu*

Main category: cs.CV

TL;DR: A continuous masked autoregressive motion transformer with gated linear attention and RMSNorm for multi-modal human motion generation, outperforming previous methods in text-to-motion, speech-to-gesture, and music-to-dance tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in whole-body multi-modal human motion generation: creating effective motion generation mechanisms and integrating various modalities (text, speech, music) into a cohesive framework, overcoming limitations of discrete masked modeling and autoregressive approaches.

Method: Developed a continuous masked autoregressive motion transformer with causal attention for sequential motion nature. Introduced gated linear attention and RMSNorm modules to focus on key actions and suppress instability. Used DiT structure for diffusion-based condition propagation and leveraged AdaLN and cross-attention to fuse text, speech, and music modalities.

Result: Experimental results demonstrate that the framework outperforms previous methods across all modalities including text-to-motion, speech-to-gesture, and music-to-dance tasks.

Conclusion: The proposed continuous masked autoregressive motion transformer with multimodal fusion capabilities effectively addresses whole-body motion generation challenges and achieves state-of-the-art performance across multiple modalities.

Abstract: Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.

</details>


### [90] [RealDPO: Real or Not Real, that is the Preference](https://arxiv.org/abs/2510.14955)
*Guo Cheng,Danni Yang,Ziqi Huang,Jianlou Si,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: RealDPO introduces a novel alignment paradigm using real-world data as positive samples for preference learning to improve complex motion synthesis in video generation, addressing the gap between generated and real-world motions.


<details>
  <summary>Details</summary>
Motivation: Existing video generative models struggle with producing natural, smooth, and contextually consistent complex motions, limiting their practical applicability. The gap between generated and real-world motions needs to be addressed.

Method: RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function, using real-world videos as positive samples and contrasting them with erroneous model outputs for iterative self-correction. Also introduces RealAction-5K dataset for post-training.

Result: Extensive experiments show RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.

Conclusion: RealDPO effectively addresses the challenge of complex motion synthesis in video generation through its novel alignment paradigm using real-world data for preference learning.

Abstract: Video generative models have recently achieved notable advancements in
synthesis quality. However, generating complex motions remains a critical
challenge, as existing models often struggle to produce natural, smooth, and
contextually consistent movements. This gap between generated and real-world
motions limits their practical applicability. To address this issue, we
introduce RealDPO, a novel alignment paradigm that leverages real-world data as
positive samples for preference learning, enabling more accurate motion
synthesis. Unlike traditional supervised fine-tuning (SFT), which offers
limited corrective feedback, RealDPO employs Direct Preference Optimization
(DPO) with a tailored loss function to enhance motion realism. By contrasting
real-world videos with erroneous model outputs, RealDPO enables iterative
self-correction, progressively refining motion quality. To support
post-training in complex motion synthesis, we propose RealAction-5K, a curated
dataset of high-quality videos capturing human daily activities with rich and
precise motion details. Extensive experiments demonstrate that RealDPO
significantly improves video quality, text alignment, and motion realism
compared to state-of-the-art models and existing preference optimization
techniques.

</details>


### [91] [MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2510.14958)
*Weikang Shi,Aldrich Yu,Rongyao Fang,Houxing Ren,Ke Wang,Aojun Zhou,Changyao Tian,Xinyu Fu,Yuxuan Hu,Zimu Lu,Linjiang Huang,Si Liu,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: MathCanvas is a framework that enables Large Multimodal Models to perform visual chain-of-thought reasoning for mathematical problems through diagram generation and strategic visual-textual reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with mathematical domains like geometry that require visual aids, and existing VCoT approaches are limited by rigid tools or poor diagram quality.

Method: Two-phase approach: Visual Manipulation pre-training on 15.2M diagram pairs and editing trajectories, followed by Strategic Visual-Aided Reasoning fine-tuning on 219K interleaved visual-textual examples.

Result: BAGEL-Canvas model achieves 86% relative improvement over LMM baselines on MathCanvas-Bench and shows strong generalization to other math benchmarks.

Conclusion: MathCanvas provides a complete toolkit for unlocking complex visual-aided reasoning in LMMs, with framework, datasets, and benchmark for evaluation.

Abstract: While Large Language Models (LLMs) have excelled in textual reasoning, they
struggle with mathematical domains like geometry that intrinsically rely on
visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often
limited by rigid external tools or fail to generate the high-fidelity,
strategically-timed diagrams necessary for complex problem-solving. To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics. Our approach consists of two phases. First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.
Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids. To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions. Our model, BAGEL-Canvas, trained under this
framework, achieves an 86% relative improvement over strong LMM baselines on
MathCanvas-Bench, demonstrating excellent generalization to other public math
benchmarks. Our work provides a complete toolkit-framework, datasets, and
benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project
Page: https://mathcanvas.github.io/

</details>


### [92] [C4D: 4D Made from 3D through Dual Correspondences](https://arxiv.org/abs/2510.14960)
*Shizun Wang,Zhenxiang Jiang,Xingyi Yang,Xinchao Wang*

Main category: cs.CV

TL;DR: C4D is a framework that extends 3D reconstruction to 4D by leveraging temporal correspondences (optical flow and point tracking) to handle dynamic scenes in monocular videos.


<details>
  <summary>Details</summary>
Motivation: Existing pointmap-based 3D reconstruction methods fail for dynamic scenes because moving objects violate multi-view geometric constraints, leading to inaccurate results.

Method: C4D captures short-term optical flow and long-term point tracking, trains a dynamic-aware point tracker to separate moving elements from static background, and introduces dynamic scene optimization objectives to recover 3D geometry and camera parameters.

Result: The framework achieves complete 4D recovery and demonstrates strong performance in depth estimation, camera pose estimation, and point tracking tasks.

Conclusion: C4D successfully extends 3D reconstruction to dynamic 4D scenes by leveraging temporal correspondences and dynamic-aware optimization, enabling integrated 4D reconstruction from monocular video.

Abstract: Recovering 4D from monocular video, which jointly estimates dynamic geometry
and camera poses, is an inevitably challenging problem. While recent
pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great
progress in reconstructing static scenes, directly applying them to dynamic
scenes leads to inaccurate results. This discrepancy arises because moving
objects violate multi-view geometric constraints, disrupting the
reconstruction. To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D. Specifically, apart from predicting pointmaps, C4D captures two types of
correspondences: short-term optical flow and long-term point tracking. We train
a dynamic-aware point tracker that provides additional mobility information,
facilitating the estimation of motion masks to separate moving elements from
the static background, thus offering more reliable guidance for dynamic scenes.
Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters. Simultaneously, the
correspondences lift 2D trajectories into smooth 3D trajectories, enabling
fully integrated 4D reconstruction. Experiments show that our framework
achieves complete 4D recovery and demonstrates strong performance across
multiple downstream tasks, including depth estimation, camera pose estimation,
and point tracking. Project Page: https://littlepure2333.github.io/C4D

</details>


### [93] [RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion](https://arxiv.org/abs/2510.14962)
*Thao Nguyen,Jiaqi Ma,Fahad Shahbaz Khan,Souhaib Ben Taieb,Salman Khan*

Main category: cs.CV

TL;DR: A new diffusion-based precipitation nowcasting method that integrates token-wise attention into both U-Net and spatio-temporal encoder to capture multi-scale spatial interactions and temporal evolution without needing separate latent modules.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for precipitation nowcasting have scalability issues - latent-space approaches require separate autoencoders adding complexity, while pixel-space approaches are computationally intensive and lack attention mechanisms for long-range dependencies.

Method: Proposes token-wise attention integrated into U-Net diffusion model and spatio-temporal encoder to dynamically capture multi-scale spatial interactions and temporal evolution, eliminating need for separate latent modules.

Result: Extensive experiments show the method significantly outperforms state-of-the-art approaches, achieving superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.

Conclusion: The proposed token-wise attention integration provides an effective solution for precipitation nowcasting that balances computational efficiency with the ability to model complex spatio-temporal dependencies.

Abstract: Precipitation nowcasting, predicting future radar echo sequences from current
observations, is a critical yet challenging task due to the inherently chaotic
and tightly coupled spatio-temporal dynamics of the atmosphere. While recent
advances in diffusion-based models attempt to capture both large-scale motion
and fine-grained stochastic variability, they often suffer from scalability
issues: latent-space approaches require a separately trained autoencoder,
adding complexity and limiting generalization, while pixel-space approaches are
computationally intensive and often omit attention mechanisms, reducing their
ability to model long-range spatio-temporal dependencies. To address these
limitations, we propose a Token-wise Attention integrated into not only the
U-Net diffusion model but also the spatio-temporal encoder that dynamically
captures multi-scale spatial interactions and temporal evolution. Unlike prior
approaches, our method natively integrates attention into the architecture
without incurring the high resource cost typical of pixel-space diffusion,
thereby eliminating the need for separate latent modules. Our extensive
experiments and visual evaluations across diverse datasets demonstrate that the
proposed method significantly outperforms state-of-the-art approaches, yielding
superior local fidelity, generalization, and robustness in complex
precipitation forecasting scenarios.

</details>


### [94] [ChangingGrounding: 3D Visual Grounding in Changing Scenes](https://arxiv.org/abs/2510.14965)
*Miao Hu,Zhiwei Huang,Tai Wang,Jiangmiao Pang,Dahua Lin,Nanning Zheng,Runsen Xu*

Main category: cs.CV

TL;DR: The paper introduces ChangingGrounding, the first benchmark for 3D visual grounding in changing scenes, and proposes Mem-ChangingGrounder, a zero-shot method that uses memory-driven exploration and multi-view fusion for efficient object localization.


<details>
  <summary>Details</summary>
Motivation: Existing 3D visual grounding methods assume static, reconstructed point clouds, requiring costly re-scans that hinder real-world deployment. The authors argue for formulating 3DVG as an active, memory-driven problem to handle changing scenes.

Method: Proposes Mem-ChangingGrounder: identifies object type from query, retrieves relevant memories to guide actions, explores target efficiently, falls back when invalid, performs multi-view scanning, and fuses multi-view evidence to get accurate bounding boxes.

Result: Mem-ChangingGrounder achieves the highest localization accuracy on the ChangingGrounding benchmark while greatly reducing exploration cost compared to other baselines.

Conclusion: The benchmark and method aim to catalyze a shift toward practical, memory-centric 3DVG research for real-world applications where scenes constantly change.

Abstract: Real-world robots localize objects from natural-language instructions while
scenes around them keep changing. Yet most of the existing 3D visual grounding
(3DVG) method still assumes a reconstructed and up-to-date point cloud, an
assumption that forces costly re-scans and hinders deployment. We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes. To set a strong reference point,
we also propose Mem-ChangingGrounder, a zero-shot method for this task that
marries cross-modal retrieval with lightweight multi-view fusion: it identifies
the object type implied by the query, retrieves relevant memories to guide
actions, then explores the target efficiently in the scene, falls back when
previous operations are invalid, performs multi-view scanning of the target,
and projects the fused evidence from multi-view scans to get accurate object
bounding boxes. We evaluate different baselines on ChangingGrounding, and our
Mem-ChangingGrounder achieves the highest localization accuracy while greatly
reducing exploration cost. We hope this benchmark and method catalyze a shift
toward practical, memory-centric 3DVG research for real-world applications.
Project page: https://hm123450.github.io/CGB/ .

</details>


### [95] [WithAnyone: Towards Controllable and ID Consistent Image Generation](https://arxiv.org/abs/2510.14975)
*Hengyuan Xu,Wei Cheng,Peng Xing,Yixiao Fang,Shuhan Wu,Rui Wang,Xianfang Zeng,Daxin Jiang,Gang Yu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper addresses the copy-paste problem in identity-consistent image generation by introducing a large-scale dataset, a new benchmark, and a contrastive identity loss method, resulting in the WithAnyone model that reduces artifacts while maintaining identity fidelity.


<details>
  <summary>Details</summary>
Motivation: Current identity-consistent generation models suffer from copy-paste artifacts where they directly replicate reference faces instead of preserving identity across natural variations in pose, expression, or lighting, which limits controllability and expressive power.

Method: Constructed MultiID-2M dataset for multi-person scenarios, introduced a benchmark for quantifying copy-paste artifacts and fidelity-variation trade-off, and proposed a training paradigm with contrastive identity loss using paired data.

Result: WithAnyone model significantly reduces copy-paste artifacts, improves controllability over pose and expression, maintains strong perceptual quality, and achieves high identity fidelity with expressive controllable generation as validated by user studies.

Conclusion: The proposed approach effectively mitigates copy-paste problems in identity-consistent generation while preserving identity similarity, enabling more expressive and controllable image generation.

Abstract: Identity-consistent generation has become an important focus in text-to-image
research, with recent models achieving notable success in producing images
aligned with a reference identity. Yet, the scarcity of large-scale paired
datasets containing multiple images of the same individual forces most
approaches to adopt reconstruction-based training. This reliance often leads to
a failure mode we term copy-paste, where the model directly replicates the
reference face rather than preserving identity across natural variations in
pose, expression, or lighting. Such over-similarity undermines controllability
and limits the expressive power of generation. To address these limitations, we
(1) construct a large-scale paired dataset MultiID-2M, tailored for
multi-person scenarios, providing diverse references for each identity; (2)
introduce a benchmark that quantifies both copy-paste artifacts and the
trade-off between identity fidelity and variation; and (3) propose a novel
training paradigm with a contrastive identity loss that leverages paired data
to balance fidelity with diversity. These contributions culminate in
WithAnyone, a diffusion-based model that effectively mitigates copy-paste while
preserving high identity similarity. Extensive qualitative and quantitative
experiments demonstrate that WithAnyone significantly reduces copy-paste
artifacts, improves controllability over pose and expression, and maintains
strong perceptual quality. User studies further validate that our method
achieves high identity fidelity while enabling expressive controllable
generation.

</details>


### [96] [Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation](https://arxiv.org/abs/2510.14976)
*Shaowei Liu,Chuan Guo,Bing Zhou,Jian Wang*

Main category: cs.CV

TL;DR: Ponimator is a framework that uses close-proximity human-human interactive poses to generate versatile interaction animations through two conditional diffusion models: one for motion generation from poses, and another for pose synthesis from single poses or text.


<details>
  <summary>Details</summary>
Motivation: Human interactive poses convey rich contextual information about interaction dynamics, and humans can intuitively infer context and anticipate dynamics from such poses. The goal is to leverage these strong priors of human behavior for versatile interaction animation.

Method: Uses two conditional diffusion models: (1) pose animator that generates dynamic motion sequences from interactive poses using temporal prior, and (2) pose generator that synthesizes interactive poses from single pose, text, or both using spatial prior. Trained on close-contact two-person poses from motion-capture datasets.

Result: Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of the framework. Supports diverse tasks including image-based interaction animation, reaction animation, and text-to-interaction synthesis.

Conclusion: Ponimator successfully transfers interaction knowledge from high-quality mocap data to open-world scenarios, leveraging interactive pose priors for versatile animation generation across multiple applications.

Abstract: Close-proximity human-human interactive poses convey rich contextual
information about interaction dynamics. Given such poses, humans can
intuitively infer the context and anticipate possible past and future dynamics,
drawing on strong priors of human behavior. Inspired by this observation, we
propose Ponimator, a simple framework anchored on proximal interactive poses
for versatile interaction animation. Our training data consists of
close-contact two-person poses and their surrounding temporal context from
motion-capture interaction datasets. Leveraging interactive pose priors,
Ponimator employs two conditional diffusion models: (1) a pose animator that
uses the temporal prior to generate dynamic motion sequences from interactive
poses, and (2) a pose generator that applies the spatial prior to synthesize
interactive poses from a single pose, text, or both when interactive poses are
unavailable. Collectively, Ponimator supports diverse tasks, including
image-based interaction animation, reaction animation, and text-to-interaction
synthesis, facilitating the transfer of interaction knowledge from high-quality
mocap data to open-world scenarios. Empirical experiments across diverse
datasets and applications demonstrate the universality of the pose prior and
the effectiveness and robustness of our framework.

</details>


### [97] [Terra: Explorable Native 3D World Model with Point Latents](https://arxiv.org/abs/2510.14977)
*Yuanhui Huang,Weiliang Chen,Wenzhao Zheng,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Terra is a native 3D world model that represents and generates explorable environments using intrinsic 3D latent space, achieving state-of-the-art performance in reconstruction and generation with high 3D consistency.


<details>
  <summary>Details</summary>
Motivation: Most existing world models rely on pixel-aligned representations, neglecting the inherent 3D nature of the physical world, which undermines 3D consistency and modeling efficiency.

Method: Proposes a point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into latent point representation and decodes as 3D Gaussian primitives, plus a sparse point flow matching network (SPFlow) for generating latent point representation.

Result: Achieves state-of-the-art performance on ScanNet v2 indoor scenes in both reconstruction and generation with high 3D consistency, enables exact multi-view consistency and flexible rendering from any viewpoint.

Conclusion: Terra demonstrates that native 3D representation and architecture enable superior world modeling with exact 3D consistency and explorable environments through progressive generation.

Abstract: World models have garnered increasing attention for comprehensive modeling of
the real world. However, most existing methods still rely on pixel-aligned
representations as the basis for world evolution, neglecting the inherent 3D
nature of the physical world. This could undermine the 3D consistency and
diminish the modeling efficiency of world models. In this paper, we present
Terra, a native 3D world model that represents and generates explorable
environments in an intrinsic 3D latent space. Specifically, we propose a novel
point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into
a latent point representation, which is subsequently decoded as 3D Gaussian
primitives to jointly model geometry and appearance. We then introduce a sparse
point flow matching network (SPFlow) for generating the latent point
representation, which simultaneously denoises the positions and features of the
point latents. Our Terra enables exact multi-view consistency with native 3D
representation and architecture, and supports flexible rendering from any
viewpoint with only a single generation process. Furthermore, Terra achieves
explorable world modeling through progressive generation in the point latent
space. We conduct extensive experiments on the challenging indoor scenes from
ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction
and generation with high 3D consistency.

</details>


### [98] [Learning an Image Editing Model without Image Editing Pairs](https://arxiv.org/abs/2510.14978)
*Nupur Kumari,Sheng-Yu Wang,Nanxuan Zhao,Yotam Nitzan,Yuheng Li,Krishna Kumar Singh,Richard Zhang,Eli Shechtman,Jun-Yan Zhu,Xun Huang*

Main category: cs.CV

TL;DR: A new training paradigm for image editing that eliminates the need for paired data by using vision-language models (VLMs) to provide direct feedback for optimization, combined with distribution matching loss for visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Current image editing models rely on supervised fine-tuning with large datasets of input-target pairs, which are hard to curate at scale. Synthetic training pairs can propagate artifacts from pretrained models.

Method: Directly optimizes a few-step diffusion model by unrolling it during training and leveraging VLM feedback to evaluate if edits follow instructions and preserve unchanged content. Incorporates distribution matching loss (DMD) to maintain visual fidelity.

Result: Performs on par with various image editing diffusion models trained on extensive supervised paired data in the few-step setting. Outperforms RL-based techniques like Flow-GRPO when using the same VLM as reward model.

Conclusion: The method successfully eliminates the need for paired data in image editing model training while achieving competitive performance through direct VLM feedback optimization and distribution matching constraints.

Abstract: Recent image editing models have achieved impressive results while following
natural language editing instructions, but they rely on supervised fine-tuning
with large datasets of input-target pairs. This is a critical bottleneck, as
such naturally occurring pairs are hard to curate at scale. Current workarounds
use synthetic training pairs that leverage the zero-shot capabilities of
existing models. However, this can propagate and magnify the artifacts of the
pretrained model into the final trained model. In this work, we present a new
training paradigm that eliminates the need for paired data entirely. Our
approach directly optimizes a few-step diffusion model by unrolling it during
training and leveraging feedback from vision-language models (VLMs). For each
input and editing instruction, the VLM evaluates if an edit follows the
instruction and preserves unchanged content, providing direct gradients for
end-to-end optimization. To ensure visual fidelity, we incorporate distribution
matching loss (DMD), which constrains generated images to remain within the
image manifold learned by pretrained models. We evaluate our method on standard
benchmarks and include an extensive ablation study. Without any paired data,
our method performs on par with various image editing diffusion models trained
on extensive supervised paired data, under the few-step setting. Given the same
VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.

</details>


### [99] [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](https://arxiv.org/abs/2510.14979)
*Haiwen Diao,Mingxuan Li,Silei Wu,Linjun Dai,Xiaohua Wang,Hanming Deng,Lewei Lu,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces NEO, a family of native Vision-Language Models built from first principles that rival modular VLMs, addressing fundamental constraints and democratizing research through reusable components.


<details>
  <summary>Details</summary>
Motivation: To overcome fundamental constraints limiting native VLMs' exploration and make research more accessible by clarifying challenges and providing guiding principles for construction.

Method: Developed NEO family of native VLMs using three core principles: aligning pixel and word representations in shared semantic space, integrating vision and language module strengths, and embodying cross-modal properties for unified encoding, aligning, and reasoning.

Result: NEO achieves performance comparable to top-tier modular VLMs across diverse real-world scenarios using only 390M image-text examples, efficiently developing visual perception from scratch while mitigating vision-language conflicts.

Conclusion: NEO serves as a cornerstone for scalable native VLMs with reusable components that create a cost-effective and extensible ecosystem, advancing the field through democratized research.

Abstract: The edifice of native Vision-Language Models (VLMs) has emerged as a rising
contender to typical modular VLMs, shaped by evolving model architectures and
training paradigms. Yet, two lingering clouds cast shadows over its widespread
exploration and promotion: (-) What fundamental constraints set native VLMs
apart from modular ones, and to what extent can these barriers be overcome? (-)
How to make research in native VLMs more accessible and democratized, thereby
accelerating progress in the field. In this paper, we clarify these challenges
and outline guiding principles for constructing native VLMs. Specifically, one
native VLM primitive should: (i) effectively align pixel and word
representations within a shared semantic space; (ii) seamlessly integrate the
strengths of formerly separate vision and language modules; (iii) inherently
embody various cross-modal properties that support unified vision-language
encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios. With only 390M image-text
examples, NEO efficiently develops visual perception from scratch while
mitigating vision-language conflicts inside a dense and monolithic model
crafted from our elaborate primitives. We position NEO as a cornerstone for
scalable and powerful native VLMs, paired with a rich set of reusable
components that foster a cost-effective and extensible ecosystem. Our code and
models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.

</details>


### [100] [Coupled Diffusion Sampling for Training-Free Multi-View Image Editing](https://arxiv.org/abs/2510.14981)
*Hadi Alzayer,Yunzhi Zhang,Chen Geng,Jia-Bin Huang,Jiajun Wu*

Main category: cs.CV

TL;DR: A diffusion sampling method for multi-view consistent image editing using pre-trained 2D models, enforcing 3D consistency through coupled sampling trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing 2D image editing models produce high-quality but inconsistent edits across multiple views of 3D scenes, while 3D optimization methods are slow and unstable with sparse views.

Method: Coupled diffusion sampling that concurrently samples from multi-view image distribution and 2D edited image distribution, using a coupling term to enforce multi-view consistency.

Result: Validated on three multi-view image editing tasks, showing effectiveness across various model architectures.

Conclusion: The framework provides a general solution for multi-view consistent editing without lengthy optimization processes.

Abstract: We present an inference-time diffusion sampling method to perform multi-view
consistent image editing using pre-trained 2D image editing models. These
models can independently produce high-quality edits for each image in a set of
multi-view images of a 3D scene or object, but they do not maintain consistency
across views. Existing approaches typically address this by optimizing over
explicit 3D representations, but they suffer from a lengthy optimization
process and instability under sparse view settings. We propose an implicit 3D
regularization approach by constraining the generated 2D image sequences to
adhere to a pre-trained multi-view image distribution. This is achieved through
coupled diffusion sampling, a simple diffusion sampling technique that
concurrently samples two trajectories from both a multi-view image distribution
and a 2D edited image distribution, using a coupling term to enforce the
multi-view consistency among the generated images. We validate the
effectiveness and generality of this framework on three distinct multi-view
image editing tasks, demonstrating its applicability across various model
architectures and highlighting its potential as a general solution for
multi-view consistent editing.

</details>
