<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO is a zero-shot video restoration method that uses Video Consistency Models (VCMs) instead of frame-by-frame image diffusion models to achieve temporally consistent high-definition video reconstruction with high computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot image restoration methods using latent diffusion models (LDMs) produce temporally inconsistent results when applied frame-by-frame to video, due to their inability to capture temporal dependencies.

Method: Proposes LVTINO, which leverages Video Consistency Models (VCMs) that distill video diffusion models into fast generators with explicit temporal causality. Uses a conditioning mechanism that bypasses automatic differentiation and requires only a few neural function evaluations.

Result: Achieves state-of-the-art video reconstruction quality with strong measurement consistency and smooth temporal transitions. Shows significant perceptual improvements over frame-by-frame image LDM methods across diverse video inverse problems.

Conclusion: LVTINO establishes a new benchmark in both reconstruction fidelity and computational efficiency for zero-shot video restoration, demonstrating the superiority of video-specific priors over image-based approaches.

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: A method for fine-grained style-controlled image generation using style extraction from reference images and alignment with text representations.


<details>
  <summary>Details</summary>
Motivation: Fine-grained styles cannot be precisely described in natural language, and stylized reference images are difficult to align with textual conditions in traditional text-to-image generation.

Method: Three-stage training style extraction method using a style encoder and style projection layer to align style representations with text representations, trained on Style30k-captions dataset containing image-style-text triads.

Result: Enables fine-grained controlled stylized image generation by injecting style representations into pretrained generative models without changing their structural framework.

Conclusion: The proposed approach successfully achieves fine-grained style control in image generation by extracting and aligning style representations from reference images with text conditions.

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: This paper introduces EvoStruggle, a dataset for temporal struggle determination during skill acquisition, featuring 61.68 hours of video recordings from 76 participants performing 18 tasks across 4 activities, with struggle segments annotated.


<details>
  <summary>Details</summary>
Motivation: Existing manipulation datasets haven't focused on how struggle evolves over time during skill acquisition, which is crucial for optimizing human learning and developing effective assistive systems.

Method: Collected a dataset with 2,793 videos and 5,385 annotated temporal struggle segments from participants repeating tasks 5 times. Defined struggle determination as a temporal action localization task and evaluated models on unseen tasks/activities.

Result: Temporal Action Localization models successfully learned to detect struggle cues, achieving 34.56% mAP across tasks and 19.24% mAP across activities, showing struggle is transferable across skill-based tasks.

Conclusion: Struggle is a transferable concept across various skill-based tasks, though struggle detection still poses challenges for further improvement.

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS is a compact foundation model using lightweight residual U-Net architecture for solving diverse PDEs, achieving SOTA generalization with fewer parameters and minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing PDE foundation models use large transformer architectures with high computational overhead, while SPUS explores the underexplored potential of lightweight U-Net architectures for efficient PDE solving.

Method: Uses a lightweight residual U-Net architecture with auto-regressive pretraining strategy that replicates numerical solver behavior to learn underlying physics, pretrained on diverse fluid dynamics PDEs.

Result: Achieves state-of-the-art generalization on 6 challenging unseen downstream PDEs across various physical systems while requiring significantly fewer parameters and minimal fine-tuning data.

Conclusion: SPUS demonstrates potential as a highly parameter-efficient foundation model for solving diverse PDE systems, offering compact and efficient alternative to large transformer-based models.

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo is an RL-based framework that optimizes identity diversity in multi-human image generation by fine-tuning flow-matching models with a compositional reward system to prevent face duplication, identity merging, and miscounting.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with multi-human prompts, often duplicating faces, merging identities, and miscounting individuals, creating an identity crisis in generative models.

Method: Uses Group-Relative Policy Optimization (GRPO) with a compositional reward that penalizes intra-image facial similarity, discourages cross-sample identity repetition, enforces accurate person counts, and preserves visual fidelity through human preference scores. A single-stage curriculum stabilizes training as complexity scales.

Result: Achieves 98.6% Unique Face Accuracy and near-perfect Global Identity Spread on DiverseHumans Testset, surpassing both open-source and proprietary methods like Gemini and GPT-Image while maintaining competitive perceptual quality.

Conclusion: DisCo establishes a scalable, annotation-free solution that resolves the long-standing identity crisis in generative models and sets a new benchmark for compositional multi-human generation.

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: The paper presents a novel visual geo-localization method that aligns visual representations with hierarchical geographic embeddings, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve worldwide visual geo-localization by developing better learned representations of geography that can accurately determine geographic locations from visual content alone.

Method: Formulates geo-localization as aligning visual representations with hierarchical geographic embeddings, and introduces efficient fusion of appearance features with semantic segmentation maps for robust visual representations.

Result: Achieved improved all-time bests in 22 out of 25 metrics across five benchmark datasets, outperforming prior state-of-the-art methods and recent Large Vision-Language Models.

Conclusion: The combination of geographic and visual representations is the primary driver of performance gains in visual geo-localization tasks.

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS is a novel data selection method for Large Vision-Language Models that clusters examples based on cross-modal attention matrix trajectories to remove redundancy, achieving 50-85% data reduction while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Data-efficient learning for LVLMs is underexplored, with no existing methods outperforming random selection at different subset sizes.

Method: Clusters examples based on trajectories of top singular values of attention matrices from fine-tuning a small proxy LVLM, then samples balanced subsets from clusters.

Result: Discards 50% of LLaVA-665k and 85% of Vision-Flan datasets while preserving LLaVA-1.5-7B performance on 10 benchmarks, with 1.2x training speedup.

Conclusion: XMAS enables efficient LVLM training by effectively removing redundancy through principled data selection based on cross-modal attention patterns.

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception is a variational flow matching method for vector-quantized image generation that combines continuous transport dynamics with explicit categorical supervision over codebook indices.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between continuous flow matching methods (which lack discrete supervision) and categorical approaches (which lack geometric awareness), enabling uncertainty quantification and temperature-controlled generation.

Method: Adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space.

Result: Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores on ImageNet-1k 256x256 generation.

Conclusion: Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: Proposes a unified deep learning framework that simultaneously generates synthetic contrast-enhanced CT images from non-contrast CT scans and segments aortic lumen and thrombus, outperforming existing methods in both tasks.


<details>
  <summary>Details</summary>
Motivation: Reduce risks associated with iodinated contrast agents in CT scans (nephrotoxicity, allergies, environmental harm) by generating synthetic contrast-enhanced images, while addressing limitations of multi-stage pipelines that cause error accumulation.

Method: Unified framework integrating conditional diffusion models with multi-task learning, sharing encoder and decoder parameters across tasks, using semi-supervised training for missing labels, and enabling end-to-end joint optimization of image synthesis and segmentation.

Result: Achieved PSNR of 25.61 dB for image synthesis (vs 23.80 dB single-task), improved lumen Dice to 0.89 (from 0.87) and thrombus Dice to 0.53 (from 0.48), reduced lumen diameter MAE to 4.19 mm (from 5.78 mm) and thrombus area error to 33.85% (from 41.45%).

Conclusion: The proposed unified framework successfully reduces contrast agent use while improving both image synthesis quality and anatomical segmentation accuracy for abdominal aortic aneurysm assessment, demonstrating superior performance over existing approaches.

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: A framework for efficiently prototyping multi-modal content analysis pipelines that converts videos into temporal semi-structured data and queryable knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Multi-modal content analysis is complex, computationally expensive, and requires significant engineering effort. Existing pre-trained models on static data are available but challenging to fuse with complex data like videos.

Method: Develop a framework that crafts candidate recipes for pipelines using pre-trained models to convert videos into temporal semi-structured data, then translates this to frame-level indexed knowledge graphs that are queryable and support continual learning.

Result: The framework enables dynamic incorporation of new domain-specific knowledge through an interactive medium and supports efficient prototyping of multi-modal analysis pipelines.

Conclusion: The proposed framework successfully addresses the challenges of multi-modal video analysis by providing an efficient prototyping system that leverages pre-trained models and enables queryable knowledge representation with continual learning capabilities.

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT is a framework that reverse-engineers website functionality into reusable tools, enabling web agents to perform high-level operations like search, filter, and sort instead of fragile step-by-step UI interactions.


<details>
  <summary>Details</summary>
Motivation: Current web agents are brittle and rely on step-by-step UI interactions with heavy LLM reasoning, which breaks under dynamic layouts and long horizons. Humans use high-level website functionality that should be exploited.

Method: WALT reverse-engineers latent website functionality into reusable invocable tools that abstract away low-level execution. Agents call tools like search(query) or create(listing) instead of reasoning about clicks and typing.

Result: On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning compared to existing methods.

Conclusion: WALT establishes a robust and generalizable paradigm for browser automation by shifting computational burden from fragile step-by-step reasoning to reliable tool invocation.

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: A semi-supervised segmentation framework that uses topological consistency across multiple predictions to preserve meaningful structures in histopathology images.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of capturing meaningful semantic structures from unlabeled data in histopathology image analysis, where objects are densely distributed and distinguishing biological structures from noise is difficult.

Method: Leverages multiple perturbed predictions through stochastic dropouts and temporal training snapshots, enforcing topological consistency. Introduces a novel matching strategy that integrates spatial overlap with global structural alignment to match topological features across predictions without ground truth.

Result: Extensive experiments demonstrate effective reduction of topological errors, resulting in more robust and accurate segmentations essential for reliable downstream analysis.

Conclusion: The proposed approach successfully identifies and preserves relevant topological features in semi-supervised segmentation, particularly beneficial for histopathology image analysis with densely distributed objects.

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: Proposes Diffusion-LPO, a listwise preference optimization framework for diffusion models that extends DPO to handle ranked image lists, improving alignment with human preferences over pairwise methods.


<details>
  <summary>Details</summary>
Motivation: Current DPO applications for diffusion models rely on pairwise preferences, but human feedback often contains implicit ranked information that provides more precise preference data than pairwise comparisons.

Method: Aggregates user feedback into ranked image lists and derives a listwise extension of DPO objective under Plackett-Luce model, enforcing consistency across entire ranking by encouraging each sample to be preferred over lower-ranked alternatives.

Result: Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment across text-to-image generation, image editing, and personalized preference alignment tasks.

Conclusion: Listwise preference optimization with Diffusion-LPO provides more effective alignment with human preferences than pairwise methods for diffusion models.

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge is a pure autoregressive unified MLLM that enables both image understanding and generation within a single next-token prediction framework using a Mixture-of-Transformers architecture and semantic-to-pixel discrete representation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of building unified MLLMs that support both understanding and generation, overcoming trade-offs between semantic alignment and pixel-level fidelity in existing approaches.

Method: Augments pre-trained visual understanding models with generative ability through Mixture-of-Transformers architecture and semantic-to-pixel discrete representation combining compact semantic tokens with fine-grained pixel tokens.

Result: Achieves competitive or superior results in both understanding and generation benchmarks, with only 7.9% increase in sequence length, while requiring less training data and reduced training time.

Conclusion: Bridge successfully demonstrates that pure autoregressive unified MLLMs can achieve strong performance in both visual understanding and generation tasks with improved efficiency.

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: A hybrid CNN-Bayesian deep learning model for oral cancer classification using small datasets, achieving better generalizability than traditional CNNs through uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Oral cancer has high mortality rates, especially in underserved areas. Early diagnosis is crucial but challenging due to limited healthcare infrastructure and data. Traditional deep learning models are overconfident and require large datasets, which are unavailable in resource-limited settings.

Method: Proposed a hybrid model combining CNN with Bayesian deep learning using variational inference for uncertainty quantification. Trained on smartphone-captured photographic color images with small datasets.

Result: Achieved 94% accuracy on similar distribution test data (comparable to traditional CNN), and 88% accuracy on diverse real-world datasets vs 72.94% for traditional CNNs. Model showed appropriate uncertainty: low confidence for misclassified samples and high confidence for correct classifications.

Conclusion: Bayesian inference effectively enhances model reliability and generalizability in data-scarce environments, improving early oral cancer diagnosis by providing uncertainty quantification.

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: CADTrans is a source-free domain adaptation method that constructs consistent assistant domains to obtain invariable feature representations and uses conditional multi-kernel MMD to align hard samples with easy samples.


<details>
  <summary>Details</summary>
Motivation: Current SFDA methods struggle with hard samples and domain bias because they can't access source domain data to obtain deterministic invariable features.

Method: Uses assistant domain module to get diversified representations from intermediate global attentions, applies multiple consistent strategies to obtain invariable features, and employs conditional multi-kernel MMD to align hard samples with easy samples.

Result: Extensive experiments on Office-31, Office-Home, VISDA-C, and DomainNet-126 benchmarks show significant performance improvements.

Conclusion: CADTrans effectively addresses SFDA challenges by constructing domain-consistent invariable feature representations and properly handling hard samples through alignment strategies.

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: A system that uses historical BLV users' questions to guide MLLMs in generating contextually-relevant image descriptions, improving relevance and user preference over generic descriptions.


<details>
  <summary>Details</summary>
Motivation: Current MLLM applications provide comprehensive but often irrelevant descriptions for BLV users, leading to inefficient information exchange. Context-aware descriptions are needed to deliver specific, relevant information.

Method: Developed a system that identifies similar visual contexts from VizWiz-LF dataset and uses associated historical questions to guide MLLM description generation for BLV users.

Result: Context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70/92) and were preferred in 54.4% of comparisons (50/92) over context-free descriptions.

Conclusion: Using historical BLV user questions to guide MLLMs significantly improves the relevance and usefulness of image descriptions for blind and low vision users.

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: ImageNet-Think is a multimodal reasoning dataset built on 250,000 ImageNet21k images, featuring structured thinking tokens and answers generated by state-of-the-art VLMs to train and evaluate reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To develop Vision Language Models with explicit reasoning capabilities and contribute to understanding multimodal reasoning mechanisms.

Method: Synthetic dataset generation using two advanced VLMs (GLM-4.1V-9B-Thinking and Kimi-VL-A3B-Thinking-2506) on 250,000 ImageNet21k images, capturing step-by-step reasoning processes and final answers.

Result: Created a comprehensive dataset with two thinking-answer sequence pairs per image, providing structured reasoning tokens and corresponding descriptive answers.

Conclusion: The dataset will be publicly available to support research in developing more robust multimodal reasoning VLMs and advance understanding of reasoning mechanisms.

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: NPN is a novel regularization method that promotes solutions in low-dimensional projections of the sensing matrix's null-space using neural networks, improving reconstruction in imaging inverse problems.


<details>
  <summary>Details</summary>
Motivation: Traditional priors ignore task-specific null-space structure in imaging inverse problems, leading to ambiguity in solutions. NPN aims to capture information orthogonal to signal components that are blind to the sensing process.

Method: Proposes Non-Linear Projections of the Null-Space (NPN) - a neural network-based regularization that enforces solutions to lie in low-dimensional projections of the sensing matrix's null-space rather than using image-domain constraints.

Result: NPN priors consistently enhance reconstruction fidelity across diverse sensing matrices and imaging problems including compressive sensing, deblurring, super-resolution, CT, and MRI with various reconstruction frameworks.

Conclusion: NPN provides interpretable and flexible regularization that captures null-space structure, is compatible with existing methods, and offers theoretical guarantees for convergence and accuracy in plug-and-play approaches.

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: An automated genomic interpretation system that transforms DNA sequences into interpretable decisions using Chaos Game Representation and Concept Bottleneck Model, with enhanced reliability features and cost-aware recommendations for clinical automation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretable genomic modeling and automated decision-making for medical automation and robotic systems, providing reliable foundations for genomic medicine.

Method: Combines Chaos Game Representation (CGR) with Concept Bottleneck Model (CBM), incorporating concept fidelity supervision, prior consistency alignment, KL distribution matching, and uncertainty calibration. Includes cost-aware recommendation layer.

Result: Achieves state-of-the-art HIV subtype classification across in-house and LANL datasets, superior concept prediction fidelity, and favorable cost-benefit trade-offs compared to baselines.

Conclusion: Establishes a reliable foundation for robotic and clinical automation in genomic medicine by delivering interpretable evidence that can be validated against biological priors and reducing unnecessary retests through cost-aware decision policies.

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1 is a reasoning-enhanced Vision-Language-Action model that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to improve step-by-step reasoning and action generation in embodied AI tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLA models lack explicit step-by-step reasoning, emit final actions without considering affordance constraints or geometric relations, and have post-training pipelines that rarely reinforce reasoning quality with weak reward design.

Method: Integrates RLVR with GRPO for systematic optimization of reasoning and execution. Uses RLVR-based post-training with verifiable rewards for region alignment, trajectory consistency, and output formatting. Develops VLA-CoT-13K dataset with chain-of-thought supervision aligned with affordance and trajectory annotations.

Result: VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods across in-domain, out-of-domain, simulation, and real-robot platforms.

Conclusion: The proposed approach successfully addresses reasoning limitations in VLA models through verifiable reward optimization and high-quality chain-of-thought supervision, demonstrating improved performance and generalization in embodied AI tasks.

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: Proposes a joint deblurring and 3D reconstruction method for macrophotography that simultaneously optimizes clear 3D models and defocus blur kernels using differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: Macrophotography suffers from severe defocus blur that hinders clear imaging and high-quality 3D reconstruction, while traditional methods require extensive data and annotations.

Method: Uses differentiable rendering to jointly optimize clear 3D models and defocus blur kernels from multi-view blurry images in a self-supervised manner.

Result: Achieves high-quality image deblurring and recovers high-fidelity 3D appearance from a small number of multi-view images.

Conclusion: The proposed framework successfully addresses macrophotography defocus blur and enables joint deblurring and 3D reconstruction with minimal input data.

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff is a novel single-step diffusion model for high-fidelity motion deblurring that reformulates deblurring as a diffusion process and trains a consistency model to enable accurate one-step deblurring with superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current diffusion models for image deblurring, including unbearable inference time and compromised fidelity, while leveraging the strong generative capabilities of pre-trained diffusion models.

Method: Reformulates motion deblurring as a diffusion-like process with progressively blurred images, trains a consistency model aligning all timesteps to clean images, integrates Kernel ControlNet for blur kernel estimation, and introduces adaptive timestep prediction.

Result: Achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching state-of-the-art models while enabling one-step deblurring.

Conclusion: FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks and establishes a robust baseline for advancing diffusion models in real-world industrial applications.

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: This paper presents a two-stage pipeline for bronze inscription recognition that handles severe visual degradation, multi-domain variability, and long-tailed character distribution using a novel LadderMoE approach.


<details>
  <summary>Details</summary>
Motivation: Bronze inscriptions are crucial for archaeological studies but automatic recognition is challenging due to visual degradation, multi-domain variability (photographs, rubbings, tracings), and extremely long-tailed character distribution.

Method: A two-stage detection-recognition pipeline with LadderMoE that augments a pretrained CLIP encoder with ladder-style MoE adapters for dynamic expert specialization and robustness across heterogeneous domains and rare classes.

Result: The method substantially outperforms state-of-the-art scene text recognition baselines, achieving superior accuracy across head, mid, and tail categories as well as all acquisition modalities (photographs, rubbings, tracings).

Conclusion: The approach establishes a strong foundation for bronze inscription recognition and downstream archaeological analysis, effectively handling the unique challenges of this domain.

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA proposes visual reprogramming for UDA, adding domain-specific visual prompts to input images instead of fine-tuning backbone parameters, enabling parameter-efficient domain adaptation with reusable backbones.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods require fine-tuning backbone parameters for each new source-target pair, leading to linear growth in parameters and storage, and preventing backbone reuse.

Method: Prepends domain-specific visual reprogramming layer to backbone that produces visual prompts as textural bias. Uses multiple objective functions to optimize intra- and inter-domain distribution differences without modifying backbone parameters.

Result: Achieves 92.8% mean accuracy on Office-31 with only 1.5M trainable parameters. Outperforms PDA by +1.6% accuracy using 46% of parameters. Beats full-backbone fine-tuning methods CDTrans and FixBi while using only 1.7% and 2.8% of their parameters.

Conclusion: VirDA enables parameter-efficient domain adaptation through visual reprogramming, allowing backbone reuse across domains while achieving competitive performance with significantly fewer parameters.

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: DFE is an unsupervised facial expression encoding method using RVQ-VAE that outperforms FACS in psychological tasks like stress detection, personality prediction, and depression detection.


<details>
  <summary>Details</summary>
Motivation: Existing facial expression coding systems like FACS have limited coverage and require costly manual annotation, creating a need for scalable, data-driven alternatives.

Method: Extracts identity-invariant expression features using 3DMM, then encodes them with Residual Vector Quantized VAE to produce discrete tokens representing reusable facial deformation patterns.

Result: DFE captures more precise facial behaviors than FACS and outperforms both FACS-based pipelines and state-of-the-art representation learning models across psychological tasks.

Conclusion: DFE serves as a scalable and effective alternative to FACS for psychological and affective computing applications, covering a wider variety of facial displays.

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: Con-NRSfM is a novel method for non-rigid structure-from-motion under conformal deformations that performs point-wise reconstruction using optimized 2D image warps in a graph-based framework, accurately computing local conformal scale and enabling precise depth estimation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing NRSfM methods that rely on strict assumptions like locally planar surfaces or locally linear deformations, and fail to recover conformal scale, while improving monocular visual deformable SLAM.

Method: Uses point-wise reconstruction with 2D selected image warps optimized through graph-based framework, employs parallel separable iterative optimization strategy, and incorporates self-supervised learning with encoder-decoder network for dense 3D point clouds.

Result: Outperforms existing approaches in reconstruction accuracy and robustness on both synthetic and real datasets, demonstrating superior performance in conformal scale computation and depth estimation.

Conclusion: The proposed Con-NRSfM method effectively eliminates restrictive assumptions of previous approaches, accurately computes local conformal scale, and enables more precise depth estimation, making it a superior solution for NRSfM under conformal deformations.

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse is a unified framework that decouples robust 3D reconstruction from inconsistent multi-view images into restoration and reconstruction subtasks, using a video diffusion model to restore image consistency before reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for robust 3D reconstruction from inconsistent multi-view images rely heavily on dense observations and complex optimization. The paper aims to address this limitation by simplifying the optimization process.

Method: Proposes UniVerse framework that first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs 3D scenes from the restored images.

Result: Extensive experiments on synthetic and real-world datasets demonstrate strong generalization capability and superior performance in robust reconstruction. The method can also control the style of reconstructed 3D scenes.

Conclusion: UniVerse effectively addresses robust 3D reconstruction from inconsistent multi-view images by leveraging diffusion models' scene priors, outperforming existing methods while enabling style control in reconstructed scenes.

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: A lightweight end-to-end template matching framework that jointly estimates position, rotation, and scaling for industrial applications, achieving high precision with 14ms inference time.


<details>
  <summary>Details</summary>
Motivation: Traditional template matching methods are inefficient under compound transformations, while deep learning approaches lack explicit geometric pose modeling, making them unsuitable for real-world industrial deployment.

Method: Proposes a lightweight framework with Template-Aware Dynamic Convolution Module (TDCM) that injects template features dynamically, uses depthwise separable convolutions and pixel shuffle for efficiency, and employs rotation-shear augmentation with pseudo labels for geometric-annotation-free training.

Result: The 3.07M parameter model achieves high precision with 14ms inference time under compound transformations, and demonstrates strong robustness in small-template and multi-object scenarios.

Conclusion: The proposed method is highly suitable for real-time industrial applications, offering efficient and precise template matching with explicit geometric pose estimation.

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: A framework for adaptive pixel reasoning in Vision-Language Models that dynamically determines when to use pixel-level operations based on query difficulty, achieving better performance while reducing unnecessary visual operations.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with fine-grained visual understanding due to information loss during image encoding and insufficient attention to critical regions. Current pixel-level approaches are inefficient and get distracted by irrelevant details.

Method: Operation-aware supervised fine-tuning followed by rollout-guided reinforcement learning that uses the model's own response feedback to determine when to invoke pixel operations based on query difficulty.

Result: Achieved 73.4% accuracy on HR-Bench 4K with only 20.1% tool usage ratio, improving accuracy while reducing tool usage by 66.5% compared to previous methods.

Conclusion: The proposed adaptive pixel reasoning framework enables VLMs to achieve superior performance on multimodal reasoning tasks while significantly reducing unnecessary visual operations through dynamic pixel operation invocation.

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: Proposes ASRS framework using augmentation sensitivity to detect error-prone chest X-ray cases, improving fairness and reliability in medical AI.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for chest X-ray interpretation show uneven accuracy across patient subgroups and hidden failures not captured by aggregate metrics, while existing error detection methods struggle with subtle within-distribution errors.

Method: ASRS applies clinically plausible rotations (±15°/±30°) and measures embedding shifts using RAD-DINO encoder to compute sensitivity scores that stratify samples into stability quartiles.

Result: Highly sensitive cases show substantially lower recall (-0.2 to -0.3) despite high AUROC and confidence, demonstrating ASRS effectively identifies error-prone cases.

Conclusion: ASRS provides a label-free method for selective prediction and clinician review, improving fairness and safety in medical AI applications.

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS is a training-free video stylization framework that generates stylized videos with rich style details and strong temporal coherence by integrating multiple stylized references into a pretrained image-to-video model.


<details>
  <summary>Details</summary>
Motivation: Video stylization is challenging because frame-by-frame image stylization hurts temporal consistency and reduces style richness, while training dedicated video models requires paired data and is computationally expensive.

Method: Integrates multiple stylized references to a pretrained I2V model, uses high-frequency compensation to constrain content layout and motion, and employs flow-based motion cues to preserve style textures in low-saliency regions.

Result: FreeViS delivers higher stylization fidelity and superior temporal consistency, outperforming recent baselines and achieving strong human preference without introducing flickers and stutters.

Conclusion: The training-free pipeline offers a practical and economic solution for high-quality, temporally coherent video stylization.

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench is a comprehensive benchmark that establishes a perception-reasoning paradigm for evaluating medical image quality using Multi-modal Large Language Models (MLLMs), covering two complementary tasks across five imaging modalities with over 3,300 assessments.


<details>
  <summary>Details</summary>
Motivation: Existing medical image quality assessment approaches are constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation, creating a gap in clinical AI safety.

Method: MedQ-Bench defines two tasks: MedQ-Perception (low-level perceptual capability via human-curated questions) and MedQ-Reasoning (no-reference and comparison reasoning tasks). The benchmark spans five imaging modalities, over forty quality attributes, and includes 2,600 perceptual queries and 708 reasoning assessments from diverse sources including clinical acquisitions, simulated degradations, and AI-generated images.

Result: Evaluation of 14 state-of-the-art MLLMs shows that models exhibit preliminary but unstable perceptual and reasoning skills with insufficient accuracy for reliable clinical use. Human-AI alignment validation confirms these limitations.

Conclusion: Current MLLMs need targeted optimization for medical image quality assessment, and MedQ-Bench is positioned to catalyze further exploration and unlock the untapped potential of MLLMs in this domain.

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer is a network that predicts full occlusion and depth orderings for all instances in a scene from a single RGB image in one forward pass, using interactions between object queries and latent mask descriptors.


<details>
  <summary>Details</summary>
Motivation: Understanding instance-wise geometries is challenging for visual models, and existing methods require expensive inputs (category labels, segmentation masks) and have high inference costs (quadratic forward passes).

Method: InstaFormer uses interactions between object queries and latent mask descriptors that semantically represent the same objects while carrying complementary information for holistic order prediction.

Result: The approach is comprehensively benchmarked and ablated to demonstrate its effectiveness in predicting occlusion and depth orderings.

Conclusion: InstaFormer mitigates limitations of existing methods by providing full instance order prediction from a single RGB image with efficient single forward pass inference.

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler is a transformer framework with Pyramidal Positional Encoding that improves neural style transfer by capturing multi-scale details and using reinforcement learning for optimization.


<details>
  <summary>Details</summary>
Motivation: Existing CNN and transformer-based NST models struggle with scaling to complex styles and high-resolution inputs efficiently.

Method: Transformer framework with Pyramidal Positional Encoding (hierarchical multi-scale encoding) and reinforcement learning for dynamic optimization.

Result: Reduced content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs with 1.39s inference time. With RL: content loss 2.03, style loss 0.75, inference 1.40s.

Conclusion: Achieves real-time, high-quality artistic rendering with broad applications in media and design.

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS is a load-balanced and efficient 3D Gaussian Splatting framework that addresses scalability issues in large scenes through depth-aware partitioning, optimization-based load balancing, and lightweight optimization techniques.


<details>
  <summary>Details</summary>
Motivation: Scaling 3D Gaussian Splatting to large unbounded scenes like city blocks is challenging due to memory constraints and inefficient partitioning methods that cause load imbalance and high overhead in existing approaches.

Method: Introduces depth-aware partitioning for fast preprocessing, optimization-based strategy to balance visible Gaussians across blocks, and lightweight techniques including visibility cropping and selective densification to reduce training costs.

Result: Achieves up to 2x faster end-to-end training time compared to state-of-the-art baselines while maintaining reconstruction quality and enabling scalability to scenes that are infeasible with vanilla 3DGS.

Conclusion: LoBE-GS successfully addresses the scalability limitations of 3D Gaussian Splatting for large scenes through efficient load balancing and optimization techniques, making large-scale urban reconstruction practical.

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: Proposes MemoryPack for dynamic context modeling and Direct Forcing to mitigate error accumulation in long-form video generation, achieving minute-level temporal consistency with linear complexity.


<details>
  <summary>Details</summary>
Motivation: Long-form video generation faces challenges in capturing long-range dependencies and preventing error accumulation in autoregressive decoding.

Method: MemoryPack: learnable context-retrieval mechanism using textual and image information for joint short- and long-term dependency modeling. Direct Forcing: single-step approximating strategy to improve training-inference alignment.

Result: Achieves minute-level temporal consistency, scales gracefully with video length, preserves computational efficiency with linear complexity, and reduces error propagation.

Conclusion: MemoryPack and Direct Forcing substantially enhance context consistency and reliability of long-form video generation, advancing practical usability of autoregressive video models.

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: This paper proposes methods for confidence calibration in 3D object detectors, focusing on both dominant and secondary class predictions through auxiliary loss terms and post-hoc calibration techniques.


<details>
  <summary>Details</summary>
Motivation: Precise object detection and uncertainty estimation are critical for safe autonomous systems, requiring proper calibration of confidence distributions across all classes in 3D object detectors.

Method: Proposed two auxiliary regularizing loss terms for calibrating either the dominant prediction or full prediction vector, and evaluated post-hoc and train-time methods including isotonic regression on CenterPoint, PillarNet and DSVT-Pillar detectors.

Result: Combining the full class prediction calibration loss term with isotonic regression achieved the best calibration for CenterPoint and PillarNet for both dominant and secondary predictions, but DSVT-Pillar could not be jointly calibrated using the same method.

Conclusion: Different 3D object detectors require tailored calibration approaches, with the proposed full class prediction regularization combined with isotonic regression being effective for some architectures but not universally applicable.

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS is a novel person search framework that leverages pre-trained diffusion models to address optimization conflicts between detection and re-identification tasks, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods use ImageNet pre-trained backbones that are suboptimal for capturing spatial context and fine-grained identity cues, and suffer from conflicting optimization objectives when sharing backbone features between detection and re-identification tasks.

Method: Proposes three specialized modules: Diffusion-Guided Region Proposal Network (DGRPN) for person localization, Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features.

Result: Sets new state-of-the-art performance on CUHK-SYSU and PRW datasets.

Conclusion: DiffPS effectively leverages diffusion prior knowledge to eliminate optimization conflicts and improve person search performance through specialized modules that enhance localization and feature representation.

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FMU integrates flow matching into HSI reconstruction using a deep unfolding framework with mean velocity loss for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imaging is costly and challenging to reconstruct from compressed measurements, with existing methods suffering from degradation and loss of spectral details.

Method: Proposes Flow-Matching-guided Unfolding network (FMU) that embeds flow matching generative prior within deep unfolding framework, enhanced with mean velocity loss for global consistency.

Result: Extensive experiments on simulated and real datasets show FMU significantly outperforms existing approaches in reconstruction quality.

Conclusion: FMU successfully combines optimization-based interpretability with flow matching generative capacity for robust HSI reconstruction.

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: Automated defect detection system for DIP components using deep learning and ConSinGAN data augmentation, with YOLOv7 achieving 95.50% accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional defect detection is time-consuming and labor-intensive, creating burden on quality inspection personnel and making quality management difficult.

Method: Uses digital camera optics and deep learning models (YOLOv3, v4, v7, v9) with ConSinGAN for data augmentation to address lack of defective component images. SCADA system with sensor architecture is also developed.

Result: YOLOv7 with ConSinGAN achieved 95.50% accuracy with 285ms detection time, superior to other YOLO versions and threshold-based approaches.

Conclusion: The proposed automated defect detection system can be easily established for various defect types and works well even with insufficient defect data.

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: FoundAD is a few-shot anomaly detection method that uses foundation visual encoders and learns a nonlinear projection operator to identify anomalies by detecting out-of-distribution regions in images.


<details>
  <summary>Details</summary>
Motivation: Few-shot anomaly detection faces challenges in differentiating normal and abnormal features with limited samples, especially under category-agnostic conditions. Foundation visual encoders can help learn general distributions of normal images.

Method: Learns a nonlinear projection operator onto the natural image manifold using foundation visual encoders. The operator detects anomalies by identifying out-of-distribution regions based on embedding differences.

Result: Achieves competitive performance in multi-class detection while using substantially fewer parameters than prior methods. Validated with multiple foundation encoders including DINOv3.

Conclusion: The approach broadens perspective on foundation features and advances few-shot anomaly detection by effectively leveraging pre-trained encoders for anomaly identification.

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT reduces Vision Transformer computational complexity for semantic segmentation by merging similar tokens using a trainable Cluster module and restoring details with a Regenerator module, achieving significant speedup with comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have high accuracy but quadratic attention complexity limits their practical use in real-world robotic systems. Token merging works for classification but is less suitable for dense prediction tasks like semantic segmentation.

Method: Expands ViT backbone with a trainable Cluster module that merges similar tokens guided by pseudo-clusters from segmentation masks, followed by a Regenerator module to restore fine details for downstream heads.

Result: Achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three datasets while maintaining comparable segmentation accuracy.

Conclusion: ClustViT effectively addresses computational complexity in Vision Transformers for semantic segmentation through token clustering and regeneration, enabling practical deployment in robotic systems.

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT introduces a unified paradigm for MLLMs to directly generate both textual and visual outputs using Visual Reference Tokens (VRTs) interleaved with text tokens, enabling dense prediction tasks like detection and segmentation without relying on indirect coordinate representations.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM approaches for vision tasks rely on indirect representations like generating coordinates as text, which limits performance and prevents dense prediction tasks such as segmentation.

Method: Proposes Patch-as-Decodable Token (PaDT) with Visual Reference Tokens (VRTs) derived from visual patch embeddings, interleaved with LLM's textual tokens. Uses lightweight decoder for predictions and dynamic embedding table expansion. Includes training strategy with random VRT selection and per-token cross-entropy loss.

Result: Achieves state-of-the-art performance across four visual perception and understanding tasks, outperforming significantly larger MLLM models.

Conclusion: PaDT provides a unified framework that enables MLLMs to directly generate diverse visual outputs, overcoming limitations of indirect representations and enabling dense prediction tasks while maintaining strong performance.

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: This paper addresses the trust deficit in online fruit/vegetable e-commerce by proposing a 'Trust Pyramid' model and 'Triangular Trust Index' to quantify quality trade-offs. It introduces TriAlignXA, an explainable AI framework that transforms algorithms from decision-makers to transparent decision-support tools, balancing biological characteristics, timeliness, and economic viability.


<details>
  <summary>Details</summary>
Motivation: The 'trust deficit' in online fruit and vegetable e-commerce arises from the inability of digital transactions to provide direct sensory perception of product quality, creating a fundamental barrier to consumer trust in online produce purchasing.

Method: The study constructs a 'Trust Pyramid' model through 'dual-source verification' and proposes the 'Triangular Trust Index' (TTI) to quantify trade-offs. It designs the TriAlignXA explainable AI framework with three engines: Bio-Adaptive Engine for quality description, Timeliness Optimization Engine for efficiency, and Economic Optimization Engine for cost control. A 'Pre-Mapping Mechanism' encodes process data into QR codes for transparency.

Result: Experiments on grading tasks demonstrate significantly higher accuracy than baseline models. Empirical evidence and theoretical analysis verify the framework's capability to balance the 'impossible triangle' of agricultural product grading constraints.

Conclusion: The research provides comprehensive support from theory to practice for building trustworthy online produce ecosystems, establishing a critical pathway from algorithmic decision-making to consumer trust through transparent, explainable AI systems.

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft is a consistent and interactive 4D Gaussian Splatting editing framework that addresses view, temporal, and non-editing region consistency issues while handling complex text instructions through LLM-based intent understanding.


<details>
  <summary>Details</summary>
Motivation: Recent 4DGS editing methods face challenges with view, temporal, and non-editing region consistency, as well as difficulty handling complex text instructions, limiting their practical usability.

Method: Proposes a 4D-aware InstructPix2Pix model with 4D VGGT geometry features, multi-view grid module for iterative refinement, Gaussian selection mechanism for non-edited region preservation, and LLM-based module for user intent understanding and instruction decomposition.

Result: The framework enables more consistent and controllable 4D scene editing compared to related works, with improved handling of complex user commands through logical decomposition of instructions.

Conclusion: 4DGS-Craft provides a comprehensive solution for consistent and interactive 4DGS editing, addressing key limitations in current methods through geometric feature integration, consistency mechanisms, and intelligent user intent interpretation.

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: Pure-Pass (PP) is a pixel-level masking mechanism that identifies pure pixels to exempt them from expensive computations in image super-resolution, improving efficiency and performance over previous methods like CAMixer.


<details>
  <summary>Details</summary>
Motivation: Existing lightweight SR methods like CAMixer have limitations including poor adaptability, coarse-grained masking, and spatial inflexibility, which hinder practical deployment due to computational complexity.

Method: PP uses fixed color center points to classify pixels into distinct categories, enabling fine-grained pixel-level masking that exempts pure pixels from expensive computations while maintaining adaptive flexibility.

Result: When integrated into ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency while saving similar computation.

Conclusion: Pure-Pass provides an effective pixel-level masking approach that enables fine-grained, spatially flexible computation savings in image super-resolution while maintaining high reconstruction quality.

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: The paper proposes a Self-correction Loop with Structured Output (SLSO) framework using GPT-4o to automatically generate jaw cyst findings on dental panoramic radiographs, showing improved accuracy over conventional methods.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of automated jaw cyst findings generation on dental radiographs by addressing inconsistencies and hallucinations in current AI systems.

Method: Developed a 10-step SLSO framework with iterative regeneration for consistency checking, using GPT-4o's multimodal capabilities. Compared against conventional Chain-of-Thought method across seven evaluation criteria.

Result: SLSO improved accuracy by 66.9% for tooth number identification, 33.3% for tooth movement, and 28.6% for root resorption. Successful cases achieved structured output after up to five regenerations, with reduced hallucinations and better negative finding descriptions.

Conclusion: The SLSO framework shows promise for automated dental finding generation but requires further refinement, especially for large lesions spanning multiple teeth, to become a practical clinical system.

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: LiLa-Net is a 3D autoencoder that efficiently encodes LiDAR point clouds from traffic environments using simplified skip connections and fewer encoder layers, achieving accurate reconstruction and strong generalization.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient 3D autoencoder for LiDAR point clouds from real traffic environments that uses minimal resources while maintaining high performance compared to state-of-the-art architectures.

Method: Proposed LiLa-Net with reduced encoder layers and simplified skip connections to create an efficient latent space for point cloud reconstruction, balancing information from skip connections and latent encoding.

Result: Achieved accurate reconstruction of original point clouds with improved reconstruction quality, strong generalization capabilities for objects beyond traffic environments, and efficient performance without extensive resource usage.

Conclusion: LiLa-Net demonstrates that simplified skip connections and reduced encoder layers can create an efficient 3D autoencoder for LiDAR point clouds that maintains high reconstruction accuracy and generalization while using fewer resources.

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: kabr-tools is an open-source package for automated multi-species behavioral monitoring using drones and machine learning, enabling scalable analysis of wildlife behavior with improved granularity compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional field observations are limited in scope, time-consuming, and labor-intensive, hindering comprehensive assessment of behavioral responses across landscapes.

Method: Integration of drone-based video with machine learning systems including object detection, tracking, and behavioral classification to extract behavioral, social, and spatial metrics from wildlife footage.

Result: Drone-based observations reduced visibility loss by 15% and captured more transitions with higher accuracy. Case studies analyzed 969 behavioral sequences, revealing species-specific behavioral patterns and spatial segregation in mixed-species herds.

Conclusion: kabr-tools enables automated behavioral monitoring at scale, offering a powerful tool for ecosystem-wide studies that advances conservation, biodiversity research, and ecological monitoring.

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing is a novel framework for semantic-aware 3D shape and texture morphing from multi-view images using mesh-guided 3D Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of previous approaches that rely on point clouds or require pre-defined homeomorphic mappings for untextured data.

Method: Leverages mesh-guided 3D Gaussian Splatting with unified deformation strategy that anchors 3D Gaussians to mesh patches, using mesh topology as geometric prior for unsupervised semantic correspondence.

Result: Outperforms prior 2D/3D methods on TexMorph benchmark, reducing color consistency error (ΔE) by 22.2% and EI by 26.2%.

Conclusion: The framework enables geometrically consistent transformations while preserving texture fidelity through topology-aware constraints, maintaining both local detail and global semantic coherence without requiring labeled data.

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: InPose: A zero-shot generalization method for pose estimation using pre-trained diffusion models conditioned on rotational measurements alone, with location measurements used to guide the likelihood term.


<details>
  <summary>Details</summary>
Motivation: Existing pose estimation methods using conditional diffusion models generalize poorly across users because location measurements are highly influenced by body size, limiting practical applications with limited sensors.

Method: Formulate pose estimation as an inverse problem using a pre-trained diffusion model conditioned only on rotational measurements, with location measurements used to derive a likelihood term that guides the pose generation.

Result: The proposed InPose method can generatively estimate highly likely pose sequences that best explain sparse on-body measurements for any user.

Conclusion: InPose enables zero-shot generalization across users by separating rotational conditioning from body-size-dependent location measurements in the pose estimation framework.

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: VGDM is a transformer-driven diffusion model for brain tumor segmentation that combines vision transformers with diffusion processes to improve accuracy and boundary precision over traditional U-Net approaches.


<details>
  <summary>Details</summary>
Motivation: Convolutional architectures like U-Net have limited capacity to capture long-range dependencies, constraining performance on complex tumor structures. Diffusion models show potential for refining segmentation boundaries in medical imaging.

Method: Embed a vision transformer at the core of the diffusion process, leveraging global contextual reasoning with iterative denoising. The transformer backbone models spatial relationships across entire MRI volumes while diffusion refinement mitigates voxel-level errors.

Result: Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance metrics.

Conclusion: The hybrid transformer-diffusion design provides improved robustness and scalability in neuro-oncology, advancing beyond conventional U-Net baselines for tumor segmentation.

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: A deep learning pipeline extracts urban areas from historical French maps (1925-1950) to create the first national-scale urban footprint dataset for this period, achieving 73% accuracy.


<details>
  <summary>Details</summary>
Motivation: Quantitative analysis of historical urban sprawl in France before 1970s is hindered by lack of nationwide digital urban footprint data.

Method: Dual-pass U-Net approach: first pass identifies confusion areas to guide data augmentation, second pass uses refined dataset and binarized output to minimize radiometric noise. Deployed on HPC cluster to process 941 high-resolution tiles.

Result: Created first open-access national-scale urban footprint dataset for 1925-1950 period with 73% overall accuracy, effectively capturing diverse urban patterns while overcoming artifacts.

Conclusion: Successfully bridges data gap for historical urban analysis in France and releases code, datasets, and nationwide urban raster to support future urbanization research.

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: Analysis of point-based tracking failure modes in surgical video object segmentation, showing it works well for tools but fails for anatomical targets due to tissue similarity and ambiguous boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the reliability and failure cases of point-based tracking in complex surgical environments, as it offers efficient zero-shot tracking but its limitations are not well studied.

Method: Systematic analysis of point-based tracking failure modes in laparoscopic cholecystectomy videos, comparing point-based tracking with segmentation mask initialization for three surgical targets (gallbladder, grasper, L-hook electrocautery).

Result: Point-based tracking is competitive for surgical tools but consistently underperforms for anatomical targets due to tissue similarity and ambiguous boundaries causing tracking failures.

Conclusion: Provides actionable recommendations for selecting and placing tracking points to improve performance in surgical video analysis, highlighting key factors influencing tracking outcomes.

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: FRIEREN is a federated learning framework for semantic segmentation that uses vision-language models and pseudo-labeling to handle domain shifts with unlabeled client data, without re-accessing the source dataset.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of domain shifts in federated learning for semantic segmentation when client data is unlabeled, overcoming limitations of existing methods that assume labeled client data or don't leverage modern vision foundation models.

Method: Leverages vision-language models with CLIP-based text embeddings for semantic disambiguation, uses a vision-language decoder, and employs weak-to-strong consistency learning for robust local training on pseudo-labels from unlabeled client data.

Result: Achieves competitive performance on synthetic-to-real and clear-to-adverse-weather benchmarks, effectively tackling the new FFREEDG task and setting a strong baseline for future research.

Conclusion: FRIEREN successfully addresses the challenging FFREEDG task by integrating vision-language modalities and consistency learning, demonstrating effective domain adaptation in federated learning with unlabeled client data.

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint is a structured prompting framework that uses action-centric knowledge to improve video anomaly detection with frozen vision-language models, achieving state-of-the-art performance without training.


<details>
  <summary>Details</summary>
Motivation: Existing prompts for video anomaly detection are too abstract and overlook fine-grained human-object interactions and action semantics that define complex anomalies in surveillance videos.

Method: Organizes prompts into semantically coherent groups (violence, property crimes, public safety) with fine-grained guiding questions that align model predictions with discriminative visual cues.

Result: Consistently improves AUC over prior baselines on UCF-Crime and XD-Violence datasets, achieving state-of-the-art performance compared to both fine-tuned and training-free methods.

Conclusion: Demonstrates the critical role of prompt granularity and establishes ASK-Hint as a new training-free, generalizable solution for explainable video anomaly detection with interpretable reasoning traces.

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify is a method that uses geometric priors from 3D self-supervised models to purify noisy 2D VLM features for 3D semantic segmentation, achieving state-of-the-art performance with only 1.5% training data.


<details>
  <summary>Details</summary>
Motivation: Current methods for transferring 2D VLM features to 3D segmentation face a trade-off: direct projection yields noisy results while geometric coherence requires expensive training pipelines and large annotated 3D datasets.

Method: Uses a Student Affinity Network to purify 2D VLM-generated 3D point features using geometric priors from a 3D self-supervised teacher model, plus a Geometry-Guided Pooling module for denoising during inference.

Result: Achieves or surpasses state-of-the-art performance on major 3D benchmarks while using only about 1.5% of the training data.

Conclusion: GeoPurify effectively mitigates the trade-off between 2D semantics and 3D geometric structure through latent geometric information and learned affinity, demonstrating superior data efficiency.

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: Noninvasive pig identification using auricular vein patterns achieves 98.12% accuracy with SVM classification, providing cost-effective alternative to physical tags.


<details>
  <summary>Details</summary>
Motivation: Current pig identification methods (ear tags, microchips) are unreliable, costly, target pure breeds, and impractical for small-scale farmers.

Method: Collected 800 ear images from 20 mixed-breed pigs using smartphone with back lighting, developed multistage computer vision pipeline for vein enhancement and feature extraction, used machine learning models for classification.

Result: SVM achieved highest accuracy of 98.12% precision across mixed-breed populations, with entire process taking average 8.3 seconds.

Conclusion: Auricular vein biometrics provide cost-effective, stress-free animal identification method that can extend precision farming benefits to resource-constrained communities.

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: A multi-category counting framework using Twins pyramid vision-transformer with multi-class counting head and Category Focus Module, achieving superior performance on dense scene counting benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address object counting in dense and occluded scenes where traditional detection methods fail, particularly for multi-category scenarios.

Method: Uses Twins pyramid vision-transformer backbone with multi-class counting head and two-task design including segmentation-based Category Focus Module to suppress inter-category cross-talk.

Result: Achieved 33%, 43% and 64% reduction in MAE on VisDrone and iSAID benchmarks compared to prior approaches, demonstrating superiority over YOLOv11 in dense scenes.

Conclusion: The method enables multi-class crowd counting in new domains, with demonstrated application to biodiversity monitoring for conservation and ecological insights.

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl enables fine-grained temporal control in text-to-video generation by using cross-attention maps to align visual concepts with timing signals during inference, without retraining.


<details>
  <summary>Details</summary>
Motivation: Current generative video models lack fine-grained temporal control, preventing users from specifying when particular visual elements should appear within generated sequences.

Method: Uses cross-attention maps from text-to-video diffusion models with a novel optimization approach that aligns temporal shape via correlation, amplifies visibility via energy, and maintains spatial focus via entropy.

Result: Enables precise temporal control while maintaining high video quality and diversity, demonstrated across applications including temporal reordering, action generation, and audio-aligned generation.

Conclusion: TempoControl provides effective temporal alignment of visual concepts during video generation inference without requiring retraining or additional supervision.

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: RewardMap is a multi-stage RL framework that improves MLLMs' fine-grained visual reasoning by introducing difficulty-aware rewards and progressive training from perception to reasoning tasks, achieving consistent performance gains across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Fine-grained visual reasoning remains challenging for MLLMs, with existing methods struggling on spatial reasoning tasks like transit maps due to sparse rewards and unstable optimization in standard RL approaches.

Method: 1) Construct ReasonMap-Plus dataset with dense reward signals via VQA tasks; 2) Develop RewardMap with difficulty-aware reward design and multi-stage RL scheme that bootstraps from simple perception to complex reasoning tasks.

Result: Models trained with RewardMap achieve 3.47% average improvement across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks, demonstrating enhanced visual understanding and reasoning capabilities.

Conclusion: RewardMap effectively addresses sparse reward challenges in fine-grained visual reasoning through dense reward signals and progressive training strategy, yielding significant performance improvements in MLLMs across diverse reasoning tasks.

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow is a new framework that leverages FLUX's strong generative priors for drag-based image editing, overcoming limitations of previous methods by using region-based editing with affine transformations and integrating personalization adapters for better subject consistency.


<details>
  <summary>Details</summary>
Motivation: Previous drag-based editing methods suffer from distortions due to insufficient priors from Stable Diffusion. With the shift to stronger DiT-based models like FLUX, there's an opportunity to improve drag editing, but direct application of point-based methods fails due to unstructured DiT features.

Method: DragFlow introduces region-based editing using affine transformations for richer feature supervision, integrates IP-Adapter for subject consistency, uses gradient mask constraints for background preservation, and employs MLLMs to resolve task ambiguities.

Result: Extensive experiments on DragBench-DR and the new ReD Bench benchmark show DragFlow surpasses both point-based and region-based baselines, achieving state-of-the-art performance in drag-based image editing.

Conclusion: DragFlow effectively harnesses FLUX's strong priors for drag-based editing through region-based supervision and personalization adapters, setting a new standard for the field with improved quality and consistency.

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: F2C proposes using key clips instead of isolated key frames for video understanding, with adaptive resolution to maintain fixed token count, achieving significant performance gains on long-form video benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Video LLMs suffer from the 'needle in a haystack' problem where massive visual tokens from raw video frames exhaust context windows. Frame-wise selection discards essential temporal dynamics, leading to suboptimal reasoning about motion and event continuity.

Method: Extends selection from isolated key frames to key clips (short, temporally coherent segments) and proposes adaptive resolution strategy that dynamically balances spatial resolution and clip length to maintain constant token count per video.

Result: Outperforms uniform sampling by up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench and MLVU benchmarks respectively. Training-free approach demonstrates importance of preserving temporal coherence.

Conclusion: The work highlights the importance of temporal coherence in frame selection and provides a practical pathway for scaling Video LLMs to real-world video understanding applications.

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: This study compares video-based 3D human pose estimation models with IMU sensors for movement assessment, finding MotionAGFormer performs best among video methods, with both technologies showing viability for out-of-lab kinematic analysis but with different trade-offs.


<details>
  <summary>Details</summary>
Motivation: To enable accurate human movement assessment outside specialized labs for telemedicine, sports science, and rehabilitation by comparing the performance of monocular video-based 3D pose estimation against IMU-based approaches.

Method: Used the VIDIMU dataset with 13 daily activities captured by commodity cameras and 5 IMUs. Evaluated joint angles from deep learning frameworks (MotionAGFormer, MotionBERT, MMPose, NVIDIA BodyTrack) against IMU data processed through OpenSim inverse kinematics, following Human3.6M format with 17 keypoints.

Result: MotionAGFormer achieved best performance with lowest RMSE (9.27°±4.80°), lowest MAE (7.86°±4.18°), highest Pearson correlation (0.86±0.15), and highest R² (0.67±0.28). Both video and IMU approaches proved viable for out-of-lab kinematic assessment.

Conclusion: Both video-based and IMU-based approaches are viable for movement assessment outside labs, with key trade-offs in cost, accessibility, and precision. The study provides guidelines for developing robust telehealth solutions and identifies where video models show clinical promise versus where IMUs perform better.

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift is a diffusion-based method that integrates AutoKL and CLIP adapters for cross-subject fMRI visual reconstruction, achieving state-of-the-art performance with minimal training time and computational resources.


<details>
  <summary>Details</summary>
Motivation: Current methods for reconstructing visual information from brain activity face challenges in cross-subject generalization due to neural variability and abstract semantic encoding in complex visual inputs.

Method: Integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. Uses pretraining on one subject followed by fine-tuning only 17% of parameters (fully connected layers) for new subjects.

Result: Achieves state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), outperforming existing methods.

Conclusion: NeuroSwift enables efficient and accurate cross-subject visual reconstruction from fMRI data with minimal computational requirements.

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP is a self-training framework that refines CLIP's visual and textual representations for fine-grained image classification using saliency-guided token fusion and LLM-derived classifiers.


<details>
  <summary>Details</summary>
Motivation: CLIP's reliance on coarse global features limits performance on fine-grained classification tasks, and prior approaches overlook spatial precision when aligning LLM descriptions with CLIP.

Method: Uses Saliency-Oriented Attention Pooling (SOAP) in TokenFusion module to create saliency-guided fine-grained tokens, two-headed LLM-derived classifier for stable pseudo-labeling, and Dynamic Knowledge Aggregation to combine fixed priors with evolving logits.

Result: Achieves consistent 2.90% average accuracy gain across 13 fine-grained benchmarks with only light adaptation requirements.

Conclusion: microCLIP effectively uncovers latent fine-grained signals in CLIP through joint refinement of visual and textual representations, enabling improved performance on fine-grained classification tasks.

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1 is the first video authenticity detector that fine-tunes a multi-modal large language model using group relative policy optimization to provide both accurate classification and interpretable explanations for AI-generated video detection.


<details>
  <summary>Details</summary>
Motivation: Address the urgent need for effective detection tools to mitigate societal risks from AI-generated videos, such as misinformation and reputational harm, while ensuring transparency through interpretable explanations for regulators and end users.

Method: Fine-tune Qwen-VL using group relative policy optimization (GRPO) with two specialized reward models targeting temporal artifacts and generation complexity, using a curated dataset of 140k real and AI-generated videos from state-of-the-art generation models.

Result: Achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Produces precise and interpretable rationales for predictions.

Conclusion: VidGuard-R1 successfully addresses the need for both accurate detection and interpretable explanations in AI-generated video authentication, demonstrating superior performance and transparency.

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Proposes Self-Forcing++ to mitigate quality degradation in long-horizon video generation by using teacher models to guide student models through self-generated long videos, achieving up to 20x length scaling without retraining.


<details>
  <summary>Details</summary>
Motivation: Address quality degradation in long video generation caused by error compounding when student models extrapolate beyond teacher training horizons, without requiring long-video supervision.

Method: Exploit teacher model knowledge to provide guidance for student models through sampled segments from self-generated long videos, maintaining temporal consistency without recomputing overlapping frames.

Result: Generates videos up to 4 minutes 15 seconds (99.9% of maximum position embedding span), 50x longer than baseline, with substantial improvements in fidelity and consistency on benchmarks.

Conclusion: The approach effectively mitigates quality degradation in long-horizon video generation, enabling significant length scaling while maintaining quality without additional supervision or retraining.

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask is a physics-guided video generation method that enables realistic rigid body control and interactions by using object velocity conditioning and a two-stage training strategy with gradual removal of motion supervision.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with physically plausible object interactions and lack physics-grounded control mechanisms, limiting their use as world simulators for robotics and embodied decision making.

Method: Two-stage training strategy that gradually removes future motion supervision via object masks, training video diffusion models on synthetic scenes with object velocity conditioning and integrating low-level motion control with high-level textual conditioning.

Result: Significant improvements in object interactions in real scenes, strong improvements over recent models of comparable size, and effective synthesis of complex dynamical phenomena.

Conclusion: KineMask demonstrates the complementary roles of low- and high-level conditioning in video diffusion models and provides a physics-guided approach for realistic video generation with controllable object interactions.

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: The paper introduces fine-grained multimodal actions incorporating proprioception, kinesthesia, force haptics, and muscle activation to enable precise control in household robots, addressing limitations of current video models.


<details>
  <summary>Details</summary>
Motivation: Current video models fail as world models due to lack of fine-grained control needed for delicate tasks and urgent situations in household robots.

Method: Developed a feature learning paradigm that aligns multimodal senses while preserving unique information from each modality, plus a regularization scheme to enhance causality of action trajectory features.

Result: Experiments show incorporating multimodal senses improves simulation accuracy and reduces temporal drift.

Conclusion: The approach demonstrates effectiveness and practicality through extensive ablation studies and downstream applications, enabling fine-grained interactions difficult to simulate with text-conditioned generative models.

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA adapts Native Sparse Attention to video-language models, enabling reliable scaling to 128K tokens and improving long-video understanding through a hardware-aware hybrid attention approach.


<details>
  <summary>Details</summary>
Motivation: Video understanding in multimodal models is limited by context length, causing models to miss key transition frames and struggle with coherence across long time scales.

Method: Adapt Native Sparse Attention (NSA) to video-language models by training Qwen2.5-VL on 216K video instruction dataset with hardware-aware hybrid attention (dense for text, sparse for video).

Result: Achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks compared to token-compression and training-free sparse baselines.

Conclusion: VideoNSA enables reliable scaling to 128K tokens with optimal global-local attention allocation, task-dependent branch usage, and learnable sparse attention that induces dynamic attention sinks.

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift is a training-free method that recalibrates noise levels for diffusion models based on resolution size, improving low-resolution image generation quality without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models trained on fixed resolutions fail to generalize to lower resolutions, preventing budget-efficient alternatives for users who don't need high-resolution images.

Method: Identifies that noise schedulers have unequal perceptual effects across resolutions - same noise level removes more signal from lower-resolution images. Proposes NoiseShift to recalibrate denoiser noise level conditioned on resolution size.

Result: Significantly improves quality at low resolutions: 15.89% FID improvement for SD3.5, 8.56% for SD3, and 2.44% for Flux-Dev on LAION-COCO; 10.36% for SD3.5, 5.19% for SD3, and 3.02% for Flux-Dev on CelebA.

Conclusion: NoiseShift effectively mitigates resolution-dependent artifacts and enhances low-resolution image generation quality, requiring no model changes and being compatible with existing models.

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: This paper studies predicting dynamic physical properties (elasticity, viscosity, dynamic friction) from videos using temporal information, comparing oracle methods, video foundation models, and MLLMs.


<details>
  <summary>Details</summary>
Motivation: To enable prediction of dynamic physical properties that require temporal information from video data, addressing the challenge of inferring properties like elasticity, viscosity, and friction through visual analysis.

Method: Collected new video datasets for each physical property with synthetic training/testing splits and real-world evaluation. Explored three approaches: (1) oracle method using classical computer vision techniques, (2) visual prompt mechanism with cross-attention on pre-trained video models, (3) prompt strategies for Multi-modal Large Language Models.

Result: Video foundation models trained generatively or self-supervised achieved similar performance (though behind oracle methods), while MLLMs were currently inferior but could be improved through better prompting strategies.

Conclusion: Video foundation models show promise for predicting dynamic physical properties, with MLLMs having potential for improvement through optimized prompting, though classical computer vision approaches still provide superior performance.

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: The paper introduces sounding object detection - a task to identify objects directly involved in producing sounds from everyday interactions, using a multimodal object-aware framework trained on egocentric videos.


<details>
  <summary>Details</summary>
Motivation: Everyday object interactions produce unique sounds, and the paper aims to develop models that can link these sounds to the specific objects involved, inspired by human perception capabilities.

Method: Uses a multimodal object-aware framework with automatic pipeline for computing segmentation masks of involved objects, guided by slot attention visual encoder to enforce object-centric focus during training on egocentric videos.

Result: Achieves state-of-the-art performance on the new sounding object detection task and existing multimodal action understanding tasks.

Conclusion: The proposed framework successfully enables models to detect objects directly involved in sound production through object-centric multimodal learning from in-the-wild videos.

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: The paper analyzes 3D Gaussian Splatting (3DGS) vulnerability to poisoning attacks and proposes a novel density-guided method that injects Gaussian points into low-density regions using KDE, creating viewpoint-dependent illusory objects while maintaining stealth.


<details>
  <summary>Details</summary>
Motivation: As 3D scene representation methods like NeRF and 3DGS become prevalent, addressing their security vulnerabilities becomes critical. The paper aims to analyze 3DGS robustness against poisoning attacks and develop effective attack methods.

Method: Proposes a density-guided poisoning method using Kernel Density Estimation (KDE) to identify low-density regions for strategic Gaussian point injection. Includes an adaptive noise strategy to disrupt multi-view consistency and a KDE-based evaluation protocol for systematic assessment.

Result: Extensive experiments show superior performance compared to state-of-the-art techniques. The method successfully embeds viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views.

Conclusion: The work demonstrates significant vulnerabilities in 3DGS to poisoning attacks and provides a systematic evaluation framework for future security research in 3D scene representations.

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: The paper introduces FOCUS, a theoretical framework and algorithms for improving multi-subject fidelity in text-to-image models by formulating subject disentanglement as stochastic optimal control over flow matching samplers.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with multi-subject prompts, showing attribute leakage, identity entanglement, and subject omissions, which limits their practical utility for complex scene generation.

Method: Two architecture-agnostic algorithms: (1) training-free test-time controller that perturbs base velocity with single-pass update, and (2) Adjoint Matching for lightweight fine-tuning that regresses control network to backward adjoint signal while preserving base-model capabilities.

Result: Both algorithms consistently improve multi-subject alignment while maintaining base-model style on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers generalize to unseen prompts.

Conclusion: FOCUS achieves state-of-the-art multi-subject fidelity across models, providing the first principled fine-tuning route explicitly designed for multi-subject generation and unifying prior attention heuristics.

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>
