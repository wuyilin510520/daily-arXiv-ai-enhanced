{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "An explainable AI system that combines a lightweight CNN classifier with a Vision-Language Model to detect, localize, and explain artifacts in 32x32 images with 96.5% accuracy and fast inference.", "motivation": "Address the challenge of verifying visual authenticity due to increasing realism of AI-generated imagery.", "method": "Combines Faster-Than-Lies CNN classifier with Qwen2-VL-7B Vision-Language Model, using autoencoder-based reconstruction error maps for artifact localization heatmaps.", "result": "Achieves 96.5% accuracy on extended CiFAKE dataset with adversarial perturbations, maintains 175ms inference time on 8-core CPUs, and generates explainable text for 70 categorized artifact types.", "conclusion": "Demonstrates feasibility of combining visual and linguistic reasoning for interpretable authenticity detection in low-resolution imagery with cross-domain applications."}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "CountFormer is a transformer-based framework for class-agnostic object counting that uses DINOv2 features and positional embeddings to recognize visual repetition and structural relationships, achieving state-of-the-art performance on complex scenes.", "motivation": "Humans can count diverse objects by perceiving visual repetition and structural relationships, but existing counting models often miscount on objects with complex shapes, internal symmetry, or overlapping components.", "method": "Built on CounTR architecture, replaces visual encoder with self-supervised DINOv2 foundation model, incorporates positional embedding fusion, and uses lightweight convolutional decoder to generate density maps.", "result": "Achieves performance comparable to state-of-the-art methods on FSC-147 dataset, with superior accuracy on structurally intricate or densely packed scenes.", "conclusion": "Integrating foundation models like DINOv2 enables counting systems to approach human-like structural perception, advancing toward truly general and exemplar-free counting."}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Cl\u00e9ment Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "A novel framework using fixed cameras and deep learning for continuous monitoring of floating debris in rivers, with geometric modeling for object size estimation.", "motivation": "Floating anthropogenic debris in rivers negatively impacts biodiversity, water quality, navigation, and recreation, requiring effective monitoring solutions.", "method": "Uses fixed in-situ cameras with deep learning models for debris detection and quantification, plus geometric modeling using camera characteristics for size estimation.", "result": "Demonstrated feasibility of automated monitoring, identified optimal deep learning models for accuracy and speed, and highlighted importance of dataset protocols.", "conclusion": "The approach enables development of robust, low-cost automated monitoring systems for urban aquatic environments using projective geometry and regression corrections."}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "RareFlow is a physics-aware super-resolution framework for remote sensing imagery that addresses out-of-distribution robustness through dual-conditioning architecture, uncertainty quantification, and multifaceted loss functions.", "motivation": "Super-resolution for remote sensing imagery often fails under out-of-distribution conditions, producing visually plausible but physically inaccurate results, especially for rare geomorphic features captured by diverse sensors.", "method": "Uses dual-conditioning architecture with Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Introduces multifaceted loss function for spectral/radiometric consistency and employs stochastic forward pass for uncertainty quantification.", "result": "In blind evaluations by geophysical experts, outputs approached ground truth fidelity, significantly outperforming state-of-the-art baselines. Quantitative gains include nearly 40% reduction in FID.", "conclusion": "RareFlow provides a robust framework for high-fidelity synthesis in data-scarce scientific domains and offers a new paradigm for controlled generation under severe domain shift."}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "A training-free method for 3D scene generation that repurposes text-to-3D object diffusion models as modular tile generators, enabling scalable synthesis of large, coherent scenes through multi-tile denoising and seamless blending.", "motivation": "Existing 3D scene generation methods are limited to single objects, require domain-specific training, or lack full 360-degree viewability, creating a need for more flexible and scalable approaches.", "method": "Reformulates scene generation as a multi-tile denoising problem where overlapping 3D regions are independently generated using text-to-3D object diffusion models and seamlessly blended via weighted averaging.", "result": "The approach enables scalable synthesis of large, coherent scenes while preserving local semantic control, supports diverse scene layouts, efficient generation, and flexible editing without requiring scene-level datasets or retraining.", "conclusion": "Establishes a simple yet powerful foundation for general-purpose, language-driven 3D scene construction by leveraging object-level priors and eliminating the need for scene-specific training."}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "LHT-CLIP is a training-free framework that improves CLIP for semantic segmentation by exploiting visual discriminability across layers, attention heads, and tokens through three techniques: semantic-spatial reweighting, selective head enhancement, and abnormal token replacement.", "motivation": "CLIP models struggle with semantic segmentation due to misalignment between image-level pre-training and pixel-level dense prediction requirements. Prior methods inherit global alignment bias from earlier layers, leading to suboptimal segmentation performance.", "method": "The framework uses three complementary techniques: semantic-spatial reweighting to restore visual discriminability, selective head enhancement to leverage consistently discriminative attention heads, and abnormal token replacement to handle anomalous tokens with sparse activation patterns.", "result": "Extensive experiments on 8 semantic segmentation benchmarks show LHT-CLIP achieves state-of-the-art performance across diverse scenarios without additional training, auxiliary networks, or extensive hyperparameter tuning.", "conclusion": "LHT-CLIP effectively restores visual discriminability in CLIP models for semantic segmentation, demonstrating superior performance and practical deployment value through systematic exploitation of layer, head, and token-level characteristics."}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride generates coherent scene-level captions for instructional videos using adaptive frame sampling, multimodal windowing, and a dynamic stride selection algorithm that balances temporal context and redundancy.", "motivation": "Scene-level captioning in instructional videos enhances learning by understanding visual cues and temporal structure, but current captions often lack coherence and quality, undermining educational intent.", "method": "Uses adaptive frame sampling and multimodal windowing to capture key transitions, employs multimodal chain-of-thought for action-object pairs, and refines them with dynamic stride window selection algorithm.", "result": "Outperforms strong baselines (VLLaMA3, GPT-4o) on both N-gram metrics (BLEU, METEOR) and semantic similarity measures (BERTScore, CLIPScore), producing more temporally coherent and informative captions.", "conclusion": "DynaStride shows promising direction for improving AI-powered instructional content generation by integrating visual semantics and temporal reasoning in coherent scene-level captions."}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "TurboPortrait3D is a low-latency method for novel-view synthesis of human portraits that combines image-to-avatar generation with diffusion models to enhance quality while maintaining 3D consistency.", "motivation": "Existing image-to-3D models for portraits produce visual artifacts, lack detail, and fail to preserve subject identity, while diffusion models are computationally expensive and not 3D-aware.", "method": "Uses a feedforward image-to-avatar pipeline to get initial 3D representation and noisy renders, then refines them with a single-step diffusion model conditioned on input images and trained for multi-view consistency.", "result": "Qualitatively and quantitatively outperforms current state-of-the-art methods for portrait novel-view synthesis while being efficient in time.", "conclusion": "Image-space diffusion models can effectively enhance existing image-to-avatar methods, maintaining 3D-awareness and running with low latency for high-quality portrait novel-view synthesis."}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "PlanarGS enhances 3D Gaussian Splatting for indoor scenes by incorporating planar and geometric priors to overcome limitations in low-texture regions, achieving superior 3D surface reconstruction.", "motivation": "3DGS struggles with ambiguous geometry in large, low-texture indoor scenes due to reliance on photometric loss alone.", "method": "Uses Language-Prompted Planar Priors (LP3) pipeline with vision-language segmentation and geometric priors, plus planar and geometric supervision terms for Gaussian optimization.", "result": "Outperforms state-of-the-art methods by large margins on standard indoor benchmarks, reconstructing accurate and detailed 3D surfaces.", "conclusion": "PlanarGS effectively addresses 3DGS limitations in indoor scenes through planar and geometric priors, enabling high-fidelity reconstruction."}}
{"id": "2510.23943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23943", "abs": "https://arxiv.org/abs/2510.23943", "authors": ["Diana Aldana", "Jo\u00e3o Paulo Lima", "Daniel Csillag", "Daniel Perazzo", "Haoan Feng", "Luiz Velho", "Tiago Novello"], "title": "Adaptive Training of INRs via Pruning and Densification", "comment": null, "summary": "Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.", "AI": {"tldr": "AIRe is an adaptive training scheme for implicit neural representations that uses neuron pruning and input frequency densification to optimize network architecture during training, achieving better size-quality trade-offs.", "motivation": "Current methods for implicit neural representations require manual selection of input frequencies and architectures, leading to parameter redundancy and heavy hyperparameter optimization.", "method": "AIRe uses neuron pruning to remove redundant neurons by identifying less-contributory neurons and applying targeted weight decay, followed by structured pruning. It then densifies input frequencies in spectrum regions where the signal underfits.", "result": "Experiments on images and SDFs show that AIRe reduces model size while preserving or improving reconstruction quality.", "conclusion": "AIRe provides an effective adaptive training scheme that automatically refines INR architecture during optimization, eliminating the need for manual hyperparameter tuning."}}
{"id": "2510.23956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23956", "abs": "https://arxiv.org/abs/2510.23956", "authors": ["Alejandro Escontrela", "Shrinu Kushagra", "Sjoerd van Steenkiste", "Yulia Rubanova", "Aleksander Holynski", "Kelsey Allen", "Kevin Murphy", "Thomas Kipf"], "title": "Neural USD: An object-centric framework for iterative editing and control", "comment": "22 pages, 16 figures, 1 table", "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .", "AI": {"tldr": "Neural USD introduces a hierarchical scene representation framework inspired by computer graphics standards, enabling precise per-object editing in generative models without unintended global changes.", "motivation": "Current controllable generative models often cause unintended global changes when trying to edit specific objects, lacking precise and iterative object editing capabilities.", "method": "Proposes Neural USD framework with structured hierarchical scene representation, applying fine-tuning to disentangle control signals for appearance, geometry, and pose per object.", "result": "The framework enables iterative and incremental workflows with precise per-object control, minimizing model-specific constraints while accommodating diverse signals.", "conclusion": "Neural USD successfully addresses challenges in precise object editing by providing structured scene representation that supports disentangled control over individual scene elements."}}
{"id": "2510.23960", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23960", "abs": "https://arxiv.org/abs/2510.23960", "authors": ["Peiyang Xu", "Minzhou Pan", "Zhaorun Chen", "Shuang Yang", "Chaowei Xiao", "Bo Li"], "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability", "comment": "42 pages, 9 figures", "summary": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.", "AI": {"tldr": "SafeVision is a novel image guardrail system that uses human-like reasoning to detect unsafe content, dynamically adapts to new safety policies without retraining, and provides transparent explanations.", "motivation": "Traditional image guardrail models have limitations: they misclassify content due to lack of semantic reasoning, struggle with emerging threats requiring costly retraining, and lack transparency in decision-making.", "method": "The approach includes: effective data collection/generation framework, policy-following training pipeline, customized loss function, and diverse QA generation/training strategy. Also introduces VisionHarm dataset with comprehensive harmful categories.", "result": "SafeVision achieves state-of-the-art performance, outperforming GPT-4o by 8.6% on VisionHarm-T and 15.5% on VisionHarm-C, while being over 16x faster.", "conclusion": "SafeVision establishes a comprehensive, policy-following, and explainable image guardrail system with dynamic adaptation to emerging threats, addressing key limitations of traditional approaches."}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "A framework that brings chain-of-thought reasoning to chest X-ray interpretation, enabling transparent stepwise reasoning that aligns with radiologists' workflow and supports clinical auditability.", "motivation": "Current vision-language models for medical image analysis are opaque and lack the transparent, stepwise reasoning that clinicians rely on for trust and verification.", "method": "Couples high-fidelity visual encoding with two-stage training: reasoning-style supervised fine-tuning followed by reinforcement learning using verifiable rewards over X-ray abnormalities.", "result": "Achieves competitive multi-label classification in out-of-distribution evaluation while improving interpretability. Reader study showed reasoning traces increased radiologist confidence, supported error auditing, and reduced report finalization time.", "conclusion": "The approach enables trustworthy, explainable AI in medical imaging where reasoning quality is as critical as prediction quality, supporting safer human-AI collaboration."}}
{"id": "2510.23978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23978", "abs": "https://arxiv.org/abs/2510.23978", "authors": ["Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints", "comment": "9 pages", "summary": "Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.", "AI": {"tldr": "Proposes joint prediction of multiple Fourier components for better quality and efficiency in arbitrary-scale super-resolution, addressing limitations of existing recurrent neural network methods.", "motivation": "Existing methods predict Fourier components one by one using recurrent neural networks, leading to performance degradation and inefficiency due to independent prediction.", "method": "Predicting multiple Fourier components jointly instead of one by one.", "result": "Improved both quality and efficiency in arbitrary-scale super-resolution.", "conclusion": "Joint prediction of multiple components is more effective than independent sequential prediction for Fourier-based super-resolution."}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "TeleEgo is a comprehensive benchmark for egocentric AI assistants that evaluates memory, understanding, and cross-memory reasoning across long-duration, streaming, multi-modal data in realistic daily contexts.", "motivation": "Existing benchmarks evaluate AI assistant capabilities in isolation, lack realistic streaming scenarios, or only support short-term tasks, failing to capture the real-world requirements of egocentric AI assistants.", "method": "Created a dataset with over 14 hours per participant of synchronized egocentric video, audio, and text across four domains, aligned on a unified timeline with human-refined visual narrations and speech transcripts. Defined 12 diagnostic subtasks across three core capabilities with 3,291 human-verified QA items.", "result": "Developed a benchmark with comprehensive evaluation metrics (Real-Time Accuracy and Memory Persistence Time) that assesses correctness, temporal responsiveness, and long-term retention in streaming settings.", "conclusion": "TeleEgo provides a realistic and comprehensive evaluation framework to advance the development of practical AI assistants capable of processing multi-modal inputs, responding in real time, and retaining evolving long-term memory."}}
{"id": "2510.24000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24000", "abs": "https://arxiv.org/abs/2510.24000", "authors": ["Heethanjan Kanagalingam", "Thenukan Pathmanathan", "Mokeeshan Vathanakumar", "Tharmakulasingam Mukunthan"], "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.", "AI": {"tldr": "Proposes AdvBlur, a novel diabetic retinopathy classification method that uses adversarial blurred images and dual-loss functions to improve domain generalization across different datasets and imaging conditions.", "motivation": "Address the limitation of existing deep learning models in maintaining robustness against distributional variations caused by different acquisition devices, demographic disparities, and imaging conditions in diabetic retinopathy detection.", "method": "Integrates adversarial blurred images into the dataset and employs a dual-loss function framework to enhance domain generalization capabilities.", "result": "Achieves competitive performance compared to state-of-the-art domain generalization DR models on unseen external datasets, with comprehensive evaluations across multiple datasets.", "conclusion": "The AdvBlur method effectively mitigates the impact of unseen distributional variations and demonstrates strong domain generalization capabilities for diabetic retinopathy classification."}}
{"id": "2510.24009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24009", "abs": "https://arxiv.org/abs/2510.24009", "authors": ["Yuan Jin", "Antonio Pepe", "Gian Marco Melito", "Yuxuan Chen", "Yunsu Byeon", "Hyeseong Kim", "Kyungwon Kim", "Doohyun Park", "Euijoon Choi", "Dosik Hwang", "Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu", "Ayman El-Ghotni", "Mohamed Nabil", "Hossam El-Kady", "Ahmed Ayyad", "Amr Nasr", "Marek Wodzinski", "Henning M\u00fcller", "Hyeongyu Kim", "Yejee Shin", "Abbas Khan", "Muhammad Asad", "Alexander Zolotarev", "Caroline Roney", "Anthony Mathur", "Martin Benning", "Gregory Slabaugh", "Theodoros Panagiotis Vagenas", "Konstantinos Georgas", "George K. Matsopoulos", "Jihan Zhang", "Zhen Zhang", "Liqin Huang", "Christian Mayer", "Heinrich M\u00e4chler", "Jan Egger"], "title": "Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge", "comment": null, "summary": "The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.", "AI": {"tldr": "The SEG.A challenge introduced a large public dataset for aortic vessel tree segmentation, benchmarking deep learning algorithms and revealing that ensemble methods outperform individual models.", "motivation": "To address the lack of shared, high-quality data that has impeded development of automated aortic vessel tree analysis from CT angiography.", "method": "Launched a multi-institutional challenge with a large public dataset, benchmarked automated algorithms on hidden test sets, and included optional surface meshing tasks for computational simulations.", "result": "3D U-Net architectures dominated top submissions, ensemble methods significantly outperformed individual models, and performance was strongly linked to algorithmic design and training data characteristics.", "conclusion": "The initiative establishes a new performance benchmark and provides a lasting resource to drive future innovation toward robust, clinically translatable tools."}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "Mars-Bench is the first standardized benchmark for evaluating foundation models on Mars-related tasks using orbital and surface imagery across 20 datasets for classification, segmentation, and object detection.", "motivation": "Mars science lacks standardized benchmarks and evaluation frameworks that have enabled progress in other domains like Earth Observation, limiting the development of foundation models for Martian tasks.", "method": "Created Mars-Bench with 20 datasets spanning classification, segmentation, and object detection tasks focused on key geologic features. Provided standardized datasets and baseline evaluations using models pre-trained on natural images, Earth satellite data, and vision-language models.", "result": "Results suggest Mars-specific foundation models may offer advantages over general-domain counterparts, indicating the potential benefits of domain-adapted pre-training for Martian tasks.", "conclusion": "Mars-Bench establishes a standardized foundation for developing and comparing machine learning models for Mars science, with data, models, and code made publicly available."}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "APT is a black-box framework that uses LLMs to generate human-readable adversarial suffixes for text-to-image models, bypassing safety filters through dual-evasion strategy and achieving high transferability.", "motivation": "Current red-teaming methods for T2I models require white-box access, use inefficient per-prompt optimization, and generate meaningless prompts that are easily blocked by filters.", "method": "Alternating optimization-finetuning pipeline between adversarial suffix optimization and LLM fine-tuning, with dual-evasion strategy using perplexity scoring and banned-token penalties.", "result": "Excellent red-teaming performance with human-readable, filter-resistant prompts, and superior zero-shot transferability to unseen prompts and commercial APIs.", "conclusion": "APT effectively exposes critical vulnerabilities in T2I models through human-readable adversarial prompts that bypass safety mechanisms."}}
{"id": "2510.24036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24036", "abs": "https://arxiv.org/abs/2510.24036", "authors": ["Xingyu Liu", "Kun Ming Goh"], "title": "ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning", "comment": "3 pages, 5 figures, 1 table", "summary": "Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.", "AI": {"tldr": "ResNet uses skip connections to solve vanishing gradient problem, enabling training of very deep networks with better accuracy and faster convergence.", "motivation": "To overcome the vanishing gradient problem that makes training very deep convolutional neural networks challenging.", "method": "Uses residual blocks with skip connections that allow gradients to flow directly through shortcut connections bypassing intermediate layers.", "result": "ResNet-18 achieved 89.9% accuracy on CIFAR-10 vs 84.1% for traditional deep CNN of similar depth, with faster convergence and more stable training.", "conclusion": "Residual Networks successfully enable training of very deep networks by addressing vanishing gradients through skip connections, achieving superior performance."}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "SNELLA is a one-stage parameter-efficient fine-tuning method that uses nonlinear low-rank decomposition and adaptive bi-level sparsity allocation to achieve state-of-the-art performance with significantly reduced memory usage compared to existing methods.", "motivation": "Current sparse tuning methods have limitations: they use a two-stage approach that overlooks parameter adjustments during fine-tuning, and they suffer from high memory usage due to storing all weight matrices in the optimizer.", "method": "SNELLA uses a one-stage approach with selective weight updates via sparse matrix addition to low-rank learnable matrices, enhanced by nonlinear kernel functions to increase rank and prevent interdependency. It also employs adaptive bi-level sparsity allocation that allows weights to compete across and within layers based on importance scores in an end-to-end manner.", "result": "SNELLA achieves SOTA performance with 1.8% higher Top-1 accuracy (91.9% vs 90.1%) on FGVC benchmark compared to SPT-LoRA, and reduces memory usage by 31.1%-39.9% across models ranging from 86M to 632M parameters.", "conclusion": "SNELLA successfully overcomes the limitations of existing sparse tuning methods by providing a more efficient one-stage approach that delivers superior performance with significantly reduced memory consumption across various vision tasks."}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "COLA is a training-free framework that uses optimal transport to align image and text features in CLIP models, improving adversarial robustness by 6.7% on average while maintaining clean accuracy.", "motivation": "CLIP models are vulnerable to adversarial attacks due to misalignment between image and text features, which worsens under perturbations. Existing methods overlook this feature gap problem.", "method": "COLA projects adversarial image embeddings onto class text feature subspace to filter distortions, then uses optimal transport to align images and texts as discrete distributions over multiple augmented views.", "result": "Extensive evaluations on 14 benchmarks show COLA improves zero-shot classification by 6.7% on average under PGD attacks on ImageNet variants while maintaining high clean accuracy.", "conclusion": "COLA effectively addresses adversarial misalignment in CLIP through optimal transport-based cross-modality alignment, providing training-free robustness enhancement compatible with existing models."}}
{"id": "2510.24078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24078", "abs": "https://arxiv.org/abs/2510.24078", "authors": ["William Yang", "Xindi Wu", "Zhiwei Deng", "Esin Tureci", "Olga Russakovsky"], "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "comment": null, "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.", "AI": {"tldr": "BOB is a fine-tuning strategy for T2I models that extracts class-agnostic attributes (background, pose) from real examples, conditions on them during fine-tuning, and marginalizes them out during generation to improve synthetic data quality for fine-grained classification.", "motivation": "Fine-tuning T2I models with few real examples for synthetic dataset generation can cause overfitting and reduce diversity, limiting effectiveness for classification tasks.", "method": "Extract class-agnostic attributes from real examples, condition T2I model fine-tuning on these attributes, then marginalize them out during generation to preserve generative prior and reduce overfitting.", "result": "BOB achieves SOTA performance in low-shot fine-grained classification, outperforming DataDream by 7.4% on Aircraft dataset and beating prior art in 18 of 24 experimental settings with 2+% accuracy improvements in 14 settings.", "conclusion": "BOB effectively mitigates overfitting in T2I fine-tuning for synthetic data generation, enabling better performance than using more real images in fine-grained classification tasks."}}
{"id": "2510.24093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24093", "abs": "https://arxiv.org/abs/2510.24093", "authors": ["Agus Gunawan", "Samuel Teodoro", "Yun Chen", "Soo Ye Kim", "Jihyong Oh", "Munchurl Kim"], "title": "OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation", "comment": "The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors", "summary": "Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.", "AI": {"tldr": "OmniText is a training-free generalist framework for Text Image Manipulation (TIM) that addresses limitations of existing text inpainting methods by enabling text removal, style control, and preventing duplicated letters through cross- and self-attention mechanisms.", "motivation": "Current diffusion-based text inpainting methods have three key limitations: inability to remove text, lack of style control over rendered text, and tendency to generate duplicated letters, which hinders their applicability to broader TIM tasks.", "method": "Proposes OmniText using cross- and self-attention mechanisms: self-attention inversion for text removal to reduce hallucinations, cross-attention redistribution to reduce text hallucination, and novel loss functions in latent optimization for controllable inpainting (cross-attention content loss for accuracy and self-attention style loss for style customization).", "result": "OmniText achieves state-of-the-art performance across multiple TIM tasks and metrics, comparable with specialist methods. Also introduces OmniText-Bench benchmark dataset for evaluating diverse TIM tasks.", "conclusion": "OmniText is the first generalist method capable of performing diverse TIM tasks including text removal, rescaling, repositioning, and insertion/editing with various styles, overcoming limitations of existing text inpainting approaches."}}
{"id": "2510.24105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24105", "abs": "https://arxiv.org/abs/2510.24105", "authors": ["Shufan Shen", "Zhaobo Qi", "Junshu Sun", "Qingming Huang", "Qi Tian", "Shuhui Wang"], "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "comment": "ICLR 2025 (Spotlight)", "summary": "The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.", "AI": {"tldr": "The paper discovers a positive correlation between interpretability and classifiability in pre-trained visual models, proposes an Inherent Interpretability Score (IIS) to quantify representation interpretability, and shows that improving classifiability also enhances interpretability.", "motivation": "To investigate whether pre-trained visual representations can simultaneously achieve high interpretability and classifiability, as current models prioritize classifiability but applications increasingly require interpretable representations.", "method": "Proposed Inherent Interpretability Score (IIS) that quantifies representation interpretability by measuring the ratio of interpretable semantics through information loss analysis when interpretations capture only interpretable parts.", "result": "Found positive correlation between interpretability and classifiability - representations with higher classifiability contain more interpretable semantics. Fine-tuning with interpretability maximization further improves classifiability, and predictions based on interpretations show less accuracy degradation.", "conclusion": "Practitioners can unify improvements in both interpretability and classifiability for pre-trained vision models, as these two properties are positively correlated rather than conflicting objectives."}}
{"id": "2510.24116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24116", "abs": "https://arxiv.org/abs/2510.24116", "authors": ["Fengming Yu", "Haiwei Pan", "Kejia Zhang", "Jian Guan", "Haiying Jiang"], "title": "UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations", "comment": "14 pages, 4 figures", "summary": "Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge", "AI": {"tldr": "UHKD is a knowledge distillation framework that uses frequency-domain analysis to transfer knowledge between heterogeneous teacher-student models, addressing architectural discrepancies that hinder traditional distillation methods.", "motivation": "Existing knowledge distillation methods work well for homogeneous models but degrade in heterogeneous scenarios due to architectural differences and semantic discrepancies in intermediate representations. Most methods focus only on logits space, underutilizing semantic information in intermediate layers.", "method": "Proposes Unified Heterogeneous Knowledge Distillation (UHKD) using Fourier transform to capture global feature information in frequency domain. Includes Feature Transformation Module (FTM) for compact frequency representations and Feature Alignment Module (FAM) for multi-level feature matching. Uses joint objective combining MSE on intermediate features and KL divergence on logits.", "result": "Achieves gains of 5.59% on CIFAR-100 and 0.83% on ImageNet-1K over the latest method, demonstrating effectiveness in unifying heterogeneous representations.", "conclusion": "UHKD effectively addresses architectural heterogeneity in knowledge distillation by leveraging frequency-domain analysis, enabling efficient utilization of visual knowledge across different model architectures."}}
{"id": "2510.24117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24117", "abs": "https://arxiv.org/abs/2510.24117", "authors": ["Zan Wang", "Siyu Chen", "Luya Mo", "Xinfeng Gao", "Yuxin Shen", "Lebin Ding", "Wei Liang"], "title": "DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery", "comment": "19 pages", "summary": "We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.", "AI": {"tldr": "DogMo is a large-scale multi-view RGB-D video dataset of canine movements, addressing limitations of existing datasets with 1.2k sequences from 10 dogs, and introduces a three-stage optimization pipeline for accurate motion recovery.", "motivation": "To overcome limitations in existing dog motion datasets, including lack of multi-view and real 3D data, limited scale, and insufficient diversity in motion and breed.", "method": "A three-stage, instance-specific optimization pipeline that progressively refines body shape and pose through coarse alignment, dense correspondence supervision, and temporal regularization, fitting the SMAL model to motion sequences.", "result": "Established four motion recovery benchmark settings supporting systematic evaluation across monocular and multi-view, RGB and RGB-D inputs, providing a foundation for accurate canine motion recovery.", "conclusion": "DogMo dataset and the proposed method provide a principled foundation for advancing research in dog motion recovery and open new directions in computer vision, graphics, and animal behavior modeling."}}
{"id": "2510.24129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24129", "abs": "https://arxiv.org/abs/2510.24129", "authors": ["Jiajian Xie", "Hubery Yin", "Chen Li", "Zhou Zhao", "Shengyu Zhang"], "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency", "comment": "17 pages, 10 figures", "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.", "AI": {"tldr": "ETC is a training-free framework that accelerates diffusion models by reusing model outputs while maintaining trajectory consistency through trend prediction and error tolerance control.", "motivation": "Current training-free acceleration methods for diffusion models ignore denoising trends and lack error control, leading to trajectory deviations and result inconsistencies when reusing model outputs across multiple steps.", "method": "ETC introduces (1) a consistent trend predictor that projects historical denoising patterns into stable future directions across multiple approximation steps, and (2) a model-specific error tolerance search mechanism that identifies transition points from semantic planning to quality refinement to derive corrective thresholds.", "result": "ETC achieves 2.65x acceleration over FLUX with only -0.074 SSIM score degradation in consistency, demonstrating significant speedup while maintaining output quality.", "conclusion": "The ETC framework effectively accelerates diffusion models by leveraging denoising trend consistency and model-specific error tolerance, enabling faster sampling without compromising generative quality."}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "Training-free framework combining object-centric approach with self-refinement to improve text-to-image model compositionality by using LLMs for layout synthesis and VLM-based reranking.", "motivation": "Modern text-to-image models struggle with compositionality, often failing to render accurate object counts, attributes, and spatial relations.", "method": "Leverage LLMs to synthesize explicit layouts from prompts, inject layouts into generation process, and use object-centric VLM to rerank candidates iteratively for prompt alignment.", "result": "Achieves stronger scene alignment with prompts compared to recent text-to-image models while preserving aesthetic quality.", "conclusion": "Unifying explicit layout-grounding with self-refine-based inference-time scaling improves layout faithfulness in text-to-image generation."}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "VC4VG is a framework for optimizing video captions specifically for text-to-video generation training, with a new benchmark showing improved caption quality leads to better video generation performance.", "motivation": "Current text-to-video generation relies on high-quality video-text pairs, but strategies for optimizing video captions specifically for T2V training are underexplored.", "method": "Proposed VC4VG framework that analyzes caption content from T2V perspective, decomposes essential elements for video reconstruction, and provides principled caption design methodology. Created VC4VG-Bench benchmark with fine-grained, multi-dimensional metrics aligned with T2V requirements.", "result": "Extensive T2V fine-tuning experiments show strong correlation between improved caption quality and video generation performance, validating the effectiveness of the approach.", "conclusion": "The VC4VG framework successfully addresses the gap in caption optimization for T2V training, with benchmark results demonstrating that better captions lead to improved video generation quality."}}
{"id": "2510.24136", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24136", "abs": "https://arxiv.org/abs/2510.24136", "authors": ["Ovi Sarkar", "Md Shafiuzzaman", "Md. Faysal Ahamed", "Golam Mahmud", "Muhammad E. H. Chowdhury"], "title": "MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images", "comment": null, "summary": "Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.", "AI": {"tldr": "Proposed MSRANetV2, a CNN architecture with ResNet50V2 backbone enhanced with residual attention and SE blocks, achieving state-of-the-art performance in colorectal cancer tissue classification on two public datasets.", "motivation": "Colorectal cancer is a leading cause of cancer mortality, and conventional diagnostic methods like colonoscopy are subjective, time-consuming, and variable. Deep learning can enhance diagnostic precision and efficiency in digital pathology.", "method": "Developed MSRANetV2 using ResNet50V2 backbone with residual attention mechanisms and squeeze-and-excitation blocks. Uses channel alignment and upsampling to fuse multi-scale features. Evaluated with five-fold stratified cross-validation on CRC-VAL-HE-7K and NCT-CRC-HE-100K datasets. Incorporated Grad-CAM for interpretability.", "result": "Achieved outstanding performance: on 7K dataset - Precision: 0.9884\u00b10.0151, Recall: 0.9900\u00b10.0151, F1: 0.9900\u00b10.0145, AUC: 0.9999\u00b10.00006, Accuracy: 0.9905\u00b10.0025; on 100K dataset - Precision: 0.9904\u00b10.0091, Recall: 0.9900\u00b10.0071, F1: 0.9900\u00b10.0071, AUC: 0.9997\u00b10.00016, Accuracy: 0.9902\u00b10.0006.", "conclusion": "MSRANetV2 is a reliable, interpretable, and high-performing model for colorectal cancer tissue classification, validated through comprehensive evaluation and visualization techniques."}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "A systematic framework for Vision-Language Models in autonomous driving that uses Mixture-of-Prompts routing, task-specific prompts with spatial reasoning, visual assembly, and optimized inference parameters to achieve state-of-the-art performance on the RoboSense Challenge.", "motivation": "To address the challenge of using Vision-Language Models for comprehensive autonomous driving scene understanding across perception, prediction, planning, and corruption detection tasks in the RoboSense Challenge at IROS 2025.", "method": "Four-component framework: 1) Mixture-of-Prompts router for question classification and expert prompt dispatch, 2) Task-specific prompts with coordinate systems, spatial reasoning, role-playing, Chain/Tree-of-Thought reasoning, and few-shot examples, 3) Visual assembly module for multi-view image composition with object crops and historical frames, 4) Task-specific inference parameter optimization.", "result": "Achieved 70.87% average accuracy on Phase-1 (clean data) and 72.85% on Phase-2 (corrupted data) using Qwen2.5-VL-72B, demonstrating substantial performance enhancement on safety-critical autonomous driving tasks.", "conclusion": "Structured prompting and spatial grounding significantly improve VLM performance on autonomous driving tasks, showing the effectiveness of the proposed systematic framework for safety-critical applications."}}
{"id": "2510.24195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24195", "abs": "https://arxiv.org/abs/2510.24195", "authors": ["Ziqi Zhou", "Yifan Hu", "Yufei Song", "Zijing Li", "Shengshan Hu", "Leo Yu Zhang", "Dezhong Yao", "Long Zheng", "Hai Jin"], "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2", "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.", "AI": {"tldr": "First cross-prompt universal adversarial attack (UAP-SAM2) against SAM2 video segmentation model that addresses architectural differences and semantic entanglement across frames.", "motivation": "SAM2's robustness remains unexplored despite its strong video segmentation capabilities, and existing SAM attacks cannot be directly transferred due to architectural differences involving prompt guidance and frame-to-frame semantic entanglement.", "method": "Proposed UAP-SAM2 with target-scanning strategy that divides frames into regions with random prompts to reduce prompt dependency, and dual semantic deviation framework that distorts semantics within frames and disrupts consistency across consecutive frames.", "result": "Extensive experiments on six datasets across two segmentation tasks show UAP-SAM2 significantly outperforms state-of-the-art attacks by a large margin.", "conclusion": "The proposed UAP-SAM2 effectively addresses the unique challenges of attacking SAM2 and demonstrates superior performance compared to existing methods."}}
{"id": "2510.24202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24202", "abs": "https://arxiv.org/abs/2510.24202", "authors": ["Anshul Kaushal", "Kunal Jangid", "Vinod K. Kurmi"], "title": "CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation", "comment": "The 36th British Machine Vision Conference (BMVC) 2025", "summary": "Accurate polyp and cardiac segmentation for early detection and treatment is\nessential for the diagnosis and treatment planning of cancer-like diseases.\nTraditional convolutional neural network (CNN) based models have represented\nlimited generalizability, robustness, and inability to handle uncertainty,\nwhich affects the segmentation performance. To solve these problems, this paper\nintroduces CLFSeg, an encoder-decoder based framework that aggregates the\nFuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy\nlogic. This module enhances the segmentation performance by identifying local\nand global features while minimizing the uncertainty, noise, and ambiguity in\nboundary regions, ensuring computing efficiency. In order to handle class\nimbalance problem while focusing on the areas of interest with tiny and\nboundary regions, binary cross-entropy (BCE) with dice loss is incorporated.\nOur proposed model exhibits exceptional performance on four publicly available\ndatasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.\nExtensive experiments and visual studies show CLFSeg surpasses the existing\nSOTA performance and focuses on relevant regions of interest in anatomical\nstructures. The proposed CLFSeg improves performance while ensuring computing\nefficiency, which makes it a potential solution for real-world medical\ndiagnostic scenarios. Project page is available at\nhttps://visdomlab.github.io/CLFSeg/", "AI": {"tldr": "CLFSeg is an encoder-decoder framework that combines fuzzy logic with convolutional layers to improve medical image segmentation for polyp and cardiac analysis, addressing uncertainty and boundary ambiguity while maintaining computational efficiency.", "motivation": "Traditional CNN models have limited generalizability, robustness, and inability to handle uncertainty in medical image segmentation, which affects performance for early cancer detection and treatment planning.", "method": "Proposes CLFSeg framework with Fuzzy-Convolutional (FC) module that aggregates convolutional layers and fuzzy logic to identify local/global features while minimizing uncertainty and noise. Uses binary cross-entropy with dice loss to handle class imbalance and focus on tiny boundary regions.", "result": "Exceptional performance on four public datasets (CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, ACDC), surpassing existing SOTA methods while focusing on relevant anatomical regions of interest.", "conclusion": "CLFSeg improves segmentation performance while ensuring computational efficiency, making it a potential solution for real-world medical diagnostic scenarios."}}
{"id": "2510.24211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24211", "abs": "https://arxiv.org/abs/2510.24211", "authors": ["Junhyuk So", "Hyunho Kook", "Chaeyeon Jang", "Eunhyeok Park"], "title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration", "comment": null, "summary": "While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.", "AI": {"tldr": "MC-SJD is a training-free, lossless parallel decoding framework that accelerates autoregressive visual generation by improving token stability across iterations through coupling-based sampling, achieving up to 4.2x image generation and 13.3x video generation speedups.", "motivation": "Autoregressive modeling for visual generation suffers from slow inference speed due to per-token generation requiring thousands of steps, limiting practical adoption despite its strong performance.", "method": "Extends Speculative Jacobi Decoding (SJD) with MC-SJD, an information-theoretic approach using coupling to maximize probability of sampling identical draft tokens across iterations while preserving lossless property. Requires only single-line modification to existing algorithm.", "result": "Achieves substantial performance gains: ~4.2x acceleration in image generation and ~13.3x acceleration in video generation compared to standard AR decoding, with no degradation in output quality.", "conclusion": "MC-SJD provides a simple yet effective solution to accelerate autoregressive visual generation by addressing token instability in SJD, enabling practical adoption through significant speed improvements while maintaining lossless generation quality."}}
{"id": "2510.24213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24213", "abs": "https://arxiv.org/abs/2510.24213", "authors": ["Haoxin Yang", "Yihong Lin", "Jingdan Kang", "Xuemiao Xu", "Yue Li", "Cheng Xu", "Shengfeng He"], "title": "Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization", "comment": null, "summary": "Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.", "AI": {"tldr": "ID\u00b2Face is a training-centric face anonymization framework that uses structured latent space disentanglement to separate identity and non-identity features, enabling direct anonymization without inference-time optimization.", "motivation": "Existing diffusion-based anonymization methods rely on inference-time interventions that cause distribution shifts and entangle identity with non-identity attributes, degrading visual quality and data utility.", "method": "Uses a conditional diffusion model with identity-masked learning, Identity-Decoupled Latent Recomposer with Identity VAE for identity features, bidirectional latent alignment for non-identity attributes, and Identity-Guided Latent Harmonizer with soft-gating. Includes Orthogonal Identity Mapping to suppress identity leakage.", "result": "Outperforms existing methods in visual quality, identity suppression, and utility preservation.", "conclusion": "ID\u00b2Face provides effective face anonymization through explicit disentanglement of identity and non-identity features in a training-centric approach, eliminating the need for inference-time optimization."}}
{"id": "2510.24214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24214", "abs": "https://arxiv.org/abs/2510.24214", "authors": ["Jinhong Deng", "Wen Li", "Joey Tianyi Zhou", "Yang He"], "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs", "comment": "NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.", "AI": {"tldr": "SCOPE is a visual token pruning method for MLLMs that jointly considers saliency and coverage to preserve semantic completeness while reducing computational overhead.", "motivation": "Existing visual token pruning methods focus only on saliency, leading to semantic incompleteness in selected tokens, while many visual tokens in MLLMs are redundant.", "method": "Proposes SCOPE score that integrates saliency with token-coverage gain, iteratively selecting tokens with highest SCOPE score based on set-coverage and token relationships.", "result": "Outperforms prior approaches on multiple vision-language benchmarks using LLaVA-1.5 and LLaVA-Next models.", "conclusion": "SCOPE effectively reduces computational overhead while maintaining semantic completeness through joint modeling of saliency and coverage."}}
{"id": "2510.24231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24231", "abs": "https://arxiv.org/abs/2510.24231", "authors": ["Waseem Shariff", "Timothy Hanley", "Maciej Stec", "Hossein Javidnia", "Peter Corcoran"], "title": "Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation", "comment": "Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference", "summary": "Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .", "AI": {"tldr": "This paper introduces the first event-based microsaccade dataset using simulated eye movements and evaluates spiking neural networks for microsaccade classification, achieving around 90% accuracy.", "motivation": "Traditional microsaccade studies using eye trackers or frame-based analysis are costly and limited in scalability and temporal resolution. Event-based sensing offers a high-speed, low-latency alternative for studying small eye movements.", "method": "Used Blender to render high-fidelity eye movement scenarios with microsaccades (0.5-2.0 degrees angular displacement), converted to event streams using v2e. Evaluated with Spiking-VGG11/13/16 models and proposed Spiking-VGG16Flow with optical flow enhancement.", "result": "Models achieved around 90% average accuracy in classifying microsaccades by angular displacement, independent of event count or duration. Successfully demonstrated spiking neural networks' capability for fine motion recognition.", "conclusion": "The work establishes a benchmark for event-based vision research and demonstrates the potential of spiking neural networks for microsaccade analysis. The dataset, code, and models will be publicly available."}}
{"id": "2510.24232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24232", "abs": "https://arxiv.org/abs/2510.24232", "authors": ["Qing Zhao", "Weijian Deng", "Pengxu Wei", "ZiYi Dong", "Hannan Lu", "Xiangyang Ji", "Liang Lin"], "title": "Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy", "comment": "NeurIPS 2025", "summary": "To improve detection robustness in adverse conditions (e.g., haze and low\nlight), image restoration is commonly applied as a pre-processing step to\nenhance image quality for the detector. However, the functional mismatch\nbetween restoration and detection networks can introduce instability and hinder\neffective integration -- an issue that remains underexplored. We revisit this\nlimitation through the lens of Lipschitz continuity, analyzing the functional\ndifferences between restoration and detection networks in both the input space\nand the parameter space. Our analysis shows that restoration networks perform\nsmooth, continuous transformations, while object detectors operate with\ndiscontinuous decision boundaries, making them highly sensitive to minor\nperturbations. This mismatch introduces instability in traditional cascade\nframeworks, where even imperceptible noise from restoration is amplified during\ndetection, disrupting gradient flow and hindering optimization. To address\nthis, we propose Lipschitz-regularized object detection (LROD), a simple yet\neffective framework that integrates image restoration directly into the\ndetector's feature learning, harmonizing the Lipschitz continuity of both tasks\nduring training. We implement this framework as Lipschitz-regularized YOLO\n(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive\nexperiments on haze and low-light benchmarks demonstrate that LR-YOLO\nconsistently improves detection stability, optimization smoothness, and overall\naccuracy.", "AI": {"tldr": "The paper proposes Lipschitz-regularized object detection (LROD) to address functional mismatch between image restoration and detection networks, improving detection robustness in adverse conditions.", "motivation": "Image restoration is commonly used as pre-processing for detection in adverse conditions, but functional mismatch between restoration and detection networks introduces instability and hinders effective integration.", "method": "Propose Lipschitz-regularized object detection (LROD) that integrates image restoration directly into detector's feature learning, harmonizing Lipschitz continuity of both tasks. Implement as LR-YOLO framework.", "result": "Extensive experiments on haze and low-light benchmarks show LR-YOLO consistently improves detection stability, optimization smoothness, and overall accuracy.", "conclusion": "LROD framework effectively addresses functional mismatch between restoration and detection networks, providing stable and accurate object detection in adverse conditions."}}
{"id": "2510.24260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24260", "abs": "https://arxiv.org/abs/2510.24260", "authors": ["Zhaotong Yang", "Yi Chen", "Yanying Li", "Shengfeng He", "Yangyang Xu", "Junyu Dong", "Jian Yang", "Yong Du"], "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity", "comment": null, "summary": "Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.", "AI": {"tldr": "Proposes DeshadowMamba, a shadow removal method using Mamba's selective state space model with CrossGate modulation and ColorShift regularization to improve structural and color consistency.", "motivation": "Existing attention-based shadow removal models mix irrelevant illumination cues, causing distorted structures and inconsistent colors. Mamba's directional state transitions offer better global context while preserving positional continuity.", "method": "Uses Mamba's selective state space model with CrossGate modulation to inject shadow-aware similarity into input gates, and ColorShift regularization with contrastive learning to suppress color contamination.", "result": "Achieves state-of-the-art visual quality and strong quantitative performance on public benchmarks.", "conclusion": "The proposed approach effectively adapts sequence modeling to shadow removal requirements, ensuring structural integrity and chromatic consistency through directional modulation and color regularization."}}
{"id": "2510.24262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24262", "abs": "https://arxiv.org/abs/2510.24262", "authors": ["Jiyu Guo", "Shuo Yang", "Yiming Huang", "Yancheng Long", "Xiaobo Xia", "Xiu Su", "Bo Zhao", "Zeke Xie", "Liqiang Nie"], "title": "UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.", "AI": {"tldr": "UtilGen is a utility-centric data augmentation framework that optimizes synthetic data generation using downstream task feedback, achieving better performance than traditional methods.", "motivation": "Existing data augmentation methods focus on visual quality metrics like fidelity and diversity but neglect task-specific requirements, which vary across different tasks and architectures.", "method": "UtilGen uses a weight allocation network to evaluate task-specific utility of synthetic samples, then employs dual-level optimization: model-level optimization tailors the generative model to the downstream task, and instance-level optimization adjusts generation policies like prompt embeddings and initial noise.", "result": "Extensive experiments on eight benchmark datasets show UtilGen consistently achieves superior performance with an average accuracy improvement of 3.87% over previous state-of-the-art methods.", "conclusion": "UtilGen validates the effectiveness of shifting from visual characteristics-centric to task utility-centric data augmentation, producing more impactful and task-relevant synthetic data."}}
{"id": "2510.24278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24278", "abs": "https://arxiv.org/abs/2510.24278", "authors": ["Pietro Bongini", "Valentina Molinari", "Andrea Costanzo", "Benedetta Tondi", "Mauro Barni"], "title": "Training-free Source Attribution of AI-generated Images via Resynthesis", "comment": "14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia", "summary": "Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.", "AI": {"tldr": "A training-free one-shot attribution method using image resynthesis outperforms existing techniques in few-shot scenarios for synthetic image source attribution.", "motivation": "Synthetic image source attribution is challenging under data scarcity conditions requiring few-shot or zero-shot classification capabilities.", "method": "Training-free one-shot attribution based on image resynthesis: generate a prompt describing the image, resynthesize with candidate sources, and attribute to model producing closest resynthesis in feature space.", "result": "Proposed resynthesis method outperforms state-of-the-art few-shot approaches and other baselines when only few samples are available for training or fine-tuning.", "conclusion": "The new dataset provides a challenging attribution framework and valuable benchmark for developing future few-shot and zero-shot methods."}}
{"id": "2510.24285", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24285", "abs": "https://arxiv.org/abs/2510.24285", "authors": ["Juntian Zhang", "Song Jin", "Chuanqi Cheng", "Yuhan Liu", "Yankai Lin", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model", "comment": null, "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.", "AI": {"tldr": "ViPER is a self-bootstrapping framework that enhances fine-grained visual perception in VLMs through a two-stage reinforcement learning approach with self-critiquing and self-prediction.", "motivation": "Address the bottleneck of limited fine-grained visual perception in VLMs, which is challenging due to data scarcity and limitations of existing methods like SFT compromising general capabilities and RFT prioritizing textual reasoning over visual perception.", "method": "Proposes a two-stage task structuring visual perception as coarse-to-fine progressive process, using self-bootstrapping framework with image-level and instance-level reconstruction integrated with two-stage reinforcement learning strategy.", "result": "Applied to Qwen2.5-VL family, produces Qwen-Viper series with average gain of 1.7% on seven benchmarks and up to 6.0% on fine-grained perception, demonstrating superior performance across vision-language scenarios while maintaining generalizability.", "conclusion": "ViPER enables self-improvement in perceptual capabilities and provides evidence for reciprocal relationship between generation and understanding, representing a breakthrough for developing more autonomous and capable VLMs."}}
{"id": "2510.24321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24321", "abs": "https://arxiv.org/abs/2510.24321", "authors": ["Ivica Dimitrovski", "Vlatko Spasev", "Ivan Kitanovski"], "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning", "comment": null, "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.", "AI": {"tldr": "Prompt learning methods outperform zero-shot CLIP and linear probing for few-shot remote sensing scene classification, with self-regulating constraints showing the best cross-domain performance.", "motivation": "Address the domain gap between general vision-language models like CLIP and remote sensing imagery, and overcome the scarcity of labeled data in remote sensing applications.", "method": "Systematically evaluate four prompt learning methods (Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, Prompting with Self-Regulating Constraints) against zero-shot CLIP with hand-crafted prompts and linear probe baselines.", "result": "Prompt learning consistently outperforms both baselines in few-shot scenarios across multiple benchmark datasets, with Prompting with Self-Regulating Constraints achieving the most robust cross-domain performance.", "conclusion": "Prompt learning provides a scalable and efficient solution for bridging domain gaps in satellite and aerial imagery, offering a strong foundation for future remote sensing research."}}
{"id": "2510.24366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24366", "abs": "https://arxiv.org/abs/2510.24366", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Ba-Thinh Lam", "Vi Vu", "Bach X. Nguyen", "Jianhua Xing", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation", "comment": "The paper is under review at Pattern Recognition Journal", "summary": "Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.", "AI": {"tldr": "A novel switching Dual-Student architecture with Loss-Aware Exponential Moving Average for semi-supervised medical image segmentation, improving collaboration and preventing error reinforcement.", "motivation": "Teacher-student frameworks in semi-supervised medical image segmentation are limited by strong correlation and unreliable knowledge transfer between teacher and student networks.", "method": "Switching Dual-Student architecture that selects the most reliable student at each iteration, plus Loss-Aware Exponential Moving Average to dynamically ensure teacher absorbs meaningful information from students.", "result": "Outperforms state-of-the-art semi-supervised methods on 3D medical image segmentation datasets, improving segmentation accuracy under limited supervision.", "conclusion": "The plug-and-play framework effectively enhances dual-student collaboration and prevents error reinforcement, demonstrating strong performance in semi-supervised medical image segmentation."}}
{"id": "2510.24374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24374", "abs": "https://arxiv.org/abs/2510.24374", "authors": ["Yuda Zou", "Zijian Zhang", "Yongchao Xu"], "title": "Decoupling What to Count and Where to See for Referring Expression Counting", "comment": null, "summary": "Referring Expression Counting (REC) extends class-level object counting to\nthe fine-grained subclass-level, aiming to enumerate objects matching a textual\nexpression that specifies both the class and distinguishing attribute. A\nfundamental challenge, however, has been overlooked: annotation points are\ntypically placed on class-representative locations (e.g., heads), forcing\nmodels to focus on class-level features while neglecting attribute information\nfrom other visual regions (e.g., legs for \"walking\"). To address this, we\npropose W2-Net, a novel framework that explicitly decouples the problem into\n\"what to count\" and \"where to see\" via a dual-query mechanism. Specifically,\nalongside the standard what-to-count (w2c) queries that localize the object, we\nintroduce dedicated where-to-see (w2s) queries. The w2s queries are guided to\nseek and extract features from attribute-specific visual regions, enabling\nprecise subclass discrimination. Furthermore, we introduce Subclass Separable\nMatching (SSM), a novel matching strategy that incorporates a repulsive force\nto enhance inter-subclass separability during label assignment. W2-Net\nsignificantly outperforms the state-of-the-art on the REC-8K dataset, reducing\ncounting error by 22.5% (validation) and 18.0% (test), and improving\nlocalization F1 by 7% and 8%, respectively. Code will be available.", "AI": {"tldr": "W2-Net addresses the overlooked challenge in Referring Expression Counting where annotation points focus on class-representative locations, causing models to neglect attribute information. It introduces dual-query mechanism and Subclass Separable Matching to improve subclass discrimination.", "motivation": "Current REC models suffer from focusing on class-level features due to annotation points being placed on class-representative locations (e.g., heads), which neglects attribute information from other visual regions (e.g., legs for \"walking\").", "method": "W2-Net uses a dual-query mechanism with what-to-count (w2c) queries for object localization and where-to-see (w2s) queries for attribute-specific feature extraction. It also introduces Subclass Separable Matching (SSM) with repulsive force for better inter-subclass separability.", "result": "W2-Net significantly outperforms state-of-the-art on REC-8K dataset, reducing counting error by 22.5% (validation) and 18.0% (test), and improving localization F1 by 7% and 8%, respectively.", "conclusion": "The proposed W2-Net framework effectively addresses the annotation bias in REC by decoupling object counting into \"what to count\" and \"where to see\", achieving substantial improvements in both counting accuracy and localization performance."}}
{"id": "2510.24378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24378", "abs": "https://arxiv.org/abs/2510.24378", "authors": ["Yann Kerverdo", "Florent Leray", "Youwan Mah\u00e9", "St\u00e9phanie Leplaideur", "Francesca Galassi"], "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool", "comment": null, "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.", "AI": {"tldr": "StrokeSeg is a lightweight, modular framework that converts research-grade stroke lesion segmentation models into deployable clinical applications with equivalent performance to original PyTorch pipelines.", "motivation": "Current deep learning frameworks like nnU-Net achieve state-of-the-art performance but are difficult to deploy clinically due to heavy dependencies and monolithic design.", "method": "Decouples preprocessing (using Anima toolbox with BIDS-compliant outputs), inference (using ONNX Runtime with Float16 quantization), and postprocessing. Provides both graphical and command-line interfaces, distributed as Python scripts and standalone Windows executable.", "result": "On 300 sub-acute and chronic stroke subjects, segmentation performance was equivalent to original PyTorch pipeline (Dice difference <10^-3), with model size reduced by about 50% through quantization.", "conclusion": "High-performing research pipelines can be successfully transformed into portable, clinically usable tools without sacrificing segmentation accuracy."}}
{"id": "2510.24379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24379", "abs": "https://arxiv.org/abs/2510.24379", "authors": ["Zhuangfan Huang", "Xiaosong Li", "Gao Wang", "Tao Ye", "Haishu Tan", "Huafeng Li"], "title": "A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset", "comment": null, "summary": "Polarization image fusion combines S0 and DOLP images to reveal surface\nroughness and material properties through complementary texture features, which\nhas important applications in camouflage recognition, tissue pathology\nanalysis, surface defect detection and other fields. To intergrate\ncoL-Splementary information from different polarized images in complex\nluminance environment, we propose a luminance-aware multi-scale network (MLSN).\nIn the encoder stage, we propose a multi-scale spatial weight matrix through a\nbrightness-branch , which dynamically weighted inject the luminance into the\nfeature maps, solving the problem of inherent contrast difference in polarized\nimages. The global-local feature fusion mechanism is designed at the bottleneck\nlayer to perform windowed self-attention computation, to balance the global\ncontext and local details through residual linking in the feature dimension\nrestructuring stage. In the decoder stage, to further improve the adaptability\nto complex lighting, we propose a Brightness-Enhancement module, establishing\nthe mapping relationship between luminance distribution and texture features,\nrealizing the nonlinear luminance correction of the fusion result. We also\npresent MSP, an 1000 pairs of polarized images that covers 17 types of indoor\nand outdoor complex lighting scenes. MSP provides four-direction polarization\nraw maps, solving the scarcity of high-quality datasets in polarization image\nfusion. Extensive experiment on MSP, PIF and GAND datasets verify that the\nproposed MLSN outperms the state-of-the-art methods in subjective and objective\nevaluations, and the MS-SSIM and SD metircs are higher than the average values\nof other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,\nrespectively. The source code and dataset is avalable at\nhttps://github.com/1hzf/MLS-UNet.", "AI": {"tldr": "A luminance-aware multi-scale network (MLSN) for polarization image fusion that addresses contrast differences in polarized images and adapts to complex lighting through dynamic luminance injection and brightness enhancement.", "motivation": "Polarization image fusion combines S0 and DOLP images to reveal surface properties, but faces challenges with inherent contrast differences and complex lighting environments that affect texture feature integration.", "method": "Proposes MLSN with multi-scale spatial weight matrix for dynamic luminance injection, global-local feature fusion with windowed self-attention, and Brightness-Enhancement module for nonlinear luminance correction. Also introduces MSP dataset with 1000 polarized image pairs.", "result": "Outperforms state-of-the-art methods on MSP, PIF and GAND datasets, with MS-SSIM and SD metrics 8.57%-63.53% higher than average values of other methods.", "conclusion": "MLSN effectively handles complex luminance environments in polarization image fusion and the new MSP dataset addresses data scarcity in this field."}}
{"id": "2510.24385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24385", "abs": "https://arxiv.org/abs/2510.24385", "authors": ["Herman Bergstr\u00f6m", "Zhongqi Yue", "Fredrik D. Johansson"], "title": "When are radiology reports useful for training medical image classifiers?", "comment": null, "summary": "Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.", "AI": {"tldr": "This paper systematically studies how radiology reports can be leveraged during training to improve medical image classification, examining both pre-training and fine-tuning approaches across different task types and dataset sizes.", "motivation": "Medical images often come with radiology reports containing expert annotations, but using these reports as inputs requires manual work. The research aims to determine when and how these reports can be leveraged during training to improve image-only classification, especially for tasks where labels are weakly associated with the text.", "method": "The study systematically evaluates how radiology reports can be used during both pre-training and fine-tuning phases, across diagnostic and prognostic tasks (e.g., 12-month readmission), and under varying training set sizes. It compares different approaches including explicit image-text alignment.", "result": "Key findings: (1) Leveraging reports during pre-training benefits downstream classification when labels are well-represented in text, but explicit image-text alignment can be detrimental when they're not; (2) Fine-tuning with reports can lead to significant improvements and sometimes has larger impact than pre-training method.", "conclusion": "The study provides actionable insights into when and how to leverage privileged text data for training medical image classifiers, while identifying gaps in current research regarding optimal use of radiology reports across different task types and settings."}}
{"id": "2510.24398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24398", "abs": "https://arxiv.org/abs/2510.24398", "authors": ["Youwan Mah\u00e9", "Elise Bannier", "St\u00e9phanie Leplaideur", "Elisa Fromont", "Francesca Galassi"], "title": "Unsupervised Detection of Post-Stroke Brain Abnormalities", "comment": null, "summary": "Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.", "AI": {"tldr": "REFLECT, a flow-based generative model, detects both focal and non-lesional abnormalities in post-stroke MRI better when trained on healthy controls rather than stroke patients.", "motivation": "Post-stroke MRI shows secondary structural changes like atrophy and ventricular enlargement, which are imaging biomarkers of recovery but poorly captured by supervised segmentation methods.", "method": "Used REFLECT, a flow-based generative model, for unsupervised detection of abnormalities. Trained two models on lesion-free slices from stroke patients (ATLAS) and on healthy controls (IXI), evaluated with dual-expert annotations and Free-Response ROC analysis.", "result": "IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43) compared to ATLAS-trained model.", "conclusion": "Training on fully healthy anatomy improves modeling of normal variability, enabling broader and more reliable detection of structural abnormalities in post-stroke patients."}}
{"id": "2510.24399", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24399", "abs": "https://arxiv.org/abs/2510.24399", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "GenTrack: A New Generation of Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack", "AI": {"tldr": "GenTrack is a novel multi-object tracking method that uses hybrid stochastic-deterministic tracking with PSO optimization and social interactions to handle varying target numbers, maintain ID consistency, and work effectively with weak detectors.", "motivation": "To address challenges in multi-object tracking including handling unknown and time-varying numbers of targets, maintaining target identity consistency, managing nonlinear dynamics, and working effectively with weak and noisy object detectors.", "method": "Hybrid tracking approach combining stochastic and deterministic methods, using PSO with fitness measures to guide particles, integrating social interactions among targets, and implementing a comprehensive state and observation model with space consistency, appearance, detection confidence, track penalties, and social scores.", "result": "Superior performance on standard benchmarks and real-world scenarios compared to state-of-the-art trackers, with reduced ID switches and track loss especially during occlusions.", "conclusion": "GenTrack provides an effective solution for robust multi-object tracking with three variants available, and the first publicly available source-code implementation facilitates further research and development in this area."}}
{"id": "2510.24410", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24410", "abs": "https://arxiv.org/abs/2510.24410", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "A Hybrid Approach for Visual Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2", "AI": {"tldr": "A visual multi-object tracking method combining stochastic particle filtering with deterministic association to maintain identifier consistency for varying numbers of targets under nonlinear dynamics.", "motivation": "To address the challenges of tracking unknown and time-varying numbers of targets with nonlinear dynamics while ensuring identifier consistency, especially during interactions and occlusions.", "method": "Uses stochastic particle filter with PSO optimization for nonlinear dynamics, deterministic association with cost matrix, smooth state updating scheme, and velocity regression for trend-seed velocities.", "result": "Superior performance compared to state-of-the-art trackers, with flexible operation for both pre-recorded videos and live camera streams.", "conclusion": "The proposed hybrid stochastic-deterministic approach effectively maintains tracking consistency and handles complex scenarios like interactions and occlusions."}}
{"id": "2510.24413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24413", "abs": "https://arxiv.org/abs/2510.24413", "authors": ["Ali Ahmad Faour", "Nabil Amacha", "Ali J. Ghandour"], "title": "50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon", "comment": null, "summary": "The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.", "AI": {"tldr": "A sensor-free approach using satellite imagery and machine learning to monitor reservoir volume in Lebanon, achieving 95% shoreline accuracy and under 1.5% error.", "motivation": "Sustainable management of Qaraaoun Reservoir requires reliable monitoring despite sensor malfunctions and limited maintenance capacity in Lebanon.", "method": "Integration of Sentinel-2/Landsat imagery with new water segmentation index and Support Vector Regression model trained on bathymetry data to estimate volume from surface area alone.", "result": "95% shoreline alignment with ground truth, optimized SVR with under 1.5% error of full capacity, and R\u00b2 exceeding 0.98.", "conclusion": "The method provides robust, cost-effective sensor-independent monitoring that can be replicated for other water bodies and supports climate change research."}}
{"id": "2510.24414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24414", "abs": "https://arxiv.org/abs/2510.24414", "authors": ["Reem Hammoud", "Abdul karim Gizzini", "Ali J. Ghandour"], "title": "XAI Evaluation Framework for Semantic Segmentation", "comment": null, "summary": "Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.", "AI": {"tldr": "This paper introduces a comprehensive evaluation framework for explainable AI (XAI) methods in semantic segmentation, addressing the gap in evaluation strategies for this specific task compared to classification.", "motivation": "As AI models are increasingly used in safety-critical domains, ensuring transparency and trust is essential. While XAI has emerged to address this, rigorous evaluation methods for semantic segmentation remain underexplored compared to classification tasks.", "method": "The paper proposes a systematic evaluation framework specifically designed for XAI in semantic segmentation, accounting for both spatial and contextual complexities. It employs pixel-level evaluation strategies and carefully designed metrics for fine-grained interpretability insights.", "result": "Simulation results using class activation mapping (CAM)-based XAI schemes demonstrate the efficiency, robustness, and reliability of the proposed methodology.", "conclusion": "The findings contribute to advancing transparent, trustworthy, and accountable semantic segmentation models by providing a comprehensive evaluation framework for XAI methods in this domain."}}
{"id": "2510.24437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24437", "abs": "https://arxiv.org/abs/2510.24437", "authors": ["Zhineng Zhao", "Zhihai He", "Zikun Zhou", "Siwei Ma", "Yaowei Wang"], "title": "Deeply-Conditioned Image Compression via Self-Generated Priors", "comment": null, "summary": "Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.", "AI": {"tldr": "DCIC-sgp is a learned image compression framework that uses self-generated priors to separate global structures from local textures, reducing geometric deformation at low bitrates and achieving significant BD-rate reductions against VVC.", "motivation": "Current learned image compression methods struggle to model complex correlation structures in natural images, particularly the entanglement of invariant global structures with transient local textures, leading to severe geometric deformation at low bitrates.", "method": "The framework uses functional decomposition with self-generated priors that encapsulate the image's structural backbone. This prior holistically modulates the entire compression pipeline, especially the analysis transform, allowing it to focus on residual high-entropy details through hierarchical, dependency-driven information disentanglement.", "result": "The method substantially mitigates geometric deformation artifacts at low bitrates and achieves significant BD-rate reductions of 14.4%, 15.7%, and 15.1% against VTM-12.1 on Kodak, CLIC, and Tecnick datasets respectively.", "conclusion": "DCIC-sgp's deeply-conditioned approach with self-generated priors effectively disentangles information streams and establishes highly competitive performance in learned image compression."}}
{"id": "2510.24448", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68T20", "I.2.10; I.4.8; I.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24448", "abs": "https://arxiv.org/abs/2510.24448", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "Rethinking Visual Intelligence: Insights from Video Pretraining", "comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2", "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.", "AI": {"tldr": "Video Diffusion Models (VDMs) pretrained on spatiotemporal data show better data efficiency and compositional understanding than LLMs in visual tasks, suggesting video pretraining provides useful inductive biases for visual foundation models.", "motivation": "LLMs have succeeded in language tasks but struggle with compositional understanding and sample efficiency in the visual domain. The paper investigates whether Video Diffusion Models can bridge this gap.", "method": "Compare pretrained LLMs and VDMs equipped with lightweight adapters on various visual benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata.", "result": "VDMs demonstrate higher data efficiency than LLMs across all benchmarks, showing better performance in visual tasks with less training data.", "conclusion": "Video pretraining provides strong inductive biases for structure and dynamics that support progress toward effective visual foundation models."}}
{"id": "2510.24456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24456", "abs": "https://arxiv.org/abs/2510.24456", "authors": ["Vivek Chetia", "Abdul Taher Khan", "Rahish Gogoi", "David Kapsian Khual", "Purnendu Bikash", "Sajal Saha"], "title": "A Critical Study towards the Detection of Parkinsons Disease using ML Technologies", "comment": null, "summary": "The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.", "AI": {"tldr": "Deep learning approach for detecting and segmenting three tea leaf diseases (Red Rust, Helopeltis, Red Spider Mite) using object detection models and custom damage area calculation.", "motivation": "To automatically classify tea leaf diseases caused by pests and pathogens, and quantify the damaged area on leaves to help with disease management.", "method": "Evaluated SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for object detection, and Mask R-CNN for instance segmentation with custom method to calculate damaged leaf portions.", "result": "Faster R-CNN ResNet50 V1 achieved better performance with 25% mAP compared to SSD MobileNet V2's 20.9% mAP. Both models showed low precision and recall values on IOU 0.50:0.95 range.", "conclusion": "Faster R-CNN ResNet50 V1 outperforms SSD MobileNet V2 for tea leaf disease detection, and Mask R-CNN with custom method enables damaged area quantification, though overall performance needs improvement."}}
{"id": "2510.24464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24464", "abs": "https://arxiv.org/abs/2510.24464", "authors": ["Charles Javerliat", "Pierre Raimbaud", "Guillaume Lavou\u00e9"], "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras", "comment": null, "summary": "Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.", "AI": {"tldr": "Kineo is a calibration-free pipeline for markerless motion capture from unsynchronized, uncalibrated RGB cameras that simultaneously calibrates cameras and reconstructs 3D keypoints at metric scale with high accuracy and efficiency.", "motivation": "Existing calibration-free motion capture methods suffer from high computational cost and reduced accuracy, limiting accessibility for non-experts and in-the-wild captures.", "method": "Leverages 2D keypoints from off-the-shelf detectors with confidence-driven spatio-temporal sampling and graph-based global optimization for simultaneous camera calibration and 3D reconstruction, including Brown-Conrady distortion coefficients.", "result": "Substantial improvements over prior methods: reduces camera translation error by 83-85%, camera angular error by 86-92%, and world mean-per-joint error by 83-91%. Processes multi-view sequences faster than their duration in some cases.", "conclusion": "Kineo provides an efficient, accurate calibration-free solution for markerless motion capture that outperforms existing methods and enables practical adoption in real-world scenarios."}}
{"id": "2510.24474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24474", "abs": "https://arxiv.org/abs/2510.24474", "authors": ["Kyungmin Lee", "Sihyun Yu", "Jinwoo Shin"], "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling", "comment": null, "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.", "AI": {"tldr": "Decoupled MeanFlow converts pretrained flow models into flow map models without architectural changes, enabling high-quality image generation in 1-4 steps with 100x faster inference.", "motivation": "Existing denoising generative models require many sampling steps due to discretization error, and flow map training typically needs architectural modifications that limit compatibility with pretrained models.", "method": "Decoupled MeanFlow conditions the final blocks of diffusion transformers on the subsequent timestep, converting flow models to flow maps without architectural changes. Combined with enhanced training techniques.", "result": "Achieves 1-step FID of 2.16 and 2.12 on ImageNet 256x256 and 512x512 respectively, surpassing prior art. With 4 steps, achieves FID of 1.51 and 1.68, nearly matching flow model performance.", "conclusion": "Training flow models first and then converting them to flow maps is more efficient than training flow maps from scratch, enabling high-quality generation with dramatically faster inference."}}
{"id": "2510.24486", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.24486", "abs": "https://arxiv.org/abs/2510.24486", "authors": ["Tinsae G. Dulecha", "Leonardo Righetto", "Ruggero Pintus", "Enrico Gobbetti", "Andrea Giachetti"], "title": "Fast and accurate neural reflectance transformation imaging through knowledge distillation", "comment": "18 pages", "summary": "Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...", "AI": {"tldr": "NeuralRTI uses neural autoencoders for superior reflectance field modeling but is computationally expensive. DisK-NeuralRTI applies knowledge distillation to reduce computational costs while maintaining quality.", "motivation": "Traditional RTI methods like PTM and HSH are compact but struggle with complex reflectance fields, while NeuralRTI provides superior quality but is computationally expensive for interactive relighting on limited hardware.", "method": "Proposes DisK-NeuralRTI using knowledge distillation to transfer knowledge from a large teacher network to a smaller student network, reducing computational costs while maintaining rendering quality.", "result": "The method successfully reduces computational requirements of NeuralRTI while preserving the quality of reflectance field modeling and interactive relighting capabilities.", "conclusion": "Knowledge distillation enables efficient NeuralRTI implementation that maintains high-quality results while being computationally feasible for interactive use on limited hardware."}}
{"id": "2510.24514", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24514", "abs": "https://arxiv.org/abs/2510.24514", "authors": ["Huanyu Zhang", "Wenshan Wu", "Chengzu Li", "Ning Shang", "Yan Xia", "Yangyu Huang", "Yifan Zhang", "Li Dong", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.", "AI": {"tldr": "Latent Sketchpad equips MLLMs with an internal visual scratchpad for generative visual thinking, enabling them to interleave textual reasoning with visual latent generation without compromising reasoning performance.", "motivation": "MLLMs excel at visual understanding but struggle with complex scenarios requiring visual planning and imagination, unlike humans who use sketching for visual thinking and idea development.", "method": "Integrates visual generation directly into MLLMs' native autoregressive reasoning process using two components: Context-Aware Vision Head for autoregressive visual representation production and pretrained Sketch Decoder for rendering interpretable sketch images.", "result": "Delivers comparable or superior reasoning performance to backbone MLLMs on MazePlanning dataset, generalizes across different MLLMs (Gemma3, Qwen2.5-VL), and enables visual thinking alongside textual reasoning.", "conclusion": "The framework extends MLLMs' reasoning capabilities to visual thinking, opening new opportunities for richer human-computer interaction and broader applications."}}
{"id": "2510.24563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24563", "abs": "https://arxiv.org/abs/2510.24563", "authors": ["Hongrui Jia", "Jitong Liao", "Xi Zhang", "Haiyang Xu", "Tianbao Xie", "Chaoya Jiang", "Ming Yan", "Si Liu", "Wei Ye", "Fei Huang"], "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents", "comment": null, "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.", "AI": {"tldr": "OSWorld-MCP is the first comprehensive benchmark for evaluating multimodal agents' tool invocation, GUI operation, and decision-making abilities in real-world computer environments, addressing the gap in fair assessment of tool usage capabilities.", "motivation": "Past evaluations focused mainly on GUI interaction skills while overlooking tool invocation abilities enabled by Model Context Protocol (MCP), creating unfair comparisons between agents with integrated tools and those evaluated only on GUI interaction.", "method": "Developed OSWorld-MCP benchmark with automated code-generation pipeline to create tools, combined with curated existing tools, resulting in 158 high-quality tools covering 7 common applications, all manually validated for functionality, applicability, and versatility.", "result": "MCP tools significantly improved task success rates (e.g., from 8.3% to 20.4% for OpenAI o3, from 40.1% to 43.3% for Claude 4 Sonnet), but even the strongest models had low tool invocation rates (only 36.3%), indicating room for improvement.", "conclusion": "OSWorld-MCP sets a new standard for evaluating multimodal agents in complex, tool-assisted environments by explicitly measuring MCP tool usage skills, providing deeper understanding of agent capabilities and highlighting the benchmark's challenging nature."}}
{"id": "2510.24579", "categories": ["cs.CV", "I.4.5; I.5"], "pdf": "https://arxiv.org/pdf/2510.24579", "abs": "https://arxiv.org/abs/2510.24579", "authors": ["Xu Jiang", "Huiying Pan", "Ligen Shi", "Jianing Sun", "Wenfeng Xu", "Xing Zhao"], "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT", "comment": "8 pages, 6 figures", "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.", "AI": {"tldr": "Deep learning method using Gaussian RBF and KAN networks to correct scatter artifacts in CBCT images by modeling rotational symmetry of scatter distribution.", "motivation": "CBCT suffers from scatter artifacts that cause CT value bias and reduced tissue contrast, degrading diagnostic accuracy.", "method": "Uses Gaussian Radial Basis Functions to model point scatter function and embeds it into Kolmogorov-Arnold Networks layers to learn high-dimensional scatter features with physical prior knowledge.", "result": "Effectively corrects scatter artifacts in reconstructed images and outperforms current methods in quantitative metrics, validated through synthetic and real-scan experiments.", "conclusion": "The proposed method successfully combines physical scatter characteristics with KAN's complex function mapping to improve scatter representation and artifact correction in CBCT imaging."}}
{"id": "2510.24640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24640", "abs": "https://arxiv.org/abs/2510.24640", "authors": ["Xin Zhang", "Yuqi Song", "Fei Zuo"], "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries", "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.", "AI": {"tldr": "A dual-branch CNN for face forgery detection that combines spatial (RGB) and frequency domain features with adaptive fusion and a unified loss function, achieving strong performance across multiple forgery types.", "motivation": "The rapid advancement of generative AI enables creation of highly realistic forged facial images, posing threats to AI security, digital media integrity, and public trust. There is urgent need for robust detection methods against malicious uses like misinformation and identity fraud.", "method": "Dual-branch CNN with RGB branch for semantic information and frequency branch for high-frequency artifacts. Uses channel attention module for adaptive feature fusion and FSC Loss (focal loss, supervised contrastive loss, frequency center margin loss) to enhance class separability.", "result": "Evaluated on DiFF benchmark with four forgery types (text-to-image, image-to-image, face swap, face edit). Achieved strong performance across all categories and outperformed average human accuracy.", "conclusion": "The model demonstrates effectiveness and potential contribution to safeguarding AI ecosystems against visual forgery attacks through its dual-domain approach and unified loss strategy."}}
{"id": "2510.24653", "categories": ["cs.CV", "cs.HC", "J.3"], "pdf": "https://arxiv.org/pdf/2510.24653", "abs": "https://arxiv.org/abs/2510.24653", "authors": ["Veronica Thai", "Rui Li", "Meng Ling", "Shuning Jiang", "Jeremy Wolfe", "Raghu Machiraju", "Yan Hu", "Zaibo Li", "Anil Parwani", "Jian Chen"], "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology", "comment": "16 pages, 9 figures, submitted to Nature Scientific Data", "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.", "AI": {"tldr": "PathoGaze1.0 is a comprehensive behavioral dataset capturing pathologists' visual search and decision-making during cancer diagnosis from whole-slide images, including eye-tracking, mouse interactions, and diagnostic decisions.", "motivation": "Pathologists' diagnostic accuracy averages around 70% with poor inter-observer consistency, and there's a lack of behavioral data to explain diagnostic errors and inconsistencies in whole-slide image interpretation.", "method": "Collected 18.69 hours of eye-tracking, mouse interaction, stimulus tracking, viewport navigation, and diagnostic decision data from 19 pathologists interpreting 397 whole-slide images using the PTAH application-grounded testbed for ecological validity.", "result": "Recorded 171,909 fixations, 263,320 saccades, and 1,867,362 mouse interaction events, creating a comprehensive behavioral dataset of pathologists' diagnostic workflow.", "conclusion": "The PathoGaze1.0 dataset provides valuable behavioral data that can help explain diagnostic errors and improve training for both pathologists and AI systems, with preregistered experiments and publicly available data."}}
{"id": "2510.24657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24657", "abs": "https://arxiv.org/abs/2510.24657", "authors": ["Xuanpu Zhang", "Xuesong Niu", "Ruidong Chen", "Dan Song", "Jianhao Zeng", "Penghui Du", "Haoxiang Cao", "Kai Wu", "An-an Liu"], "title": "Group Relative Attention Guidance for Image Editing", "comment": null, "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.", "AI": {"tldr": "Proposes Group Relative Attention Guidance (GRAG) for fine-grained control over image editing intensity in Diffusion-in-Transformer models by reweighting attention delta values.", "motivation": "Existing Diffusion-in-Transformer editing methods lack effective control over editing degree, limiting customization capabilities.", "method": "Analyzes MM-Attention mechanism in DiT models, identifies bias vectors as inherent editing behavior, and proposes GRAG to reweight delta values between tokens and biases to modulate editing focus.", "result": "GRAG enables continuous and fine-grained control over editing intensity without tuning, integrates with existing frameworks in 4 lines of code, and outperforms Classifier-Free Guidance in smoothness and precision.", "conclusion": "GRAG provides an effective solution for controlling editing intensity in Diffusion-in-Transformer models, enhancing editing quality with minimal implementation effort."}}
{"id": "2510.24667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24667", "abs": "https://arxiv.org/abs/2510.24667", "authors": ["Mia Kan", "Yilin Liu", "Niloy Mitra"], "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips", "comment": "Website: https://kan32501.github.io/sage.github.io/", "summary": "Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.", "AI": {"tldr": "SAGE is a zero-shot method for video transitions that combines structural guidance with generative synthesis to create smooth, semantically consistent transitions between diverse clips with large temporal gaps.", "motivation": "Existing video transition methods struggle with bridging diverse clips involving large temporal gaps or significant semantic differences, creating artifacts and breaking temporal coherence.", "method": "SAGE uses structure-aware generative approach with line maps and motion flow for structural guidance, combined with generative synthesis, without requiring fine-tuning.", "result": "Extensive experiments show SAGE outperforms both classical and generative baselines (FILM, TVG, DiffMorpher, VACE, GI) on quantitative metrics and user studies.", "conclusion": "SAGE enables smooth, semantically consistent transitions between diverse video clips without fine-tuning, bridging the gap for content-aware and visually coherent transitions."}}
{"id": "2510.24688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24688", "abs": "https://arxiv.org/abs/2510.24688", "authors": ["Yun Zhang", "Zhaoliang Zheng", "Johnson Liu", "Zhiyu Huang", "Zewei Zhou", "Zonglin Meng", "Tianhui Cai", "Jiaqi Ma"], "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection", "comment": null, "summary": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.", "AI": {"tldr": "MIC-BEV is a Transformer-based BEV perception framework for infrastructure-based multi-camera 3D object detection that handles variable cameras and sensor degradation, achieving SOTA performance.", "motivation": "Existing camera-based detection models underperform in infrastructure scenarios due to multi-view setups, diverse camera configurations, degraded inputs, and varying road layouts.", "method": "Uses Transformer-based BEV framework with graph-enhanced fusion module that integrates multi-view features using geometric relationships between cameras and BEV cells, plus visual cues.", "result": "Achieves state-of-the-art performance on both synthetic M2I dataset and real-world RoScenes dataset, with strong robustness under extreme weather and sensor degradation.", "conclusion": "MIC-BEV demonstrates strong potential for real-world deployment in intelligent transportation systems with infrastructure-based perception."}}
{"id": "2510.24709", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.", "AI": {"tldr": "Vision Transformers naturally develop object binding capabilities during self-supervised pretraining, allowing them to determine whether image patches belong to the same object, which actively guides attention and serves the learning objective.", "motivation": "To investigate whether object binding - the ability to group features into coherent object representations - naturally emerges in pre-trained Vision Transformers, rather than being explicitly imposed through architectural constraints like Slot Attention.", "method": "Used a similarity probe to decode 'IsSameObject' (whether two patches belong to the same object) from patch embeddings across ViT layers, comparing self-supervised models (DINO, MAE, CLIP) with ImageNet-supervised models.", "result": "Self-supervised ViTs achieved over 90% accuracy in detecting same-object patches, while supervised models showed markedly weaker binding capabilities. IsSameObject is encoded in a low-dimensional subspace and actively guides attention.", "conclusion": "Object binding emerges naturally in self-supervised ViTs as an ability acquired through specific pretraining objectives, challenging the view that ViTs lack object binding and demonstrating how symbolic knowledge of object composition emerges in connectionist systems."}}
{"id": "2510.24711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24711", "abs": "https://arxiv.org/abs/2510.24711", "authors": ["Yujie Wei", "Shiwei Zhang", "Hangjie Yuan", "Yujin Han", "Zhekai Chen", "Jiayu Wang", "Difan Zou", "Xihui Liu", "Yingya Zhang", "Yu Liu", "Hongming Shan"], "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.", "AI": {"tldr": "ProMoE is a novel Mixture-of-Experts framework for Diffusion Transformers that uses a two-step router with explicit routing guidance to address the challenges of applying MoE to visual tokens, achieving state-of-the-art performance on ImageNet.", "motivation": "Existing attempts to apply MoE to Diffusion Transformers have shown limited gains due to fundamental differences between language and visual tokens. Visual tokens exhibit spatial redundancy and functional heterogeneity, which hinders expert specialization in vision MoE applications.", "method": "ProMoE features a two-step router with explicit routing guidance: 1) conditional routing that partitions image tokens into conditional and unconditional sets based on functional roles, and 2) prototypical routing with learnable prototypes that refines assignments of conditional image tokens based on semantic content. It also includes a routing contrastive loss to enhance intra-expert coherence and inter-expert diversity.", "result": "Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives, showing significant improvements over previous MoE approaches for Diffusion Transformers.", "conclusion": "The proposed ProMoE framework successfully addresses the challenges of applying MoE to visual tokens through its two-step routing mechanism and explicit semantic guidance, establishing that such guidance is crucial for effective vision MoE systems."}}
{"id": "2510.24717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24717", "abs": "https://arxiv.org/abs/2510.24717", "authors": ["Haoge Deng", "Ting Pan", "Fan Zhang", "Yang Liu", "Zhuoyan Luo", "Yufeng Cui", "Wenxuan Wang", "Chunhua Shen", "Shiguang Shan", "Zhaoxiang Zhang", "Xinlong Wang"], "title": "Uniform Discrete Diffusion with Metric Path for Video Generation", "comment": "19 pages, 10 figures", "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA", "AI": {"tldr": "URSA is a discrete generative modeling framework for scalable video generation that bridges the performance gap with continuous approaches through iterative global refinement of discrete tokens.", "motivation": "Discrete approaches for video generation lag behind continuous-space methods due to error accumulation and long-context inconsistency issues.", "method": "URSA formulates video generation as iterative global refinement of discrete spatiotemporal tokens, using Linearized Metric Path and Resolution-dependent Timestep Shifting mechanisms, with asynchronous temporal fine-tuning for unified task handling.", "result": "URSA outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods on challenging video and image generation benchmarks.", "conclusion": "URSA successfully bridges the gap between discrete and continuous approaches for scalable video generation, enabling efficient high-resolution synthesis and long-duration generation with fewer inference steps."}}
{"id": "2510.24718", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24718", "abs": "https://arxiv.org/abs/2510.24718", "authors": ["Chonghyuk Song", "Michal Stary", "Boyuan Chen", "George Kopanas", "Vincent Sitzmann"], "title": "Generative View Stitching", "comment": "Project website: https://andrewsonga.github.io/gvs", "summary": "Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.", "AI": {"tldr": "Generative View Stitching (GVS) enables stable, collision-free camera-guided video generation by sampling entire sequences in parallel and using diffusion stitching with Omni Guidance for temporal consistency.", "motivation": "Autoregressive video diffusion models cannot incorporate future conditioning, leading to collisions with generated scenes when following predefined camera trajectories, causing model collapse.", "method": "Proposes GVS sampling algorithm that extends diffusion stitching from robot planning to video generation, compatible with any Diffusion Forcing-trained model. Introduces Omni Guidance for past/future conditioning and loop-closing mechanism.", "result": "GVS achieves stable, collision-free, frame-to-frame consistent video generation that closes loops for various predefined camera paths, including impossible geometries like the Impossible Staircase.", "conclusion": "The method successfully addresses limitations of autoregressive models in camera-guided generation, enabling faithful scene generation along complex camera trajectories with long-range coherence."}}
