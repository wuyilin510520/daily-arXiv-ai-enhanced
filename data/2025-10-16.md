<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 99]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms](https://arxiv.org/abs/2510.12901)
*Haithem Turki,Qi Wu,Xin Kang,Janick Martinez Esturo,Shengyu Huang,Ruilong Li,Zan Gojcic,Riccardo de Lutio*

Main category: cs.CV

TL;DR: SimULi enables real-time rendering of arbitrary camera models and LiDAR data for autonomous robot testing, addressing cross-sensor inconsistencies and outperforming existing methods in speed and fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing neural rendering methods have limitations in rendering speed, support for high-distortion lenses, and handling cross-sensor inconsistencies between cameras and LiDAR, which hinders their use in rigorous autonomous robot testing.

Method: Extends 3DGUT with LiDAR support using automated tiling for spinning LiDAR models and ray-based culling. Uses factorized 3D Gaussian representation and anchoring strategy to address cross-sensor inconsistencies.

Result: Renders 10-20x faster than ray tracing approaches and 1.5-10x faster than prior rasterization-based methods. Reduces mean camera and depth error by up to 40% compared to existing methods. Matches or exceeds state-of-the-art fidelity on autonomous driving datasets.

Conclusion: SimULi provides a comprehensive solution for multi-sensor simulation that overcomes key limitations of existing methods, enabling high-fidelity, real-time testing of autonomous robots with both camera and LiDAR data.

Abstract: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

</details>


### [2] [State-Change Learning for Prediction of Future Events in Endoscopic Videos](https://arxiv.org/abs/2510.12904)
*Saurav Sharma,Chinedu Innocent Nwoye,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: SurgFUTR reframes surgical future prediction as state-change learning using a teacher-student architecture with Sinkhorn-Knopp clustering and Action Dynamics module, achieving consistent improvements across multiple procedures and prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Current surgical AI focuses on understanding current events rather than predicting future ones, lacking unified approaches for both short-term and long-term prediction. Existing methods are task-specific, rely on coarse supervision, and struggle with generalization across different surgical contexts.

Method: SurgFUTR uses a teacher-student architecture where video clips are compressed into state representations via Sinkhorn-Knopp clustering. The teacher learns from current and future clips, while the student predicts future states from current videos alone using an Action Dynamics module.

Result: Experiments across four datasets and three procedures show consistent improvements. Cross-procedure transfer validates the method's generalizability across different surgical contexts.

Conclusion: Reframing surgical future prediction as state-change learning rather than raw observation forecasting enables more robust and generalizable prediction across multiple surgical procedures and time horizons.

Abstract: Surgical future prediction, driven by real-time AI analysis of surgical
video, is critical for operating room safety and efficiency. It provides
actionable insights into upcoming events, their timing, and risks-enabling
better resource allocation, timely instrument readiness, and early warnings for
complications (e.g., bleeding, bile duct injury). Despite this need, current
surgical AI research focuses on understanding what is happening rather than
predicting future events. Existing methods target specific tasks in isolation,
lacking unified approaches that span both short-term (action triplets, events)
and long-term horizons (remaining surgery duration, phase transitions). These
methods rely on coarse-grained supervision while fine-grained surgical action
triplets and steps remain underexplored. Furthermore, methods based only on
future feature prediction struggle to generalize across different surgical
contexts and procedures. We address these limits by reframing surgical future
prediction as state-change learning. Rather than forecasting raw observations,
our approach classifies state transitions between current and future timesteps.
We introduce SurgFUTR, implementing this through a teacher-student
architecture. Video clips are compressed into state representations via
Sinkhorn-Knopp clustering; the teacher network learns from both current and
future clips, while the student network predicts future states from current
videos alone, guided by our Action Dynamics (ActDyn) module. We establish
SFPBench with five prediction tasks spanning short-term (triplets, events) and
long-term (remaining surgery duration, phase and step transitions) horizons.
Experiments across four datasets and three procedures show consistent
improvements. Cross-procedure transfer validates generalizability.

</details>


### [3] [Robust Plant Disease Diagnosis with Few Target-Domain Samples](https://arxiv.org/abs/2510.12909)
*Takafumi Nogami,Satoshi Kagiwada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: TMPS is a target-aware metric learning framework that improves plant disease diagnosis robustness by using limited target domain samples to address domain gaps and symptom variability.


<details>
  <summary>Details</summary>
Motivation: Deep learning plant disease diagnosis systems fail to maintain accuracy on images from different conditions than training due to domain gaps and symptom variability, limiting real-world deployment.

Method: Target-Aware Metric Learning with Prioritized Sampling (TMPS) - a metric learning framework that leverages limited labeled target domain samples to improve generalization across domains.

Result: TMPS achieves significant improvements: 7.3 and 3.6 points higher average macro F1 score compared to combined training and fine-tuning baselines, and 18.7/17.1 points over baseline and conventional metric learning.

Conclusion: TMPS effectively addresses domain adaptation challenges in plant disease diagnosis using limited target domain samples, demonstrating strong performance improvements on a large-scale dataset with 223,073 images across 23 fields.

Abstract: Various deep learning-based systems have been proposed for accurate and
convenient plant disease diagnosis, achieving impressive performance. However,
recent studies show that these systems often fail to maintain diagnostic
accuracy on images captured under different conditions from the training
environment -- an essential criterion for model robustness. Many deep learning
methods have shown high accuracy in plant disease diagnosis. However, they
often struggle to generalize to images taken in conditions that differ from the
training setting. This drop in performance stems from the subtle variability of
disease symptoms and domain gaps -- differences in image context and
environment. The root cause is the limited diversity of training data relative
to task complexity, making even advanced models vulnerable in unseen domains.
To tackle this challenge, we propose a simple yet highly adaptable learning
framework called Target-Aware Metric Learning with Prioritized Sampling (TMPS),
grounded in metric learning. TMPS operates under the assumption of access to a
limited number of labeled samples from the target (deployment) domain and
leverages these samples effectively to improve diagnostic robustness. We assess
TMPS on a large-scale automated plant disease diagnostic task using a dataset
comprising 223,073 leaf images sourced from 23 agricultural fields, spanning 21
diseases and healthy instances across three crop species. By incorporating just
10 target domain samples per disease into training, TMPS surpasses models
trained using the same combined source and target samples, and those fine-tuned
with these target samples after pre-training on source data. It achieves
average macro F1 score improvements of 7.3 and 3.6 points, respectively, and a
remarkable 18.7 and 17.1 point improvement over the baseline and conventional
metric learning.

</details>


### [4] [Unifying Vision-Language Latents for Zero-label Image Caption Enhancement](https://arxiv.org/abs/2510.12931)
*Sanghyun Byun,Jung Ick Guack,Mohanad Odema,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CV

TL;DR: ViZer is a zero-label enhancement framework that improves vision-language models' captioning capabilities without requiring labeled data by aligning vision and language representations during training.


<details>
  <summary>Details</summary>
Motivation: Current VLMs rely heavily on labeled image datasets, limiting scalability and underutilizing vast amounts of unlabeled image data. There's a need for methods that can enhance VLMs without requiring text labels.

Method: ViZer actively aligns vision and language representation features during training, enabling existing VLMs to generate improved captions without requiring text labels or full retraining.

Result: ViZer shows consistent qualitative improvements when applied to SmolVLM-Base and Qwen2-VL, producing more grounded and descriptive captions than baseline models. Automated metrics like CIDEr and BERTScore often penalize details absent in reference captions.

Conclusion: ViZer provides a practical framework for zero-label enhancement in vision-language tasks, demonstrating that qualitative improvements in caption quality can be achieved without labeled data through better vision-language alignment.

Abstract: Vision-language models (VLMs) achieve remarkable performance through
large-scale image-text pretraining. However, their reliance on labeled image
datasets limits scalability and leaves vast amounts of unlabeled image data
underutilized. To address this, we propose Unified Vision-Language Alignment
for Zero-Label Enhancement (ViZer), an enhancement training framework that
enables zero-label learning in image captioning, providing a practical starting
point for broader zero-label adaptation in vision-language tasks. Unlike prior
approaches that rely on human or synthetically annotated datasets, ViZer
actively aligns vision and language representation features during training,
enabling existing VLMs to generate improved captions without requiring text
labels or full retraining. We demonstrate ViZer's advantage in qualitative
evaluation, as automated caption metrics such as CIDEr and BERTScore often
penalize details that are absent in reference captions. Applying ViZer on
SmolVLM-Base and Qwen2-VL, we observe consistent qualitative improvements,
producing captions that are more grounded and descriptive than their baseline.

</details>


### [5] [Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation](https://arxiv.org/abs/2510.12953)
*Xiao He,Huangxuan Zhao,Guojia Wan,Wei Zhou,Yanxing Liu,Juhua Liu,Yongchao Xu,Yong Luo,Dacheng Tao,Bo Du*

Main category: cs.CV

TL;DR: FetalMind is a medical AI system designed for fetal ultrasound that addresses challenges in multi-view reasoning, disease diversity, and image heterogeneity through Salient Epistemic Disentanglement and a large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: Existing medical vision-language models perform poorly on fetal ultrasound due to challenges like multi-view image reasoning, numerous diseases, and image diversity compared to structured adult imaging.

Method: Proposed Salient Epistemic Disentanglement (SED) that injects expert-curated bipartite graph to decouple view-disease associations and uses reinforcement learning for clinically faithful preference selection. Also created FetalSigma-1M dataset with 20K reports from 12 medical centers.

Result: FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable.

Conclusion: FetalMind successfully bridges the gap in fetal ultrasound AI by addressing domain-specific challenges through clinical workflow-guided design and large-scale dataset curation, demonstrating superior performance and clinical alignment.

Abstract: Recent medical vision-language models have shown promise on tasks such as
VQA, report generation, and anomaly detection. However, most are adapted to
structured adult imaging and underperform in fetal ultrasound, which poses
challenges of multi-view image reasoning, numerous diseases, and image
diversity. To bridge this gap, we introduce FetalMind, a medical AI system
tailored to fetal ultrasound for both report generation and diagnosis. Guided
by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which
injects an expert-curated bipartite graph into the model to decouple
view-disease associations and to steer preference selection along clinically
faithful steps via reinforcement learning. This design mitigates variability
across diseases and heterogeneity across views, reducing learning bottlenecks
while aligning the model's inference with obstetric practice. To train
FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale
fetal ultrasound report corpus, comprising 20K reports from twelve medical
centers, addressing the scarcity of domain data. Extensive experiments show
that FetalMind outperforms open- and closed-source baselines across all
gestational stages, achieving +14% average gains and +61.2% higher accuracy on
critical conditions while remaining efficient, stable, and scalable. Project
Page: https://hexiao0275.github.io/FetalMind.

</details>


### [6] [CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected Guidance for SD/SDXL Latent Diffusion Models](https://arxiv.org/abs/2510.12954)
*Denis Rychkovskiy,GPT-5*

Main category: cs.CV

TL;DR: CADE 2.5 is a sampler-level guidance stack for SD/SDXL latent diffusion models that improves image quality through frequency-decoupled guidance, energy rescaling, and zero-projection, with additional stabilization via QSilk Micrograin Stabilizer.


<details>
  <summary>Details</summary>
Motivation: To enhance image generation quality in stable diffusion models by improving sharpness, prompt adherence, and artifact control without requiring retraining.

Method: Uses ZeResFDG module with frequency-decoupled guidance, energy rescaling, and zero-projection, combined with spectral EMA for mode switching and QSilk Micrograin Stabilizer for inference-time stabilization.

Result: Improves sharpness, prompt adherence, and artifact control across SD/SDXL samplers at moderate guidance scales, with enhanced robustness and natural high-frequency micro-texture at high resolutions.

Conclusion: CADE 2.5 provides effective guidance enhancement for latent diffusion models that works with existing samplers and requires no retraining, offering improved image quality and stability.

Abstract: We introduce CADE 2.5 (Comfy Adaptive Detail Enhancer), a sampler-level
guidance stack for SD/SDXL latent diffusion models. The central module,
ZeResFDG, unifies (i) frequency-decoupled guidance that reweights low- and
high-frequency components of the guidance signal, (ii) energy rescaling that
matches the per-sample magnitude of the guided prediction to the positive
branch, and (iii) zero-projection that removes the component parallel to the
unconditional direction. A lightweight spectral EMA with hysteresis switches
between a conservative and a detail-seeking mode as structure crystallizes
during sampling. Across SD/SDXL samplers, ZeResFDG improves sharpness, prompt
adherence, and artifact control at moderate guidance scales without any
retraining. In addition, we employ a training-free inference-time stabilizer,
QSilk Micrograin Stabilizer (quantile clamp + depth/edge-gated micro-detail
injection), which improves robustness and yields natural high-frequency
micro-texture at high resolutions with negligible overhead. For completeness we
note that the same rule is compatible with alternative parameterizations (e.g.,
velocity), which we briefly discuss in the Appendix; however, this paper
focuses on SD/SDXL latent diffusion models.

</details>


### [7] [Scope: Selective Cross-modal Orchestration of Visual Perception Experts](https://arxiv.org/abs/2510.12974)
*Tianyu Zhang,Suyuchen Wang,Chao Wang,Juan Rodriguez,Ahmed Masry,Xiangru Jian,Yoshua Bengio,Perouz Taslakian*

Main category: cs.CV

TL;DR: SCOPE is a Mixture-of-Encoders framework that dynamically selects one specialized vision encoder per image-text pair using instance-level routing, outperforming models that use all encoders simultaneously while reducing computation by 24-49%.


<details>
  <summary>Details</summary>
Motivation: Vision-language models benefit from multiple vision encoders, but naive stacking yields diminishing returns while multiplying inference costs. The goal is to achieve better performance with less computation through intelligent encoder selection.

Method: SCOPE maintains a shared encoder and a pool of routed encoders. A lightweight router uses cross-attention between text prompts and shared visual features to select the optimal encoder via instance-level routing. Training uses dual entropy regularization with auxiliary losses to balance load distribution and routing confidence.

Result: SCOPE with one shared plus one routed encoder outperforms models using all four extra encoders simultaneously, while reducing compute by 24-49%. This demonstrates that intelligent encoder selection beats brute-force aggregation.

Conclusion: SCOPE challenges the prevailing paradigm in multi-encoder VLMs by showing that dynamic encoder selection through instance-level routing is more effective than using all encoders simultaneously, achieving better performance with significantly reduced computation.

Abstract: Vision-language models (VLMs) benefit from multiple vision encoders, but
naively stacking them yields diminishing returns while multiplying inference
costs. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that
dynamically selects one specialized encoder per image-text pair via
instance-level routing, unlike token-level routing in traditional MoE. SCOPE
maintains a shared encoder and a pool of routed encoders. A lightweight router
uses cross-attention between text prompts and shared visual features to select
the optimal encoder from the routed encoders. To train this router, we
introduce dual entropy regularization with auxiliary losses to balance
dataset-level load distribution with instance-level routing confidence.
Remarkably, SCOPE with one shared plus one routed encoder outperforms models
using all four extra encoders simultaneously, while reducing compute by
24-49\%. This demonstrates that intelligent encoder selection beats brute-force
aggregation, challenging the prevailing paradigm in multi-encoder VLMs.

</details>


### [8] [SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding](https://arxiv.org/abs/2510.13016)
*Tanveer Hannan,Shuaicong Wu,Mark Weber,Suprosanna Shit,Jindong Gu,Rajat Koner,Aljoša Ošep,Laura Leal-Taixé,Thomas Seidl*

Main category: cs.CV

TL;DR: The paper introduces SVAG, a new task for spatio-temporal video action grounding that requires detecting, tracking, and temporally localizing objects based on natural language action descriptions, along with a benchmark dataset and baseline model.


<details>
  <summary>Details</summary>
Motivation: Existing video understanding methods focus on coarse-grained action recognition or generic object tracking, but overlook the challenge of jointly detecting and tracking multiple objects according to their actions with temporal grounding.

Method: Proposed SVAGFormer, a baseline framework that adapts state-of-the-art vision language models for joint spatial and temporal grounding, and introduced SVAG-Bench dataset with 688 videos and 19,590 annotated records.

Result: Empirical results show existing models perform poorly on SVAG, especially in dense or complex scenes, highlighting the need for better reasoning about fine-grained object-action interactions in long videos.

Conclusion: The SVAG task presents significant challenges for current models and underscores the importance of advanced reasoning capabilities for fine-grained object-action interactions in video understanding.

Abstract: Understanding fine-grained actions and accurately localizing their
corresponding actors in space and time are fundamental capabilities for
advancing next-generation AI systems, including embodied agents, autonomous
platforms, and human-AI interaction frameworks. Despite recent progress in
video understanding, existing methods predominantly address either
coarse-grained action recognition or generic object tracking, thereby
overlooking the challenge of jointly detecting and tracking multiple objects
according to their actions while grounding them temporally. To address this
gap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task
that requires models to simultaneously detect, track, and temporally localize
all referent objects in videos based on natural language descriptions of their
actions. To support this task, we construct SVAG-Bench, a large-scale benchmark
comprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering
a diverse range of objects, actions, and real-world scenes. We further propose
SVAGFormer, a baseline framework that adapts state of the art vision language
models for joint spatial and temporal grounding, and introduce SVAGEval, a
standardized evaluation toolkit for fair and reproducible benchmarking.
Empirical results show that existing models perform poorly on SVAG,
particularly in dense or complex scenes, underscoring the need for more
advanced reasoning over fine-grained object-action interactions in long videos.

</details>


### [9] [SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models](https://arxiv.org/abs/2510.13042)
*Zhengxu Tang,Zizheng Wang,Luning Wang,Zitao Shuai,Chenhao Zhang,Siyu Qian,Yirui Wu,Bohao Wang,Haosong Rao,Zhenyu Yang,Chenwei Wu*

Main category: cs.CV

TL;DR: SeqBench is a new benchmark for evaluating sequential narrative coherence in text-to-video generation, featuring 320 prompts and 2,560 annotated videos from 8 models, with a DTG-based automatic metric that correlates well with human judgments.


<details>
  <summary>Details</summary>
Motivation: Current T2V models struggle with coherent sequential narratives and logical progression through multiple events, while existing benchmarks focus mainly on visual quality rather than narrative coherence over extended sequences.

Method: Created SeqBench dataset with 320 prompts across various narrative complexities, collected 2,560 human-annotated videos from 8 state-of-the-art T2V models, and designed a Dynamic Temporal Graphs (DTG)-based automatic evaluation metric to capture long-range dependencies and temporal ordering efficiently.

Result: The DTG-based metric shows strong correlation with human annotations. Evaluation reveals current T2V models fail to maintain consistent object states across multi-action sequences, produce physically implausible results in multi-object scenarios, and struggle with preserving realistic timing and ordering between sequential actions.

Conclusion: SeqBench provides the first systematic framework for evaluating narrative coherence in T2V generation and offers concrete insights for improving sequential reasoning capabilities in future video generation models.

Abstract: Text-to-video (T2V) generation models have made significant progress in
creating visually appealing videos. However, they struggle with generating
coherent sequential narratives that require logical progression through
multiple events. Existing T2V benchmarks primarily focus on visual quality
metrics but fail to evaluate narrative coherence over extended sequences. To
bridge this gap, we present SeqBench, a comprehensive benchmark for evaluating
sequential narrative coherence in T2V generation. SeqBench includes a carefully
designed dataset of 320 prompts spanning various narrative complexities, with
2,560 human-annotated videos generated from 8 state-of-the-art T2V models.
Additionally, we design a Dynamic Temporal Graphs (DTG)-based automatic
evaluation metric, which can efficiently capture long-range dependencies and
temporal ordering while maintaining computational efficiency. Our DTG-based
metric demonstrates a strong correlation with human annotations. Through
systematic evaluation using SeqBench, we reveal critical limitations in current
T2V models: failure to maintain consistent object states across multi-action
sequences, physically implausible results in multi-object scenarios, and
difficulties in preserving realistic timing and ordering relationships between
sequential actions. SeqBench provides the first systematic framework for
evaluating narrative coherence in T2V generation and offers concrete insights
for improving sequential reasoning capabilities in future models. Please refer
to https://videobench.github.io/SeqBench.github.io/ for more details.

</details>


### [10] [SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion](https://arxiv.org/abs/2510.13044)
*Jungbin Cho,Minsu Kim,Jisoo Kim,Ce Zheng,Laszlo A. Jeni,Ming-Hsuan Yang,Youngjae Yu,Seonjoo Kim*

Main category: cs.CV

TL;DR: SceneAdapt is a framework that injects scene awareness into text-to-motion models by leveraging disjoint datasets through two adaptation stages: inbetweening and scene-aware inbetweening.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation approaches address either motion semantics or scene-awareness in isolation, and constructing large-scale datasets with both rich text-motion coverage and precise scene interactions is extremely challenging.

Method: Uses two adaptation stages: 1) inbetweening with keyframing layers that modulate motion latents while preserving the latent manifold, and 2) scene-aware inbetweening with a scene-conditioning layer that injects scene geometry by adaptively querying local context through cross-attention.

Result: Experimental results show that SceneAdapt effectively injects scene awareness into text-to-motion models.

Conclusion: The framework successfully bridges disjoint datasets to inject scene-awareness into text-to-motion models, with code and models to be released.

Abstract: Human motion is inherently diverse and semantically rich, while also shaped
by the surrounding scene. However, existing motion generation approaches
address either motion semantics or scene-awareness in isolation, since
constructing large-scale datasets with both rich text--motion coverage and
precise scene interactions is extremely challenging. In this work, we introduce
SceneAdapt, a framework that injects scene awareness into text-conditioned
motion models by leveraging disjoint scene--motion and text--motion datasets
through two adaptation stages: inbetweening and scene-aware inbetweening. The
key idea is to use motion inbetweening, learnable without text, as a proxy task
to bridge two distinct datasets and thereby inject scene-awareness to
text-to-motion models. In the first stage, we introduce keyframing layers that
modulate motion latents for inbetweening while preserving the latent manifold.
In the second stage, we add a scene-conditioning layer that injects scene
geometry by adaptively querying local context through cross-attention.
Experimental results show that SceneAdapt effectively injects scene awareness
into text-to-motion models, and we further analyze the mechanisms through which
this awareness emerges. Code and models will be released.

</details>


### [11] [One Dimensional CNN ECG Mamba for Multilabel Abnormality Classification in 12 Lead ECG](https://arxiv.org/abs/2510.13046)
*Huawei Jiang,Husna Mutahira,Gan Huang,Mannan Saeed Muhammad*

Main category: cs.CV

TL;DR: A hybrid 1D CNN-Mamba model for ECG classification that combines convolutional feature extraction with selective state space modeling, achieving superior performance on PhysioNet datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models like residual networks and transformers have limited performance when processing long sequential ECG signals. State space models offer an efficient alternative for better sequence modeling.

Method: Proposed 1D CNN Electrocardiogram Mamba - a hybrid framework combining convolutional feature extraction with Mamba (selective state space model), built upon Vision Mamba for enhanced temporal dependency representation.

Result: Achieved substantially higher AUPRC and AUROC scores than best previously published algorithms on twelve lead ECGs from PhysioNet Computing in Cardiology Challenges 2020 and 2021.

Conclusion: Mamba-based architectures show strong potential for advancing reliable ECG classification, supporting early diagnosis, personalized treatment, and enhancing accessibility in telemedicine and resource-constrained healthcare systems.

Abstract: Accurate detection of cardiac abnormalities from electrocardiogram recordings
is regarded as essential for clinical diagnostics and decision support.
Traditional deep learning models such as residual networks and transformer
architectures have been applied successfully to this task, but their
performance has been limited when long sequential signals are processed.
Recently, state space models have been introduced as an efficient alternative.
In this study, a hybrid framework named One Dimensional Convolutional Neural
Network Electrocardiogram Mamba is introduced, in which convolutional feature
extraction is combined with Mamba, a selective state space model designed for
effective sequence modeling. The model is built upon Vision Mamba, a
bidirectional variant through which the representation of temporal dependencies
in electrocardiogram data is enhanced. Comprehensive experiments on the
PhysioNet Computing in Cardiology Challenges of 2020 and 2021 were conducted,
and superior performance compared with existing methods was achieved.
Specifically, the proposed model achieved substantially higher AUPRC and AUROC
scores than those reported by the best previously published algorithms on
twelve lead electrocardiograms. These results demonstrate the potential of
Mamba-based architectures to advance reliable ECG classification. This
capability supports early diagnosis and personalized treatment, while enhancing
accessibility in telemedicine and resource-constrained healthcare systems.

</details>


### [12] [True Self-Supervised Novel View Synthesis is Transferable](https://arxiv.org/abs/2510.13063)
*Thomas W. Mitchel,Hyunwoo Ryu,Vincent Sitzmann*

Main category: cs.CV

TL;DR: XFactor is the first geometry-free self-supervised model for novel view synthesis that achieves pose transferability across different scenes without using 3D inductive biases or explicit pose parameterizations.


<details>
  <summary>Details</summary>
Motivation: Prior self-supervised NVS models lack transferability - their predicted poses don't work across different scenes, indicating they're not truly learning camera geometry.

Method: Combines pair-wise pose estimation with input/output augmentation to disentangle camera pose from scene content, using unconstrained latent pose variables without 3D biases or SE(3) parameterization.

Result: XFactor significantly outperforms prior pose-free NVS transformers, achieves true pose transferability, and shows latent poses are highly correlated with real-world poses.

Conclusion: Geometry-free self-supervised NVS with transferable poses is possible without traditional 3D geometric constraints, enabling novel view synthesis that generalizes across scenes.

Abstract: In this paper, we identify that the key criterion for determining whether a
model is truly capable of novel view synthesis (NVS) is transferability:
Whether any pose representation extracted from one video sequence can be used
to re-render the same camera trajectory in another. We analyze prior work on
self-supervised NVS and find that their predicted poses do not transfer: The
same set of poses lead to different camera trajectories in different 3D scenes.
Here, we present XFactor, the first geometry-free self-supervised model capable
of true NVS. XFactor combines pair-wise pose estimation with a simple
augmentation scheme of the inputs and outputs that jointly enables
disentangling camera pose from scene content and facilitates geometric
reasoning. Remarkably, we show that XFactor achieves transferability with
unconstrained latent pose variables, without any 3D inductive biases or
concepts from multi-view geometry -- such as an explicit parameterization of
poses as elements of SE(3). We introduce a new metric to quantify
transferability, and through large-scale experiments, we demonstrate that
XFactor significantly outperforms prior pose-free NVS transformers, and show
that latent poses are highly correlated with real-world poses through probing
experiments.

</details>


### [13] [Direction-aware multi-scale gradient loss for infrared and visible image fusion](https://arxiv.org/abs/2510.13067)
*Kaixuan Yang,Wei Xiang,Zhenshuai Chen,Tong Jin,Yunpeng Liu*

Main category: cs.CV

TL;DR: The paper proposes a direction-aware, multi-scale gradient loss for infrared and visible image fusion that preserves gradient direction information across scales, improving edge fidelity and texture preservation without changing model architectures.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based approaches use gradient magnitude loss which removes directional information, leading to ambiguous supervision and suboptimal edge fidelity in fused images.

Method: Introduces a direction-aware, multi-scale gradient loss that supervises horizontal and vertical gradient components separately while preserving their sign across different scales.

Result: Experiments show the approach produces sharper, better-aligned edges and richer texture preservation compared to traditional gradient magnitude methods.

Conclusion: The proposed direction-aware gradient loss provides clear directional guidance and improves fusion quality without requiring changes to model architectures or training protocols.

Abstract: Infrared and visible image fusion aims to integrate complementary information
from co-registered source images to produce a single, informative result. Most
learning-based approaches train with a combination of structural similarity
loss, intensity reconstruction loss, and a gradient-magnitude term. However,
collapsing gradients to their magnitude removes directional information,
yielding ambiguous supervision and suboptimal edge fidelity. We introduce a
direction-aware, multi-scale gradient loss that supervises horizontal and
vertical components separately and preserves their sign across scales. This
axis-wise, sign-preserving objective provides clear directional guidance at
both fine and coarse resolutions, promoting sharper, better-aligned edges and
richer texture preservation without changing model architectures or training
protocols. Experiments on open-source model and multiple public benchmarks
demonstrate effectiveness of our approach.

</details>


### [14] [Unsupervised Domain Adaptation via Content Alignment for Hippocampus Segmentation](https://arxiv.org/abs/2510.13075)
*Hoda Kalabizadeh,Ludovica Griffanti,Pak-Hei Yeung,Ana I. L. Namburete,Nicola K. Dinsdale,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: Novel unsupervised domain adaptation framework for cross-domain hippocampus segmentation that addresses both style and content domain shifts through z-normalization and bidirectional deformable image registration.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for medical image segmentation struggle with domain shifts across different datasets, particularly variations in image appearance (style) and population-dependent anatomical characteristics (content).

Method: Combines efficient style harmonization through z-normalization with a bidirectional deformable image registration (DIR) strategy. The DIR network is jointly trained with segmentation and discriminator networks to guide registration with respect to ROI and generate anatomically plausible transformations.

Result: Outperforms existing baselines across all experiments. Achieves up to 15% relative improvement in Dice score when transferring from young, healthy populations to clinical dementia patients, with largest gains in scenarios with substantial content shift.

Conclusion: The framework is effective for accurate hippocampus segmentation across diverse populations, particularly addressing the challenge of content variations in cross-domain medical image analysis.

Abstract: Deep learning models for medical image segmentation often struggle when
deployed across different datasets due to domain shifts - variations in both
image appearance, known as style, and population-dependent anatomical
characteristics, referred to as content. This paper presents a novel
unsupervised domain adaptation framework that directly addresses domain shifts
encountered in cross-domain hippocampus segmentation from MRI, with specific
emphasis on content variations. Our approach combines efficient style
harmonisation through z-normalisation with a bidirectional deformable image
registration (DIR) strategy. The DIR network is jointly trained with
segmentation and discriminator networks to guide the registration with respect
to a region of interest and generate anatomically plausible transformations
that align source images to the target domain. We validate our approach through
comprehensive evaluations on both a synthetic dataset using Morpho-MNIST (for
controlled validation of core principles) and three MRI hippocampus datasets
representing populations with varying degrees of atrophy. Across all
experiments, our method outperforms existing baselines. For hippocampus
segmentation, when transferring from young, healthy populations to clinical
dementia patients, our framework achieves up to 15% relative improvement in
Dice score compared to standard augmentation methods, with the largest gains
observed in scenarios with substantial content shift. These results highlight
the efficacy of our approach for accurate hippocampus segmentation across
diverse populations.

</details>


### [15] [Counting Hallucinations in Diffusion Models](https://arxiv.org/abs/2510.13080)
*Shuai Fu,Jian Zhou,Qi Chen,Huang Jing,Huy Anh Nguyen,Xiaohan Liu,Zhixiong Zeng,Lin Ma,Quanshi Zhang,Qi Wu*

Main category: cs.CV

TL;DR: This paper introduces a systematic framework for quantifying counting hallucinations in diffusion probabilistic models (DPMs), where models generate incorrect numbers of objects despite such patterns being absent from training data.


<details>
  <summary>Details</summary>
Motivation: Current DPMs often produce hallucinated samples that conflict with real-world knowledge, but there's a lack of feasible methodologies for systematically quantifying such hallucinations, hindering progress in addressing this challenge.

Method: Constructed CountHalluSet dataset suite with well-defined counting criteria (ToyShape, SimObject, RealHand), developed standardized evaluation protocol for counting hallucinations, and systematically examined how different sampling conditions affect hallucination levels.

Result: Systematic analysis revealed that common evaluation metrics like FID fail to capture counting hallucinations consistently, and different sampling conditions (solver type, ODE solver order, sampling steps, initial noise) significantly affect counting hallucination levels.

Conclusion: This work takes the first step toward systematically quantifying hallucinations in diffusion models and offers new insights for investigating hallucination phenomena in image generation, highlighting the need for better evaluation metrics that capture factual consistency.

Abstract: Diffusion probabilistic models (DPMs) have demonstrated remarkable progress
in generative tasks, such as image and video synthesis. However, they still
often produce hallucinated samples (hallucinations) that conflict with
real-world knowledge, such as generating an implausible duplicate cup floating
beside another cup. Despite their prevalence, the lack of feasible
methodologies for systematically quantifying such hallucinations hinders
progress in addressing this challenge and obscures potential pathways for
designing next-generation generative models under factual constraints. In this
work, we bridge this gap by focusing on a specific form of hallucination, which
we term counting hallucination, referring to the generation of an incorrect
number of instances or structured objects, such as a hand image with six
fingers, despite such patterns being absent from the training data. To this
end, we construct a dataset suite CountHalluSet, with well-defined counting
criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets,
we develop a standardized evaluation protocol for quantifying counting
hallucinations, and systematically examine how different sampling conditions in
DPMs, including solver type, ODE solver order, sampling steps, and initial
noise, affect counting hallucination levels. Furthermore, we analyze their
correlation with common evaluation metrics such as FID, revealing that this
widely used image quality metric fails to capture counting hallucinations
consistently. This work aims to take the first step toward systematically
quantifying hallucinations in diffusion models and offer new insights into the
investigation of hallucination phenomena in image generation.

</details>


### [16] [Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation](https://arxiv.org/abs/2510.13084)
*Yi Zuo,Zitao Wang,Lingling Li,Xu Liu,Fang Liu,Licheng Jiao*

Main category: cs.CV

TL;DR: Edit-Your-Interest is a lightweight, text-driven, zero-shot video editing method that uses spatio-temporal feature memory to reduce computational overhead while maintaining temporal consistency and visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods suffer from high computational overhead, memory consumption, and produce undesirable temporal inconsistencies and artifacts like blurring and mosaic patterns.

Method: Introduces Spatio-Temporal Feature Memory (SFM) to cache features from previous frames, Feature Most-Similar Propagation (FMP) for temporal consistency, and uses cross-attention maps to automatically extract masks for target objects.

Result: Extensive experiments show Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and visual fidelity.

Conclusion: The proposed method provides superior effectiveness and practicality for text-driven video editing while maintaining computational efficiency and visual quality.

Abstract: Text-to-image (T2I) diffusion models have recently demonstrated significant
progress in video editing.
  However, existing video editing methods are severely limited by their high
computational overhead and memory consumption.
  Furthermore, these approaches often sacrifice visual fidelity, leading to
undesirable temporal inconsistencies and artifacts such as blurring and
pronounced mosaic-like patterns.
  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video
editing method.
  Edit-Your-Interest introduces a spatio-temporal feature memory to cache
features from previous frames, significantly reducing computational overhead
compared to full-sequence spatio-temporal modeling approaches.
  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),
which is designed to efficiently cache and retain the crucial image tokens
processed by spatial attention.
  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP
propagates the most relevant tokens from previous frames to subsequent ones,
preserving temporal consistency.
  Finally, we introduce an SFM update algorithm that continuously refreshes the
cached features, ensuring their long-term relevance and effectiveness
throughout the video sequence.
  Furthermore, we leverage cross-attention maps to automatically extract masks
for the instances of interest.
  These masks are seamlessly integrated into the diffusion denoising process,
enabling fine-grained control over target objects and allowing
Edit-Your-Interest to perform highly accurate edits while robustly preserving
the background integrity.
  Extensive experiments decisively demonstrate that the proposed
Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and
visual fidelity, validating its superior effectiveness and practicality.

</details>


### [17] [EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception](https://arxiv.org/abs/2510.13105)
*Xijun Wang,Tanay Sharma,Achin Kulshrestha,Abhimitra Meka,Aveek Purohit,Dinesh Manocha*

Main category: cs.CV

TL;DR: EgoSocial dataset and EgoSoD method address AI's lack of social awareness in AR/VR by detecting optimal intervention timing in social interactions using multimodal contextual cues.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack social awareness for determining when to intervene as AI assistants in AR/VR contexts, leading to disruptive responses that negatively impact user focus during natural conversations.

Method: Created EgoSocial dataset with 13,500 social video-question pairs, analyzed OLLMs' limitations, and proposed EgoSoD - an end-to-end method integrating multimodal cues into a social thinking graph to dynamically model participants and interactions.

Result: OLLMs struggle with intervention timing detection (14.4% for Gemini 2.5 Pro). EgoSoD improves Phi-4 by 45.6% and Gemini 2.5 Pro by 9.9% on Intervention Timing, and improves Phi-4 by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.

Conclusion: EgoSocial dataset and EgoSoD method effectively enhance AI's social awareness for determining optimal intervention timing in AR/VR environments, significantly outperforming existing OLLMs.

Abstract: As AR/VR technologies become integral to daily life, there's a growing need
for AI that understands human social dynamics from an egocentric perspective.
However, current LLMs often lack the social awareness to discern when to
intervene as AI assistant. This leads to constant, socially unaware responses
that may disrupt natural conversation and negatively impact user focus. To
address these limitations, we introduce EgoSocial, a large-scale egocentric
dataset with 13,500 social video-question pairs, specifically designed to
benchmark intervention in social interaction perception. We also present an
in-depth analysis of current omnimodal LLMs (OLLMs) to assess their
effectiveness in detecting diverse social contextual cues. Experiments show
that OLLMs still struggle to detect the intervention timing (14.4% for Gemini
2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method
for robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD
integrates multimodal contextual cues (e.g., audio and visual cues) into a
social thinking graph, dynamically modeling participants and interactions. Our
method proactively detects intervention timing and social interactions,
precisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and
Gemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4
by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.
We will release the dataset and code soon.

</details>


### [18] [DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2510.13108)
*Jingyu Song,Zhenxin Li,Shiyi Lan,Xinglong Sun,Nadine Chang,Maying Shen,Joshua Chen,Katherine A. Skinner,Jose M. Alvarez*

Main category: cs.CV

TL;DR: DriveCritic is a framework for evaluating autonomous driving planners that better aligns with human judgment by incorporating context awareness through a curated dataset and VLM-based evaluator.


<details>
  <summary>Details</summary>
Motivation: Existing metrics like EPDMS lack context awareness in nuanced scenarios, making it challenging to benchmark autonomous driving planners according to human judgment.

Method: Two key contributions: DriveCritic dataset with challenging scenarios annotated with human preferences, and DriveCritic model - a VLM-based evaluator fine-tuned using two-stage supervised and reinforcement learning to integrate visual and symbolic context.

Result: DriveCritic significantly outperforms existing metrics and baselines in matching human preferences and demonstrates strong context awareness.

Conclusion: Provides a more reliable, human-aligned foundation for evaluating autonomous driving systems.

Abstract: Benchmarking autonomous driving planners to align with human judgment remains
a critical challenge, as state-of-the-art metrics like the Extended Predictive
Driver Model Score (EPDMS) lack context awareness in nuanced scenarios. To
address this, we introduce DriveCritic, a novel framework featuring two key
contributions: the DriveCritic dataset, a curated collection of challenging
scenarios where context is critical for correct judgment and annotated with
pairwise human preferences, and the DriveCritic model, a Vision-Language Model
(VLM) based evaluator. Fine-tuned using a two-stage supervised and
reinforcement learning pipeline, the DriveCritic model learns to adjudicate
between trajectory pairs by integrating visual and symbolic context.
Experiments show DriveCritic significantly outperforms existing metrics and
baselines in matching human preferences and demonstrates strong context
awareness. Overall, our work provides a more reliable, human-aligned foundation
to evaluating autonomous driving systems.

</details>


### [19] [VPREG: An Optimal Control Formulation for Diffeomorphic Image Registration Based on the Variational Principle Grid Generation Method](https://arxiv.org/abs/2510.13109)
*Zicong Zhou,Baihan Zhao,Andreas Mang,Guojun Liao*

Main category: cs.CV

TL;DR: VPreg is a novel diffeomorphic image registration method that ensures positive Jacobian determinants and provides accurate inverse transformations within the diffeomorphism group, outperforming state-of-the-art methods in brain scan registration.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing mesh generation and diffeomorphic image registration methods by achieving better registration accuracy while controlling transformation quality, ensuring diffeomorphic properties essential for neuroimaging workflows.

Method: Uses a Variational Principle (VP) grid generation approach that constructs non-folding grids with prescribed Jacobian determinant and curl, generating inverse transformations within the diffeomorphism group rather than image space.

Result: VPreg outperforms ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt in 150 brain scan registrations from OASIS-1 dataset, showing superior Dice scores for 35 regions of interest, better transformation regularity, and more accurate inverse maps.

Conclusion: VPreg provides a significant improvement in diffeomorphic image registration with guaranteed diffeomorphic properties, superior registration accuracy, and more reliable inverse transformations compared to existing state-of-the-art methods.

Abstract: This paper introduces VPreg, a novel diffeomorphic image registration method.
This work provides several improvements to our past work on mesh generation and
diffeomorphic image registration. VPreg aims to achieve excellent registration
accuracy while controlling the quality of the registration transformations. It
ensures a positive Jacobian determinant of the spatial transformation and
provides an accurate approximation of the inverse of the registration, a
crucial property for many neuroimaging workflows. Unlike conventional methods,
VPreg generates this inverse transformation within the group of diffeomorphisms
rather than operating on the image space. The core of VPreg is a grid
generation approach, referred to as \emph{Variational Principle} (VP), which
constructs non-folding grids with prescribed Jacobian determinant and curl.
These VP-generated grids guarantee diffeomorphic spatial transformations
essential for computational anatomy and morphometry, and provide a more
accurate inverse than existing methods. To assess the potential of the proposed
approach, we conduct a performance analysis for 150 registrations of brain
scans from the OASIS-1 dataset. Performance evaluation based on Dice scores for
35 regions of interest, along with an empirical analysis of the properties of
the computed spatial transformations, demonstrates that VPreg outperforms
state-of-the-art methods in terms of Dice scores, regularity properties of the
computed transformation, and accuracy and consistency of the provided inverse
map. We compare our results to ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt.

</details>


### [20] [OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment](https://arxiv.org/abs/2510.13131)
*Rongjun Chen,Chengsi Yao,Jinchang Ren,Xianxian Zeng,Peixian Wang,Jun Yuan,Jiawen Li,Huimin Zhao,Xu Lu*

Main category: cs.CV

TL;DR: The paper proposes OS-HGAdapter, a method that uses LLM's open semantic knowledge to address text-image alignment imbalance by enhancing text modality entropy and using hypergraph adapters for cross-modal connections.


<details>
  <summary>Details</summary>
Motivation: To solve the imbalance in cross-modal retrieval caused by different information entropy between texts and images, and to reproduce human-like alignment capabilities using LLM's semantic knowledge.

Method: Two-step approach: 1) LLM-based prompt template to enhance text polysemy description and increase text modality entropy; 2) Hypergraph adapter to construct multilateral text-image connections and correct matching errors while reducing semantic noise.

Result: Achieved 16.8% improvement in text-to-image retrieval and 40.1% improvement in image-to-text retrieval on Flickr30K and MS-COCO benchmarks, establishing new state-of-the-art performance.

Conclusion: The proposed OS-HGAdapter effectively addresses cross-modal alignment imbalance through LLM-enhanced semantic entropy and hypergraph-based adaptation, demonstrating significant performance gains in retrieval tasks.

Abstract: Text-image alignment constitutes a foundational challenge in multimedia
content understanding, where effective modeling of cross-modal semantic
correspondences critically enhances retrieval system performance through joint
embedding space optimization. Given the inherent difference in information
entropy between texts and images, conventional approaches often show an
imbalance in the mutual retrieval of these two modalities. To address this
particular challenge, we propose to use the open semantic knowledge of Large
Language Model (LLM) to fill for the entropy gap and reproduce the alignment
ability of humans in these tasks. Our entropy-enhancing alignment is achieved
through a two-step process: 1) a new prompt template that does not rely on
explicit knowledge in the task domain is designed to use LLM to enhance the
polysemy description of the text modality. By analogy, the information entropy
of the text modality relative to the visual modality is increased; 2) A
hypergraph adapter is used to construct multilateral connections between the
text and image modalities, which can correct the positive and negative matching
errors for synonymous semantics in the same fixed embedding space, whilst
reducing the noise caused by open semantic entropy by mapping the reduced
dimensions back to the original dimensions. Comprehensive evaluations on the
Flickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic
Hypergraph Adapter (OS-HGAdapter), showcasing 16.8\% (text-to-image) and 40.1\%
(image-to-text) cross-modal retrieval gains over existing methods while
establishing new state-of-the-art performance in semantic alignment tasks.

</details>


### [21] [Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN](https://arxiv.org/abs/2510.13137)
*Madhumati Pol,Anvay Anturkar,Anushka Khot,Ayush Andure,Aniruddha Ghosh,Anvit Magadum,Anvay Bahadur*

Main category: cs.CV

TL;DR: Comparison of 3D CNNs and LSTMs for real-time ASL recognition, showing 3D CNNs achieve higher accuracy (92.4%) but with higher computational cost, while LSTMs offer better efficiency with 86.7% accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate different neural network architectures for real-time American Sign Language recognition, addressing the trade-off between recognition accuracy and computational efficiency for practical assistive technologies.

Method: Evaluated 3D CNNs and LSTM networks on a dataset of 1,200 ASL signs across 50 classes, comparing accuracy, computational efficiency, and latency under similar training conditions. Also tested a hybrid 3D CNN-LSTM model.

Result: 3D CNNs achieved 92.4% recognition accuracy but required 3.2% more processing time per frame. LSTMs maintained 86.7% accuracy with significantly lower resource consumption. Hybrid model showed decent performance.

Conclusion: Context-dependent architecture selection is crucial for practical implementation, with trade-offs between recognition precision and real-time operational requirements in edge computing environments.

Abstract: This study investigates the performance of 3D Convolutional Neural Networks
(3D CNNs) and Long Short-Term Memory (LSTM) networks for real-time American
Sign Language (ASL) recognition. Though 3D CNNs are good at spatiotemporal
feature extraction from video sequences, LSTMs are optimized for modeling
temporal dependencies in sequential data. We evaluate both architectures on a
dataset containing 1,200 ASL signs across 50 classes, comparing their accuracy,
computational efficiency, and latency under similar training conditions.
Experimental results demonstrate that 3D CNNs achieve 92.4% recognition
accuracy but require 3.2% more processing time per frame compared to LSTMs,
which maintain 86.7% accuracy with significantly lower resource consumption.
The hybrid 3D CNNLSTM model shows decent performance, which suggests that
context-dependent architecture selection is crucial for practical
implementation.This project provides professional benchmarks for developing
assistive technologies, highlighting trade-offs between recognition precision
and real-time operational requirements in edge computing environments.

</details>


### [22] [Foveation Improves Payload Capacity in Steganography](https://arxiv.org/abs/2510.13151)
*Lifeng Qiu Lin,Henry Kam,Qi Sun,Kaan Akşit*

Main category: cs.CV

TL;DR: The paper presents a steganography method that significantly improves capacity limits from 100 to 500 bits while maintaining high accuracy (1 failure bit out of 2000) and good visual quality (31.47 dB PSNR, 0.13 LPIPS).


<details>
  <summary>Details</summary>
Motivation: To enhance steganography in visual media for applications like metadata embedding and watermarking by overcoming existing capacity limitations while preserving visual quality.

Method: Utilizes efficient latent representations and foveated rendering to train models that create multi-modal latent representations with novel perceptual design.

Result: Achieved 5x improvement in capacity (500 bits vs 100 bits), high accuracy with only 1 failure bit out of 2000 test bits, and maintained visual quality with 31.47 dB PSNR and 0.13 LPIPS.

Conclusion: The novel perceptual design effectively enables multi-modal latent representations in steganography, demonstrating significant improvements in capacity and accuracy while preserving visual quality.

Abstract: Steganography finds its use in visual medium such as providing metadata and
watermarking. With support of efficient latent representations and foveated
rendering, we trained models that improve existing capacity limits from 100 to
500 bits, while achieving better accuracy of up to 1 failure bit out of 2000,
at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB
PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in
creating multi-modal latent representations in steganography.

</details>


### [23] [DP-TTA: Test-time Adaptation for Transient Electromagnetic Signal Denoising via Dictionary-driven Prior Regularization](https://arxiv.org/abs/2510.13160)
*Meng Yang,Kecheng Chen,Wei Luo,Xianjie Chen,Yong Jia,Mingyue Wang,Fanqiang Lin*

Main category: cs.CV

TL;DR: The paper proposes DP-TTA, a test-time adaptation method for TEM signal denoising that uses dictionary-driven prior regularization to handle noise differences across geographical regions.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning TEM denoising models trained on simulated or single real-world scenario data fail in new environments due to different noise characteristics from varying geological conditions, equipment, and external interference.

Method: Proposed Dictionary-driven Prior Regularization Test-time Adaptation (DP-TTA) that encodes intrinsic TEM signal characteristics (exponential decay, smoothness) via dictionary learning as prior knowledge, then uses self-supervised losses for dynamic adaptation during testing.

Result: Extensive experiments show the method achieves much better performance than existing TEM denoising methods and TTA methods.

Conclusion: The proposed DP-TTA effectively handles cross-region TEM denoising challenges by leveraging intrinsic physical characteristics as prior knowledge for test-time adaptation.

Abstract: Transient Electromagnetic (TEM) method is widely used in various geophysical
applications, providing valuable insights into subsurface properties. However,
time-domain TEM signals are often submerged in various types of noise. While
recent deep learning-based denoising models have shown strong performance,
these models are mostly trained on simulated or single real-world scenario
data, overlooking the significant differences in noise characteristics from
different geographical regions. Intuitively, models trained in one environment
often struggle to perform well in new settings due to differences in geological
conditions, equipment, and external interference, leading to reduced denoising
performance. To this end, we propose the Dictionary-driven Prior Regularization
Test-time Adaptation (DP-TTA). Our key insight is that TEM signals possess
intrinsic physical characteristics, such as exponential decay and smoothness,
which remain consistent across different regions regardless of external
conditions. These intrinsic characteristics serve as ideal prior knowledge for
guiding the TTA strategy, which helps the pre-trained model dynamically adjust
parameters by utilizing self-supervised losses, improving denoising performance
in new scenarios. To implement this, we customized a network, named DTEMDNet.
Specifically, we first use dictionary learning to encode these intrinsic
characteristics as a dictionary-driven prior, which is integrated into the
model during training. At the testing stage, this prior guides the model to
adapt dynamically to new environments by minimizing self-supervised losses
derived from the dictionary-driven consistency and the signal one-order
variation. Extensive experimental results demonstrate that the proposed method
achieves much better performance than existing TEM denoising methods and TTA
methods.

</details>


### [24] [STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control](https://arxiv.org/abs/2510.13186)
*Zhen Li,Xibin Jin,Guoliang Li,Shuai Wang,Miaowen Wen,Huseyin Arslan,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.CV

TL;DR: Edge Gaussian splatting (EGS) trains global 3D scene models from distributed clients. This paper proposes STT-GS strategy with two-stage sampling and transmission to maximize GS quality under communication constraints, using feature-domain clustering and joint client selection/power control.


<details>
  <summary>Details</summary>
Motivation: Traditional edge resource management methods focus on communication throughput or general learning performance, but EGS specifically aims to maximize Gaussian splatting quality, making existing approaches inapplicable. There's a need for GS-oriented optimization that considers heterogeneous view contributions.

Method: Proposes STT-GS strategy: 1) Sample subset of images as pilot data using feature-domain clustering to select representative data, 2) Prioritize communication resources to more valuable clients based on pilot evaluation, 3) Joint client selection and power control framework using penalty alternating majorization minimization algorithm.

Result: Significantly outperforms existing benchmarks on real-world datasets. GS-oriented objective can be accurately predicted with low sampling ratios (e.g., 10%). Achieves excellent tradeoff between view contributions and communication costs.

Conclusion: The proposed STT-GS strategy effectively addresses the causality dilemma in EGS by using two-stage sampling and transmission, enabling efficient resource allocation that maximizes Gaussian splatting quality while minimizing communication overhead.

Abstract: Edge Gaussian splatting (EGS), which aggregates data from distributed clients
and trains a global GS model at the edge server, is an emerging paradigm for
scene reconstruction. Unlike traditional edge resource management methods that
emphasize communication throughput or general-purpose learning performance, EGS
explicitly aims to maximize the GS qualities, rendering existing approaches
inapplicable. To address this problem, this paper formulates a novel
GS-oriented objective function that distinguishes the heterogeneous view
contributions of different clients. However, evaluating this function in turn
requires clients' images, leading to a causality dilemma. To this end, this
paper further proposes a sample-then-transmit EGS (or STT-GS for short)
strategy, which first samples a subset of images as pilot data from each client
for loss prediction. Based on the first-stage evaluation, communication
resources are then prioritized towards more valuable clients. To achieve
efficient sampling, a feature-domain clustering (FDC) scheme is proposed to
select the most representative data and pilot transmission time minimization
(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint
client selection and power control (JCSPC) framework to maximize the
GS-oriented function under communication resource constraints. Despite the
nonconvexity of the problem, we propose a low-complexity efficient solution
based on the penalty alternating majorization minimization (PAMM) algorithm.
Experiments unveil that the proposed scheme significantly outperforms existing
benchmarks on real-world datasets. It is found that the GS-oriented objective
can be accurately predicted with low sampling ratios (e.g.,10%), and our method
achieves an excellent tradeoff between view contributions and communication
costs.

</details>


### [25] [Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion](https://arxiv.org/abs/2510.13198)
*Rongtao Xu,Jinzhou Lin,Jialei Zhou,Jiahua Dong,Changwei Wang,Ruisheng Wang,Li Guo,Shibiao Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: CIGOcc is a two-stage camera-based occupancy prediction framework that fuses multiple feature representations (segmentation, graphics, depth) using deformable multi-level fusion and SAM knowledge distillation, achieving state-of-the-art performance on SemanticKITTI.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on structural modifications with limited performance, while rich diversity of features in 2D images remains underutilized. The paper explores representation fusion to better leverage multi-level features.

Method: Two-stage framework extracting segmentation, graphics, and depth features from input images, using deformable multi-level fusion mechanism to fuse these features, and incorporating knowledge distillation from SAM.

Result: Achieves state-of-the-art performance on SemanticKITTI benchmark without increasing training costs.

Conclusion: CIGOcc demonstrates that effective multi-level representation fusion significantly improves camera-based occupancy prediction performance, providing a new direction beyond structural modifications.

Abstract: Camera-based occupancy prediction is a mainstream approach for 3D perception
in autonomous driving, aiming to infer complete 3D scene geometry and semantics
from 2D images. Almost existing methods focus on improving performance through
structural modifications, such as lightweight backbones and complex cascaded
frameworks, with good yet limited performance. Few studies explore from the
perspective of representation fusion, leaving the rich diversity of features in
2D images underutilized. Motivated by this, we propose \textbf{CIGOcc, a
two-stage occupancy prediction framework based on multi-level representation
fusion. \textbf{CIGOcc extracts segmentation, graphics, and depth features from
an input image and introduces a deformable multi-level fusion mechanism to fuse
these three multi-level features. Additionally, CIGOcc incorporates knowledge
distilled from SAM to further enhance prediction accuracy. Without increasing
training costs, CIGOcc achieves state-of-the-art performance on the
SemanticKITTI benchmark. The code is provided in the supplementary material and
will be released https://github.com/VitaLemonTea1/CIGOcc

</details>


### [26] [Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences](https://arxiv.org/abs/2510.13201)
*Jing Yang,Qiyao Wei,Jiaxin Pei*

Main category: cs.CV

TL;DR: Paper Copilot is a system that creates digital archives of peer reviews across CS venues, enabling large-scale study of peer review evolution and supporting evidence-based improvements to the system.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AI conferences is straining the peer-review system, causing heavy workloads, expertise mismatches, inconsistent standards, superficial reviews, and limited accountability. Ad-hoc policy changes create confusion and opacity in how papers are accepted.

Method: Developed Paper Copilot system that creates durable digital archives of peer reviews across computer-science venues, providing an open dataset for studying peer review at scale. Conducted large-scale empirical analysis of ICLR reviews spanning multiple years.

Result: Created infrastructure and dataset that support reproducible research on peer review evolution. The system enables tracking changes, diagnosing failure modes, and informing evidence-based improvements.

Conclusion: Paper Copilot provides resources to help the community work toward a more robust, transparent, and reliable peer-review system by enabling large-scale study of review practices and their evolution over time.

Abstract: The rapid growth of AI conferences is straining an already fragile
peer-review system, leading to heavy reviewer workloads, expertise mismatches,
inconsistent evaluation standards, superficial or templated reviews, and
limited accountability under compressed timelines. In response, conference
organizers have introduced new policies and interventions to preserve review
standards. Yet these ad-hoc changes often create further concerns and confusion
about the review process, leaving how papers are ultimately accepted - and how
practices evolve across years - largely opaque. We present Paper Copilot, a
system that creates durable digital archives of peer reviews across a wide
range of computer-science venues, an open dataset that enables researchers to
study peer review at scale, and a large-scale empirical analysis of ICLR
reviews spanning multiple years. By releasing both the infrastructure and the
dataset, Paper Copilot supports reproducible research on the evolution of peer
review. We hope these resources help the community track changes, diagnose
failure modes, and inform evidence-based improvements toward a more robust,
transparent, and reliable peer-review system.

</details>


### [27] [MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation](https://arxiv.org/abs/2510.13208)
*Lianlian Liu,YongKang He,Zhaojie Chu,Xiaofen Xing,Xiangmin Xu*

Main category: cs.CV

TL;DR: MimicParts is a framework that generates stylized 3D human motion from speech using part-aware style injection and denoising to capture regional motion differences and adapt to speech rhythm/emotion changes.


<details>
  <summary>Details</summary>
Motivation: Current methods oversimplify stylistic diversity and ignore regional motion style differences, limiting motion realism. They also fail to dynamically adapt motion style to speech rhythm and emotion changes.

Method: Divides body into regions for localized motion style encoding, uses part-aware style injection and part-aware denoising network with attention blocks that allow rhythm/emotion cues to guide each body region precisely.

Result: Outperforms existing methods, generating more natural and expressive 3D human motion sequences that better align with speech variations.

Conclusion: The proposed part-aware approach effectively captures fine-grained regional motion differences and enables dynamic style adaptation to speech rhythm and emotion, enhancing motion realism.

Abstract: Generating stylized 3D human motion from speech signals presents substantial
challenges, primarily due to the intricate and fine-grained relationships among
speech signals, individual styles, and the corresponding body movements.
Current style encoding approaches either oversimplify stylistic diversity or
ignore regional motion style differences (e.g., upper vs. lower body), limiting
motion realism. Additionally, motion style should dynamically adapt to changes
in speech rhythm and emotion, but existing methods often overlook this. To
address these issues, we propose MimicParts, a novel framework designed to
enhance stylized motion generation based on part-aware style injection and
part-aware denoising network. It divides the body into different regions to
encode localized motion styles, enabling the model to capture fine-grained
regional differences. Furthermore, our part-aware attention block allows rhythm
and emotion cues to guide each body region precisely, ensuring that the
generated motion aligns with variations in speech rhythm and emotional state.
Experimental results show that our method outperforming existing methods
showcasing naturalness and expressive 3D human motion sequences.

</details>


### [28] [Prompt-based Adaptation in Large-scale Vision Models: A Survey](https://arxiv.org/abs/2510.13219)
*Xi Xiao,Yunbei Zhang,Lin Zhao,Yiyang Liu,Xiaoying Liao,Zheda Mai,Xingjian Li,Xiao Wang,Hao Xu,Jihun Hamm,Xue Lin,Min Xu,Qifan Wang,Tianyang Wang,Cheng Han*

Main category: cs.CV

TL;DR: This survey provides a comprehensive analysis of Visual Prompting (VP) and Visual Prompt Tuning (VPT), conceptualizing them under a unified Prompt-based Adaptation (PA) framework with taxonomy covering learnable, generative, and non-learnable prompts organized by injection granularity.


<details>
  <summary>Details</summary>
Motivation: To address the blurred conceptual boundaries between VP and VPT in current research, provide systematic distinction between these techniques, and establish a unified framework for understanding prompt-based adaptation methods in computer vision.

Method: Revisits VP and VPT designs from first principles, conceptualizes them within a unified Prompt-based Adaptation (PA) framework, provides taxonomy categorizing methods into learnable, generative, and non-learnable prompts with pixel-level and token-level injection granularity.

Result: Establishes the first comprehensive survey dedicated to PA's methodologies and applications, examining integrations across medical imaging, 3D point clouds, vision-language tasks, test-time adaptation, and trustworthy AI, while summarizing benchmarks and identifying challenges.

Conclusion: The survey provides a clear roadmap for researchers and practitioners to understand and explore the evolving landscape of prompt-based adaptation research, offering systematic organization and future directions for this emerging field.

Abstract: In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune''
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA's integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.

</details>


### [29] [Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects](https://arxiv.org/abs/2510.13226)
*Hang-Cheng Dong,Yibo Jiao,Fupeng Wei,Guodong Liu,Dong Ye,Bingguo Liu*

Main category: cs.CV

TL;DR: Proposes a sample-centric multi-task learning framework for industrial surface defect inspection that jointly learns sample-level classification and pixel-level segmentation to improve reliability of quality control decisions.


<details>
  <summary>Details</summary>
Motivation: Address the mismatch between pixel-centric optimization and sample-level quality control decisions in industrial defect inspection, where extreme foreground-background imbalance, defect sparsity, and low contrast cause existing models to achieve good pixel metrics but poor sample-level stability.

Method: Multi-task learning framework with shared encoder that jointly learns sample-level defect classification and pixel-level mask localization. Sample-level supervision modulates feature distribution and boosts recall for small/low-contrast defects, while segmentation preserves boundary details.

Result: Substantially improves reliability of sample-level decisions and completeness of defect localization on two benchmark datasets, with proposed decision-linked metrics (Seg_mIoU and Seg_Recall) showing better performance than classical mIoU.

Conclusion: Sample-centric approach with joint classification and segmentation effectively addresses the optimization mismatch in industrial QC, providing more stable sample-level decisions and better defect localization, especially for sparse or slender defects.

Abstract: Industrial surface defect inspection for sample-wise quality control (QC)
must simultaneously decide whether a given sample contains defects and localize
those defects spatially. In real production lines, extreme
foreground-background imbalance, defect sparsity with a long-tailed scale
distribution, and low contrast are common. As a result, pixel-centric training
and evaluation are easily dominated by large homogeneous regions, making it
difficult to drive models to attend to small or low-contrast defects-one of the
main bottlenecks for deployment. Empirically, existing models achieve strong
pixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the
sample level, especially for sparse or slender defects. The root cause is a
mismatch between the optimization objective and the granularity of QC
decisions. To address this, we propose a sample-centric multi-task learning
framework and evaluation suite. Built on a shared-encoder architecture, the
method jointly learns sample-level defect classification and pixel-level mask
localization. Sample-level supervision modulates the feature distribution and,
at the gradient level, continually boosts recall for small and low-contrast
defects, while the segmentation branch preserves boundary and shape details to
enhance per-sample decision stability and reduce misses. For evaluation, we
propose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias
of classical mIoU caused by empty or true-negative samples and tightly couple
localization quality with sample-level decisions. Experiments on two benchmark
datasets demonstrate that our approach substantially improves the reliability
of sample-level decisions and the completeness of defect localization.

</details>


### [30] [What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging](https://arxiv.org/abs/2510.13232)
*Inha Kang,Youngsun Lim,Seonho Lee,Jiho Choi,Junsuk Choe,Hyunjung Shim*

Main category: cs.CV

TL;DR: The paper addresses the affirmative bias in vision-language models by proposing CoVAND dataset and NegToMe module to improve negation understanding in described object detection tasks.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art VLMs suffer from critical failures in understanding negation (affirmative bias), particularly in described object detection tasks, limiting their real-world applicability.

Method: Two main contributions: (1) CoVAND dataset pipeline using chain-of-thought and VQA-based approach for high-quality negation data, and (2) NegToMe text token merging module that groups negation cues with attributes into coherent semantic phrases to prevent structural loss during tokenization.

Result: Significant performance improvements on challenging negation benchmarks with lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval and demonstrating generalization to state-of-the-art VLMs.

Conclusion: This work marks a crucial step forward in addressing negation understanding for real-world detection applications through architectural and data-driven solutions.

Abstract: State-of-the-art vision-language models (VLMs) suffer from a critical failure
in understanding negation, often referred to as affirmative bias. This
limitation is particularly severe in described object detection (DOD) tasks. To
address this, we propose two primary contributions: (1) a new dataset pipeline
and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a
dataset constructed with a systematic chain-of-thought (CoT) and VQA-based
pipeline to generate high-quality, instance-grounded negation data. Second, we
propose NegToMe, a novel text token merging module that directly tackles the
architectural cause of affirmative bias. NegToMe fundamentally addresses the
structural loss of negation cues in tokenization, grouping them with attributes
into coherent semantic phrases. It maintains correct polarity at the input
level, enabling robust negation understanding even with limited data. For
instance, to prevent a model from treating the fragmented tokens "not" and
"girl" as simply "girl", NegToMe binds them into a single token whose meaning
is correctly distinguished from that of "girl" alone. This module is integrated
with a parameter-efficient and strategic LoRA fine-tuning approach. Our method
significantly improves performance on challenging negation benchmarks with a
lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval
and demonstrating generalization to SoTA VLMs. This work marks a crucial step
forward in addressing negation understanding for real-world detection
applications.

</details>


### [31] [UniVector: Unified Vector Extraction via Instance-Geometry Interaction](https://arxiv.org/abs/2510.13234)
*Yinglong Yan,Jun Yue,Shaobo Xia,Hanmeng Sun,Tianxu Ying,Chengcheng Wu,Sifan Lan,Min He,Pedram Ghamisi,Leyuan Fang*

Main category: cs.CV

TL;DR: UniVector is a unified framework that extracts multiple vector types (polygons, polylines, line segments) from raster images using a single model, overcoming limitations of existing single-type methods.


<details>
  <summary>Details</summary>
Motivation: Existing vector extraction methods are tailored to single vector types, requiring separate models for different structures, which limits their ability to capture complex structures due to independent treatment of instance and geometric attributes.

Method: UniVector encodes vectors as structured queries containing both instance- and geometry-level information, iteratively updates them through an interaction module for cross-level context exchange, and applies a dynamic shape constraint to refine global structures and key points.

Result: UniVector sets a new state of the art on both single- and multi-structure vector extraction tasks, as demonstrated on the newly introduced Multi-Vector dataset containing diverse polygons, polylines, and line segments.

Conclusion: The proposed UniVector framework successfully unifies multiple vector type extraction within a single model through instance-geometry interaction, achieving superior performance across various vector extraction scenarios.

Abstract: Vector extraction retrieves structured vector geometry from raster images,
offering high-fidelity representation and broad applicability. Existing
methods, however, are usually tailored to a single vector type (e.g., polygons,
polylines, line segments), requiring separate models for different structures.
This stems from treating instance attributes (category, structure) and
geometric attributes (point coordinates, connections) independently, limiting
the ability to capture complex structures. Inspired by the human brain's
simultaneous use of semantic and spatial interactions in visual perception, we
propose UniVector, a unified VE framework that leverages instance-geometry
interaction to extract multiple vector types within a single model. UniVector
encodes vectors as structured queries containing both instance- and
geometry-level information, and iteratively updates them through an interaction
module for cross-level context exchange. A dynamic shape constraint further
refines global structures and key points. To benchmark multi-structure
scenarios, we introduce the Multi-Vector dataset with diverse polygons,
polylines, and line segments. Experiments show UniVector sets a new state of
the art on both single- and multi-structure VE tasks. Code and dataset will be
released at https://github.com/yyyyll0ss/UniVector.

</details>


### [32] [EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking](https://arxiv.org/abs/2510.13235)
*Yukuan Zhang,Jiarui Zhao,Shangqing Nie,Jin Kuang,Shengsheng Wang*

Main category: cs.CV

TL;DR: EPIPTrack is a unified multimodal vision-language tracking framework that uses explicit and implicit prompts to dynamically model targets and align semantics, outperforming existing trackers on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal tracking methods rely on static textual descriptions from LLMs, which lack adaptability to real-time target state changes and are prone to hallucinations.

Method: Uses explicit prompts (spatial motion to natural language) and implicit prompts (pseudo-words with learnable descriptors) that dynamically adjust via CLIP text encoder, plus a Discriminative Feature Augmentor for enhanced representations.

Result: Extensive experiments on MOT17, MOT20, and DanceTrack show EPIPTrack outperforms existing trackers in diverse scenarios with robust adaptability and superior performance.

Conclusion: The proposed EPIPTrack framework effectively addresses limitations of static textual descriptions by leveraging dynamic multimodal prompts for improved target tracking.

Abstract: Multimodal semantic cues, such as textual descriptions, have shown strong
potential in enhancing target perception for tracking. However, existing
methods rely on static textual descriptions from large language models, which
lack adaptability to real-time target state changes and prone to
hallucinations. To address these challenges, we propose a unified multimodal
vision-language tracking framework, named EPIPTrack, which leverages explicit
and implicit prompts for dynamic target modeling and semantic alignment.
Specifically, explicit prompts transform spatial motion information into
natural language descriptions to provide spatiotemporal guidance. Implicit
prompts combine pseudo-words with learnable descriptors to construct
individualized knowledge representations capturing appearance attributes. Both
prompts undergo dynamic adjustment via the CLIP text encoder to respond to
changes in target state. Furthermore, we design a Discriminative Feature
Augmentor to enhance visual and cross-modal representations. Extensive
experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack
outperforms existing trackers in diverse scenarios, exhibiting robust
adaptability and superior performance.

</details>


### [33] [Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models](https://arxiv.org/abs/2510.13237)
*Haochuan Xu,Yun Sing Koh,Shuhuai Huang,Zirun Zhou,Di Wang,Jun Sakuma,Jingfeng Zhang*

Main category: cs.CV

TL;DR: This paper proposes EDPA, a model-agnostic adversarial patch attack for Vision-Language-Action (VLA) models that disrupts semantic alignment between visual and textual representations, along with a defense strategy using adversarial fine-tuning of the visual encoder.


<details>
  <summary>Details</summary>
Motivation: VLA models have shown revolutionary progress in robot learning but their adversarial robustness remains underexplored, creating security vulnerabilities in robotic systems.

Method: Proposes Embedding Disruption Patch Attack (EDPA) that generates adversarial patches to disrupt semantic alignment and maximize latent representation discrepancies, plus an adversarial fine-tuning defense for visual encoders.

Result: EDPA substantially increases task failure rates of state-of-the-art VLA models on the LIBERO benchmark, while the proposed defense effectively mitigates this performance degradation.

Conclusion: The work highlights critical security vulnerabilities in VLA models and provides both attack and defense strategies, demonstrating the importance of adversarial robustness in vision-language-action systems for robotics.

Abstract: Vision-Language-Action (VLA) models have achieved revolutionary progress in
robot learning, enabling robots to execute complex physical robot tasks from
natural language instructions. Despite this progress, their adversarial
robustness remains underexplored. In this work, we propose both adversarial
patch attack and corresponding defense strategies for VLA models. We first
introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic
adversarial attack that generates patches directly placeable within the
camera's view. In comparison to prior methods, EDPA can be readily applied to
different VLA models without requiring prior knowledge of the model
architecture, or the controlled robotic manipulator. EDPA constructs these
patches by (i) disrupting the semantic alignment between visual and textual
latent representations, and (ii) maximizing the discrepancy of latent
representations between adversarial and corresponding clean visual inputs.
Through the optimization of these objectives, EDPA distorts the VLA's
interpretation of visual information, causing the model to repeatedly generate
incorrect actions and ultimately result in failure to complete the given
robotic task. To counter this, we propose an adversarial fine-tuning scheme for
the visual encoder, in which the encoder is optimized to produce similar latent
representations for both clean and adversarially perturbed visual inputs.
Extensive evaluations on the widely recognized LIBERO robotic simulation
benchmark demonstrate that EDPA substantially increases the task failure rate
of cutting-edge VLA models, while our proposed defense effectively mitigates
this degradation. The codebase is accessible via the homepage at
https://edpa-attack.github.io/.

</details>


### [34] [FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding](https://arxiv.org/abs/2510.13243)
*Francesco Barbato,Matteo Caligiuri,Pietro Zanuttigh*

Main category: cs.CV

TL;DR: FlyAwareV2 is a multimodal dataset combining real and synthetic UAV imagery for urban scene understanding, featuring RGB, depth, and semantic labels across diverse environmental conditions, with benchmarks for semantic segmentation and domain adaptation studies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of collecting and annotating real-world UAV data, which is extremely costly and difficult, by providing a comprehensive multimodal dataset for urban scene understanding tasks.

Method: Built upon SynDrone and FlyAware datasets, FlyAwareV2 introduces multimodal data (RGB, depth, semantic labels) across varying weather and daytime conditions, computes depth maps for real samples using monocular depth estimation, and provides benchmarks for semantic segmentation.

Result: The dataset provides rich annotations and environmental diversity, enabling research on UAV-based 3D urban scene understanding and facilitating studies on synthetic-to-real domain adaptation.

Conclusion: FlyAwareV2 serves as a valuable resource for developing computer vision algorithms for UAV applications in urban environments, overcoming data collection limitations through its multimodal approach and environmental diversity.

Abstract: The development of computer vision algorithms for Unmanned Aerial Vehicle
(UAV) applications in urban environments heavily relies on the availability of
large-scale datasets with accurate annotations. However, collecting and
annotating real-world UAV data is extremely challenging and costly. To address
this limitation, we present FlyAwareV2, a novel multimodal dataset encompassing
both real and synthetic UAV imagery tailored for urban scene understanding
tasks. Building upon the recently introduced SynDrone and FlyAware datasets,
FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,
depth, semantic labels) across diverse environmental conditions including
varying weather and daytime; 2) Depth maps for real samples computed via
state-of-the-art monocular depth estimation; 3) Benchmarks for RGB and
multimodal semantic segmentation on standard architectures; 4) Studies on
synthetic-to-real domain adaptation to assess the generalization capabilities
of models trained on the synthetic data. With its rich set of annotations and
environmental diversity, FlyAwareV2 provides a valuable resource for research
on UAV-based 3D urban scene understanding.

</details>


### [35] [CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation](https://arxiv.org/abs/2510.13245)
*Li Liang,Bo Miao,Xinyu Wang,Naveed Akhtar,Jordan Vice,Ajmal Mian*

Main category: cs.CV

TL;DR: SketchSem3D is the first large-scale benchmark for generating 3D outdoor semantic scenes from sketches and satellite images, with a proposed Cylinder Mamba Diffusion (CymbaDiff) method that enhances spatial coherence.


<details>
  <summary>Details</summary>
Motivation: Advances in outdoor 3D semantic scene generation are constrained by the absence of publicly available, well-annotated datasets.

Method: Proposed Cylinder Mamba Diffusion (CymbaDiff) that imposes structured spatial ordering, captures cylindrical continuity and vertical hierarchy, and preserves physical neighborhood relationships and global context.

Result: Extensive experiments on SketchSem3D demonstrate that CymbaDiff achieves superior semantic consistency, spatial realism, and cross-dataset generalization.

Conclusion: The SketchSem3D benchmark and CymbaDiff method significantly advance outdoor 3D semantic scene generation capabilities.

Abstract: Outdoor 3D semantic scene generation produces realistic and semantically rich
environments for applications such as urban simulation and autonomous driving.
However, advances in this direction are constrained by the absence of publicly
available, well-annotated datasets. We introduce SketchSem3D, the first
large-scale benchmark for generating 3D outdoor semantic scenes from abstract
freehand sketches and pseudo-labeled annotations of satellite images.
SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based
KITTI-360 (containing LiDAR voxels along with their corresponding sketches and
annotated satellite images), to enable standardized, rigorous, and diverse
evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that
significantly enhances spatial coherence in outdoor 3D scene generation.
CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical
continuity and vertical hierarchy, and preserves both physical neighborhood
relationships and global context within the generated scenes. Extensive
experiments on SketchSem3D demonstrate that CymbaDiff achieves superior
semantic consistency, spatial realism, and cross-dataset generalization. The
code and dataset will be available at
https://github.com/Lillian-research-hub/CymbaDiff

</details>


### [36] [Real-Time Crowd Counting for Embedded Systems with Lightweight Architecture](https://arxiv.org/abs/2510.13250)
*Zhiyuan Zhao,Yubin Wen,Siyu Yang,Lichen Ning,Yuandong Liu,Junyu Gao*

Main category: cs.CV

TL;DR: A super real-time crowd counting model with stem-encoder-decoder structure that achieves the fastest inference speed while maintaining competitive accuracy, designed specifically for embedded systems.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting methods have excessive parameters and complex calculations, making them unsuitable for real-time applications on embedded systems in fields like intelligent security and public safety management.

Method: Uses a stem-encoder-decoder structure with large convolution kernels in stem network for receptive field, conditional channel weighting and multi-branch local fusion block in encoder for multi-scale feature merging with low computation, and feature pyramid networks to address incomplete fusion.

Result: Achieves 381.7 FPS on NVIDIA GTX 1080Ti and 71.9 FPS on NVIDIA Jetson TX1, making it the fastest crowd counting model while maintaining competitive accuracy on three benchmarks.

Conclusion: The proposed network is suitable for super real-time crowd counting on embedded systems, offering the fastest inference speed with acceptable accuracy trade-offs.

Abstract: Crowd counting is a task of estimating the number of the crowd through
images, which is extremely valuable in the fields of intelligent security,
urban planning, public safety management, and so on. However, the existing
counting methods have some problems in practical application on embedded
systems for these fields, such as excessive model parameters, abundant complex
calculations, etc. The practical application of embedded systems requires the
model to be real-time, which means that the model is fast enough. Considering
the aforementioned problems, we design a super real-time model with a
stem-encoder-decoder structure for crowd counting tasks, which achieves the
fastest inference compared with state-of-the-arts. Firstly, large convolution
kernels in the stem network are used to enlarge the receptive field, which
effectively extracts detailed head information. Then, in the encoder part, we
use conditional channel weighting and multi-branch local fusion block to merge
multi-scale features with low computational consumption. This part is crucial
to the super real-time performance of the model. Finally, the feature pyramid
networks are added to the top of the encoder to alleviate its incomplete fusion
problems. Experiments on three benchmarks show that our network is suitable for
super real-time crowd counting on embedded systems, ensuring competitive
accuracy. At the same time, the proposed network reasoning speed is the
fastest. Specifically, the proposed network achieves 381.7 FPS on NVIDIA GTX
1080Ti and 71.9 FPS on NVIDIA Jetson TX1.

</details>


### [37] [Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs](https://arxiv.org/abs/2510.13251)
*Minji Kim,Taekyung Kim,Bohyung Han*

Main category: cs.CV

TL;DR: VideoLLMs perform temporal reasoning through early cross-frame interactions and middle-layer video-language integration, with effective information pathways that can be selectively used while pruning 58% of attention edges without performance loss.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms of Video Large Language Models (VideoLLMs) - specifically where and how they extract and propagate video and textual information for temporal reasoning tasks like video question answering.

Method: Used mechanistic interpretability techniques to analyze information flow in VideoLLMs across diverse VideoQA tasks, examining cross-frame interactions and video-language integration patterns.

Result: Revealed consistent patterns: (1) temporal reasoning starts with cross-frame interactions in early-to-middle layers, (2) video-language integration occurs in middle layers via alignment between video representations and temporal linguistic embeddings, (3) answer generation happens in middle-to-late layers, (4) VideoLLMs can maintain performance by selecting effective pathways while pruning 58% of attention edges.

Conclusion: The findings provide a blueprint for how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization.

Abstract: Video Large Language Models (VideoLLMs) extend the capabilities of
vision-language models to spatiotemporal inputs, enabling tasks such as video
question answering (VideoQA). Despite recent advances in VideoLLMs, their
internal mechanisms on where and how they extract and propagate video and
textual information remain less explored. In this study, we investigate the
internal information flow of VideoLLMs using mechanistic interpretability
techniques. Our analysis reveals consistent patterns across diverse VideoQA
tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame
interactions in early-to-middle layers, (2) followed by progressive
video-language integration in middle layers. This is facilitated by alignment
between video representations and linguistic embeddings containing temporal
concepts. (3) Upon completion of this integration, the model is ready to
generate correct answers in middle-to-late layers. (4) Based on our analysis,
we show that VideoLLMs can retain their VideoQA performance by selecting these
effective information pathways while suppressing a substantial amount of
attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a
blueprint on how VideoLLMs perform temporal reasoning and offer practical
insights for improving model interpretability and downstream generalization.
Our project page with the source code is available at
https://map-the-flow.github.io

</details>


### [38] [End-to-End Multi-Modal Diffusion Mamba](https://arxiv.org/abs/2510.13253)
*Chunhao Lu,Qiang Lu,Meichen Dong,Jake Luo*

Main category: cs.CV

TL;DR: MDM (Multi-modal Diffusion Mamba) is a unified architecture that uses Mamba-based diffusion with a shared variational autoencoder to process multiple modalities, achieving superior performance in image generation and text tasks while competing with SOTA models.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal models use separate encoders and decoders for different modalities, which hinders joint representation learning and unified processing across modalities.

Method: Proposes MDM architecture using Mamba-based multi-step selection diffusion model with unified variational autoencoder for both encoding and decoding, progressively generating and refining modality-specific information.

Result: MDM significantly outperforms existing end-to-end models (MonoFormer, LlamaGen, Chameleon) and competes effectively with SOTA models like GPT-4V, Gemini Pro, and Mistral in image generation, captioning, VQA, text comprehension, and reasoning tasks.

Conclusion: MDM effectively unifies multi-modal processing while maintaining computational efficiency, establishing a new direction for end-to-end multi-modal architectures.

Abstract: Current end-to-end multi-modal models utilize different encoders and decoders
to process input and output information. This separation hinders the joint
representation learning of various modalities. To unify multi-modal processing,
we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM
utilizes a Mamba-based multi-step selection diffusion model to progressively
generate and refine modality-specific information through a unified variational
autoencoder for both encoding and decoding. This innovative approach allows MDM
to achieve superior performance when processing high-dimensional data,
particularly in generating high-resolution images and extended text sequences
simultaneously. Our evaluations in areas such as image generation, image
captioning, visual question answering, text comprehension, and reasoning tasks
demonstrate that MDM significantly outperforms existing end-to-end models
(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA
models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's
effectiveness in unifying multi-modal processes while maintaining computational
efficiency, establishing a new direction for end-to-end multi-modal
architectures.

</details>


### [39] [MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models](https://arxiv.org/abs/2510.13276)
*Keyan Zhou,Zecheng Tang,Lingfeng Ming,Guanghao Zhou,Qiguang Chen,Dan Qiao,Zheming Yang,Libo Qin,Minghui Qiu,Juntao Li,Min Zhang*

Main category: cs.CV

TL;DR: MMLongCite is a new benchmark for evaluating large vision language models' faithfulness in long-context scenarios across text, images, and videos, revealing current models' limitations.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of long-context faithfulness focus mainly on text-only domains, while multimodal assessments are limited to short contexts, creating a gap in understanding LVLMs' performance in real-world long-context applications.

Method: Developed MMLongCite benchmark with 8 distinct tasks spanning 6 context length intervals, incorporating diverse modalities including text, images, and videos to evaluate LVLMs' fidelity.

Result: Evaluation of state-of-the-art LVLMs revealed limited faithfulness in handling long multimodal contexts, with performance affected by context length and position of crucial content.

Conclusion: There is a significant gap in LVLMs' ability to effectively utilize long multimodal contexts, highlighting the need for improved long-context faithfulness in multimodal AI systems.

Abstract: The rapid advancement of large vision language models (LVLMs) has led to a
significant expansion of their context windows. However, an extended context
window does not guarantee the effective utilization of the context, posing a
critical challenge for real-world applications. Current evaluations of such
long-context faithfulness are predominantly focused on the text-only domain,
while multimodal assessments remain limited to short contexts. To bridge this
gap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate
the fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8
distinct tasks spanning 6 context length intervals and incorporates diverse
modalities, including text, images, and videos. Our evaluation of
state-of-the-art LVLMs reveals their limited faithfulness in handling long
multimodal contexts. Furthermore, we provide an in-depth analysis of how
context length and the position of crucial content affect the faithfulness of
these models.

</details>


### [40] [Universal Image Restoration Pre-training via Masked Degradation Classification](https://arxiv.org/abs/2510.13282)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yinghao Chen,Yanye Lu*

Main category: cs.CV

TL;DR: MaskDCPT is a pre-training method that uses masked degradation classification and image reconstruction to learn generalized representations for image restoration tasks, achieving significant performance improvements across various degradation types.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional pre-training methods by using degradation type as weak supervision while leveraging image reconstruction to enhance performance and robustness in image restoration tasks.

Method: Uses an encoder and two decoders: encoder extracts features from masked low-quality images, classification decoder identifies degradation types, and reconstruction decoder reconstructs high-quality images. Combines masked image modeling and contrastive learning.

Result: Achieves minimum 3.77 dB PSNR increase in 5D all-in-one restoration and 34.8% PIQE reduction in real-world scenarios. Shows strong generalization to unseen degradation types and levels. Works well for both CNNs and Transformers.

Conclusion: MaskDCPT provides an effective pre-training approach for universal image restoration, demonstrating superior performance and generalization capabilities. Also releases UIR-2.5M dataset with 2.5M paired samples across 19 degradation types.

Abstract: This study introduces a Masked Degradation Classification Pre-Training method
(MaskDCPT), designed to facilitate the classification of degradation types in
input images, leading to comprehensive image restoration pre-training. Unlike
conventional pre-training methods, MaskDCPT uses the degradation type of the
image as an extremely weak supervision, while simultaneously leveraging the
image reconstruction to enhance performance and robustness. MaskDCPT includes
an encoder and two decoders: the encoder extracts features from the masked
low-quality input image. The classification decoder uses these features to
identify the degradation type, whereas the reconstruction decoder aims to
reconstruct a corresponding high-quality image. This design allows the
pre-training to benefit from both masked image modeling and contrastive
learning, resulting in a generalized representation suited for restoration
tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained
encoder can be used to address universal image restoration and achieve
outstanding performance. Implementing MaskDCPT significantly improves
performance for both convolution neural networks (CNNs) and Transformers, with
a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and
a 34.8% reduction in PIQE compared to baseline in real-world degradation
scenarios. It also emergences strong generalization to previously unseen
degradation types and levels. In addition, we curate and release the UIR-2.5M
dataset, which includes 2.5 million paired restoration samples across 19
degradation types and over 200 degradation levels, incorporating both synthetic
and real-world data. The dataset, source code, and models are available at
https://github.com/MILab-PKU/MaskDCPT.

</details>


### [41] [Automated document processing system for government agencies using DBNET++ and BART models](https://arxiv.org/abs/2510.13303)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: An automatic document classification system that detects text in images and classifies documents into four categories (Invoice, Report, Letter, Form) using DBNet++ for text detection and BART for classification.


<details>
  <summary>Details</summary>
Motivation: To address practical challenges in document classification including variable illumination, arbitrary orientation, curved/occluded text, low resolution, and distant text in both offline images and real-time camera capture scenarios.

Method: Four-stage pipeline: image capture/preprocessing, text detection using DBNet++ (Differentiable Binarization Network Plus), text classification using BART (Bidirectional and Auto-Regressive Transformers), with PyQt5 user interface.

Result: Achieved 92.88% text detection accuracy on Total-Text dataset with challenging high-resolution images after 10 hours of testing.

Conclusion: The proposed approach is effective for practical, mixed-source document categorization in unconstrained imaging scenarios.

Abstract: An automatic document classification system is presented that detects textual
content in images and classifies documents into four predefined categories
(Invoice, Report, Letter, and Form). The system supports both offline images
(e.g., files on flash drives, HDDs, microSD) and real-time capture via
connected cameras, and is designed to mitigate practical challenges such as
variable illumination, arbitrary orientation, curved or partially occluded
text, low resolution, and distant text. The pipeline comprises four stages:
image capture and preprocessing, text detection [1] using a DBNet++
(Differentiable Binarization Network Plus) detector, and text classification
[2] using a BART (Bidirectional and Auto-Regressive Transformers) classifier,
all integrated within a user interface implemented in Python with PyQt5. The
achieved results by the system for text detection in images were good at about
92.88% through 10 hours on Total-Text dataset that involve high resolution
images simulate a various and very difficult challenges. The results indicate
the proposed approach is effective for practical, mixed-source document
categorization in unconstrained imaging scenarios.

</details>


### [42] [Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning](https://arxiv.org/abs/2510.13307)
*Yang Li,Aming Wu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: This paper proposes a novel approach for 3D Novel Class Discovery (3D-NCD) using structural causal modeling to learn segmentation of unlabeled novel classes by leveraging supervision from labeled base classes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of segmenting novel 3D classes without direct supervision by establishing precise correlations between point representations and class labels, avoiding confusion in novel class inference through causal relationships.

Method: The method introduces a structural causal model (SCM) to reformulate 3D-NCD, featuring joint learning of causal representation and reasoning. It analyzes hidden confounders in base class representations, creates causal representation prototypes that eliminate confounders, and uses graph structures to model causal relationships between base and novel class prototypes.

Result: Extensive experiments and visualization results on both 3D and 2D NCD semantic segmentation demonstrate the superior performance of the proposed method.

Conclusion: The paper concludes that imposing causal relationships as strong correlated constraints effectively uncovers essential point cloud representations that accurately correspond to classes, enabling successful novel class discovery in 3D segmentation tasks.

Abstract: In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation
(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes
using only the supervision from labeled (base) 3D classes. The key to this task
is to setup the exact correlations between the point representations and their
base class labels, as well as the representation correlations between the
points from base and novel classes. A coarse or statistical correlation
learning may lead to the confusion in novel class inference. lf we impose a
causal relationship as a strong correlated constraint upon the learning
process, the essential point cloud representations that accurately correspond
to the classes should be uncovered. To this end, we introduce a structural
causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,
i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we
first analyze hidden confounders in the base class representations and the
causal relationships between the base and novel classes through SCM. We devise
a causal representation prototype that eliminates confounders to capture the
causal representations of base classes. A graph structure is then used to model
the causal relationships between the base classes' causal representation
prototypes and the novel class prototypes, enabling causal reasoning from base
to novel classes. Extensive experiments and visualization results on 3D and 2D
NCD semantic segmentation demonstrate the superiorities of our method.

</details>


### [43] [InstantSfM: Fully Sparse and Parallel Structure-from-Motion](https://arxiv.org/abs/2510.13310)
*Jiankun Zhong,Zitong Zhan,Quankai Gao,Ziyu Chen,Haozhe Lou,Jiageng Mao,Ulrich Neumann,Yue Wang*

Main category: cs.CV

TL;DR: A GPU-accelerated SfM method that achieves 40x speedup over COLMAP while maintaining comparable accuracy, handling up to 5000 images where deep learning methods fail.


<details>
  <summary>Details</summary>
Motivation: Traditional SfM methods like COLMAP have computational bottlenecks with large-scale scenarios, while deep learning approaches face GPU memory limitations with thousands of views. There's a need for scalable, fast SfM that maintains accuracy.

Method: Leverages GPU parallel computation to accelerate all critical SfM pipeline stages, extending sparse-aware bundle adjustment techniques to both BA and global positioning within a unified global SfM framework.

Result: Achieves up to 40x speedup over COLMAP while maintaining comparable or improved reconstruction accuracy, successfully handling 5000-image datasets where VGGSfM and VGGT run out of memory.

Conclusion: GPU parallel computation effectively addresses SfM scalability issues, providing dramatic speed improvements without sacrificing reconstruction quality, making large-scale 3D reconstruction more practical.

Abstract: Structure-from-Motion (SfM), a method that recovers camera poses and scene
geometry from uncalibrated images, is a central component in robotic
reconstruction and simulation. Despite the state-of-the-art performance of
traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive
CPU-specialized implementations of bundle adjustment (BA) or global positioning
(GP) introduce significant computational overhead when handling large-scale
scenarios, leading to a trade-off between accuracy and speed in SfM. Moreover,
the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes
with the curse of limited flexibility, as they lack support for various
external optimization options. On the other hand, while deep learning based SfM
pipelines like VGGSfM and VGGT enable feed-forward 3D reconstruction, they are
unable to scale to thousands of input views at once as GPU memory consumption
increases sharply as the number of input views grows. In this paper, we unleash
the full potential of GPU parallel computation to accelerate each critical
stage of the standard SfM pipeline. Building upon recent advances in
sparse-aware bundle adjustment optimization, our design extends these
techniques to accelerate both BA and GP within a unified global SfM framework.
Through extensive experiments on datasets of varying scales (e.g. 5000 images
where VGGSfM and VGGT run out of memory), our method demonstrates up to about
40 times speedup over COLMAP while achieving consistently comparable or even
improved reconstruction accuracy. Our project page can be found at
https://cre185.github.io/InstantSfM/.

</details>


### [44] [Self-Augmented Visual Contrastive Decoding](https://arxiv.org/abs/2510.13315)
*Eun Woo Im,Muhammad Kashif Ali,Vivek Gupta*

Main category: cs.CV

TL;DR: A novel training-free decoding strategy for Large Vision-Language Models that reduces hallucinations by using self-augmentation prompting and adaptive thresholding.


<details>
  <summary>Details</summary>
Motivation: LVLMs inherit hallucination tendencies from language models, and existing visual contrastive decoding methods use generic augmentations that ignore text query context, limiting effectiveness.

Method: Two key contributions: 1) Self-augmentation prompting that aligns semantics between query and visual augmentation using model's intrinsic knowledge, 2) Adaptive thresholding algorithm that adjusts next token candidate size based on output sparsity using full logit distribution information.

Result: Extensive experiments across four LVLMs and seven benchmarks show significant improvement in factual consistency compared to state-of-the-art decoding methods.

Conclusion: The work demonstrates the importance of integrating query-dependent augmentation and entropy-aware decoding for improving effective generation in LVLMs.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal
capabilities, but they inherit the tendency to hallucinate from their
underlying language models. While visual contrastive decoding has been proposed
to mitigate this issue, existing methods often apply generic visual
augmentations that disregard the specific context provided by the text query,
limiting their effectiveness. This study introduces a novel training-free
decoding strategy that addresses these limitations, featuring two key
contributions. First, a self-augmentation prompting strategy that leverages the
intrinsic knowledge of the model to dynamically align semantics between the
query and the visual augmentation. Second, an adaptive thresholding algorithm
that adaptively adjusts next token candidate size based on the output sparsity,
utilizing full information from the logit distribution. Extensive experiments
across four LVLMs and seven benchmarks demonstrate that the proposed decoding
significantly enhances factual consistency compared to state-of-the-art
decoding methods. This work highlights the importance of integrating
query-dependent augmentation and entropy-aware decoding for improving effective
generation of LVLMs.

</details>


### [45] [Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests](https://arxiv.org/abs/2510.13316)
*Fitim Abdullahu,Helmut Grabner*

Main category: cs.CV

TL;DR: The paper explores how Large Multimodal Models (LMMs), particularly GPT-4o, capture visual interestingness concepts and compares their assessments with human judgments, finding partial alignment that enables effective labeling for training ranking models.


<details>
  <summary>Details</summary>
Motivation: To understand how well LMMs capture visual interestingness concepts and examine the alignment between human assessments and AI predictions, given that attention and interestingness are crucial in daily life and consumption.

Method: Comparative analysis between human assessments and GPT-4o's predictions of visual interestingness, using the partial alignment to label image pairs for training a learning-to-rank model.

Result: Partial alignment between humans and GPT-4o was found, with GPT-4o capturing the concept better than state-of-the-art methods, enabling effective labeling of image pairs according to interestingness.

Conclusion: The findings allow for knowledge distillation into ranking models and pave the way for deeper understanding of human interest through LMM capabilities.

Abstract: Our daily life is highly influenced by what we consume and see. Attracting
and holding one's attention -- the definition of (visual) interestingness -- is
essential. The rise of Large Multimodal Models (LMMs) trained on large-scale
visual and textual data has demonstrated impressive capabilities. We explore
these models' potential to understand to what extent the concepts of visual
interestingness are captured and examine the alignment between human
assessments and GPT-4o's, a leading LMM, predictions through comparative
analysis. Our studies reveal partial alignment between humans and GPT-4o. It
already captures the concept as best compared to state-of-the-art methods.
Hence, this allows for the effective labeling of image pairs according to their
(commonly) interestingness, which are used as training data to distill the
knowledge into a learning-to-rank model. The insights pave the way for a deeper
understanding of human interest.

</details>


### [46] [Removing Cost Volumes from Optical Flow Estimators](https://arxiv.org/abs/2510.13317)
*Simon Kiefhaber,Stefan Roth,Simone Schaub-Meyer*

Main category: cs.CV

TL;DR: Training strategy that removes cost volumes from optical flow estimators after sufficient training, improving speed and reducing memory while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Cost volumes are computationally expensive and memory-intensive, limiting processing speed and input resolution in optical flow estimation.

Method: Introduce training strategy that removes cost volumes once other network parts are sufficiently trained, creating three models for different compute budgets.

Result: Most accurate model achieves state-of-the-art accuracy with 1.2x faster inference and 6x lower memory footprint; fastest model processes Full HD at 20 FPS using only 500MB GPU memory.

Conclusion: Cost volumes can be effectively removed from optical flow estimators through proper training strategy, enabling significant speed improvements and memory reduction without sacrificing accuracy.

Abstract: Cost volumes are used in every modern optical flow estimator, but due to
their computational and space complexity, they are often a limiting factor
regarding both processing speed and the resolution of input frames. Motivated
by our empirical observation that cost volumes lose their importance once all
other network parts of, e.g., a RAFT-based pipeline have been sufficiently
trained, we introduce a training strategy that allows removing the cost volume
from optical flow estimators throughout training. This leads to significantly
improved inference speed and reduced memory requirements. Using our training
strategy, we create three different models covering different compute budgets.
Our most accurate model reaches state-of-the-art accuracy while being
$1.2\times$ faster and having a $6\times$ lower memory footprint than
comparable models; our fastest model is capable of processing Full HD frames at
$20\,\mathrm{FPS}$ using only $500\,\mathrm{MB}$ of GPU memory.

</details>


### [47] [DEF-YOLO: Leveraging YOLO for Concealed Weapon Detection in Thermal Imagin](https://arxiv.org/abs/2510.13326)
*Divya Bhardwaj,Arnav Ramamoorthy,Poonam Goyal*

Main category: cs.CV

TL;DR: Proposes DEF-YOLO, a YOLOv8-based architecture with deformable convolutions for concealed weapon detection in thermal imagery, and introduces the first large-scale Thermal Imaging Concealed Weapon (TICW) dataset.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing imaging modalities (poor resolution, privacy concerns) and provide real-time, low-cost, privacy-preserved surveillance for concealed weapon detection using thermal imaging.

Method: Enhanced YOLOv8 with deformable convolutions at SPPF layer and backbone/neck layers to extract multi-scale features, enabling adaptive focus on thermal homogeneous regions. Uses focal loss to handle class imbalance.

Result: The proposed DEF-YOLO architecture achieves state-of-the-art performance for concealed weapon detection in thermal imagery while maintaining speed and throughput.

Conclusion: The work establishes a new benchmark for concealed weapon detection in thermal imaging through the proposed DEF-YOLO architecture and the first large-scale TICW dataset.

Abstract: Concealed weapon detection aims at detecting weapons hidden beneath a
person's clothing or luggage. Various imaging modalities like Millimeter Wave,
Microwave, Terahertz, Infrared, etc., are exploited for the concealed weapon
detection task. These imaging modalities have their own limitations, such as
poor resolution in microwave imaging, privacy concerns in millimeter wave
imaging, etc. To provide a real-time, 24 x 7 surveillance, low-cost, and
privacy-preserved solution, we opted for thermal imaging in spite of the lack
of availability of a benchmark dataset. We propose a novel approach and a
dataset for concealed weapon detection in thermal imagery. Our YOLO-based
architecture, DEF-YOLO, is built with key enhancements in YOLOv8 tailored to
the unique challenges of concealed weapon detection in thermal vision. We adopt
deformable convolutions at the SPPF layer to exploit multi-scale features;
backbone and neck layers to extract low, mid, and high-level features, enabling
DEF-YOLO to adaptively focus on localization around the objects in thermal
homogeneous regions, without sacrificing much of the speed and throughput. In
addition to these simple yet effective key architectural changes, we introduce
a new, large-scale Thermal Imaging Concealed Weapon dataset, TICW, featuring a
diverse set of concealed weapons and capturing a wide range of scenarios. To
the best of our knowledge, this is the first large-scale contributed dataset
for this task. We also incorporate focal loss to address the significant class
imbalance inherent in the concealed weapon detection task. The efficacy of the
proposed work establishes a new benchmark through extensive experimentation for
concealed weapon detection in thermal imagery.

</details>


### [48] [Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models](https://arxiv.org/abs/2510.13331)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: Group-VQ improves VQ-VAEs by using group-wise codebook optimization to address codebook collapse issues, achieving better reconstruction quality and allowing flexible post-training codebook size adjustments.


<details>
  <summary>Details</summary>
Motivation: To solve persistent codebook collapse problems in VQ-VAEs where existing methods using implicit static codebooks or joint optimization constrain learning capability and reduce reconstruction quality.

Method: Proposes Group-VQ with group-wise codebook optimization where each group is optimized independently with joint optimization within groups, plus a training-free codebook resampling method for post-training size adjustments.

Result: Improved reconstruction performance across various image reconstruction settings and achieved desired flexibility in adjusting codebook size after training.

Conclusion: Group-VQ provides better trade-off between codebook utilization and reconstruction performance while enabling flexible post-training codebook size adjustments.

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised
learning through reconstruction tasks to represent continuous vectors using the
closest vectors in a codebook. However, issues such as codebook collapse
persist in the VQ model. To address these issues, existing approaches employ
implicit static codebooks or jointly optimize the entire codebook, but these
methods constrain the codebook's learning capability, leading to reduced
reconstruction quality. In this paper, we propose Group-VQ, which performs
group-wise optimization on the codebook. Each group is optimized independently,
with joint optimization performed within groups. This approach improves the
trade-off between codebook utilization and reconstruction performance.
Additionally, we introduce a training-free codebook resampling method, allowing
post-training adjustment of the codebook size. In image reconstruction
experiments under various settings, Group-VQ demonstrates improved performance
on reconstruction metrics. And the post-training codebook sampling method
achieves the desired flexibility in adjusting the codebook size.

</details>


### [49] [No-Reference Rendered Video Quality Assessment: Dataset and Metrics](https://arxiv.org/abs/2510.13349)
*Sipeng Yang,Jiayu Ji,Qingchuan Zhu,Zhiyao Yang,Xiaogang Jin*

Main category: cs.CV

TL;DR: A new no-reference video quality assessment metric and dataset specifically designed for rendered videos, addressing the limitations of existing camera-focused metrics in evaluating temporal artifacts common in computer graphics applications.


<details>
  <summary>Details</summary>
Motivation: Existing NR-VQA methods are biased for rendered videos because they focus on camera-captured content and don't properly handle temporal artifacts that are more prevalent in rendered videos used in games, VR, and AR applications.

Method: Created a large rendering-oriented video dataset with subjective quality annotations across various 3D scenes and rendering settings. Designed a NR-VQA metric that evaluates both image quality and temporal stability specifically for rendered videos.

Result: The proposed metric demonstrates superior performance compared to existing NR-VQA metrics when applied to rendered videos, and can effectively benchmark supersampling methods and assess frame generation strategies.

Conclusion: The developed dataset and metric provide a specialized solution for quality assessment of rendered videos, filling an important gap in computer graphics applications where traditional camera-focused metrics are inadequate.

Abstract: Quality assessment of videos is crucial for many computer graphics
applications, including video games, virtual reality, and augmented reality,
where visual performance has a significant impact on user experience. When test
videos cannot be perfectly aligned with references or when references are
unavailable, the significance of no-reference video quality assessment (NR-VQA)
methods is undeniable. However, existing NR-VQA datasets and metrics are
primarily focused on camera-captured videos; applying them directly to rendered
videos would result in biased predictions, as rendered videos are more prone to
temporal artifacts. To address this, we present a large rendering-oriented
video dataset with subjective quality annotations, as well as a designed NR-VQA
metric specific to rendered videos. The proposed dataset includes a wide range
of 3D scenes and rendering settings, with quality scores annotated for various
display types to better reflect real-world application scenarios. Building on
this dataset, we calibrate our NR-VQA metric to assess rendered video quality
by looking at both image quality and temporal stability. We compare our metric
to existing NR-VQA metrics, demonstrating its superior performance on rendered
videos. Finally, we demonstrate that our metric can be used to benchmark
supersampling methods and assess frame generation strategies in real-time
rendering.

</details>


### [50] [Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity](https://arxiv.org/abs/2510.13364)
*MingZe Tang,Jubal Chandy Jacob*

Main category: cs.CV

TL;DR: Simple prompts outperform detailed ones in zero-shot classification of human postures using modern VLMs, with MetaCLIP 2 and OpenCLIP showing best results with basic prompts while SigLip benefits from descriptive prompts.


<details>
  <summary>Details</summary>
Motivation: To understand how prompt design affects zero-shot classification of visually similar categories like human postures in data-scarce conditions.

Method: Evaluated modern VLMs (OpenCLIP, MetaCLIP 2, SigLip) on 285-image COCO dataset using three-tiered prompt design with increasing linguistic detail.

Result: Highest-performing models (MetaCLIP 2 and OpenCLIP) achieved best results with simplest prompts (68.8% accuracy), while detailed prompts degraded performance. SigLip improved with descriptive prompts for ambiguous classes.

Conclusion: Simple prompts work best for high-performing VLMs, revealing 'prompt overfitting' phenomenon where adding detail harms performance.

Abstract: Recent Vision-Language Models (VLMs) enable zero-shot classification by
aligning images and text in a shared space, a promising approach for
data-scarce conditions. However, the influence of prompt design on recognizing
visually similar categories, such as human postures, is not well understood.
This study investigates how prompt specificity affects the zero-shot
classification of sitting, standing, and walking/running on a small, 285-image
COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,
and SigLip, were evaluated using a three-tiered prompt design that
systematically increases linguistic detail. Our findings reveal a compelling,
counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and
OpenCLIP), the simplest, most basic prompts consistently achieve the best
results. Adding descriptive detail significantly degrades performance for
instance, MetaCLIP 2's multi-class accuracy drops from 68.8\% to 55.1\% a
phenomenon we term "prompt overfitting". Conversely, the lower-performing
SigLip model shows improved classification on ambiguous classes when given more
descriptive, body-cue-based prompts.

</details>


### [51] [DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning](https://arxiv.org/abs/2510.13375)
*Tianyuan Yuan,Yicheng Liu,Chenhao Lu,Zhuoguang Chen,Tao Jiang,Hang Zhao*

Main category: cs.CV

TL;DR: DepthVLA is a Vision-Language-Action model that enhances spatial reasoning by incorporating a pretrained depth prediction module, achieving superior performance in manipulation tasks compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current VLA models suffer from limited spatial reasoning capabilities inherited from VLMs, leading to degraded performance on tasks requiring precise spatial understanding. Existing approaches rely on extensive action-data pretraining which is inefficient and still insufficient.

Method: DepthVLA uses a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial awareness through pretrained depth prediction.

Result: DepthVLA outperforms state-of-the-art approaches significantly: 78.5% vs 65.0% progress in real-world tasks, 94.9% vs 93.6% in LIBERO simulator, and 74.8% vs 58.8% in Simpler simulator.

Conclusion: Explicitly incorporating spatial awareness through pretrained depth prediction is an effective approach for enhancing VLA models' spatial reasoning capabilities, leading to improved performance in manipulation tasks across both real-world and simulated environments.

Abstract: Vision-Language-Action (VLA) models have recently shown impressive
generalization and language-guided manipulation capabilities. However, their
performance degrades on tasks requiring precise spatial reasoning due to
limited spatial reasoning inherited from Vision-Language Models (VLMs).
Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D
space, which reduces training efficiency and is still insufficient for accurate
spatial understanding. In this work, we present DepthVLA, a simple yet
effective VLA architecture that explicitly incorporates spatial awareness
through a pretrained depth prediction module. DepthVLA adopts a
mixture-of-transformers design that unifies a VLM, a depth transformer, and an
action expert with fully shared attentions, forming an end-to-end model with
enhanced spatial reasoning. Extensive evaluations in both real-world and
simulated environments show that DepthVLA outperforms state-of-the-art
approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.
93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.
Our code will be made publicly available.

</details>


### [52] [Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering](https://arxiv.org/abs/2510.13381)
*Siddharth Tourani,Jayaram Reddy,Akash Kumbar,Satyajit Tourani,Nishant Goyal,Madhava Krishna,N. Dinesh Reddy,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: A novel method that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) for dynamic scene rendering and reconstruction, achieving state-of-the-art performance without requiring LiDAR data or ground-truth 3D motion annotations.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing 3DGS methods that require camera and LiDAR data, ground-truth 3D segmentations, and motion data for dynamic urban scene modeling, by exploring whether 2D object-agnostic priors (depth and point tracking) with SDF representation can relax these requirements.

Method: Integrates SDFs with 3DGS in a unified optimization framework, using 2D object-agnostic priors (depth and point tracking) coupled with SDF representation for dynamic objects to enhance geometric accuracy and improve deformation modeling.

Result: Achieves state-of-the-art performance in rendering metrics without LiDAR data on urban scenes. When incorporating LiDAR, further improves reconstruction and novel view generation across diverse object categories without ground-truth 3D motion annotation. Enables scene editing tasks including scene decomposition and composition.

Conclusion: The proposed SDF-3DGS integration creates a more robust object representation that is more adaptable and precise, successfully relaxing the requirements of existing methods while maintaining high performance and enabling additional scene editing capabilities.

Abstract: Dynamic scene rendering and reconstruction play a crucial role in computer
vision and augmented reality. Recent methods based on 3D Gaussian Splatting
(3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban
scenes they require both camera and LiDAR data, ground-truth 3D segmentations
and motion data in the form of tracklets or pre-defined object templates such
as SMPL. In this work, we explore whether a combination of 2D object agnostic
priors in the form of depth and point tracking coupled with a signed distance
function (SDF) representation for dynamic objects can be used to relax some of
these requirements. We present a novel approach that integrates Signed Distance
Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust
object representation by harnessing the strengths of both methods. Our unified
optimization framework enhances the geometric accuracy of 3D Gaussian splatting
and improves deformation modeling within the SDF, resulting in a more adaptable
and precise representation. We demonstrate that our method achieves
state-of-the-art performance in rendering metrics even without LiDAR data on
urban scenes. When incorporating LiDAR, our approach improved further in
reconstructing and generating novel views across diverse object categories,
without ground-truth 3D motion annotation. Additionally, our method enables
various scene editing tasks, including scene decomposition, and scene
composition.

</details>


### [53] [Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment](https://arxiv.org/abs/2510.13390)
*Feng-Qi Cui,Yu-Tong Guo,Tianyue Zheng,Jinyang Huang*

Main category: cs.CV

TL;DR: GLSDA is a WiFi-based gesture recognition framework that uses large foundation models for semantic distillation and alignment, achieving superior performance in both in-domain and cross-domain scenarios while reducing model size and latency.


<details>
  <summary>Details</summary>
Motivation: To address limited generalization and semantic expressiveness in existing WiFi gesture recognition methods, which suffer from domain-sensitive Channel State Information and lack of high-level gesture abstraction.

Method: Uses dual-path CSI encoding (CSI-Ratio phase sequences and Doppler spectrograms), Multiscale Semantic Encoder with cross-modal attention, Semantic-Aware Soft Supervision for inter-class correlations, and Robust Dual-Distillation to compress model into lightweight student network.

Result: Outperforms state-of-the-art methods on Widar3.0 benchmark in both in-domain and cross-domain gesture recognition, while significantly reducing model size and inference latency.

Conclusion: Provides a scalable and deployable solution for generalized RF-based gesture interfaces in real-world AIoT applications.

Abstract: WiFi-based gesture recognition has emerged as a promising RF sensing paradigm
for enabling non-contact and privacy-preserving human-computer interaction in
AIoT environments. However, existing methods often suffer from limited
generalization and semantic expressiveness due to the domain-sensitive nature
of Channel State Information and the lack of high-level gesture abstraction. To
address these challenges, we propose a novel generalization framework, termed
Large-Model-Aware Semantic Distillation and Alignment (GLSDA), which leverages
the semantic prior of pre-trained large foundation models to enhance gesture
representation learning in both in-domain and cross-domain scenarios.
Specifically, we first design a dual-path CSI encoding pipeline that captures
geometric and dynamic gesture patterns via CSI-Ratio phase sequences and
Doppler spectrograms. These representations are then fed into a Multiscale
Semantic Encoder, which learns robust temporal embeddings and aligns them with
gesture semantics through cross-modal attention mechanisms. To further enhance
category discrimination, we introduce a Semantic-Aware Soft Supervision scheme
that encodes inter-class correlations and reduces label ambiguity, especially
for semantically similar gestures. Finally, we develop a Robust
Dual-Distillation strategy to compress the aligned model into a lightweight
student network, jointly distilling intermediate features and semantic-informed
soft labels from the teacher model. Extensive experiments on the Widar3.0
benchmark show that GLSDA consistently outperforms state-of-the-art methods in
both in-domain and cross-domain gesture recognition tasks, while significantly
reducing model size and inference latency. Our method offers a scalable and
deployable solution for generalized RF-based gesture interfaces in real-world
AIoT applications.

</details>


### [54] [Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.13394)
*Xinmiao Huang,Qisong He,Zhenglin Huang,Boxuan Wang,Zhuoyun Li,Guangliang Cheng,Yi Dong,Xiaowei Huang*

Main category: cs.CV

TL;DR: Spatial-DISE is a new benchmark for evaluating Vision Language Models' spatial reasoning abilities, addressing gaps in existing benchmarks by covering four fundamental spatial reasoning quadrants with automatically generated diverse data.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are inadequate for assessing spatial reasoning, especially intrinsic-dynamic spatial reasoning which is fundamental to human cognition. This gap limits VLMs' real-world applications in robotics, AR, and autonomous navigation.

Method: Developed a unified benchmark with cognitively grounded taxonomy categorizing tasks into four quadrants: Intrinsic-Static, Intrinsic-Dynamic, Extrinsic-Static, and Extrinsic-Dynamic. Created scalable automated pipeline to generate diverse and verifiable spatial reasoning questions.

Result: Generated Spatial-DISE dataset with 559 evaluation VQA pairs and 12K+ training pairs. Evaluation of 28 state-of-the-art VLMs revealed large consistent gaps to human competence, especially on multi-step multi-view spatial reasoning.

Conclusion: Spatial-DISE provides robust framework, valuable dataset, and clear direction for future research toward human-like spatial intelligence. Current VLMs significantly lag behind human spatial reasoning capabilities.

Abstract: Spatial reasoning ability is crucial for Vision Language Models (VLMs) to
support real-world applications in diverse domains including robotics,
augmented reality, and autonomous navigation. Unfortunately, existing
benchmarks are inadequate in assessing spatial reasoning ability, especially
the \emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of
human spatial cognition. In this paper, we propose a unified benchmark,
\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that
categorizes tasks into four fundamental quadrants:
\textbf{I}ntrinsic-\textbf{S}tatic, Intrinsic-\textbf{D}ynamic,
\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,
to address the issue of data scarcity, we develop a scalable and automated
pipeline to generate diverse and verifiable spatial reasoning questions,
resulting in a new \textbf{Spatial-DISE} dataset that includes Spatial-DISE
Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA
pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals
that, current VLMs have a large and consistent gap to human competence,
especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a
robust framework, valuable dataset, and clear direction for future research
toward human-like spatial intelligence. Benchmark, dataset, and code will be
publicly released.

</details>


### [55] [Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation](https://arxiv.org/abs/2510.13418)
*Yifu Luo,Xinhao Hu,Keyu Fan,Haoyuan Sun,Zeyu Chen,Bo Xia,Tiantian Zhang,Yongzhe Chang,Xueqian Wang*

Main category: cs.CV

TL;DR: Mask-GRPO is the first RL method for masked generative models in text-to-image generation, redefining transition probability and formulating unmasking as multi-step decision making, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Most RL approaches for text-to-image generation focus on diffusion or autoregressive models, overlooking masked generative models which represent an important alternative paradigm.

Method: Proposes Mask-GRPO incorporating Group Relative Policy Optimization into masked generative models by redefining transition probability and formulating unmasking as multi-step decision making, with strategies like removing KL constraint, applying reduction strategy, and filtering low-quality samples.

Result: Substantial improvements on standard T2I benchmarks and preference alignment, outperforming existing state-of-the-art approaches when applied to base model Show-o.

Conclusion: Mask-GRPO successfully bridges the gap in RL approaches for masked generative models in text-to-image generation, demonstrating the effectiveness of this overlooked paradigm.

Abstract: Reinforcement learning (RL) has garnered increasing attention in
text-to-image (T2I) generation. However, most existing RL approaches are
tailored to either diffusion models or autoregressive models, overlooking an
important alternative: masked generative models. In this work, we propose
Mask-GRPO, the first method to incorporate Group Relative Policy Optimization
(GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine
the transition probability, which is different from current approaches, and
formulate the unmasking process as a multi-step decision-making problem. To
further enhance our method, we explore several useful strategies, including
removing the KL constraint, applying the reduction strategy, and filtering out
low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with
substantial improvements on standard T2I benchmarks and preference alignment,
outperforming existing state-of-the-art approaches. The code is available on
https://github.com/xingzhejun/Mask-GRPO

</details>


### [56] [Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter](https://arxiv.org/abs/2510.13419)
*Jianhui Zhang,Sheng Cheng,Qirui Sun,Jia Liu,Wang Luyang,Chaoyu Feng,Chen Fang,Lei Lei,Jue Wang,Shuaicheng Liu*

Main category: cs.CV

TL;DR: Patch-Adapter is a framework for high-resolution text-guided image inpainting that achieves 4K+ resolution while maintaining content consistency and prompt alignment through a two-stage adapter architecture.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited to lower resolutions and struggle with content consistency and prompt alignment, which become more challenging at higher resolutions and with complex textures.

Method: Uses a two-stage adapter architecture: (1) Dual Context Adapter for global structural consistency at reduced resolutions, and (2) Reference Patch Adapter with patch-level attention mechanism for full-resolution inpainting with local detail fidelity.

Result: Achieves 4K+ resolution inpainting, resolves artifacts common in large-scale inpainting, and outperforms existing methods on OpenImages and Photo-Concept-Bucket datasets in both perceptual quality and text-prompt adherence.

Conclusion: Patch-Adapter effectively addresses the scalability gap in high-resolution inpainting by decoupling global semantics from localized refinement, achieving state-of-the-art performance.

Abstract: In this work, we present Patch-Adapter, an effective framework for
high-resolution text-guided image inpainting. Unlike existing methods limited
to lower resolutions, our approach achieves 4K+ resolution while maintaining
precise content consistency and prompt alignment, two critical challenges in
image inpainting that intensify with increasing resolution and texture
complexity. Patch-Adapter leverages a two-stage adapter architecture to scale
the diffusion model's resolution from 1K to 4K+ without requiring structural
overhauls: (1) Dual Context Adapter learns coherence between masked and
unmasked regions at reduced resolutions to establish global structural
consistency; and (2) Reference Patch Adapter implements a patch-level attention
mechanism for full-resolution inpainting, preserving local detail fidelity
through adaptive feature fusion. This dual-stage architecture uniquely
addresses the scalability gap in high-resolution inpainting by decoupling
global semantics from localized refinement. Experiments demonstrate that
Patch-Adapter not only resolves artifacts common in large-scale inpainting but
also achieves state-of-the-art performance on the OpenImages and
Photo-Concept-Bucket datasets, outperforming existing methods in both
perceptual quality and text-prompt adherence.

</details>


### [57] [CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation](https://arxiv.org/abs/2510.13432)
*Yushan Han,Hui Zhang,Honglei Zhang,Chuntao Ding,Yuanzhouhan Cao,Yidong Li*

Main category: cs.CV

TL;DR: CoDS is a collaborative perception method that addresses feature discrepancies in heterogeneous autonomous driving scenarios through domain separation, achieving a balance between detection accuracy and inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative perception methods assume identical encoders for all agents, which doesn't hold in real-world heterogeneous scenarios. Current approaches are vulnerable to noise from domain gaps and use inefficient transformer-based modules.

Method: CoDS uses two feature alignment modules: Lightweight Spatial-Channel Resizer (LSCR) for spatial and channel alignment, and Distribution Alignment via Domain Separation (DADS) with encoder-specific and encoder-agnostic modules. It employs Domain Alignment Mutual Information (DAMI) loss for training and uses a fully convolutional architecture.

Result: Extensive experiments show CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and achieves a trade-off between detection accuracy and inference efficiency.

Conclusion: CoDS successfully addresses feature discrepancies in heterogeneous collaborative perception through domain separation techniques while maintaining high inference efficiency through its fully convolutional design.

Abstract: Collaborative perception has been proven to improve individual perception in
autonomous driving through multi-agent interaction. Nevertheless, most methods
often assume identical encoders for all agents, which does not hold true when
these models are deployed in real-world applications. To realize collaborative
perception in actual heterogeneous scenarios, existing methods usually align
neighbor features to those of the ego vehicle, which is vulnerable to noise
from domain gaps and thus fails to address feature discrepancies effectively.
Moreover, they adopt transformer-based modules for domain adaptation, which
causes the model inference inefficiency on mobile devices. To tackle these
issues, we propose CoDS, a Collaborative perception method that leverages
Domain Separation to address feature discrepancies in heterogeneous scenarios.
The CoDS employs two feature alignment modules, i.e., Lightweight
Spatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation
(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)
loss to ensure effective feature alignment. Specifically, the LSCR aligns the
neighbor feature across spatial and channel dimensions using a lightweight
convolutional layer. Subsequently, the DADS mitigates feature distribution
discrepancy with encoder-specific and encoder-agnostic domain separation
modules. The former removes domain-dependent information and the latter
captures task-related information. During training, the DAMI loss maximizes the
mutual information between aligned heterogeneous features to enhance the domain
separation process. The CoDS employs a fully convolutional architecture, which
ensures high inference efficiency. Extensive experiments demonstrate that the
CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and
achieves a trade-off between detection accuracy and inference efficiency.

</details>


### [58] [Beyond Pixels: A Differentiable Pipeline for Probing Neuronal Selectivity in 3D](https://arxiv.org/abs/2510.13433)
*Pavithra Elumalai,Mohammad Bashiri,Goirik Chakrabarty,Suhas Shrinivasan,Fabian H. Sinz*

Main category: cs.CV

TL;DR: A differentiable rendering pipeline that optimizes deformable meshes in 3D to probe neuronal selectivity to interpretable 3D scene properties like shape, pose, and lighting.


<details>
  <summary>Details</summary>
Motivation: Current approaches mainly operate on 2D pixels, making it difficult to isolate neuronal selectivity for physical scene properties. Visual perception relies on inference of 3D scene properties, so understanding how neurons enable robust perception requires characterizing their selectivity to physically interpretable factors.

Method: Introduced a differentiable rendering pipeline that optimizes deformable meshes directly in 3D. Parameterizes mesh deformations with radial basis functions and learns offsets and scales that maximize neuronal responses while enforcing geometric regularity.

Result: Applied to models of monkey area V4, the approach enables probing neuronal selectivity to interpretable 3D factors such as pose and lighting.

Conclusion: This approach bridges inverse graphics with systems neuroscience, offering a way to probe neural selectivity with physically grounded, 3D stimuli beyond conventional pixel-based methods.

Abstract: Visual perception relies on inference of 3D scene properties such as shape,
pose, and lighting. To understand how visual sensory neurons enable robust
perception, it is crucial to characterize their selectivity to such physically
interpretable factors. However, current approaches mainly operate on 2D pixels,
making it difficult to isolate selectivity for physical scene properties. To
address this limitation, we introduce a differentiable rendering pipeline that
optimizes deformable meshes to obtain MEIs directly in 3D. The method
parameterizes mesh deformations with radial basis functions and learns offsets
and scales that maximize neuronal responses while enforcing geometric
regularity. Applied to models of monkey area V4, our approach enables probing
neuronal selectivity to interpretable 3D factors such as pose and lighting.
This approach bridges inverse graphics with systems neuroscience, offering a
way to probe neural selectivity with physically grounded, 3D stimuli beyond
conventional pixel-based methods.

</details>


### [59] [Near-Infrared Hyperspectral Imaging Applications in Food Analysis -- Improving Algorithms and Methodologies](https://arxiv.org/abs/2510.13452)
*Ole-Christian Galbo Engstrøm*

Main category: cs.CV

TL;DR: NIR-HSI with CNN models outperforms PLS for food quality analysis when both chemical and visual information are relevant. CNN with spectral convolution layer enhances performance, but PLS remains best for mean chemical content analysis. CNN also produces better chemical maps than PLS.


<details>
  <summary>Details</summary>
Motivation: To investigate the application of near-infrared hyperspectral imaging for food quality analysis and compare CNN and PLS modeling approaches.

Method: Four studies using NIR-HSI, comparing CNN models (including 2D CNN with spectral convolution layer) with PLS regression for various food quality parameters.

Result: CNN with joint spatio-spectral analysis outperforms PLS when chemical and physical visual information are relevant. CNN with spectral convolution enhances predictive performance. PLS performs equally well for mean chemical content analysis. CNN produces better chemical maps than PLS.

Conclusion: CNN approaches are superior for complex spatio-spectral analysis, while PLS remains effective for mean chemical content analysis. Two Python packages were developed to facilitate fast PLS modeling and cross-validation.

Abstract: This thesis investigates the application of near-infrared hyperspectral
imaging (NIR-HSI) for food quality analysis. The investigation is conducted
through four studies operating with five research hypotheses. For several
analyses, the studies compare models based on convolutional neural networks
(CNNs) and partial least squares (PLS). Generally, joint spatio-spectral
analysis with CNNs outperforms spatial analysis with CNNs and spectral analysis
with PLS when modeling parameters where chemical and physical visual
information are relevant. When modeling chemical parameters with a
2-dimensional (2D) CNN, augmenting the CNN with an initial layer dedicated to
performing spectral convolution enhances its predictive performance by learning
a spectral preprocessing similar to that applied by domain experts. Still,
PLS-based spectral modeling performs equally well for analysis of the mean
content of chemical parameters in samples and is the recommended approach.
Modeling the spatial distribution of chemical parameters with NIR-HSI is
limited by the ability to obtain spatially resolved reference values.
Therefore, a study used bulk mean references for chemical map generation of fat
content in pork bellies. A PLS-based approach gave non-smooth chemical maps and
pixel-wise predictions outside the range of 0-100\%. Conversely, a 2D CNN
augmented with a spectral convolution layer mitigated all issues arising with
PLS. The final study attempted to model barley's germinative capacity by
analyzing NIR spectra, RGB images, and NIR-HSI images. However, the results
were inconclusive due to the dataset's low degree of germination. Additionally,
this thesis has led to the development of two open-sourced Python packages. The
first facilitates fast PLS-based modeling, while the second facilitates very
fast cross-validation of PLS and other classical machine learning models with a
new algorithm.

</details>


### [60] [VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator](https://arxiv.org/abs/2510.13454)
*Hyojun Go,Dominik Narnhofer,Goutam Bhat,Prune Truong,Federico Tombari,Konrad Schindler*

Main category: cs.CV

TL;DR: VIST3A is a framework that combines text-to-video generators with 3D reconstruction models through model stitching and alignment to create high-quality text-to-3D generation systems.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of modern latent text-to-video models and feedforward 3D reconstruction systems by combining them into a powerful text-to-3D generation framework.

Method: Uses model stitching to connect text-to-video generators with 3D decoders by identifying compatible layers, and applies direct reward finetuning to align the generator with the 3D decoder for consistent geometry.

Result: VIST3A significantly outperforms prior text-to-3D models that output Gaussian splats, and also enables high-quality text-to-pointmap generation when using appropriate 3D base models.

Conclusion: The proposed framework successfully combines text-to-video generation and 3D reconstruction capabilities, demonstrating improved performance over existing methods and flexibility across different model pairings.

Abstract: The rapid progress of large, pretrained models for both visual content
generation and 3D reconstruction opens up new possibilities for text-to-3D
generation. Intuitively, one could obtain a formidable 3D scene generator if
one were able to combine the power of a modern latent text-to-video model as
"generator" with the geometric abilities of a recent (feedforward) 3D
reconstruction system as "decoder". We introduce VIST3A, a general framework
that does just that, addressing two main challenges. First, the two components
must be joined in a way that preserves the rich knowledge encoded in their
weights. We revisit model stitching, i.e., we identify the layer in the 3D
decoder that best matches the latent representation produced by the
text-to-video generator and stitch the two parts together. That operation
requires only a small dataset and no labels. Second, the text-to-video
generator must be aligned with the stitched 3D decoder, to ensure that the
generated latents are decodable into consistent, perceptually convincing 3D
scene geometry. To that end, we adapt direct reward finetuning, a popular
technique for human preference alignment. We evaluate the proposed VIST3A
approach with different video generators and 3D reconstruction models. All
tested pairings markedly improve over prior text-to-3D models that output
Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also
enables high-quality text-to-pointmap generation.

</details>


### [61] [Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition](https://arxiv.org/abs/2510.13464)
*Emily Miller,Michael Milford,Muhammad Burhan Hafez,SD Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: The paper proposes three training-free uncertainty metrics for Visual Place Recognition that estimate prediction confidence by analyzing similarity score patterns, enabling robust place matching across varying environmental conditions without additional training or computational overhead.


<details>
  <summary>Details</summary>
Motivation: VPR systems face challenges with varying visual environments, lighting, seasons, and viewpoints. Failure-critical applications like SLAM loop closure require robust uncertainty estimation for place matching to ensure reliability.

Method: Three training-free uncertainty metrics: Similarity Distribution (SD) measures score separation between candidates; Ratio Spread (RS) evaluates competitive ambiguity among top matches; Statistical Uncertainty (SU) combines SD and RS as a unified metric that generalizes across datasets and methods without validation data.

Result: Comprehensive evaluation across 9 VPR methods and 6 datasets shows the metrics excel at discriminating correct/incorrect matches, outperform existing approaches, maintain negligible computational overhead, and improve precision-recall performance for real-time applications.

Conclusion: The proposed uncertainty metrics provide effective, training-free confidence estimation for VPR that generalizes across methods and datasets, making them deployable for real-time robotic applications with improved reliability across varied environmental conditions.

Abstract: Visual Place Recognition (VPR) enables robots and autonomous vehicles to
identify previously visited locations by matching current observations against
a database of known places. However, VPR systems face significant challenges
when deployed across varying visual environments, lighting conditions, seasonal
changes, and viewpoints changes. Failure-critical VPR applications, such as
loop closure detection in simultaneous localization and mapping (SLAM)
pipelines, require robust estimation of place matching uncertainty. We propose
three training-free uncertainty metrics that estimate prediction confidence by
analyzing inherent statistical patterns in similarity scores from any existing
VPR method. Similarity Distribution (SD) quantifies match distinctiveness by
measuring score separation between candidates; Ratio Spread (RS) evaluates
competitive ambiguity among top-scoring locations; and Statistical Uncertainty
(SU) is a combination of SD and RS that provides a unified metric that
generalizes across datasets and VPR methods without requiring validation data
to select the optimal metric. All three metrics operate without additional
model training, architectural modifications, or computationally expensive
geometric verification. Comprehensive evaluation across nine state-of-the-art
VPR methods and six benchmark datasets confirms that our metrics excel at
discriminating between correct and incorrect VPR matches, and consistently
outperform existing approaches while maintaining negligible computational
overhead, making it deployable for real-time robotic applications across varied
environmental conditions with improved precision-recall performance.

</details>


### [62] [ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition](https://arxiv.org/abs/2510.13493)
*Deeptimaan Banerjee,Prateek Gothwal,Ashis Kumer Biswas*

Main category: cs.CV

TL;DR: ExpressNet-MoE is a hybrid deep learning model combining CNNs and Mixture of Experts framework for facial emotion recognition, achieving state-of-the-art performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world facial emotion recognition faces challenges including variable head positions, occlusions, illumination shifts, and demographic diversity. Current models have limitations in engagement detection for applications like virtual learning and customer services.

Method: The model uses a hybrid approach with CNN-based feature extractors for multi-scale feature extraction (global and local facial features), a Mixture of Experts module for adaptive feature selection, and a residual network backbone for deep feature learning.

Result: Achieved accuracies of 74.77% on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on FER-2013, outperforming current state-of-the-art methods.

Conclusion: The model demonstrates strong adaptability and generalization across diverse datasets, showing potential for developing practical end-to-end emotion recognition systems in real-world applications.

Abstract: In many domains, including online education, healthcare, security, and
human-computer interaction, facial emotion recognition (FER) is essential.
Real-world FER is still difficult despite its significance because of some
factors such as variable head positions, occlusions, illumination shifts, and
demographic diversity. Engagement detection, which is essential for
applications like virtual learning and customer services, is frequently
challenging due to FER limitations by many current models. In this article, we
propose ExpressNet-MoE, a novel hybrid deep learning model that blends both
Convolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to
overcome the difficulties. Our model dynamically chooses the most pertinent
expert networks, thus it aids in the generalization and providing flexibility
to model across a wide variety of datasets. Our model improves on the accuracy
of emotion recognition by utilizing multi-scale feature extraction to collect
both global and local facial features. ExpressNet-MoE includes numerous
CNN-based feature extractors, a MoE module for adaptive feature selection, and
finally a residual network backbone for deep feature learning. To demonstrate
efficacy of our proposed model we evaluated on several datasets, and compared
with current state-of-the-art methods. Our model achieves accuracies of 74.77%
on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on
FER-2013. The results show how adaptive our model is and how it may be used to
develop end-to-end emotion recognition systems in practical settings.
Reproducible codes and results are made publicly accessible at
https://github.com/DeeptimaanB/ExpressNet-MoE.

</details>


### [63] [UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning](https://arxiv.org/abs/2510.13515)
*Tiancheng Gu,Kaicheng Yang,Kaichen Zhang,Xiang An,Ziyong Feng,Yueyi Zhang,Weidong Cai,Jiankang Deng,Lidong Bing*

Main category: cs.CV

TL;DR: UniME-V2 enhances multimodal embedding models by using MLLMs to generate semantic matching scores for hard negative mining and soft label training, achieving SOTA performance on MMEB benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal embedding models struggle with capturing subtle semantic differences, lack diversity in negative samples, and have limited discriminative ability for false and hard negatives.

Method: Constructs hard negative set via global retrieval, uses MLLM-as-a-Judge to generate semantic alignment scores, employs these scores for hard negative mining and as soft labels to align similarity matrices, and introduces UniME-V2-Reranker with joint pairwise and listwise optimization.

Result: Achieves state-of-the-art performance on MMEB benchmark and multiple retrieval tasks, demonstrating superior discriminative capacity across all evaluated tasks.

Conclusion: The proposed UniME-V2 framework effectively addresses limitations of existing methods by leveraging MLLMs for semantic understanding, enabling better hard negative mining and improved representation learning in multimodal embedding models.

Abstract: Universal multimodal embedding models are foundational to various tasks.
Existing approaches typically employ in-batch negative mining by measuring the
similarity of query-candidate pairs. However, these methods often struggle to
capture subtle semantic differences among candidates and lack diversity in
negative samples. Moreover, the embeddings exhibit limited discriminative
ability in distinguishing false and hard negatives. In this paper, we leverage
the advanced understanding capabilities of MLLMs to enhance representation
learning and present a novel Universal Multimodal Embedding (UniME-V2) model.
Our approach first constructs a potential hard negative set through global
retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes
MLLMs to assess the semantic alignment of query-candidate pairs and generate
soft semantic matching scores. These scores serve as a foundation for hard
negative mining, mitigating the impact of false negatives and enabling the
identification of diverse, high-quality hard negatives. Furthermore, the
semantic matching scores are used as soft labels to mitigate the rigid
one-to-one mapping constraint. By aligning the similarity matrix with the soft
semantic matching score matrix, the model learns semantic distinctions among
candidates, significantly enhancing its discriminative capacity. To further
improve performance, we propose UniME-V2-Reranker, a reranking model trained on
our mined hard negatives through a joint pairwise and listwise optimization
approach. We conduct comprehensive experiments on the MMEB benchmark and
multiple retrieval tasks, demonstrating that our method achieves
state-of-the-art performance on average across all tasks.

</details>


### [64] [High Semantic Features for the Continual Learning of Complex Emotions: a Lightweight Solution](https://arxiv.org/abs/2510.13534)
*Thibault Geoffroy,gauthier Gerspacher,Lionel Prevost*

Main category: cs.CV

TL;DR: The paper proposes using Action Units (facial muscle movements) as non-transient features for incremental learning of complex emotion recognition, achieving 0.75 accuracy on CFEE dataset with a lightweight model.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in incremental learning for emotion recognition, particularly when learning complex emotions after basic ones, by finding stable features that persist across tasks.

Method: Using Action Units (describing facial muscle movements) as non-transient, semantic features for incremental learning of complex emotions, comparing them with shallow and deep CNN features.

Result: Action Units outperform both shallow and deep CNN features, achieving 0.75 accuracy on CFEE dataset for incremental complex emotion learning, with favorable comparison to state-of-the-art methods.

Conclusion: Action Units are effective non-transient features that enable successful incremental learning of complex emotions while preventing catastrophic forgetting, resulting in a lightweight model with small memory footprint.

Abstract: Incremental learning is a complex process due to potential catastrophic
forgetting of old tasks when learning new ones. This is mainly due to transient
features that do not fit from task to task. In this paper, we focus on complex
emotion recognition. First, we learn basic emotions and then, incrementally,
like humans, complex emotions. We show that Action Units, describing facial
muscle movements, are non-transient, highly semantical features that outperform
those extracted by both shallow and deep convolutional neural networks. Thanks
to this ability, our approach achieves interesting results when learning
incrementally complex, compound emotions with an accuracy of 0.75 on the CFEE
dataset and can be favorably compared to state-of-the-art results. Moreover, it
results in a lightweight model with a small memory footprint.

</details>


### [65] [Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos](https://arxiv.org/abs/2510.13540)
*Maximilian Weiherer,Antonia von Riedheim,Vanessa Brébant,Bernhard Egger,Christoph Palm*

Main category: cs.CV

TL;DR: A neural parametric 3D breast shape model and reconstruction pipeline using monocular RGB video for accurate breast geometry recovery without specialized hardware.


<details>
  <summary>Details</summary>
Motivation: To provide a low-cost, accessible alternative to expensive commercial 3D breast scanning solutions that doesn't require specialized hardware or proprietary software.

Method: Uses Structure-from-motion pipeline with a parametric breast model that decomposes the implicit breast domain into multiple local neural SDF regions anchored at anatomical landmarks (liRBSM).

Result: Achieves high-quality 3D breast geometry reconstruction within less than 2mm error margin, outperforming the global iRBSM model with more detailed surfaces in under 6 minutes.

Conclusion: The proposed localized implicit breast shape model (liRBSM) enables fast, accurate, and accessible 3D breast reconstruction from monocular video, making it publicly available as open-source.

Abstract: We present a neural parametric 3D breast shape model and, based on this
model, introduce a low-cost and accessible 3D surface reconstruction pipeline
capable of recovering accurate breast geometry from a monocular RGB video. In
contrast to widely used, commercially available yet prohibitively expensive 3D
breast scanning solutions and existing low-cost alternatives, our method
requires neither specialized hardware nor proprietary software and can be used
with any device that is able to record RGB videos. The key building blocks of
our pipeline are a state-of-the-art, off-the-shelf Structure-from-motion
pipeline, paired with a parametric breast model for robust and metrically
correct surface reconstruction. Our model, similarly to the recently proposed
implicit Regensburg Breast Shape Model (iRBSM), leverages implicit neural
representations to model breast shapes. However, unlike the iRBSM, which
employs a single global neural signed distance function (SDF), our approach --
inspired by recent state-of-the-art face models -- decomposes the implicit
breast domain into multiple smaller regions, each represented by a local neural
SDF anchored at anatomical landmark positions. When incorporated into our
surface reconstruction pipeline, the proposed model, dubbed liRBSM (short for
localized iRBSM), significantly outperforms the iRBSM in terms of
reconstruction quality, yielding more detailed surface reconstruction than its
global counterpart. Overall, we find that the introduced pipeline is able to
recover high-quality 3D breast geometry within an error margin of less than 2
mm. Our method is fast (requires less than six minutes), fully transparent and
open-source, and -- together with the model -- publicly available at
https://rbsm.re-mic.de/local-implicit.

</details>


### [66] [Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU](https://arxiv.org/abs/2510.13546)
*Ruiqi Ye,Mikel Luján*

Main category: cs.CV

TL;DR: This paper compares GPU vs FPGA acceleration for feature detectors in Visual SLAM, finding GPUs better for traditional detectors (FAST, Harris) but FPGAs superior for learning-based detectors (SuperPoint) in performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Feature detection is time-consuming in SLAM systems deployed on power-constrained platforms like drones. With both GPUs and SoCs with integrated FPGAs widely available, there's a need to understand which hardware acceleration approach performs better for different types of feature detectors in V-SLAM pipelines.

Method: Comparative study of hardware-accelerated feature detectors (FAST, Harris, SuperPoint) implemented on both GPU (Nvidia Jetson Orin) and FPGA (AMD Versal) platforms within a Visual SLAM pipeline. Evaluated run-time performance, energy efficiency, and accuracy across different dataset sequences.

Result: For non-learning-based detectors (FAST, Harris), GPU implementations achieved better performance and energy efficiency than FPGA counterparts. For learning-based detector (SuperPoint), FPGA implementation achieved up to 3.1× performance and 1.4× energy efficiency improvements over GPU. FPGA-accelerated V-SLAM achieved comparable performance to GPU-accelerated V-SLAM, with better FPS in 2 out of 5 sequences. GPU-accelerated V-SLAM was generally more accurate.

Conclusion: Hardware acceleration choice depends on detector type: GPUs are better for traditional feature detectors while FPGAs excel for learning-based detectors. Hardware acceleration enables less frequent bundle adjustment without accuracy loss, improving overall V-SLAM pipeline performance.

Abstract: Feature detection is a common yet time-consuming module in Simultaneous
Localization and Mapping (SLAM) implementations, which are increasingly
deployed on power-constrained platforms, such as drones. Graphics Processing
Units (GPUs) have been a popular accelerator for computer vision in general,
and feature detection and SLAM in particular.
  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable
Gate Array (FPGA) are also widely available. This paper presents the first
study of hardware-accelerated feature detectors considering a Visual SLAM
(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated
FAST, Harris, and SuperPoint implementations against the FPGA-accelerated
counterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).
  The evaluation shows that when using a non-learning-based feature detector
such as FAST and Harris, their GPU implementations, and the GPU-accelerated
V-SLAM can achieve better run-time performance and energy efficiency than the
FAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.
However, when considering a learning-based detector such as SuperPoint, its
FPGA implementation can achieve better run-time performance and energy
efficiency (up to 3.1$\times$ and 1.4$\times$ improvements, respectively) than
the GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable
run-time performance compared to the GPU-accelerated V-SLAM, with better FPS in
2 out of 5 dataset sequences. When considering the accuracy, the results show
that the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated
V-SLAM in general. Last but not least, the use of hardware acceleration for
feature detection could further improve the performance of the V-SLAM pipeline
by having the global bundle adjustment module invoked less frequently without
sacrificing accuracy.

</details>


### [67] [Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents](https://arxiv.org/abs/2510.13557)
*David Freire-Obregón,José Salas-Cáceres,Javier Lorenzo-Navarro,Oliverio J. Santana,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

TL;DR: This paper introduces an agent-based benchmark to study how cultural composition and image blur affect facial expression recognition robustness, revealing asymmetric degradation patterns between cultural groups.


<details>
  <summary>Details</summary>
Motivation: FER systems need robustness across cultural variations and degraded visual conditions, but current evaluations assume homogeneous data and high-quality images.

Method: Agent-based streaming benchmark with CLIP features and lightweight residual adapters, testing monocultural and mixed populations on a 5x5 lattice with Gaussian blur.

Result: Asian (JAFFE) populations maintain better performance at low blur but drop sharply at intermediate stages, while Western (KDEF) populations degrade more uniformly. Mixed populations show intermediate patterns.

Conclusion: Cultural composition and interaction structure significantly influence FER robustness as perceptual conditions deteriorate, with balanced mixtures mitigating early degradation but imbalanced settings amplifying weaknesses.

Abstract: Facial expression recognition (FER) must remain robust under both cultural
variation and perceptually degraded visual conditions, yet most existing
evaluations assume homogeneous data and high-quality imagery. We introduce an
agent-based, streaming benchmark that reveals how cross-cultural composition
and progressive blurring interact to shape face recognition robustness. Each
agent operates in a frozen CLIP feature space with a lightweight residual
adapter trained online at sigma=0 and fixed during testing. Agents move and
interact on a 5x5 lattice, while the environment provides inputs with
sigma-scheduled Gaussian blur. We examine monocultural populations
(Western-only, Asian-only) and mixed environments with balanced (5/5) and
imbalanced (8/2, 2/8) compositions, as well as different spatial contact
structures. Results show clear asymmetric degradation curves between cultural
groups: JAFFE (Asian) populations maintain higher performance at low blur but
exhibit sharper drops at intermediate stages, whereas KDEF (Western)
populations degrade more uniformly. Mixed populations exhibit intermediate
patterns, with balanced mixtures mitigating early degradation, but imbalanced
settings amplify majority-group weaknesses under high blur. These findings
quantify how cultural composition and interaction structure influence the
robustness of FER as perceptual conditions deteriorate.

</details>


### [68] [XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation](https://arxiv.org/abs/2510.13565)
*Huawei Sun,Zixu Wang,Xiangyuan Peng,Julius Ott,Georg Stettinger,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: XD-RCDepth is a lightweight radar-camera fusion depth estimation architecture that reduces parameters by 29.7% while maintaining accuracy through explainability-aligned and depth-distribution knowledge distillation strategies.


<details>
  <summary>Details</summary>
Motivation: Depth estimation is crucial for autonomous driving, and radar-camera fusion provides robustness in adverse conditions by offering complementary geometric cues.

Method: Proposes XD-RCDepth with two knowledge-distillation strategies: explainability-aligned distillation that transfers teacher's saliency structure to student, and depth-distribution distillation that recasts depth regression as soft classification over discretized bins.

Result: Reduces parameters by 29.7% relative to state-of-the-art lightweight baseline while maintaining comparable accuracy, reduces MAE by 7.97% compared to direct training, and achieves competitive accuracy with real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.

Conclusion: The proposed lightweight architecture with knowledge distillation strategies effectively reduces model size while preserving performance, making it suitable for real-time autonomous driving applications.

Abstract: Depth estimation remains central to autonomous driving, and radar-camera
fusion offers robustness in adverse conditions by providing complementary
geometric cues. In this paper, we present XD-RCDepth, a lightweight
architecture that reduces the parameters by 29.7% relative to the
state-of-the-art lightweight baseline while maintaining comparable accuracy. To
preserve performance under compression and enhance interpretability, we
introduce two knowledge-distillation strategies: an explainability-aligned
distillation that transfers the teacher's saliency structure to the student,
and a depth-distribution distillation that recasts depth regression as soft
classification over discretized bins. Together, these components reduce the MAE
compared with direct training with 7.97% and deliver competitive accuracy with
real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.

</details>


### [69] [Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues](https://arxiv.org/abs/2510.13620)
*Chen Chen,Kangcheng Bin,Ting Hu,Jiahao Qi,Xingyue Liu,Tianpeng Liu,Zhen Liu,Yongxiang Liu,Ping Zhong*

Main category: cs.CV

TL;DR: The paper introduces ATR-UMOD, a high-diversity UAV dataset with RGB-IR image pairs covering various altitudes, angles, and time/weather conditions, and proposes PCDF, a prompt-guided dynamic fusion method for adaptive multimodal object detection.


<details>
  <summary>Details</summary>
Motivation: Existing UAV datasets struggle to capture real-world complexity for limited imaging conditions, necessitating a more comprehensive dataset and adaptive fusion methods for robust around-the-clock object detection.

Method: Proposed PCDF (prompt-guided condition-aware dynamic fusion) that encodes imaging conditions as text prompts and uses task-specific soft-gating to adaptively reassign multimodal contributions. Includes condition-decoupling module for practical use without annotations.

Result: Experiments on the ATR-UMOD dataset demonstrate the effectiveness of the proposed PCDF method for adaptive multimodal object detection.

Conclusion: The ATR-UMOD dataset and PCDF method provide a comprehensive solution for robust UAV-based object detection across diverse real-world conditions, with the condition-decoupling module ensuring practical applicability.

Abstract: Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and
infrared (IR) images facilitates robust around-the-clock detection, driven by
advancements in deep learning techniques and the availability of high-quality
dataset. However, the existing dataset struggles to fully capture real-world
complexity for limited imaging conditions. To this end, we introduce a
high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes
from 80m to 300m, angles from 0{\deg} to 75{\deg}, and all-day, all-year time
variations in rich weather and illumination conditions. Moreover, each RGB-IR
image pair is annotated with 6 condition attributes, offering valuable
high-level contextual information. To meet the challenge raised by such diverse
conditions, we propose a novel prompt-guided condition-aware dynamic fusion
(PCDF) to adaptively reassign multimodal contributions by leveraging annotated
condition cues. By encoding imaging conditions as text prompts, PCDF
effectively models the relationship between conditions and multimodal
contributions through a task-specific soft-gating transformation. A
prompt-guided condition-decoupling module further ensures the availability in
practice without condition annotations. Experiments on ATR-UMOD dataset reveal
the effectiveness of PCDF.

</details>


### [70] [AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset](https://arxiv.org/abs/2510.13630)
*Amjid Ali,Zulfiqar Ahmad Khan,Altaf Hussain,Muhammad Munsif,Adnan Hussain,Sung Wook Baik*

Main category: cs.CV

TL;DR: AVAR-Net is a lightweight audio-visual anomaly recognition framework that combines audio and visual features using early fusion and temporal modeling, achieving state-of-the-art performance on new VAAR dataset and XD-Violence benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly recognition methods rely solely on visual data, making them unreliable under challenging conditions like occlusion, low illumination, and adverse weather. The absence of large-scale synchronized audio-visual datasets has hindered multimodal progress.

Method: AVAR-Net uses Wav2Vec2 for audio feature extraction, MobileViT for visual feature extraction, early fusion mechanism to combine modalities, and Multi-Stage Temporal Convolutional Network (MTCN) to learn long-range temporal dependencies for spatiotemporal reasoning.

Result: Achieves 89.29% accuracy on VAAR dataset and 88.56% Average Precision on XD-Violence dataset, improving Average Precision by 2.8% over existing state-of-the-art methods.

Conclusion: The framework demonstrates effectiveness, efficiency, and generalization capability, while the new VAAR dataset serves as a valuable benchmark for advancing multimodal anomaly recognition research.

Abstract: Anomaly recognition plays a vital role in surveillance, transportation,
healthcare, and public safety. However, most existing approaches rely solely on
visual data, making them unreliable under challenging conditions such as
occlusion, low illumination, and adverse weather. Moreover, the absence of
large-scale synchronized audio-visual datasets has hindered progress in
multimodal anomaly recognition. To address these limitations, this study
presents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition
framework designed for real-world environments. AVAR-Net consists of four main
modules: an audio feature extractor, a video feature extractor, fusion
strategy, and a sequential pattern learning network that models cross-modal
relationships for anomaly recognition. Specifically, the Wav2Vec2 model
extracts robust temporal features from raw audio, while MobileViT captures both
local and global visual representations from video frames. An early fusion
mechanism combines these modalities, and a Multi-Stage Temporal Convolutional
Network (MTCN) model that learns long-range temporal dependencies within the
fused representation, enabling robust spatiotemporal reasoning. A novel
Visual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as
a medium-scale benchmark containing 3,000 real-world videos with synchronized
audio across ten diverse anomaly classes. Experimental evaluations demonstrate
that AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on
the XD-Violence dataset, improving Average Precision by 2.8% over existing
state-of-the-art methods. These results highlight the effectiveness,
efficiency, and generalization capability of the proposed framework, as well as
the utility of VAAR as a benchmark for advancing multimodal anomaly recognition
research.

</details>


### [71] [Challenges, Advances, and Evaluation Metrics in Medical Image Enhancement: A Systematic Literature Review](https://arxiv.org/abs/2510.13638)
*Chun Wai Chin,Haniza Yazid,Hoi Leong Lee*

Main category: cs.CV

TL;DR: A systematic review of 39 studies on medical image enhancement, analyzing challenges, methods, and evaluation metrics across different imaging modalities.


<details>
  <summary>Details</summary>
Motivation: Medical images often suffer from noise, artifacts, and low contrast that limit diagnostic potential, requiring robust enhancement methods to improve quality and interpretability.

Method: Systematic literature review following PRISMA approach, analyzing 39 peer-reviewed studies on medical image enhancement techniques and evaluation metrics.

Result: Low contrast and noise are most frequent challenges; MRI and multi-modal imaging receive most attention; 29 studies use conventional methods, 9 use deep learning, 1 hybrid; 65 IQA metrics identified, predominantly non-reference-based.

Conclusion: Identifies current limitations, research gaps, and future directions, noting specialized modalities remain underexplored and highlighting the need for advanced enhancement methods.

Abstract: Medical image enhancement is crucial for improving the quality and
interpretability of diagnostic images, ultimately supporting early detection,
accurate diagnosis, and effective treatment planning. Despite advancements in
imaging technologies such as X-ray, CT, MRI, and ultrasound, medical images
often suffer from challenges like noise, artifacts, and low contrast, which
limit their diagnostic potential. Addressing these challenges requires robust
preprocessing, denoising algorithms, and advanced enhancement methods, with
deep learning techniques playing an increasingly significant role. This
systematic literature review, following the PRISMA approach, investigates the
key challenges, recent advancements, and evaluation metrics in medical image
enhancement. By analyzing findings from 39 peer-reviewed studies, this review
provides insights into the effectiveness of various enhancement methods across
different imaging modalities and the importance of evaluation metrics in
assessing their impact. Key issues like low contrast and noise are identified
as the most frequent, with MRI and multi-modal imaging receiving the most
attention, while specialized modalities such as histopathology, endoscopy, and
bone scintigraphy remain underexplored. Out of the 39 studies, 29 utilize
conventional mathematical methods, 9 focus on deep learning techniques, and 1
explores a hybrid approach. In terms of image quality assessment, 18 studies
employ both reference-based and non-reference-based metrics, 9 rely solely on
reference-based metrics, and 12 use only non-reference-based metrics, with a
total of 65 IQA metrics introduced, predominantly non-reference-based. This
review highlights current limitations, research gaps, and potential future
directions for advancing medical image enhancement.

</details>


### [72] [Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.13643)
*Akib Mohammed Khan,Bartosz Krawczyk*

Main category: cs.CV

TL;DR: This paper examines adversarial vulnerability and uncertainty calibration in DINOv2-based few-shot anomaly detectors, finding they are susceptible to attacks and poorly calibrated, then proposes Platt scaling as a baseline solution.


<details>
  <summary>Details</summary>
Motivation: To investigate two unexamined questions about DINOv2-based anomaly detectors: their susceptibility to adversarial attacks and the calibration quality of their anomaly scores for reliable uncertainty estimation.

Method: Built on AnomalyDINO, a training-free deep nearest-neighbor detector using DINOv2 features. For attacks, attached a lightweight linear head to frozen features for gradient-based perturbations. For uncertainty, applied post-hoc Platt scaling to anomaly scores.

Result: Adversarial attacks (FGSM) consistently degraded performance metrics across datasets. Raw anomaly scores were poorly calibrated, showing confidence-correctness gaps. Platt scaling enabled attack detection via higher predictive entropy on perturbed inputs and reduced calibration error.

Conclusion: DINOv2-based anomaly detectors have concrete vulnerabilities requiring adversarial robustness and principled uncertainty quantification as essential capabilities for trustworthy real-world deployment.

Abstract: Foundation models such as DINOv2 have shown strong performance in few-shot
anomaly detection, yet two key questions remain unexamined: (i) how susceptible
are these detectors to adversarial perturbations; and (ii) how well do their
anomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a
training-free deep nearest-neighbor detector over DINOv2 features, we present
one of the first systematic studies of adversarial attacks and uncertainty
estimation in this setting. To enable white-box gradient attacks while
preserving test-time behavior, we attach a lightweight linear head to frozen
DINOv2 features only for crafting perturbations. Using this heuristic, we
evaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe
consistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible
perturbations can flip nearest-neighbor relations in feature space to induce
confident misclassification. Complementing robustness, we probe reliability and
find that raw anomaly scores are poorly calibrated, revealing a gap between
confidence and correctness that limits safety-critical use. As a simple, strong
baseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly
scores for uncertainty estimation. The resulting calibrated posteriors yield
significantly higher predictive entropy on adversarially perturbed inputs than
on clean ones, enabling a practical flagging mechanism for attack detection
while reducing calibration error (ECE). Our findings surface concrete
vulnerabilities in DINOv2-based few-shot anomaly detectors and establish an
evaluation protocol and baseline for robust, uncertainty-aware anomaly
detection. We argue that adversarial robustness and principled uncertainty
quantification are not optional add-ons but essential capabilities if anomaly
detection systems are to be trustworthy and ready for real-world deployment.

</details>


### [73] [Local-Global Context-Aware and Structure-Preserving Image Super-Resolution](https://arxiv.org/abs/2510.13649)
*Sanchar Palit,Subhasis Chaudhuri,Biplab Banerjee*

Main category: cs.CV

TL;DR: A contextually precise image super-resolution framework using Local-Global Context-Aware Attention and distribution-aligned conditioning to generate high-quality, structurally consistent images from degraded inputs.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based super-resolution methods struggle with diverse and highly degraded images, leading to noise amplification and incorrect content generation.

Method: Proposes Local-Global Context-Aware Attention to maintain pixel relationships, and distribution-perceptual-aligned conditioning in pixel space to enhance perceptual fidelity while preserving structural information.

Result: Extensive experiments on multiple benchmarks show the approach produces high-fidelity, perceptually accurate reconstructions with reduced artifacts.

Conclusion: The framework effectively addresses limitations of existing methods by maintaining structural consistency and generating realistic detail restoration in super-resolution tasks.

Abstract: Diffusion models have recently achieved significant success in various image
manipulation tasks, including image super-resolution and perceptual quality
enhancement. Pretrained text-to-image models, such as Stable Diffusion, have
exhibited strong capabilities in synthesizing realistic image content, which
makes them particularly attractive for addressing super-resolution tasks. While
some existing approaches leverage these models to achieve state-of-the-art
results, they often struggle when applied to diverse and highly degraded
images, leading to noise amplification or incorrect content generation. To
address these limitations, we propose a contextually precise image
super-resolution framework that effectively maintains both local and global
pixel relationships through Local-Global Context-Aware Attention, enabling the
generation of high-quality images. Furthermore, we propose a distribution- and
perceptual-aligned conditioning mechanism in the pixel space to enhance
perceptual fidelity. This mechanism captures fine-grained pixel-level
representations while progressively preserving and refining structural
information, transitioning from local content details to the global structural
composition. During inference, our method generates high-quality images that
are structurally consistent with the original content, mitigating artifacts and
ensuring realistic detail restoration. Extensive experiments on multiple
super-resolution benchmarks demonstrate the effectiveness of our approach in
producing high-fidelity, perceptually accurate reconstructions.

</details>


### [74] [EditCast3D: Single-Frame-Guided 3D Editing with Video Propagation and View Selection](https://arxiv.org/abs/2510.13652)
*Huaizhi Qu,Ruichen Zhang,Shuqing Luo,Luchao Qi,Zhihao Zhang,Xiaoming Liu,Roni Sengupta,Tianlong Chen*

Main category: cs.CV

TL;DR: EditCast3D is a pipeline that uses video generation foundation models to propagate 2D edits across entire 3D datasets before reconstruction, enabling efficient and scalable 3D editing while maintaining multi-view consistency.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models show remarkable progress in image editing but extending them to 3D editing is challenging due to heavy computational demands and impractical iterative editing strategies with closed-source APIs.

Method: Proposes EditCast3D pipeline that propagates edits from a single first frame across the entire dataset using video generation models, then uses view selection strategy to identify consistent views and performs feedforward reconstruction without costly refinement.

Result: EditCast3D demonstrates superior editing quality and high efficiency compared to state-of-the-art 3D editing baselines on commonly used datasets.

Conclusion: EditCast3D establishes a scalable and general paradigm for integrating foundation models into 3D editing pipelines, minimizing reliance on expensive image editing and mitigating prompt ambiguities.

Abstract: Recent advances in foundation models have driven remarkable progress in image
editing, yet their extension to 3D editing remains underexplored. A natural
approach is to replace the image editing modules in existing workflows with
foundation models. However, their heavy computational demands and the
restrictions and costs of closed-source APIs make plugging these models into
existing iterative editing strategies impractical. To address this limitation,
we propose EditCast3D, a pipeline that employs video generation foundation
models to propagate edits from a single first frame across the entire dataset
prior to reconstruction. While editing propagation enables dataset-level
editing via video models, its consistency remains suboptimal for 3D
reconstruction, where multi-view alignment is essential. To overcome this,
EditCast3D introduces a view selection strategy that explicitly identifies
consistent and reconstruction-friendly views and adopts feedforward
reconstruction without requiring costly refinement. In combination, the
pipeline both minimizes reliance on expensive image editing and mitigates
prompt ambiguities that arise when applying foundation models independently
across images. We evaluate EditCast3D on commonly used 3D editing datasets and
compare it against state-of-the-art 3D editing baselines, demonstrating
superior editing quality and high efficiency. These results establish
EditCast3D as a scalable and general paradigm for integrating foundation models
into 3D editing pipelines. The code is available at
https://github.com/UNITES-Lab/EditCast3D

</details>


### [75] [OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild](https://arxiv.org/abs/2510.13660)
*Hongyu Qu,Jianan Wei,Xiangbo Shu,Yazhou Yao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: OmniGaze is a semi-supervised framework for 3D gaze estimation that uses large-scale unlabeled data from diverse real-world environments to overcome domain bias and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Current 3D gaze estimation methods struggle with generalization across domains due to limited annotated datasets and insufficient diversity in labeled data.

Method: OmniGaze uses pseudo-labeling with a reward model that assesses pseudo label reliability using visual embeddings and semantic cues from a Multimodal Large Language Model, then selects high-quality pseudo labels for training.

Result: Achieves state-of-the-art performance on five datasets in both in-domain and cross-domain settings, and shows robust zero-shot generalization on four unseen datasets.

Conclusion: OmniGaze effectively leverages unlabeled data to improve gaze estimation generalization and can serve as a scalable data engine for the task.

Abstract: Current 3D gaze estimation methods struggle to generalize across diverse data
domains, primarily due to i) the scarcity of annotated datasets, and ii) the
insufficient diversity of labeled data. In this work, we present OmniGaze, a
semi-supervised framework for 3D gaze estimation, which utilizes large-scale
unlabeled data collected from diverse and unconstrained real-world environments
to mitigate domain bias and generalize gaze estimation in the wild. First, we
build a diverse collection of unlabeled facial images, varying in facial
appearances, background environments, illumination conditions, head poses, and
eye occlusions. In order to leverage unlabeled data spanning a broader
distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a
reward model to assess the reliability of pseudo labels. Beyond pseudo labels
as 3D direction vectors, the reward model also incorporates visual embeddings
extracted by an off-the-shelf visual encoder and semantic cues from gaze
perspective generated by prompting a Multimodal Large Language Model to compute
confidence scores. Then, these scores are utilized to select high-quality
pseudo labels and weight them for loss computation. Extensive experiments
demonstrate that OmniGaze achieves state-of-the-art performance on five
datasets under both in-domain and cross-domain settings. Furthermore, we also
evaluate the efficacy of OmniGaze as a scalable data engine for gaze
estimation, which exhibits robust zero-shot generalization on four unseen
datasets.

</details>


### [76] [CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas](https://arxiv.org/abs/2510.13669)
*Zian Li,Muhan Zhang*

Main category: cs.CV

TL;DR: CanvasMAR is a novel video masked autoregressive model that addresses slow-start and error accumulation issues through a canvas mechanism and compositional guidance, achieving high-quality video generation with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Video masked autoregressive models suffer from slow-start problems due to lack of global structure early in sampling and error accumulation across spatial and temporal dimensions during autoregression.

Method: Introduces a canvas mechanism (blurred global prediction of next frame) as starting point for masked generation, compositional classifier-free guidance for spatial and temporal conditioning, and noise-based canvas augmentation for robustness.

Result: Achieves high-quality video generation on BAIR and Kinetics-600 benchmarks with fewer autoregressive steps, remarkable performance among autoregressive models on Kinetics-600, and rivals diffusion-based methods.

Conclusion: CanvasMAR effectively mitigates key limitations of video MAR models through its canvas mechanism and compositional guidance, enabling faster and more coherent video synthesis while maintaining competitive performance.

Abstract: Masked autoregressive models (MAR) have recently emerged as a powerful
paradigm for image and video generation, combining the flexibility of masked
modeling with the potential of continuous tokenizer. However, video MAR models
suffer from two major limitations: the slow-start problem, caused by the lack
of a structured global prior at early sampling stages, and error accumulation
across the autoregression in both spatial and temporal dimensions. In this
work, we propose CanvasMAR, a novel video MAR model that mitigates these issues
by introducing a canvas mechanism--a blurred, global prediction of the next
frame, used as the starting point for masked generation. The canvas provides
global structure early in sampling, enabling faster and more coherent frame
synthesis. Furthermore, we introduce compositional classifier-free guidance
that jointly enlarges spatial (canvas) and temporal conditioning, and employ
noise-based canvas augmentation to enhance robustness. Experiments on the BAIR
and Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality
videos with fewer autoregressive steps. Our approach achieves remarkable
performance among autoregressive models on Kinetics-600 dataset and rivals
diffusion-based methods.

</details>


### [77] [NTIRE 2025 Challenge on Low Light Image Enhancement: Methods and Results](https://arxiv.org/abs/2510.13670)
*Xiaoning Liu,Zongwei Wu,Florin-Alexandru Vasluianu,Hailong Yan,Bin Ren,Yulun Zhang,Shuhang Gu,Le Zhang,Ce Zhu,Radu Timofte,Kangbiao Shi,Yixu Feng,Tao Hu,Yu Cao,Peng Wu,Yijin Liang,Yanning Zhang,Qingsen Yan,Han Zhou,Wei Dong,Yan Min,Mohab Kishawy,Jun Chen,Pengpeng Yu,Anjin Park,Seung-Soo Lee,Young-Joon Park,Zixiao Hu,Junyv Liu,Huilin Zhang,Jun Zhang,Fei Wan,Bingxin Xu,Hongzhe Liu,Cheng Xu,Weiguo Pan,Songyin Dai,Xunpeng Yi,Qinglong Yan,Yibing Zhang,Jiayi Ma,Changhui Hu,Kerui Hu,Donghang Jing,Tiesheng Chen,Zhi Jin,Hongjun Wu,Biao Huang,Haitao Ling,Jiahao Wu,Dandan Zhan,G Gyaneshwar Rao,Vijayalaxmi Ashok Aralikatti,Nikhil Akalwadi,Ramesh Ashok Tabib,Uma Mudenagudi,Ruirui Lin,Guoxi Huang,Nantheera Anantrasirichai,Qirui Yang,Alexandru Brateanu,Ciprian Orhei,Cosmin Ancuti,Daniel Feijoo,Juan C. Benito,Álvaro García,Marcos V. Conde,Yang Qin,Raul Balmez,Anas M. Ali,Bilel Benjdira,Wadii Boulila,Tianyi Mao,Huan Zheng,Yanyan Wei,Shengeng Tang,Dan Guo,Zhao Zhang,Sabari Nathan,K Uma,A Sasithradevi,B Sathya Bama,S. Mohamed Mansoor Roomi,Ao Li,Xiangtao Zhang,Zhe Liu,Yijie Tang,Jialong Tang,Zhicheng Fu,Gong Chen,Joe Nasti,John Nicholson,Zeyu Xiao,Zhuoyuan Li,Ashutosh Kulkarni,Prashant W. Patil,Santosh Kumar Vipparthi,Subrahmanyam Murala,Duan Liu,Weile Li,Hangyuan Lu,Rixian Liu,Tengfeng Wang,Jinxing Liang,Chenxin Yu*

Main category: cs.CV

TL;DR: Review of NTIRE 2025 Low-Light Image Enhancement Challenge with 762 participants and 28 valid submissions, evaluating state-of-the-art methods for brighter, clearer images.


<details>
  <summary>Details</summary>
Motivation: To identify effective networks for producing brighter, clearer, and visually compelling images under diverse challenging low-light conditions.

Method: Comprehensive review and evaluation of solutions submitted to the NTIRE 2025 LLIE Challenge, analyzing proposed networks and approaches.

Result: Significant progress in low-light image enhancement demonstrated through 28 valid submissions from 762 registered participants.

Conclusion: The challenge showcases substantial advancements in LLIE technology, with effective networks emerging for improved image enhancement under challenging low-light conditions.

Abstract: This paper presents a comprehensive review of the NTIRE 2025 Low-Light Image
Enhancement (LLIE) Challenge, highlighting the proposed solutions and final
outcomes. The objective of the challenge is to identify effective networks
capable of producing brighter, clearer, and visually compelling images under
diverse and challenging conditions. A remarkable total of 762 participants
registered for the competition, with 28 teams ultimately submitting valid
entries. This paper thoroughly evaluates the state-of-the-art advancements in
LLIE, showcasing the significant progress.

</details>


### [78] [Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning](https://arxiv.org/abs/2510.13675)
*Hongkuan Zhou,Lavdim Halilaj,Sebastian Monka,Stefan Schmid,Yuqicheng Zhu,Jingcheng Wu,Nadeem Nazer,Steffen Staab*

Main category: cs.CV

TL;DR: KnowCoL is a knowledge-guided contrastive learning framework for open-domain visual entity recognition that combines images and text descriptions in a shared semantic space using Wikidata's structured information to improve zero-shot recognition, especially for rare and unseen entities.


<details>
  <summary>Details</summary>
Motivation: Open-domain visual entity recognition operates under open-set conditions with most target entities unseen during training and exhibiting long-tail distributions, making it challenging due to limited supervision, visual ambiguity, and semantic disambiguation needs.

Method: Proposes Knowledge-guided Contrastive Learning (KnowCoL) that combines images and text descriptions into a shared semantic space grounded by structured information from Wikidata, leveraging entity descriptions, type hierarchies, and relational context.

Result: Evaluation on OVEN benchmark shows significant improvements: smallest model improves accuracy on unseen entities by 10.5% compared to state-of-the-art, despite being 35 times smaller. Using visual, textual, and structured knowledge greatly improves accuracy, especially for rare entities.

Conclusion: The proposed framework effectively addresses open-domain visual entity recognition challenges by integrating multiple knowledge sources, demonstrating strong performance improvements particularly for rare and unseen entities while maintaining model efficiency.

Abstract: Open-domain visual entity recognition aims to identify and link entities
depicted in images to a vast and evolving set of real-world concepts, such as
those found in Wikidata. Unlike conventional classification tasks with fixed
label sets, it operates under open-set conditions, where most target entities
are unseen during training and exhibit long-tail distributions. This makes the
task inherently challenging due to limited supervision, high visual ambiguity,
and the need for semantic disambiguation. In this work, we propose a
Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both
images and text descriptions into a shared semantic space grounded by
structured information from Wikidata. By abstracting visual and textual inputs
to a conceptual level, the model leverages entity descriptions, type
hierarchies, and relational context to support zero-shot entity recognition. We
evaluate our approach on the OVEN benchmark, a large-scale open-domain visual
recognition dataset with Wikidata IDs as the label space. Our experiments show
that using visual, textual, and structured knowledge greatly improves accuracy,
especially for rare and unseen entities. Our smallest model improves the
accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite
being 35 times smaller.

</details>


### [79] [FlashWorld: High-quality 3D Scene Generation within Seconds](https://arxiv.org/abs/2510.13678)
*Xinyang Li,Tengfei Wang,Zixiao Gu,Shengchuan Zhang,Chunchao Guo,Liujuan Cao*

Main category: cs.CV

TL;DR: FlashWorld is a fast 3D scene generation model that produces 3D Gaussian representations directly from single images or text prompts, achieving 10-100x speedup over previous methods while maintaining superior rendering quality.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional multi-view-oriented approaches that generate multi-view images first then reconstruct 3D scenes, which are slow and inefficient. The authors aim to develop a faster 3D generation method that directly produces 3D representations.

Method: Uses a dual-mode pre-training phase followed by cross-mode post-training distillation. First pre-trains a dual-mode multi-view diffusion model supporting both MV-oriented and 3D-oriented generation. Then uses cross-mode distillation to match distributions from 3D-oriented to high-quality MV-oriented mode, reducing denoising steps and enhancing quality while maintaining 3D consistency.

Result: Achieves 10-100x faster generation than previous works while possessing superior rendering quality. The method effectively integrates strengths of both paradigms and shows strong generalization to out-of-distribution inputs through extensive experiments.

Conclusion: FlashWorld successfully shifts from MV-oriented to 3D-oriented paradigm, demonstrating that direct 3D Gaussian representation generation can achieve both speed and quality improvements in 3D scene generation from single images or text prompts.

Abstract: We propose FlashWorld, a generative model that produces 3D scenes from a
single image or text prompt in seconds, 10~100$\times$ faster than previous
works while possessing superior rendering quality. Our approach shifts from the
conventional multi-view-oriented (MV-oriented) paradigm, which generates
multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach
where the model directly produces 3D Gaussian representations during multi-view
generation. While ensuring 3D consistency, 3D-oriented method typically suffers
poor visual quality. FlashWorld includes a dual-mode pre-training phase
followed by a cross-mode post-training phase, effectively integrating the
strengths of both paradigms. Specifically, leveraging the prior from a video
diffusion model, we first pre-train a dual-mode multi-view diffusion model,
which jointly supports MV-oriented and 3D-oriented generation modes. To bridge
the quality gap in 3D-oriented generation, we further propose a cross-mode
post-training distillation by matching distribution from consistent 3D-oriented
mode to high-quality MV-oriented mode. This not only enhances visual quality
while maintaining 3D consistency, but also reduces the required denoising steps
for inference. Also, we propose a strategy to leverage massive single-view
images and text prompts during this process to enhance the model's
generalization to out-of-distribution inputs. Extensive experiments demonstrate
the superiority and efficiency of our method.

</details>


### [80] [Generating healthy counterfactuals with denoising diffusion bridge models](https://arxiv.org/abs/2510.13684)
*Ana Lawry Aguila,Peirong Liu,Marina Crespo Aguirre,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: The paper proposes using denoising diffusion bridge models (DDBMs) to generate healthy counterfactuals from pathological medical images, outperforming previous methods in segmentation and anomaly detection tasks.


<details>
  <summary>Details</summary>
Motivation: Generating healthy counterfactuals from pathological images is important for medical applications like anomaly detection and analysis tools designed for healthy scans, but existing methods struggle to balance pathology removal with preservation of individual anatomical characteristics.

Method: The authors use denoising diffusion bridge models (DDBMs) that condition the diffusion process on both the initial healthy image and a corresponding synthetically generated pathological image, treating pathology as a structurally informative prior.

Result: The proposed DDBM approach outperforms previously proposed diffusion models and fully supervised approaches at segmentation and anomaly detection tasks.

Conclusion: DDBMs effectively generate counterfactuals that closely match patient anatomy while selectively removing pathology, demonstrating superior performance over existing methods.

Abstract: Generating healthy counterfactuals from pathological images holds significant
promise in medical imaging, e.g., in anomaly detection or for application of
analysis tools that are designed for healthy scans. These counterfactuals
should represent what a patient's scan would plausibly look like in the absence
of pathology, preserving individual anatomical characteristics while modifying
only the pathological regions. Denoising diffusion probabilistic models (DDPMs)
have become popular methods for generating healthy counterfactuals of pathology
data. Typically, this involves training on solely healthy data with the
assumption that a partial denoising process will be unable to model disease
regions and will instead reconstruct a closely matched healthy counterpart.
More recent methods have incorporated synthetic pathological images to better
guide the diffusion process. However, it remains challenging to guide the
generative process in a way that effectively balances the removal of anomalies
with the retention of subject-specific features. To solve this problem, we
propose a novel application of denoising diffusion bridge models (DDBMs) -
which, unlike DDPMs, condition the diffusion process not only on the initial
point (i.e., the healthy image), but also on the final point (i.e., a
corresponding synthetically generated pathological image). Treating the
pathological image as a structurally informative prior enables us to generate
counterfactuals that closely match the patient's anatomy while selectively
removing pathology. The results show that our DDBM outperforms previously
proposed diffusion models and fully supervised approaches at segmentation and
anomaly detection tasks.

</details>


### [81] [Risk-adaptive Activation Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2510.13698)
*Jonghyun Park,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: RAS (Risk-adaptive Activation Steering) is a method that reformulates queries to enhance cross-modal attention to safety-critical image regions for accurate risk assessment, then adaptively steers activations to generate safe responses without iterative output adjustments.


<details>
  <summary>Details</summary>
Motivation: Current AI models struggle with multimodal queries containing harmful intent in images. Training-based safety alignment is costly, while inference-time methods cause excessive refusals and slow inference due to iterative adjustments.

Method: Reformulate queries to strengthen cross-modal attention to safety-critical image regions for risk assessment, then use assessed risk to adaptively steer activations to generate safe responses without iterative output adjustments.

Result: Significantly reduces attack success rates, preserves general task performance, and improves inference speed over prior inference-time defenses across multiple benchmarks on multimodal safety and utility.

Conclusion: RAS effectively addresses multimodal safety challenges by enabling accurate risk assessment and adaptive activation steering, achieving better safety performance while maintaining utility and inference efficiency.

Abstract: One of the key challenges of modern AI models is ensuring that they provide
helpful responses to benign queries while refusing malicious ones. But often,
the models are vulnerable to multimodal queries with harmful intent embedded in
images. One approach for safety alignment is training with extensive safety
datasets at the significant costs in both dataset curation and training.
Inference-time alignment mitigates these costs, but introduces two drawbacks:
excessive refusals from misclassified benign queries and slower inference speed
due to iterative output adjustments. To overcome these limitations, we propose
to reformulate queries to strengthen cross-modal attention to safety-critical
image regions, enabling accurate risk assessment at the query level. Using the
assessed risk, it adaptively steers activations to generate responses that are
safe and helpful without overhead from iterative output adjustments. We call
this Risk-adaptive Activation Steering (RAS). Extensive experiments across
multiple benchmarks on multimodal safety and utility demonstrate that the RAS
significantly reduces attack success rates, preserves general task performance,
and improves inference speed over prior inference-time defenses.

</details>


### [82] [MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion](https://arxiv.org/abs/2510.13702)
*Minjung Shin,Hyunin Cho,Sooyeon Go,Jin-Hwa Kim,Youngjung Uh*

Main category: cs.CV

TL;DR: MVCustom is a diffusion-based framework that achieves both multi-view camera pose control and prompt-based customization with geometric consistency, addressing limitations in existing models.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view generation models lack customization with geometric consistency, while customization models lack explicit viewpoint control. This gap motivates the need for a unified approach to multi-view customization.

Method: MVCustom uses a feature-field representation to learn subject identity and geometry, enhanced with dense spatio-temporal attention for temporal coherence. It employs depth-aware feature rendering and consistent-aware latent completion for geometric consistency and perspective alignment.

Result: Extensive experiments show MVCustom is the only framework that simultaneously achieves faithful multi-view generation and customization, outperforming existing approaches.

Conclusion: MVCustom successfully addresses the multi-view customization task by combining camera pose control with prompt-based customization while maintaining geometric consistency.

Abstract: Multi-view generation with camera pose control and prompt-based customization
are both essential elements for achieving controllable generative models.
However, existing multi-view generation models do not support customization
with geometric consistency, whereas customization models lack explicit
viewpoint control, making them challenging to unify. Motivated by these gaps,
we introduce a novel task, multi-view customization, which aims to jointly
achieve multi-view camera pose control and customization. Due to the scarcity
of training data in customization, existing multi-view generation models, which
inherently rely on large-scale datasets, struggle to generalize to diverse
prompts. To address this, we propose MVCustom, a novel diffusion-based
framework explicitly designed to achieve both multi-view consistency and
customization fidelity. In the training stage, MVCustom learns the subject's
identity and geometry using a feature-field representation, incorporating the
text-to-video diffusion backbone enhanced with dense spatio-temporal attention,
which leverages temporal coherence for multi-view consistency. In the inference
stage, we introduce two novel techniques: depth-aware feature rendering
explicitly enforces geometric consistency, and consistent-aware latent
completion ensures accurate perspective alignment of the customized subject and
surrounding backgrounds. Extensive experiments demonstrate that MVCustom is the
only framework that simultaneously achieves faithful multi-view generation and
customization.

</details>


### [83] [Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm](https://arxiv.org/abs/2510.13720)
*Fabio Musio,Norman Juchler,Kaiyuan Yang,Suprosanna Shit,Chinmay Prabhakar,Bjoern Menze,Sven Hirsch*

Main category: cs.CV

TL;DR: This paper presents a baseline algorithm for extracting and curating centerline graphs from the Circle of Willis (CoW) using U-Net-based skeletonization with A* graph connection, achieving high anatomical accuracy and feature robustness.


<details>
  <summary>Details</summary>
Motivation: The Circle of Willis is crucial for cerebrovascular pathologies, but conventional skeletonization struggles with its complex geometry and there's scarcity of public centerline datasets, hindering automated quantitative analysis.

Method: Used thinning-based skeletonization on TopCoW dataset (200 stroke patients with MRA/CTA), developed baseline algorithm combining U-Net-based skeletonization with A* graph connection for centerline extraction and feature curation.

Result: Achieved high graph topology accuracy (F1=1), average Euclidean node distance <1 voxel, strong feature robustness (median relative errors <5%, Pearson correlations >0.95), and successfully predicted fetal PCA variants and detected modality differences.

Conclusion: Learning-based skeletonization with graph connection enables anatomically plausible centerline extraction, emphasizing importance of evaluating anatomical accuracy and feature robustness beyond voxel-based measures. Dataset and algorithm released for further research.

Abstract: The Circle of Willis (CoW) is a critical network of arteries in the brain,
often implicated in cerebrovascular pathologies. Voxel-level segmentation is an
important first step toward an automated CoW assessment, but a full
quantitative analysis requires centerline representations. However,
conventional skeletonization techniques often struggle to extract reliable
centerlines due to the CoW's complex geometry, and publicly available
centerline datasets remain scarce. To address these challenges, we used a
thinning-based skeletonization algorithm to extract and curate centerline
graphs and morphometric features from the TopCoW dataset, which includes 200
stroke patients, each imaged with MRA and CTA. The curated graphs were used to
develop a baseline algorithm for centerline and feature extraction, combining
U-Net-based skeletonization with A* graph connection. Performance was evaluated
on a held-out test set, focusing on anatomical accuracy and feature robustness.
Further, we used the extracted features to predict the frequency of fetal PCA
variants, confirm theoretical bifurcation optimality relations, and detect
subtle modality differences. The baseline algorithm consistently reconstructed
graph topology with high accuracy (F1 = 1), and the average Euclidean node
distance between reference and predicted graphs was below one voxel. Features
such as segment radius, length, and bifurcation ratios showed strong
robustness, with median relative errors below 5% and Pearson correlations above
0.95. Our results demonstrate the utility of learning-based skeletonization
combined with graph connection for anatomically plausible centerline
extraction. We emphasize the importance of going beyond simple voxel-based
measures by evaluating anatomical accuracy and feature robustness. The dataset
and baseline algorithm have been released to support further method development
and clinical research.

</details>


### [84] [LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration](https://arxiv.org/abs/2510.13729)
*Aymeric Fleith,Julian Zirbel,Daniel Cremers,Niclas Zeller*

Main category: cs.CV

TL;DR: LiFMCR is a novel dataset for multi-camera light field registration with synchronized images from two high-resolution plenoptic cameras and ground truth poses from a Vicon system.


<details>
  <summary>Details</summary>
Motivation: Existing light field datasets are limited to single-camera setups and lack external ground truth, making rigorous evaluation of multi-camera registration methods difficult.

Method: Provides two baseline registration approaches: RANSAC-based 3D transformation estimation using cross-view point clouds, and plenoptic PnP algorithm estimating 6-DoF poses from single light field images, both integrating the plenoptic camera model.

Result: Experiments show strong alignment with ground truth, supporting reliable multi-view light field processing.

Conclusion: LiFMCR enables accurate and scalable multi-camera registration for light field applications.

Abstract: We present LiFMCR, a novel dataset for the registration of multiple micro
lens array (MLA)-based light field cameras. While existing light field datasets
are limited to single-camera setups and typically lack external ground truth,
LiFMCR provides synchronized image sequences from two high-resolution Raytrix
R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)
poses recorded by a Vicon motion capture system. This unique combination
enables rigorous evaluation of multi-camera light field registration methods.
  As a baseline, we provide two complementary registration approaches: a robust
3D transformation estimation via a RANSAC-based method using cross-view point
clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from
single light field images. Both explicitly integrate the plenoptic camera
model, enabling accurate and scalable multi-camera registration. Experiments
show strong alignment with the ground truth, supporting reliable multi-view
light field processing.
  Project page: https://lifmcr.github.io/

</details>


### [85] [Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI Synthesis](https://arxiv.org/abs/2510.13735)
*Zhenxuan Zhang,Peiyuan Jing,Zi Wang,Ula Briski,Coraline Beitone,Yue Yang,Yinzhe Wu,Fanwen Wang,Liutao Yang,Jiahao Huang,Zhifan Gao,Zhaolin Chen,Kh Tohidul Islam,Guang Yang,Peter J. Lally*

Main category: cs.CV

TL;DR: CSS-Diff framework synthesizes high-field MRI from low-field MRI using cyclic self-supervised diffusion with anatomical preservation, achieving state-of-the-art performance in image quality and structural fidelity.


<details>
  <summary>Details</summary>
Motivation: Low-field MRI is cheaper and more accessible but suffers from poor resolution and noise. There's a need to bridge the clinical fidelity gap in high-field MRI synthesis while preserving anatomical accuracy and enhancing structural details.

Method: Proposes cyclic self-supervised diffusion (CSS-Diff) framework with cycle-consistent constraint, slice-wise gap perception network for inter-slice alignment via contrastive learning, and local structure correction network for enhanced feature restoration through self-reconstruction of masked patches.

Result: Achieved state-of-the-art performance: 31.80 ± 2.70 dB PSNR, 0.943 ± 0.102 SSIM, 0.0864 ± 0.0689 LPIPS. Significantly improved anatomical preservation - left cerebral white matter error dropped from 12.1% to 2.1%, cortex error from 4.2% to 3.7%.

Conclusion: CSS-Diff can synthesize high-field MRI images that are both quantitatively reliable and anatomically consistent, effectively bridging the domain gap while preserving fine-grained structural details.

Abstract: Synthesizing high-quality images from low-field MRI holds significant
potential. Low-field MRI is cheaper, more accessible, and safer, but suffers
from low resolution and poor signal-to-noise ratio. This synthesis process can
reduce reliance on costly acquisitions and expand data availability. However,
synthesizing high-field MRI still suffers from a clinical fidelity gap. There
is a need to preserve anatomical fidelity, enhance fine-grained structural
details, and bridge domain gaps in image contrast. To address these issues, we
propose a \emph{cyclic self-supervised diffusion (CSS-Diff)} framework for
high-field MRI synthesis from real low-field MRI data. Our core idea is to
reformulate diffusion-based synthesis under a cycle-consistent constraint. It
enforces anatomical preservation throughout the generative process rather than
just relying on paired pixel-level supervision. The CSS-Diff framework further
incorporates two novel processes. The slice-wise gap perception network aligns
inter-slice inconsistencies via contrastive learning. The local structure
correction network enhances local feature restoration through
self-reconstruction of masked and perturbed patches. Extensive experiments on
cross-field synthesis tasks demonstrate the effectiveness of our method,
achieving state-of-the-art performance (e.g., 31.80 $\pm$ 2.70 dB in PSNR,
0.943 $\pm$ 0.102 in SSIM, and 0.0864 $\pm$ 0.0689 in LPIPS). Beyond pixel-wise
fidelity, our method also preserves fine-grained anatomical structures compared
with the original low-field MRI (e.g., left cerebral white matter error drops
from 12.1$\%$ to 2.1$\%$, cortex from 4.2$\%$ to 3.7$\%$). To conclude, our
CSS-Diff can synthesize images that are both quantitatively reliable and
anatomically consistent.

</details>


### [86] [Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs](https://arxiv.org/abs/2510.13740)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: LogViG is a novel hybrid CNN-GNN model that uses Logarithmic Scalable Graph Construction (LSGC) to enhance vision graph neural networks by limiting long-range links and incorporating multi-scale high-resolution features, outperforming existing ViG, CNN, and ViT architectures.


<details>
  <summary>Details</summary>
Motivation: Existing graph construction methods like KNN are expensive on larger images, and methods like SVGA with fixed step scales can lead to over-squashing and miss multiple connections that could be gained from long-range links.

Method: Proposed Logarithmic Scalable Graph Construction (LSGC) to limit long-range links, and LogViG - a hybrid CNN-GNN model with high-resolution branch and multi-scale feature fusion between high and low-resolution branches.

Result: LogViG beats existing ViG, CNN, and ViT architectures in accuracy, GMACs, and parameters. Ti-LogViG achieves 79.9% top-1 accuracy on ImageNet-1K, 1.7% higher than Vision GNN with 24.3% parameter reduction and 35.3% GMACs reduction.

Conclusion: Leveraging long-range links in graph construction through LSGC can exceed current state-of-the-art ViG performance, demonstrating the effectiveness of the proposed approach.

Abstract: Vision graph neural networks (ViG) have demonstrated promise in vision tasks
as a competitive alternative to conventional convolutional neural nets (CNN)
and transformers (ViTs); however, common graph construction methods, such as
k-nearest neighbor (KNN), can be expensive on larger images. While methods such
as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step
scale can lead to over-squashing and missing multiple connections to gain the
same information that could be gained from a long-range link. Through this
observation, we propose a new graph construction method, Logarithmic Scalable
Graph Construction (LSGC) to enhance performance by limiting the number of
long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model
that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and
high-resolution architectures, we introduce and apply a high-resolution branch
and fuse features between our high-resolution and low-resolution branches for a
multi-scale high-resolution Vision GNN network. Extensive experiments show that
LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,
GMACs, and parameters on image classification and semantic segmentation tasks.
Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on
ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average
accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%
reduction in GMACs. Our work shows that leveraging long-range links in graph
construction for ViGs through our proposed LSGC can exceed the performance of
current state-of-the-art ViGs. Code is available at
https://github.com/mmunir127/LogViG-Official.

</details>


### [87] [UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy](https://arxiv.org/abs/2510.13745)
*Tianshuo Xu,Kai Wang,Zhifei Chen,Leyi Wu,Tianshui Wen,Fei Chao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: UniCalli is a unified diffusion framework that jointly trains Chinese calligraphy recognition and generation, achieving state-of-the-art results by leveraging mutual constraints between tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods either create high-quality isolated characters but ignore page-level aesthetics, or attempt page synthesis at the expense of calligraphic correctness.

Method: Uses asymmetric noising and rasterized box map for spatial priors, trained on synthetic, labeled, and unlabeled data. Jointly trains recognition and generation tasks for mutual constraints.

Result: Achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition. Successfully extends to other ancient scripts like Oracle bone inscriptions and Egyptian hieroglyphs.

Conclusion: The unified framework fosters concept-level abstractions that improve both tasks, especially in limited-data regimes, and demonstrates successful generalization to multiple ancient writing systems.

Abstract: Computational replication of Chinese calligraphy remains challenging.
Existing methods falter, either creating high-quality isolated characters while
ignoring page-level aesthetics like ligatures and spacing, or attempting page
synthesis at the expense of calligraphic correctness. We introduce
\textbf{UniCalli}, a unified diffusion framework for column-level recognition
and generation. Training both tasks jointly is deliberate: recognition
constrains the generator to preserve character structure, while generation
provides style and layout priors. This synergy fosters concept-level
abstractions that improve both tasks, especially in limited-data regimes. We
curated a dataset of over 8,000 digitized pieces, with ~4,000 densely
annotated. UniCalli employs asymmetric noising and a rasterized box map for
spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The
model achieves state-of-the-art generative quality with superior ligature
continuity and layout fidelity, alongside stronger recognition. The framework
successfully extends to other ancient scripts, including Oracle bone
inscriptions and Egyptian hieroglyphs. Code and data can be viewed in
\href{https://github.com/EnVision-Research/UniCalli}{this URL}.

</details>


### [88] [InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue](https://arxiv.org/abs/2510.13747)
*Wenwen Tong,Hewei Guo,Dongchuan Ran,Jiangnan Chen,Jiefan Lu,Kaibin Wang,Keqiang Li,Xiaoxu Zhu,Jiakui Li,Kehan Li,Xueheng Li,Lumin Li,Chenxu Guo,Jiasheng Zhou,Jiandong Chen,Xianye Wu,Jiahao Wang,Silei Wu,Lei Chen,Hanming Deng,Yuxuan Song,Dinghao Zhou,Guiping Zhong,Ken Zheng,Shiyin Kang,Lewei Lu*

Main category: cs.CV

TL;DR: InteractiveOmni is a unified open-source omni-modal LLM (4B-8B parameters) that integrates vision, audio, language, and speech capabilities for multi-turn audio-visual interactions, achieving SOTA performance with efficient model scaling.


<details>
  <summary>Details</summary>
Motivation: To create a lightweight yet comprehensive omni-modal model that can handle complex multi-turn audio-visual interactions with human-like conversational abilities, addressing the need for accessible foundation models for next-generation interactive systems.

Method: Integrates vision encoder, audio encoder, LLM, and speech decoder into unified architecture; uses multi-stage training including pre-training for omni-modal understanding and post-training with speech conversation and audio-visual interaction; curates multi-turn training dataset for long-term conversational ability.

Result: Significantly outperforms leading open-source models in multi-turn audio-visual experience; InteractiveOmni-4B comparable to Qwen2.5-Omni-7B on general benchmarks while using 50% model size; achieves SOTA results across image, audio, video understanding and speech generation tasks.

Conclusion: InteractiveOmni provides an accessible, open-source foundation for next-generation intelligent interactive systems with strong multi-modal understanding and speech generation capabilities, particularly excelling in long-term memory and multi-turn interactions.

Abstract: We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities. To
achieve this, we integrate the vision encoder, audio encoder, large language
model, and speech decoder into a unified model for understanding and generation
tasks. We design a multi-stage training strategy to ensure robust cross-modal
capabilities, including pre-training for omni-modal understanding, followed by
post-training with speech conversation and audio-visual interaction. To enable
human-like long-term conversational ability, we meticulously curate a
multi-turn training dataset that enhances the model's ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the multi-turn speech interaction benchmark. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
is comparable to the much larger model like Qwen2.5-Omni-7B on general
benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.

</details>


### [89] [RECODE: Reasoning Through Code Generation for Visual Question Answering](https://arxiv.org/abs/2510.13756)
*Junhong Shen,Mu Cai,Bo Hu,Ameet Talwalkar,David A Ross,Cordelia Schmid,Alireza Fathi*

Main category: cs.CV

TL;DR: RECODE is an agentic framework that uses derendering (reverse-engineering visuals into executable code) to enable verifiable visual reasoning in MLLMs, significantly outperforming existing methods on visual reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models struggle with precise reasoning for structured visuals like charts and diagrams because pixel-based perception lacks verification mechanisms.

Method: Proposes RECODE framework that generates multiple candidate programs to reproduce input images, uses a critic to select the most faithful reconstruction, and iteratively refines the code through derendering.

Result: Significantly outperforms methods that don't leverage code or only use code for auxiliary purposes on benchmarks like CharXiv, ChartQA, and Geometry3K.

Conclusion: Grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.

Abstract: Multimodal Large Language Models (MLLMs) struggle with precise reasoning for
structured visuals like charts and diagrams, as pixel-based perception lacks a
mechanism for verification. To address this, we propose to leverage derendering
-- the process of reverse-engineering visuals into executable code -- as a new
modality for verifiable visual reasoning. Specifically, we propose RECODE, an
agentic framework that first generates multiple candidate programs to reproduce
the input image. It then uses a critic to select the most faithful
reconstruction and iteratively refines the code. This process not only
transforms an ambiguous perceptual task into a verifiable, symbolic problem,
but also enables precise calculations and logical inferences later on. On
various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,
RECODE significantly outperforms methods that do not leverage code or only use
code for drawing auxiliary lines or cropping. Our work demonstrates that
grounding visual perception in executable code provides a new path toward more
accurate and verifiable multimodal reasoning.

</details>


### [90] [Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark](https://arxiv.org/abs/2510.13759)
*Kai Zou,Ziqi Huang,Yuhao Dong,Shulin Tian,Dian Zheng,Hongbo Liu,Jingwen He,Bin Liu,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: Uni-MMMU is a comprehensive benchmark that systematically evaluates the bidirectional synergy between visual understanding and generation across eight reasoning-centric domains, revealing performance gaps in current unified multimodal models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks rarely examine the true integration of visual understanding and generation in unified multimodal models, either treating them in isolation or overlooking tasks that inherently couple both abilities.

Method: Developed Uni-MMMU benchmark with bidirectionally coupled tasks across eight domains (science, coding, mathematics, puzzles, etc.) that require models to either use understanding to guide generation or use generation as cognitive scaffold for reasoning, incorporating verifiable intermediate steps and reproducible scoring.

Result: Extensive evaluation revealed substantial performance disparities and cross-modal dependencies among state-of-the-art unified, generation-only, and understanding-only models, showing when and how these abilities reinforce each other.

Conclusion: Uni-MMMU establishes a reliable foundation for advancing unified multimodal models by systematically examining the bidirectional synergy between visual understanding and generation.

Abstract: Unified multimodal models aim to jointly enable visual understanding and
generation, yet current benchmarks rarely examine their true integration.
Existing evaluations either treat the two abilities in isolation or overlook
tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.
Each task is bidirectionally coupled, demanding models to (i) leverage
conceptual understanding to guide precise visual synthesis, or (ii) utilize
generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
incorporates verifiable intermediate reasoning steps, unique ground truths, and
a reproducible scoring protocol for both textual and visual outputs. Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.

</details>


### [91] [Scaling Vision Transformers for Functional MRI with Flat Maps](https://arxiv.org/abs/2510.13768)
*Connor Lane,Daniel Z. Kaplan,Tanishq Mathew Abraham,Paul S. Scotti*

Main category: cs.CV

TL;DR: Transforms 4D fMRI data into 2D activity flat map videos and trains Vision Transformers using spatiotemporal masked autoencoder framework on large-scale fMRI data, achieving strong representation learning for brain state and trait decoding.


<details>
  <summary>Details</summary>
Motivation: To bridge the modality gap between fMRI and natural images for adapting deep learning architectures, and to build foundation models for fMRI data through open science.

Method: Transform 4D volumetric fMRI data into videos of 2D fMRI activity flat maps, then train Vision Transformers on 2.3K hours of fMRI data using spatiotemporal masked autoencoder (MAE) framework.

Result: Masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Model learns rich representations supporting fine-grained state decoding across subjects and subject-specific trait decoding across brain state changes.

Conclusion: Successfully developed foundation models for fMRI data that can decode both brain states and individual traits, with performance scaling predictably with dataset size. This represents an important step in fMRI deep learning adaptation.

Abstract: A key question for adapting modern deep learning architectures to functional
MRI (fMRI) is how to represent the data for model input. To bridge the modality
gap between fMRI and natural images, we transform the 4D volumetric fMRI data
into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K
hours of fMRI flat map videos from the Human Connectome Project using the
spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI
modeling performance improves with dataset size according to a strict power
scaling law. Downstream classification benchmarks show that our model learns
rich representations supporting both fine-grained state decoding across
subjects, as well as subject-specific trait decoding across changes in brain
state. This work is part of an ongoing open science project to build foundation
models for fMRI data. Our code and datasets are available at
https://github.com/MedARC-AI/fmri-fm.

</details>


### [92] [Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based Story Continuation](https://arxiv.org/abs/2510.13787)
*Seyed Mohammad Mousavi,Morteza Analoui*

Main category: cs.CV

TL;DR: AVC is a diffusion-based framework for story continuation that adaptively uses prior visual context through CLIP-based retrieval and controlled conditioning in early diffusion stages, with improved data quality from LLM re-captioning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively utilizing prior visual context while ensuring semantic alignment with current textual input in story continuation tasks.

Method: Uses CLIP model to retrieve semantically aligned images from previous frames, adaptively restricts visual influence to early diffusion stages when no relevant image is found, and improves data quality through LLM-based re-captioning.

Result: AVC achieves superior coherence, semantic consistency, and visual fidelity compared to strong baselines, especially in cases where prior visuals conflict with current input.

Conclusion: The adaptive visual conditioning framework effectively balances visual context utilization with semantic alignment, demonstrating improved performance in challenging story continuation scenarios.

Abstract: Story continuation focuses on generating the next image in a narrative
sequence so that it remains coherent with both the ongoing text description and
the previously observed images. A central challenge in this setting lies in
utilizing prior visual context effectively, while ensuring semantic alignment
with the current textual input. In this work, we introduce AVC (Adaptive Visual
Conditioning), a framework for diffusion-based story continuation. AVC employs
the CLIP model to retrieve the most semantically aligned image from previous
frames. Crucially, when no sufficiently relevant image is found, AVC adaptively
restricts the influence of prior visuals to only the early stages of the
diffusion process. This enables the model to exploit visual context when
beneficial, while avoiding the injection of misleading or irrelevant
information. Furthermore, we improve data quality by re-captioning a noisy
dataset using large language models, thereby strengthening textual supervision
and semantic alignment. Quantitative results and human evaluations demonstrate
that AVC achieves superior coherence, semantic consistency, and visual fidelity
compared to strong baselines, particularly in challenging cases where prior
visuals conflict with the current input.

</details>


### [93] [NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models](https://arxiv.org/abs/2510.13793)
*Nir Goren,Oren Katzir,Abhinav Nakarmi,Eyal Ronen,Mahmood Sharif,Or Patashnik*

Main category: cs.CV

TL;DR: NoisePrints is a lightweight watermarking scheme for diffusion models that uses the random seed as proof of authorship without modifying the generation process, enabling efficient verification without model weights.


<details>
  <summary>Details</summary>
Motivation: With the rapid adoption of diffusion models, proving authorship and protecting copyright have become critical, especially when model owners keep models private and third-party verification is needed.

Method: Utilizes the random seed used to initialize the diffusion process as watermark, incorporates hash function into noise sampling to prevent seed recovery, and uses cryptographic zero-knowledge proofs to prove ownership without revealing the seed.

Result: Validated on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output without requiring model weights.

Conclusion: NoisePrints provides a practical and scalable solution for diffusion model watermarking that is robust to manipulations and increases difficulty of watermark removal by keeping the seed secret.

Abstract: With the rapid adoption of diffusion models for visual content generation,
proving authorship and protecting copyright have become critical. This
challenge is particularly important when model owners keep their models private
and may be unwilling or unable to handle authorship issues, making third-party
verification essential. A natural solution is to embed watermarks for later
verification. However, existing methods require access to model weights and
rely on computationally heavy procedures, rendering them impractical and
non-scalable. To address these challenges, we propose , a lightweight
watermarking scheme that utilizes the random seed used to initialize the
diffusion process as a proof of authorship without modifying the generation
process. Our key observation is that the initial noise derived from a seed is
highly correlated with the generated visual content. By incorporating a hash
function into the noise sampling process, we further ensure that recovering a
valid seed from the content is infeasible. We also show that sampling an
alternative seed that passes verification is infeasible, and demonstrate the
robustness of our method under various manipulations. Finally, we show how to
use cryptographic zero-knowledge proofs to prove ownership without revealing
the seed. By keeping the seed secret, we increase the difficulty of watermark
removal. In our experiments, we validate NoisePrints on multiple
state-of-the-art diffusion models for images and videos, demonstrating
efficient verification using only the seed and output, without requiring access
to model weights.

</details>


### [94] [Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs](https://arxiv.org/abs/2510.13795)
*Yi Zhang,Bolin Ni,Xin-Sheng Chen,Heng-Rui Zhang,Yongming Rao,Houwen Peng,Qinglin Lu,Han Hu,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CV

TL;DR: The paper introduces Honey-Data-15M, a 15M QA pair dataset with enhanced data quality and Chain-of-Thought reasoning, along with HoneyPipe data curation pipeline, achieving SOTA performance with Bee-8B model that competes with semi-open models.


<details>
  <summary>Details</summary>
Motivation: Fully open MLLMs lag behind proprietary counterparts due to poor data quality in existing open-source datasets, which suffer from widespread noise and lack complex reasoning data like Chain-of-Thought.

Method: Created Honey-Data-15M dataset with 15M QA pairs using multiple cleaning techniques and dual-level CoT enrichment strategy, developed HoneyPipe data curation pipeline with DataStudio framework, and trained Bee-8B model on this dataset.

Result: Bee-8B establishes new SOTA for fully open MLLMs, achieving performance competitive with and sometimes surpassing recent semi-open models like InternVL3.5-8B.

Conclusion: A principled focus on data quality is key to developing fully open MLLMs that are highly competitive with semi-open counterparts, demonstrated through comprehensive resources including dataset, pipeline, training recipes, and model weights.

Abstract: Fully open multimodal large language models (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for supervised fine-tuning (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as Chain-of-Thought (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the Honey-Data-15M corpus; the full-stack suite
comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.

</details>


### [95] [Reasoning in Space via Grounding in the World](https://arxiv.org/abs/2510.13800)
*Yiming Chen,Zekun Qi,Wenyao Zhang,Xin Jin,Li Zhang,Peidong Liu*

Main category: cs.CV

TL;DR: GS-Reasoner is a 3D LLM that achieves autoregressive grounding without external modules by using a dual-path pooling mechanism to create unified 3D representations, bridging 3D visual grounding and spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing 3D LLMs lack unified representations that capture both semantic and geometric information, leading to poor grounding performance or excessive reliance on external modules, which hinders the integration of grounding and spatial reasoning.

Method: Proposes a dual-path pooling mechanism that aligns geometric features with semantic and positional cues to create unified image patch-based 3D representations without increasing input tokens. Also introduces Grounded Chain-of-Thought (GCoT) dataset with 3D bounding box annotations and reasoning paths.

Result: GS-Reasoner achieves autoregressive grounding without external modules and delivers performance comparable to state-of-the-art models. It shows impressive results on 3D visual grounding and significantly enhances spatial reasoning capabilities.

Conclusion: GS-Reasoner establishes a unified and self-contained framework for 3D spatial reasoning, demonstrating that effective 3D visual grounding serves as the cornerstone for spatial reasoning capabilities.

Abstract: In this paper, we claim that 3D visual grounding is the cornerstone of
spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to
explore the effective spatial representations that bridge the gap between them.
Existing 3D LLMs suffer from the absence of a unified 3D representation capable
of jointly capturing semantic and geometric information. This deficiency is
manifested either in poor performance on grounding or in an excessive reliance
on external modules, ultimately hindering the seamless integration of grounding
and spatial reasoning. To address this, we propose a simple yet effective
dual-path pooling mechanism that tightly aligns geometric features with both
semantic and positional cues, constructing a unified image patch-based 3D
representation that encapsulates all essential information without increasing
the number of input tokens. Leveraging this holistic representation,
GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely
without external modules while delivering performance comparable to
state-of-the-art models, establishing a unified and self-contained framework
for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we
introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is
meticulously curated to include both 3D bounding box annotations for objects
referenced in reasoning questions and step-by-step reasoning paths that
integrate grounding as a core component of the problem-solving process.
Extensive experiments demonstrate that GS-Reasoner achieves impressive results
on 3D visual grounding, which in turn significantly enhances its spatial
reasoning capabilities, leading to state-of-the-art performance.

</details>


### [96] [Trace Anything: Representing Any Video in 4D via Trajectory Fields](https://arxiv.org/abs/2510.13802)
*Xinhang Liu,Yuxi Xiao,Donny Y. Chen,Jiashi Feng,Yu-Wing Tai,Chi-Keung Tang,Bingyi Kang*

Main category: cs.CV

TL;DR: Trace Anything represents videos as trajectory fields where each pixel has a continuous 3D trajectory over time, and uses a neural network to predict these trajectories in one feed-forward pass using B-spline control points.


<details>
  <summary>Details</summary>
Motivation: To create effective spatio-temporal representations for video dynamics by modeling pixels as continuous 3D trajectories over time, which serves as the fundamental primitive element of video dynamics.

Method: Proposes representing videos as Trajectory Fields - dense mappings assigning continuous 3D trajectory functions to each pixel. Uses a neural network called Trace Anything that predicts trajectory fields in one feed-forward pass by outputting B-spline control points for each pixel.

Result: Achieves state-of-the-art performance on new trajectory field estimation benchmark and competitive results on established point-tracking benchmarks. Offers significant efficiency gains through one-pass prediction without iterative optimization. Exhibits emergent abilities including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.

Conclusion: The Trajectory Field representation and Trace Anything model provide an effective, efficient approach to video dynamics modeling that enables various downstream applications through its continuous trajectory prediction capabilities.

Abstract: Effective spatio-temporal representation is fundamental to modeling,
understanding, and predicting dynamics in videos. The atomic unit of a video,
the pixel, traces a continuous 3D trajectory over time, serving as the
primitive element of dynamics. Based on this principle, we propose representing
any video as a Trajectory Field: a dense mapping that assigns a continuous 3D
trajectory function of time to each pixel in every frame. With this
representation, we introduce Trace Anything, a neural network that predicts the
entire trajectory field in a single feed-forward pass. Specifically, for each
pixel in each frame, our model predicts a set of control points that
parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at
arbitrary query time instants. We trained the Trace Anything model on
large-scale 4D data, including data from our new platform, and our experiments
demonstrate that: (i) Trace Anything achieves state-of-the-art performance on
our new benchmark for trajectory field estimation and performs competitively on
established point-tracking benchmarks; (ii) it offers significant efficiency
gains thanks to its one-pass paradigm, without requiring iterative optimization
or auxiliary estimators; and (iii) it exhibits emergent abilities, including
goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.
Project page: https://trace-anything.github.io/.

</details>


### [97] [Generative Universal Verifier as Multimodal Meta-Reasoner](https://arxiv.org/abs/2510.13804)
*Xinchen Zhang,Xiaoying Zhang,Youbin Wu,Yanbin Cao,Renrui Zhang,Ruihang Chu,Ling Yang,Yujiu Yang*

Main category: cs.CV

TL;DR: The paper introduces Generative Universal Verifier, a plugin for multimodal models that enables reflection and refinement of visual outcomes during reasoning. It includes a benchmark (ViVerBench), an omni-capable verifier (OmniVerifier-7B), and a test-time scaling paradigm (OmniVerifier-TTS) that improves multimodal reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models consistently underperform in reliable visual verification tasks, showing a substantial gap from human-level capability. There is a need for systems that can provide reflection and refinement during multimodal reasoning processes.

Method: Built ViVerBench benchmark with 16 categories of visual verification tasks. Designed automated pipelines to construct large-scale visual verification data. Trained OmniVerifier-7B as the first omni-capable generative verifier. Proposed OmniVerifier-TTS, a sequential test-time scaling paradigm for iterative fine-grained optimization.

Result: OmniVerifier-7B achieved notable gains on ViVerBench (+8.3). OmniVerifier-TTS achieved improvements on T2I-ReasonBench (+3.7) and GenEval++ (+4.3), outperforming existing parallel test-time scaling methods like Best-of-N. Identified three atomic capabilities in visual verification that generalize and interact synergistically.

Conclusion: The Generative Universal Verifier advances reliable reflection during generation and scalable test-time refinement, marking progress toward more trustworthy and controllable next-generation reasoning systems in multimodal AI.

Abstract: We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.

</details>


### [98] [VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models](https://arxiv.org/abs/2510.13808)
*Dominick Reilly,Manish Kumar Govind,Le Xue,Srijan Das*

Main category: cs.CV

TL;DR: VisCoP introduces visual probes to adapt VLMs to novel domains with minimal parameter modification, outperforming existing methods across cross-view, cross-modal, and cross-task settings while preserving source knowledge.


<details>
  <summary>Details</summary>
Motivation: Large VLMs suffer performance degradation on novel domains with distribution shifts, while existing domain adaptation methods cause limited feature learning or catastrophic forgetting of prior capabilities.

Method: Augment VLM's vision encoder with compact learnable visual probes for efficient domain-specific adaptation with minimal modification to pretrained parameters.

Result: Outperforms existing adaptation strategies across cross-view (exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human understanding to robot control) settings.

Conclusion: VisCoP enables superior performance on target domains while effectively retaining source-domain knowledge through efficient domain adaptation.

Abstract: Large Vision-Language Models (VLMs) excel at general visual reasoning tasks
but exhibit sharp performance degradation when applied to novel domains with
substantial distribution shifts from pretraining data. Existing domain
adaptation approaches finetune different VLM components, but this often results
in limited domain-specific feature learning or catastrophic forgetting of prior
capabilities. To address these issues, we introduce Vision Contextualized
Probing (VisCoP), which augments the VLM's vision encoder with a compact set of
learnable visual probes. These probes enable efficient domain-specific
adaptation with minimal modification to pretrained parameters. We evaluate
VisCoP across three challenging domain adaptation settings-cross-view
(exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human
understanding to robot control). Experiments show that VisCoP consistently
outperforms existing adaptation strategies, achieving superior performance on
target domains while effectively retaining source-domain knowledge.

</details>


### [99] [PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.13809)
*Sihui Ji,Xi Chen,Xin Tao,Pengfei Wan,Hengshuang Zhao*

Main category: cs.CV

TL;DR: PhysMaster is a framework that enhances physics-awareness in video generation by learning physical representations from input images and using reinforcement learning with human feedback to optimize these representations, enabling more physically plausible video generation.


<details>
  <summary>Details</summary>
Motivation: Current video generation models produce visually realistic videos but often violate physical laws, limiting their ability to serve as accurate 'world models' and generate physically plausible content.

Method: PhysMaster uses PhysEncoder to extract physical information from input images as conditioning for video generation. It applies reinforcement learning with human feedback and Direct Preference Optimization (DPO) to optimize physical representations in an end-to-end manner.

Result: PhysMaster demonstrates improved physics-awareness in video generation, proving effective on simple proxy tasks and showing generalizability to various physical scenarios.

Conclusion: PhysMaster provides a generic, plug-in solution for physics-aware video generation that unifies solutions for various physical processes through representation learning in a reinforcement learning paradigm.

Abstract: Video generation models nowadays are capable of generating visually realistic
videos, but often fail to adhere to physical laws, limiting their ability to
generate physically plausible videos and serve as ''world models''. To address
this issue, we propose PhysMaster, which captures physical knowledge as a
representation for guiding video generation models to enhance their
physics-awareness. Specifically, PhysMaster is based on the image-to-video task
where the model is expected to predict physically plausible dynamics from the
input image. Since the input image provides physical priors like relative
positions and potential interactions of objects in the scenario, we devise
PhysEncoder to encode physical information from it as an extra condition to
inject physical knowledge into the video generation process. The lack of proper
supervision on the model's physical performance beyond mere appearance
motivates PhysEncoder to apply reinforcement learning with human feedback to
physical representation learning, which leverages feedback from generation
models to optimize physical representations with Direct Preference Optimization
(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for
improving physics-awareness of PhysEncoder and thus of video generation,
proving its ability on a simple proxy task and generalizability to wide-ranging
physical scenarios. This implies that our PhysMaster, which unifies solutions
for various physical processes via representation learning in the reinforcement
learning paradigm, can act as a generic and plug-in solution for physics-aware
video generation and broader applications.

</details>
