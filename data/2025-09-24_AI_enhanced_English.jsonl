{"id": "2509.18159", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18159", "abs": "https://arxiv.org/abs/2509.18159", "authors": ["Akwasi Asare", "Ulas Bagci"], "title": "PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset", "comment": null, "summary": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related\nmorbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as\ncritical precursors according to the World Health Organization (WHO). Early and\naccurate segmentation of polyps during colonoscopy is essential for reducing\nCRC progression, yet manual delineation is labor-intensive and prone to\nobserver variability. Deep learning methods have demonstrated strong potential\nfor automated polyp analysis, but their limited interpretability remains a\nbarrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an\nexplainable deep learning framework that integrates the U-Net architecture with\nGradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp\nsegmentation. The model was trained and evaluated on the Kvasir-SEG dataset of\n1000 annotated endoscopic images. Experimental results demonstrate robust\nsegmentation performance, achieving a mean Intersection over Union (IoU) of\n0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)\non training and validation sets. Grad-CAM visualizations further confirmed that\npredictions were guided by clinically relevant regions, enhancing transparency\nand trust in the model's decisions. By coupling high segmentation accuracy with\ninterpretability, PolypSeg-GradCAM represents a step toward reliable,\ntrustworthy AI-assisted colonoscopy and improved early colorectal cancer\nprevention.", "AI": {"tldr": "PolypSeg-GradCAM is an explainable deep learning framework that combines U-Net with Grad-CAM for transparent polyp segmentation in colonoscopy images, achieving high accuracy while providing interpretable visualizations.", "motivation": "Colorectal cancer is a major global health concern, with polyps as critical precursors. Manual polyp segmentation is labor-intensive and subjective, while existing deep learning methods lack interpretability needed for clinical adoption.", "method": "The framework integrates U-Net architecture with Gradient-weighted Class Activation Mapping (Grad-CAM) for explainable polyp segmentation. The model was trained and evaluated on the Kvasir-SEG dataset containing 1000 annotated endoscopic images.", "result": "Achieved robust segmentation performance with mean IoU of 0.9257 on test set and consistently high Dice coefficients (F-score > 0.96) on training/validation sets. Grad-CAM visualizations confirmed predictions were guided by clinically relevant regions.", "conclusion": "PolypSeg-GradCAM represents a significant step toward reliable, trustworthy AI-assisted colonoscopy by combining high segmentation accuracy with interpretability, potentially improving early colorectal cancer prevention."}}
{"id": "2509.18160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18160", "abs": "https://arxiv.org/abs/2509.18160", "authors": ["Akwasi Asare", "Isaac Baffour Senkyire", "Emmanuel Freeman", "Simon Hilary Ayinedenaba Aluze-Ele", "Kelvin Kwao"], "title": "PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis", "comment": null, "summary": "Diabetic retinopathy is a leading cause of vision loss among adults and a\nmajor global health challenge, particularly in underserved regions. This study\npresents PerceptronCARE, a deep learning-based teleophthalmology application\ndesigned for automated diabetic retinopathy detection using retinal images. The\nsystem was developed and evaluated using multiple convolutional neural\nnetworks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine\nthe optimal balance between accuracy and computational efficiency. The final\nmodel classifies disease severity with an accuracy of 85.4%, enabling real-time\nscreening in clinical and telemedicine settings. PerceptronCARE integrates\ncloud-based scalability, secure patient data management, and a multi-user\nframework, facilitating early diagnosis, improving doctor-patient interactions,\nand reducing healthcare costs. This study highlights the potential of AI-driven\ntelemedicine solutions in expanding access to diabetic retinopathy screening,\nparticularly in remote and resource-constrained environments.", "AI": {"tldr": "PerceptronCARE is a deep learning-based teleophthalmology application that uses retinal images for automated diabetic retinopathy detection with 85.4% accuracy, designed for real-time screening in clinical and telemedicine settings.", "motivation": "Diabetic retinopathy is a leading cause of vision loss globally, especially in underserved regions, creating a need for accessible and efficient screening solutions.", "method": "Developed using multiple convolutional neural networks (ResNet-18, EfficientNet-B0, SqueezeNet) to optimize accuracy and computational efficiency, with cloud-based scalability and secure patient data management.", "result": "The final model achieves 85.4% accuracy in disease severity classification, enabling real-time screening capabilities.", "conclusion": "AI-driven telemedicine solutions like PerceptronCARE can significantly expand access to diabetic retinopathy screening, particularly in remote and resource-constrained environments, while improving healthcare efficiency and reducing costs."}}
{"id": "2509.18165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18165", "abs": "https://arxiv.org/abs/2509.18165", "authors": ["Xiuding Cai", "Yaoyao Zhu", "Linjie Fu", "Dong Miao", "Yu Yao"], "title": "Self Identity Mapping", "comment": "Early accepted by Neural Networks 2025", "summary": "Regularization is essential in deep learning to enhance generalization and\nmitigate overfitting. However, conventional techniques often rely on\nheuristics, making them less reliable or effective across diverse settings. We\npropose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic\nregularization framework that leverages an inverse mapping mechanism to enhance\nrepresentation learning. By reconstructing the input from its transformed\noutput, SIM reduces information loss during forward propagation and facilitates\nsmoother gradient flow. To address computational inefficiencies, We instantiate\nSIM as $ \\rho\\text{SIM} $ by incorporating patch-level feature sampling and\nprojection-based method to reconstruct latent features, effectively lowering\ncomplexity. As a model-agnostic, task-agnostic regularizer, SIM can be\nseamlessly integrated as a plug-and-play module, making it applicable to\ndifferent network architectures and tasks.\n  We extensively evaluate $\\rho\\text{SIM}$ across three tasks: image\nclassification, few-shot prompt learning, and domain generalization.\nExperimental results show consistent improvements over baseline methods,\nhighlighting $\\rho\\text{SIM}$'s ability to enhance representation learning\nacross various tasks. We also demonstrate that $\\rho\\text{SIM}$ is orthogonal\nto existing regularization methods, boosting their effectiveness. Moreover, our\nresults confirm that $\\rho\\text{SIM}$ effectively preserves semantic\ninformation and enhances performance in dense-to-dense tasks, such as semantic\nsegmentation and image translation, as well as in non-visual domains including\naudio classification and time series anomaly detection. The code is publicly\navailable at https://github.com/XiudingCai/SIM-pytorch.", "AI": {"tldr": "SIM is a data-intrinsic regularization framework that uses inverse mapping to reconstruct inputs from transformed outputs, reducing information loss and improving gradient flow. The efficient variant \u03c1SIM uses patch-level sampling and projection to lower complexity.", "motivation": "Conventional regularization techniques are often heuristic-based and unreliable across diverse settings. There's a need for more effective, model-agnostic regularization methods that can enhance representation learning.", "method": "Proposes Self Identity Mapping (SIM) - an inverse mapping mechanism that reconstructs input from output to preserve information. \u03c1SIM implements this efficiently using patch-level feature sampling and projection-based reconstruction of latent features.", "result": "Consistent improvements across image classification, few-shot prompt learning, and domain generalization tasks. \u03c1SIM is orthogonal to existing methods, boosting their effectiveness. Works well in dense-to-dense tasks and non-visual domains like audio classification and time series anomaly detection.", "conclusion": "SIM/\u03c1SIM is an effective, plug-and-play regularization framework that enhances representation learning across diverse tasks and architectures while being computationally efficient."}}
{"id": "2509.18170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18170", "abs": "https://arxiv.org/abs/2509.18170", "authors": ["Zhanting Zhou", "Jinbo Wang", "Zeqin Wu", "Fengli Zhang"], "title": "MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion", "comment": null, "summary": "We study gradient inversion in the challenging single round averaged gradient\nSAG regime where per sample cues are entangled within a single batch mean\ngradient. We introduce MAGIA a momentum based adaptive correction on gradient\ninversion attack a novel label inference free framework that senses latent per\nimage signals by probing random data subsets. MAGIA objective integrates two\ncore innovations 1 a closed form combinatorial rescaling that creates a\nprovably tighter optimization bound and 2 a momentum based mixing of whole\nbatch and subset losses to ensure reconstruction robustness. Extensive\nexperiments demonstrate that MAGIA significantly outperforms advanced methods\nachieving high fidelity multi image reconstruction in large batch scenarios\nwhere prior works fail. This is all accomplished with a computational footprint\ncomparable to standard solvers and without requiring any auxiliary information.", "AI": {"tldr": "MAGIA is a momentum-based adaptive correction framework for gradient inversion attacks that enables high-fidelity multi-image reconstruction in single-round averaged gradient scenarios without requiring label inference or auxiliary information.", "motivation": "To address the challenge of gradient inversion in the single round averaged gradient (SAG) regime where per-sample cues are entangled within batch mean gradients, overcoming limitations of prior methods that fail in large batch scenarios.", "method": "MAGIA uses momentum-based adaptive correction with two core innovations: 1) closed-form combinatorial rescaling for tighter optimization bounds, and 2) momentum-based mixing of whole-batch and subset losses for reconstruction robustness. It probes random data subsets to sense latent per-image signals without requiring label inference.", "result": "Extensive experiments show MAGIA significantly outperforms advanced methods, achieving high-fidelity multi-image reconstruction in large batch scenarios where prior works fail, with computational footprint comparable to standard solvers.", "conclusion": "MAGIA provides an effective framework for gradient inversion attacks in challenging SAG regimes, demonstrating robust performance without requiring auxiliary information while maintaining computational efficiency."}}
{"id": "2509.18174", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18174", "abs": "https://arxiv.org/abs/2509.18174", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motasim Hamed", "Ahmad Bastati", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR", "comment": null, "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.", "AI": {"tldr": "Baseer is a vision-language model fine-tuned specifically for Arabic document OCR, achieving state-of-the-art performance with WER of 0.25 by leveraging domain-specific adaptation of general-purpose MLLMs.", "motivation": "Arabic document OCR remains challenging due to cursive script, diverse fonts, diacritics, and right-to-left orientation. Existing MLLMs perform poorly on Arabic despite advances in high-resource languages.", "method": "Fine-tuned a pre-trained MLLM using decoder-only strategy with large-scale dataset combining synthetic and real-world Arabic documents. Also created Misraj-DocOCR benchmark for evaluation.", "result": "Baseer significantly outperforms existing open-source and commercial solutions, achieving WER of 0.25 and establishing new state-of-the-art in Arabic document OCR.", "conclusion": "Domain-specific adaptation of general-purpose MLLMs is highly beneficial for high-accuracy OCR on morphologically rich languages like Arabic, establishing a strong baseline for future work."}}
{"id": "2509.18176", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18176", "abs": "https://arxiv.org/abs/2509.18176", "authors": ["Wendong Yao", "Saeed Azadnejad", "Binhua Huang", "Shane Donohue", "Soumyabrata Dev"], "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland", "comment": "This paper is submitted to IEEE Transactions on Geoscience and Remote\n  Sensing", "summary": "Monitoring ground displacement is crucial for urban infrastructure stability\nand mitigating geological hazards. However, forecasting future deformation from\nsparse Interferometric Synthetic Aperture Radar (InSAR) time-series data\nremains a significant challenge. This paper introduces a novel deep learning\nframework that transforms these sparse point measurements into a dense\nspatio-temporal tensor. This methodological shift allows, for the first time,\nthe direct application of advanced computer vision architectures to this\nforecasting problem. We design and implement a hybrid Convolutional Neural\nNetwork and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to\nsimultaneously learn spatial patterns and temporal dependencies from the\ngenerated data tensor. The model's performance is benchmarked against powerful\nmachine learning baselines, Light Gradient Boosting Machine and LASSO\nregression, using Sentinel-1 data from eastern Ireland. Results demonstrate\nthat the proposed architecture provides significantly more accurate and\nspatially coherent forecasts, establishing a new performance benchmark for this\ntask. Furthermore, an interpretability analysis reveals that baseline models\noften default to simplistic persistence patterns, highlighting the necessity of\nour integrated spatio-temporal approach to capture the complex dynamics of\nground deformation. Our findings confirm the efficacy and potential of\nspatio-temporal deep learning for high-resolution deformation forecasting.", "AI": {"tldr": "A novel deep learning framework transforms sparse InSAR time-series data into dense spatio-temporal tensors, enabling accurate ground deformation forecasting using a hybrid CNN-LSTM model that outperforms traditional machine learning methods.", "motivation": "Forecasting future ground deformation from sparse InSAR time-series data is challenging but crucial for urban infrastructure stability and geological hazard mitigation.", "method": "Hybrid CNN-LSTM model designed to simultaneously learn spatial patterns and temporal dependencies from generated dense spatio-temporal tensors, benchmarked against LightGBM and LASSO regression using Sentinel-1 data.", "result": "The proposed architecture provides significantly more accurate and spatially coherent forecasts than baseline models, establishing a new performance benchmark for deformation forecasting.", "conclusion": "Spatio-temporal deep learning is effective for high-resolution deformation forecasting, with interpretability analysis showing baseline models default to simplistic patterns while the integrated approach captures complex dynamics."}}
{"id": "2509.18177", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18177", "abs": "https://arxiv.org/abs/2509.18177", "authors": ["George Corr\u00eaa de Ara\u00fajo", "Helena de Almeida Maia", "Helio Pedrini"], "title": "A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts", "comment": "WIP", "summary": "In this paper, we present the Scrapbook framework, a novel methodology\ndesigned to generate extensive datasets for probing the learned concepts of\nartificial intelligence (AI) models. The framework focuses on fundamental\nconcepts such as object recognition, absolute and relative positions, and\nattribute identification. By generating datasets with a large number of\nquestions about individual concepts and a wide linguistic variation, the\nScrapbook framework aims to validate the model's understanding of these basic\nelements before tackling more complex tasks. Our experimental findings reveal\nthat, while contemporary models demonstrate proficiency in recognizing and\nenumerating objects, they encounter challenges in comprehending positional\ninformation and addressing inquiries with additional constraints. Specifically,\nthe MobileVLM-V2 model showed significant answer disagreements and plausible\nwrong answers, while other models exhibited a bias toward affirmative answers\nand struggled with questions involving geometric shapes and positional\ninformation, indicating areas for improvement in understanding and consistency.\nThe proposed framework offers a valuable instrument for generating diverse and\ncomprehensive datasets, which can be utilized to systematically assess and\nenhance the performance of AI models.", "AI": {"tldr": "The Scrapbook framework generates extensive datasets to test AI models' understanding of basic concepts like object recognition, positions, and attributes through diverse linguistic questions.", "motivation": "To validate AI models' understanding of fundamental concepts before tackling complex tasks, as current models show proficiency in object recognition but struggle with positional information and constrained questions.", "method": "The Scrapbook framework generates datasets with large numbers of questions about individual concepts and wide linguistic variation to systematically probe AI models' learned concepts.", "result": "Contemporary models demonstrate proficiency in object recognition but struggle with positional information and constrained questions. MobileVLM-V2 showed significant answer disagreements, while other models exhibited bias toward affirmative answers and difficulties with geometric shapes and positional information.", "conclusion": "The Scrapbook framework provides a valuable tool for generating diverse datasets to systematically assess and enhance AI model performance, revealing specific areas needing improvement in understanding and consistency."}}
{"id": "2509.18179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18179", "abs": "https://arxiv.org/abs/2509.18179", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes", "comment": "13 pages, 7 Figures", "summary": "With the increasing integration of multimodal AI systems in creative\nworkflows, understanding information loss in vision-language-vision pipelines\nhas become important for evaluating system limitations. However, the\ndegradation that occurs when visual content passes through textual\nintermediation remains poorly quantified. In this work, we provide empirical\nanalysis of the describe-then-generate bottleneck, where natural language\nserves as an intermediate representation for visual information. We generated\n150 image pairs through the describe-then-generate pipeline and applied\nexisting metrics (LPIPS, SSIM, and color distance) to measure information\npreservation across perceptual, structural, and chromatic dimensions. Our\nevaluation reveals that 99.3% of samples exhibit substantial perceptual\ndegradation and 91.5% demonstrate significant structural information loss,\nproviding empirical evidence that the describe-then-generate bottleneck\nrepresents a measurable and consistent limitation in contemporary multimodal\nsystems.", "AI": {"tldr": "Empirical analysis shows significant information loss in vision-language-vision pipelines, with 99.3% perceptual degradation and 91.5% structural loss when using natural language as intermediate representation.", "motivation": "Understanding information loss in multimodal AI systems is crucial as they become more integrated in creative workflows, but degradation through textual intermediation remains poorly quantified.", "method": "Generated 150 image pairs through describe-then-generate pipeline and measured information preservation using LPIPS, SSIM, and color distance metrics across perceptual, structural, and chromatic dimensions.", "result": "99.3% of samples showed substantial perceptual degradation and 91.5% demonstrated significant structural information loss, indicating consistent limitations in current multimodal systems.", "conclusion": "The describe-then-generate bottleneck represents a measurable and consistent limitation in contemporary multimodal systems, providing empirical evidence of substantial information loss."}}
{"id": "2509.18182", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18182", "abs": "https://arxiv.org/abs/2509.18182", "authors": ["Isabelle Tingzon", "Yoji Toriumi", "Caroline Gevaert"], "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines", "comment": "Accepted at the 2nd Workshop on Computer Vision for Developing\n  Countries (CV4DC) at ICCV 2025", "summary": "Detailed structural building information is used to estimate potential damage\nfrom hazard events like cyclones, floods, and landslides, making them critical\nfor urban resilience planning and disaster risk reduction. However, such\ninformation is often unavailable in many small island developing states (SIDS)\nin climate-vulnerable regions like the Caribbean. To address this data gap, we\npresent an AI-driven workflow to automatically infer rooftop attributes from\nhigh-resolution satellite imagery, with Saint Vincent and the Grenadines as our\ncase study. Here, we compare the utility of geospatial foundation models\ncombined with shallow classifiers against fine-tuned deep learning models for\nrooftop classification. Furthermore, we assess the impact of incorporating\nadditional training data from neighboring SIDS to improve model performance.\nOur best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof\nmaterial classification, respectively. Combined with local capacity building,\nour work aims to provide SIDS with novel capabilities to harness AI and Earth\nObservation (EO) data to enable more efficient, evidence-based urban\ngovernance.", "AI": {"tldr": "AI-driven workflow using satellite imagery to automatically infer rooftop attributes for disaster risk assessment in small island developing states, achieving F1 scores of 0.88 for roof pitch and 0.83 for roof material classification.", "motivation": "Small island developing states (SIDS) lack detailed structural building information needed for urban resilience planning and disaster risk reduction, particularly for hazard events like cyclones, floods, and landslides.", "method": "Comparison of geospatial foundation models with shallow classifiers versus fine-tuned deep learning models for rooftop classification from high-resolution satellite imagery, with assessment of incorporating additional training data from neighboring SIDS.", "result": "Best models achieved F1 scores of 0.88 for roof pitch classification and 0.83 for roof material classification.", "conclusion": "The AI-driven approach combined with local capacity building provides SIDS with novel capabilities to use AI and Earth Observation data for more efficient, evidence-based urban governance."}}
{"id": "2509.18183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18183", "abs": "https://arxiv.org/abs/2509.18183", "authors": ["Jinyue Bian", "Zhaoxing Zhang", "Zhengyu Liang", "Shiwei Zheng", "Shengtao Zhang", "Rong Shen", "Chen Yang", "Anzhou Hou"], "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation", "comment": null, "summary": "The Visual-Language-Action (VLA) models can follow text instructions\naccording to visual observations of the surrounding environment. This ability\nto map multimodal inputs to actions is derived from the training of the VLA\nmodel on extensive standard demonstrations. These visual observations captured\nby third-personal global and in-wrist local cameras are inevitably varied in\nnumber and perspective across different environments, resulting in significant\ndifferences in the visual features. This perspective heterogeneity constrains\nthe generality of VLA models. In light of this, we first propose the\nlightweight module VLA-LPAF to foster the perspective adaptivity of VLA models\nusing only 2D data. VLA-LPAF is finetuned using images from a single view and\nfuses other multiview observations in the latent space, which effectively and\nefficiently bridge the gap caused by perspective inconsistency. We instantiate\nour VLA-LPAF framework with the VLA model RoboFlamingo to construct\nRoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves\naround 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a\ncustomized simulation benchmark. We also demonstrate the developed viewadaptive\ncharacteristics of the proposed RoboFlamingo-LPAF through real-world tasks.", "AI": {"tldr": "Proposes VLA-LPAF, a lightweight module that enhances Visual-Language-Action models' perspective adaptivity by fusing multiview observations in latent space using only 2D data.", "motivation": "VLA models face perspective heterogeneity issues due to varied camera views across environments, which constrains their generality and performance.", "method": "Developed VLA-LPAF module that is finetuned with single-view images and fuses multiview observations in latent space to bridge perspective inconsistency gaps. Implemented as RoboFlamingo-LPAF.", "result": "Achieved significant improvements: 8% task success rate on CALVIN, 15% on LIBERO, and 30% on customized simulation benchmark. Demonstrated view-adaptive capabilities in real-world tasks.", "conclusion": "VLA-LPAF effectively and efficiently addresses perspective heterogeneity in VLA models, enabling better generalization across different camera views and environments."}}
{"id": "2509.18184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18184", "abs": "https://arxiv.org/abs/2509.18184", "authors": ["Yifeng Cheng", "Alois Knoll", "Hu Cao"], "title": "URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation", "comment": "This work is accepted by Visual Intelligence Journal", "summary": "Event cameras provide high temporal resolution, high dynamic range, and low\nlatency, offering significant advantages over conventional frame-based cameras.\nIn this work, we introduce an uncertainty-aware refinement network called URNet\nfor event-based stereo depth estimation. Our approach features a local-global\nrefinement module that effectively captures fine-grained local details and\nlong-range global context. Additionally, we introduce a Kullback-Leibler (KL)\ndivergence-based uncertainty modeling method to enhance prediction reliability.\nExtensive experiments on the DSEC dataset demonstrate that URNet consistently\noutperforms state-of-the-art (SOTA) methods in both qualitative and\nquantitative evaluations.", "AI": {"tldr": "URNet is an uncertainty-aware refinement network for event-based stereo depth estimation that uses local-global refinement and KL divergence-based uncertainty modeling to achieve state-of-the-art performance on the DSEC dataset.", "motivation": "Event cameras offer advantages like high temporal resolution, high dynamic range, and low latency compared to conventional frame-based cameras, but existing methods for stereo depth estimation from event data need improvement in capturing fine details and providing reliable uncertainty estimates.", "method": "The approach introduces URNet with a local-global refinement module to capture both fine-grained local details and long-range global context, plus a Kullback-Leibler divergence-based uncertainty modeling method to enhance prediction reliability.", "result": "Extensive experiments on the DSEC dataset show that URNet consistently outperforms state-of-the-art methods in both qualitative and quantitative evaluations.", "conclusion": "The proposed URNet with its local-global refinement and uncertainty modeling effectively improves event-based stereo depth estimation, demonstrating superior performance over existing methods."}}
{"id": "2509.18185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18185", "abs": "https://arxiv.org/abs/2509.18185", "authors": ["Giammarco La Barbera", "Enzo Bonnot", "Thomas Isla", "Juan Pablo de la Plata", "Joy-Rose Dunoyer de Segonzac", "Jennifer Attali", "C\u00e9cile Lozach", "Alexandre Bellucci", "Louis Marcellin", "Laure Fournier", "Sabine Sarnacki", "Pietro Gori", "Isabelle Bloch"], "title": "Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases", "comment": "Computer-Aided Pelvic Imaging for Female Health (CAPI) - Workshop\n  MICCAI 2025", "summary": "Endometriosis often leads to chronic pelvic pain and possible nerve\ninvolvement, yet imaging the peripheral nerves remains a challenge. We\nintroduce Visionerves, a novel hybrid AI framework for peripheral nervous\nsystem recognition from multi-gradient DWI and morphological MRI data. Unlike\nconventional tractography, Visionerves encodes anatomical knowledge through\nfuzzy spatial relationships, removing the need for selection of manual ROIs.\nThe pipeline comprises two phases: (A) automatic segmentation of anatomical\nstructures using a deep learning model, and (B) tractography and nerve\nrecognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in\n10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated\nsubstantial improvements over standard tractography, with Dice score\nimprovements of up to 25% and spatial errors reduced to less than 5 mm. This\nautomatic and reproducible approach enables detailed nerve analysis and paves\nthe way for non-invasive diagnosis of endometriosis-related neuropathy, as well\nas other conditions with nerve involvement.", "AI": {"tldr": "Visionerves is a hybrid AI framework that combines deep learning and symbolic spatial reasoning for automatic peripheral nerve recognition from MRI data, showing significant improvements over standard tractography for endometriosis patients.", "motivation": "Endometriosis causes chronic pelvic pain with potential nerve involvement, but current imaging methods struggle to visualize peripheral nerves effectively. Conventional tractography requires manual ROI selection and lacks anatomical knowledge integration.", "method": "A two-phase hybrid AI approach: (A) deep learning model for automatic anatomical structure segmentation, and (B) symbolic spatial reasoning using fuzzy spatial relationships for tractography and nerve recognition, eliminating the need for manual ROIs.", "result": "Tested on lumbosacral plexus in 10 women with endometriosis, Visionerves achieved up to 25% improvement in Dice scores and reduced spatial errors to less than 5mm compared to standard tractography.", "conclusion": "This automatic, reproducible framework enables detailed nerve analysis and provides a foundation for non-invasive diagnosis of endometriosis-related neuropathy and other nerve-involved conditions."}}
{"id": "2509.18187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18187", "abs": "https://arxiv.org/abs/2509.18187", "authors": ["Muhammad Naveed", "Nazia Perwaiz", "Sidra Sultana", "Mohaira Ahmad", "Muhammad Moazam Fraz"], "title": "V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling", "comment": null, "summary": "Road traffic accidents remain a major public health challenge, particularly\nin countries with heterogeneous road conditions, mixed traffic flow, and\nvariable driving discipline, such as Pakistan. Reliable detection of unsafe\ndriving behaviours is a prerequisite for improving road safety, enabling\nadvanced driver assistance systems (ADAS), and supporting data driven decisions\nin insurance and fleet management. Most of existing datasets originate from the\ndeveloped countries with limited representation of the behavioural diversity\nobserved in emerging economies and the driver's face recording voilates the\nprivacy preservation. We present V-SenseDrive, the first privacy-preserving\nmultimodal driver behaviour dataset collected entirely within the Pakistani\ndriving environment. V-SenseDrive combines smartphone based inertial and GPS\nsensor data with synchronized road facing video to record three target driving\nbehaviours (normal, aggressive, and risky) on multiple types of roads,\nincluding urban arterials, secondary roads, and motorways. Data was gathered\nusing a custom Android application designed to capture high frequency\naccelerometer, gyroscope, and GPS streams alongside continuous video, with all\nsources precisely time aligned to enable multimodal analysis. The focus of this\nwork is on the data acquisition process, covering participant selection,\ndriving scenarios, environmental considerations, and sensor video\nsynchronization techniques. The dataset is structured into raw, processed, and\nsemantic layers, ensuring adaptability for future research in driver behaviour\nclassification, traffic safety analysis, and ADAS development. By representing\nreal world driving in Pakistan, V-SenseDrive fills a critical gap in the global\nlandscape of driver behaviour datasets and lays the groundwork for context\naware intelligent transportation solutions.", "AI": {"tldr": "V-SenseDrive is the first privacy-preserving multimodal driver behavior dataset collected in Pakistan, combining smartphone sensor data with synchronized road-facing video to capture diverse driving behaviors in heterogeneous road conditions.", "motivation": "Existing datasets lack representation of behavioral diversity in emerging economies like Pakistan, and often violate privacy by recording drivers' faces. There's a need for context-aware datasets that reflect real-world driving conditions in developing countries.", "method": "Used a custom Android app to capture high-frequency accelerometer, gyroscope, GPS streams alongside continuous road-facing video, with precise time alignment. Data was collected on various road types (urban arterials, secondary roads, motorways) targeting three driving behaviors (normal, aggressive, risky).", "result": "Created a structured dataset with raw, processed, and semantic layers, enabling multimodal analysis of driver behavior in Pakistani driving environments while preserving privacy.", "conclusion": "V-SenseDrive fills a critical gap in global driver behavior datasets by representing real-world driving in Pakistan, providing a foundation for context-aware intelligent transportation solutions and ADAS development."}}
{"id": "2509.18189", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18189", "abs": "https://arxiv.org/abs/2509.18189", "authors": ["Daxiang Dong", "Mingming Zheng", "Dong Xu", "Bairong Zhuang", "Wenyu Zhang", "Chunhua Luo", "Haoran Wang", "Zijian Zhao", "Jie Li", "Yuxuan Li", "Hanjun Zhong", "Mengyue Liu", "Jieting Chen", "Shupeng Li", "Lun Tian", "Yaping Feng", "Xin Li", "Donggang Jiang", "Yong Chen", "Yehua Xu", "Duohao Qin", "Chen Feng", "Dan Wang", "Henghua Zhang", "Jingjing Ha", "Jinhui He", "Yanfeng Zhai", "Chengxin Zheng", "Jiayi Mao", "Jiacheng Chen", "Ruchang Yao", "Ziye Yuan", "Jianmin Wu", "Guangjun Xie", "Dou Shen"], "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models", "comment": "12 pages", "summary": "We present Qianfan-VL, a series of multimodal large language models ranging\nfrom 3B to 70B parameters, achieving state-of-the-art performance through\ninnovative domain enhancement techniques. Our approach employs multi-stage\nprogressive training and high-precision data synthesis pipelines, which prove\nto be critical technologies for enhancing domain-specific capabilities while\nmaintaining strong general performance. Qianfan-VL achieves comparable results\nto leading open-source models on general benchmarks, with state-of-the-art\nperformance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and\nMMStar. The domain enhancement strategy delivers significant advantages in OCR\nand document understanding, validated on both public benchmarks (OCRBench 873,\nDocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B\nvariants incorporate long chain-of-thought capabilities, demonstrating superior\nperformance on mathematical reasoning (MathVista 78.6%) and logical inference\ntasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating\nthe capability of large-scale AI infrastructure to train SOTA-level multimodal\nmodels with over 90% scaling efficiency on 5000 chips for a single task. This\nwork establishes an effective methodology for developing domain-enhanced\nmultimodal models suitable for diverse enterprise deployment scenarios.", "AI": {"tldr": "Qianfan-VL is a series of multimodal large language models (3B-70B parameters) that achieve state-of-the-art performance through domain enhancement techniques, multi-stage progressive training, and high-precision data synthesis pipelines.", "motivation": "To develop effective domain-enhanced multimodal models suitable for diverse enterprise deployment scenarios, addressing the need for specialized capabilities while maintaining strong general performance.", "method": "Multi-stage progressive training and high-precision data synthesis pipelines for domain enhancement, trained entirely on Baidu's Kunlun P800 chips with over 90% scaling efficiency on 5000 chips.", "result": "Achieves SOTA performance on benchmarks including CCBench, SEEDBench IMG, ScienceQA, MMStar, OCRBench (873), DocVQA (94.75%), and MathVista (78.6%). Models incorporate long chain-of-thought capabilities for superior mathematical reasoning and logical inference.", "conclusion": "Establishes an effective methodology for developing domain-enhanced multimodal models, validating the capability of large-scale AI infrastructure to train SOTA-level multimodal models for enterprise deployment."}}
{"id": "2509.18190", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18190", "abs": "https://arxiv.org/abs/2509.18190", "authors": ["Junseong Shin", "Seungwoo Chung", "Yunjeong Yang", "Tae Hyun Kim"], "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing", "comment": null, "summary": "Dehazing involves removing haze or fog from images to restore clarity and\nimprove visibility by estimating atmospheric scattering effects. While deep\nlearning methods show promise, the lack of paired real-world training data and\nthe resulting domain gap hinder generalization to real-world scenarios. In this\ncontext, physics-grounded learning becomes crucial; however, traditional\nmethods based on the Atmospheric Scattering Model (ASM) often fall short in\nhandling real-world complexities and diverse haze patterns. To solve this\nproblem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM\nas an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),\nHazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,\nenhancing real-world dehazing performance with only a single inference step.\nAdditionally, we introduce a non-homogeneous haze generation method using\nMarkov Chain Brownian Motion (MCBM) to address the scarcity of paired\nreal-world data. By simulating realistic haze patterns through MCBM, we enhance\nthe adaptability of HazeFlow to diverse real-world scenarios. Through extensive\nexperiments, we demonstrate that HazeFlow achieves state-of-the-art performance\nacross various real-world dehazing benchmark datasets.", "AI": {"tldr": "HazeFlow is a novel ODE-based framework that reformulates atmospheric scattering model as an ODE for single-step real-world image dehazing, addressing data scarcity through MCBM-based haze generation.", "motivation": "Deep learning dehazing methods struggle with real-world generalization due to lack of paired training data and domain gaps. Traditional physics-based methods using Atmospheric Scattering Model fail to handle complex real-world haze patterns.", "method": "Proposes HazeFlow, an ODE-based framework inspired by Rectified Flow that learns optimal ODE trajectories to map hazy to clean images in single inference. Introduces non-homogeneous haze generation using Markov Chain Brownian Motion to create realistic training data.", "result": "HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets through extensive experiments.", "conclusion": "The ODE-based formulation and MCBM haze generation enable effective real-world dehazing with improved generalization and single-step inference capability."}}
{"id": "2509.18193", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18193", "abs": "https://arxiv.org/abs/2509.18193", "authors": ["Omar H. Khater", "Abdul Jabbar Siddiqui", "Aiman El-Maleh", "M. Shamim Hossain"], "title": "TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection", "comment": null, "summary": "Deploying deep learning models in agriculture is difficult because edge\ndevices have limited resources, but this work presents a compressed version of\nEcoWeedNet using structured channel pruning, quantization-aware training (QAT),\nand acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the\nchallenges of pruning complex architectures with residual shortcuts, attention\nmechanisms, concatenations, and CSP blocks, the model size was reduced by up to\n68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at\nFP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the\npruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n\n(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%\nmAP50, proving it to be both efficient and effective for precision agriculture.", "AI": {"tldr": "This paper presents a compressed version of EcoWeedNet using structured channel pruning, quantization-aware training, and TensorRT acceleration, achieving significant model size reduction and faster inference while maintaining high performance on weed detection tasks.", "motivation": "Deploying deep learning models in agriculture is challenging due to limited resources on edge devices, necessitating efficient model compression techniques for practical applications in precision agriculture.", "method": "The authors used structured channel pruning, quantization-aware training (QAT), and acceleration with NVIDIA's TensorRT on Jetson Orin Nano to compress EcoWeedNet, overcoming challenges from complex architectural elements like residual shortcuts, attention mechanisms, concatenations, and CSP blocks.", "result": "The compressed model achieved 68.5% size reduction, 3.2 GFLOPs computation reduction, and 184 FPS inference speed (28.7% faster than baseline). On CottonWeedDet12 dataset, it outperformed YOLO11n and YOLO12n with 83.7% precision, 77.5% recall, and 85.9% mAP50.", "conclusion": "The pruned EcoWeedNet demonstrates both efficiency and effectiveness for precision agriculture applications, providing a viable solution for deploying deep learning models on resource-constrained edge devices in agricultural settings."}}
{"id": "2509.18284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18284", "abs": "https://arxiv.org/abs/2509.18284", "authors": ["Yi Gu", "Kuniaki Saito", "Jiaxin Ma"], "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction", "comment": "MICCAI 2025", "summary": "As medical diagnoses increasingly leverage multimodal data, machine learning\nmodels are expected to effectively fuse heterogeneous information while\nremaining robust to missing modalities. In this work, we propose a novel\nmultimodal learning framework that integrates enhanced modalities dropout and\ncontrastive learning to address real-world limitations such as modality\nimbalance and missingness. Our approach introduces learnable modality tokens\nfor improving missingness-aware fusion of modalities and augments conventional\nunimodal contrastive objectives with fused multimodal representations. We\nvalidate our framework on large-scale clinical datasets for disease detection\nand prediction tasks, encompassing both visual and tabular modalities.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance, particularly in challenging and practical scenarios where only a\nsingle modality is available. Furthermore, we show its adaptability through\nsuccessful integration with a recent CT foundation model. Our findings\nhighlight the effectiveness, efficiency, and generalizability of our approach\nfor multimodal learning, offering a scalable, low-cost solution with\nsignificant potential for real-world clinical applications. The code is\navailable at https://github.com/omron-sinicx/medical-modality-dropout.", "AI": {"tldr": "A novel multimodal learning framework that uses enhanced modality dropout and contrastive learning to handle missing modalities and modality imbalance in medical diagnosis, achieving state-of-the-art performance particularly in single-modality scenarios.", "motivation": "Medical diagnoses increasingly use multimodal data, but real-world limitations include modality imbalance and missing modalities. Models need to effectively fuse heterogeneous information while remaining robust when some modalities are unavailable.", "method": "Proposes a framework integrating enhanced modality dropout and contrastive learning. Uses learnable modality tokens for missingness-aware fusion and augments unimodal contrastive objectives with fused multimodal representations. Validated on clinical datasets with visual and tabular modalities.", "result": "Achieves state-of-the-art performance, especially in challenging scenarios with only single modality available. Successfully integrates with CT foundation model, demonstrating adaptability. Shows effectiveness, efficiency, and generalizability.", "conclusion": "The approach offers a scalable, low-cost solution with significant potential for real-world clinical applications, effectively addressing modality imbalance and missingness in multimodal medical learning."}}
{"id": "2509.18308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18308", "abs": "https://arxiv.org/abs/2509.18308", "authors": ["Yixin Zhang", "Ryan Chamberlain", "Lawrance Ngo", "Kevin Kramer", "Maciej A. Mazurowski"], "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model", "comment": "submitted to WACV 2026 application track, model weights available at:\n  https://github.com/mazurowski-lab/PulmonaryEmbolismSegmentation", "summary": "In this study, we curated a densely annotated in-house dataset comprising 490\nCTPA scans. Using this dataset, we systematically evaluated nine widely used\nsegmentation architectures from both the CNN and Vision Transformer (ViT)\nfamilies, initialized with either pretrained or random weights, under a unified\ntesting framework as a performance audit. Our study leads to several important\nobservations: (1) 3D U-Net with a ResNet encoder remains a highly effective\narchitecture for PE segmentation; (2) 3D models are particularly well-suited to\nthis task given the morphological characteristics of emboli; (3) CNN-based\nmodels generally yield superior performance compared to their ViT-based\ncounterparts in PE segmentation; (4) classification-based pretraining, even on\nlarge PE datasets, can adversely impact segmentation performance compared to\ntraining from scratch, suggesting that PE classification and segmentation may\nrely on different sets of discriminative features; (5) different model\narchitectures show a highly consistent pattern of segmentation performance when\ntrained on the same data; and (6) while central and large emboli can be\nsegmented with satisfactory accuracy, distal emboli remain challenging due to\nboth task complexity and the scarcity of high-quality datasets. Besides these\nfindings, our best-performing model achieves a mean Dice score of 0.7131 for\nsegmentation. It detects 181 emboli with 49 false positives and 28 false\nnegatives from 60 in-house testing scans. Its generalizability is further\nvalidated on public datasets.", "AI": {"tldr": "Systematic evaluation of 9 segmentation architectures for pulmonary embolism (PE) segmentation from CTPA scans, revealing that 3D U-Net with ResNet encoder performs best, CNN models outperform ViT models, and pretraining can hurt segmentation performance.", "motivation": "To conduct a comprehensive performance audit of various segmentation architectures for pulmonary embolism segmentation from CT pulmonary angiography (CTPA) scans, addressing the need for reliable PE detection methods.", "method": "Used a densely annotated in-house dataset of 490 CTPA scans to evaluate 9 segmentation architectures (CNN and ViT families) with pretrained or random weights under a unified testing framework.", "result": "Best model achieved mean Dice score of 0.7131, detecting 181 emboli with 49 false positives and 28 false negatives from 60 testing scans. 3D U-Net with ResNet encoder performed best, CNN models outperformed ViT models, and pretraining often degraded segmentation performance.", "conclusion": "3D models are well-suited for PE segmentation due to emboli morphology, segmentation and classification rely on different features, distal emboli remain challenging, and consistent performance patterns exist across architectures trained on the same data."}}
{"id": "2509.18309", "categories": ["cs.CV", "cs.LG", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18309", "abs": "https://arxiv.org/abs/2509.18309", "authors": ["Alessa Carbo", "Eric Nalisnick"], "title": "Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach", "comment": null, "summary": "Handshapes serve a fundamental phonological role in signed languages, with\nAmerican Sign Language employing approximately 50 distinct shapes.\nHowever,computational approaches rarely model handshapes explicitly, limiting\nboth recognition accuracy and linguistic analysis.We introduce a novel graph\nneural network that separates temporal dynamics from static handshape\nconfigurations. Our approach combines anatomically-informed graph structures\nwith contrastive learning to address key challenges in handshape recognition,\nincluding subtle interclass distinctions and temporal variations. We establish\nthe first benchmark for structured handshape recognition in signing sequences,\nachieving 46% accuracy across 37 handshape classes (with baseline methods\nachieving 25%).", "AI": {"tldr": "A novel graph neural network that separates temporal dynamics from static handshape configurations for improved handshape recognition in sign language.", "motivation": "Current computational approaches rarely model handshapes explicitly, limiting both recognition accuracy and linguistic analysis in sign language processing.", "method": "Combines anatomically-informed graph structures with contrastive learning to address challenges in handshape recognition, including subtle interclass distinctions and temporal variations.", "result": "Established the first benchmark for structured handshape recognition in signing sequences, achieving 46% accuracy across 37 handshape classes (baseline methods achieved 25%).", "conclusion": "The proposed approach significantly improves handshape recognition accuracy and provides a structured framework for linguistic analysis of sign languages."}}
{"id": "2509.18326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18326", "abs": "https://arxiv.org/abs/2509.18326", "authors": ["Chun Kit Wong", "Anders N. Christensen", "Cosmin I. Bercea", "Julia A. Schnabel", "Martin G. Tolsgaard", "Aasa Feragen"], "title": "Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound", "comment": "MICCAI 2025", "summary": "Reliable out-of-distribution (OOD) detection is important for safe deployment\nof deep learning models in fetal ultrasound amidst heterogeneous image\ncharacteristics and clinical settings. OOD detection relies on estimating a\nclassification model's uncertainty, which should increase for OOD samples.\nWhile existing research has largely focused on uncertainty quantification\nmethods, this work investigates the impact of the classification task itself.\nThrough experiments with eight uncertainty quantification methods across four\nclassification tasks, we demonstrate that OOD detection performance\nsignificantly varies with the task, and that the best task depends on the\ndefined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an\nimage characteristic shift or ii) an anatomical feature shift. Furthermore, we\nreveal that superior OOD detection does not guarantee optimal abstained\nprediction, underscoring the necessity to align task selection and uncertainty\nstrategies with the specific downstream application in medical image analysis.", "AI": {"tldr": "This paper investigates how different classification tasks affect out-of-distribution (OOD) detection performance in fetal ultrasound imaging, showing that task selection significantly impacts OOD detection effectiveness depending on the type of distribution shift.", "motivation": "Reliable OOD detection is crucial for safe deployment of deep learning models in fetal ultrasound, where heterogeneous image characteristics and clinical settings exist. While previous research focused on uncertainty quantification methods, this work examines the impact of the classification task itself on OOD detection performance.", "method": "The study conducted experiments with eight uncertainty quantification methods across four different classification tasks to evaluate OOD detection performance. The research analyzed how OOD detection varies based on whether the OOD samples result from image characteristic shifts or anatomical feature shifts.", "result": "Results show that OOD detection performance significantly varies with the classification task, and the best task depends on the defined in-distribution vs. out-of-distribution criteria. Superior OOD detection does not necessarily guarantee optimal abstained prediction performance.", "conclusion": "Task selection and uncertainty strategies must be aligned with specific downstream applications in medical image analysis, as the effectiveness of OOD detection depends on both the classification task and the type of distribution shift encountered."}}
{"id": "2509.18350", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18350", "abs": "https://arxiv.org/abs/2509.18350", "authors": ["Oussema Dhaouadi", "Riccardo Marin", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata", "comment": "Accepted at NeurIPS 2025", "summary": "Accurate visual localization from aerial views is a fundamental problem with\napplications in mapping, large-area inspection, and search-and-rescue\noperations. In many scenarios, these systems require high-precision\nlocalization while operating with limited resources (e.g., no internet\nconnection or GNSS/GPS support), making large image databases or heavy 3D\nmodels impractical. Surprisingly, little attention has been given to leveraging\northographic geodata as an alternative paradigm, which is lightweight and\nincreasingly available through free releases by governmental authorities (e.g.,\nthe European Union). To fill this gap, we propose OrthoLoC, the first\nlarge-scale dataset comprising 16,425 UAV images from Germany and the United\nStates with multiple modalities. The dataset addresses domain shifts between\nUAV imagery and geospatial data. Its paired structure enables fair benchmarking\nof existing solutions by decoupling image retrieval from feature matching,\nallowing isolated evaluation of localization and calibration performance.\nThrough comprehensive evaluation, we examine the impact of domain shifts, data\nresolutions, and covisibility on localization accuracy. Finally, we introduce a\nrefinement technique called AdHoP, which can be integrated with any feature\nmatcher, improving matching by up to 95% and reducing translation error by up\nto 63%. The dataset and code are available at:\nhttps://deepscenario.github.io/OrthoLoC.", "AI": {"tldr": "OrthoLoC is a large-scale dataset for visual localization using orthographic geodata, addressing domain shifts between UAV imagery and geospatial data, with a refinement technique called AdHoP that improves matching performance.", "motivation": "Enable high-precision visual localization in resource-constrained environments where large image databases or 3D models are impractical, by leveraging lightweight orthographic geodata that is increasingly available from governmental sources.", "method": "Created OrthoLoC dataset with 16,425 UAV images from Germany and US with multiple modalities, using paired structure to decouple image retrieval from feature matching. Introduced AdHoP refinement technique that can be integrated with any feature matcher.", "result": "Comprehensive evaluation shows AdHoP improves matching by up to 95% and reduces translation error by up to 63%. The dataset enables fair benchmarking of localization solutions by isolating evaluation of localization and calibration performance.", "conclusion": "Orthographic geodata provides a viable alternative paradigm for visual localization in resource-constrained scenarios, with the OrthoLoC dataset and AdHoP technique significantly advancing the field by addressing domain shifts and improving matching accuracy."}}
{"id": "2509.18354", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.18354", "abs": "https://arxiv.org/abs/2509.18354", "authors": ["Mehrdad Moradi", "Shengzhe Chen", "Hao Yan", "Kamran Paynabar"], "title": "A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data", "comment": "12 pages, 10 figures, 1 table. Preprint submitted to a CVF conference", "summary": "Anomaly detection in images is typically addressed by learning from\ncollections of training data or relying on reference samples. In many\nreal-world scenarios, however, such training data may be unavailable, and only\nthe test image itself is provided. We address this zero-shot setting by\nproposing a single-image anomaly localization method that leverages the\ninductive bias of convolutional neural networks, inspired by Deep Image Prior\n(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key\nassumption is that natural images often exhibit unified textures and patterns,\nand that anomalies manifest as localized deviations from these repetitive or\nstochastic patterns. To learn the deep image prior, we design a patch-based\ntraining framework where the input image is fed directly into the network for\nself-reconstruction, rather than mapping random noise to the image as done in\nDIP. To avoid the model simply learning an identity mapping, we apply masking,\npatch shuffling, and small Gaussian noise. In addition, we use a perceptual\nloss based on inner-product similarity to capture structure beyond pixel\nfidelity. Our approach needs no external training data, labels, or references,\nand remains robust in the presence of noise or missing pixels. SSDnet achieves\n0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the\nfabric dataset, outperforming state-of-the-art methods. The implementation code\nwill be released at https://github.com/mehrdadmoradi124/SSDnet", "AI": {"tldr": "SSDnet is a zero-shot anomaly detection method that uses deep image prior for single-image anomaly localization without external training data, achieving state-of-the-art performance on benchmark datasets.", "motivation": "Many real-world scenarios lack training data or reference samples, requiring anomaly detection using only the test image itself. Existing methods typically rely on training collections, which may be unavailable.", "method": "Proposes a patch-based training framework where the input image is fed directly into the network for self-reconstruction using masking, patch shuffling, and Gaussian noise to avoid identity mapping. Uses perceptual loss based on inner-product similarity for better structure capture.", "result": "Achieves 0.99 AUROC and 0.60 AUPRC on MVTec-AD, and 0.98 AUROC and 0.67 AUPRC on fabric dataset, outperforming state-of-the-art methods.", "conclusion": "SSDnet provides an effective zero-shot solution for anomaly detection that requires no external data, labels, or references, and remains robust to noise and missing pixels."}}
{"id": "2509.18369", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18369", "abs": "https://arxiv.org/abs/2509.18369", "authors": ["Riad Ahmed Anonto", "Sardar Md. Saffat Zabin", "M. Saifur Rahman"], "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning", "comment": null, "summary": "Grounding vision--language models in low-resource languages remains\nchallenging, as they often produce fluent text about the wrong objects. This\nstems from scarce paired data, translation pivots that break alignment, and\nEnglish-centric pretraining that ignores target-language semantics. We address\nthis with a compute-aware Bengali captioning pipeline trained on LaBSE-verified\nEN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT\nyields stable visual patches, a Bengali-native mBART-50 decodes, and a\nlightweight bridge links the modalities. Our core novelty is a tri-loss\nobjective: Patch-Alignment Loss (PAL) aligns real and synthetic patch\ndescriptors using decoder cross-attention, InfoNCE enforces global\nreal--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained\npatch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces\nspurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR\n27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,\nBERTScore-F1 75.40), outperforming strong CE baselines and narrowing the\nreal--synthetic centroid gap by 41%.", "AI": {"tldr": "A novel Bengali captioning pipeline with tri-loss objective (PAL+InfoNCE+OT) improves vision-language grounding in low-resource languages by aligning real and synthetic visual patches and reducing spurious matches.", "motivation": "Grounding vision-language models in low-resource languages like Bengali is challenging due to scarce paired data, translation alignment issues, and English-centric pretraining that ignores target-language semantics.", "method": "Uses a compute-aware pipeline with frozen MaxViT for visual patches, Bengali-native mBART-50 decoder, and lightweight bridge. Core innovation is tri-loss objective: Patch-Alignment Loss (PAL) for patch descriptor alignment, InfoNCE for global real-synthetic separation, and Sinkhorn-based OT for balanced patch correspondence.", "result": "Achieves strong performance on Flickr30k-1k (BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40), outperforming CE baselines and reducing real-synthetic centroid gap by 41%.", "conclusion": "The PAL+InfoNCE+OT synergy effectively improves grounding in low-resource languages, demonstrating significant gains in captioning quality while addressing alignment challenges between real and synthetic data."}}
{"id": "2509.18372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18372", "abs": "https://arxiv.org/abs/2509.18372", "authors": ["Reeshad Khan", "John Gauch"], "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning", "comment": null, "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.", "AI": {"tldr": "TinyBEV is a compact, real-time camera-only BEV framework that distills full-stack autonomous driving capabilities from a large teacher model into a 28M-parameter student model, achieving significant parameter reduction while maintaining performance.", "motivation": "To bridge the gap between large-scale multi-modal perception-planning models and deployment-ready real-time autonomy by creating a resource-efficient camera-only solution that retains full-stack driving intelligence.", "method": "Uses model-agnostic, multi-stage distillation combining feature-level, output-level, and adaptive region-aware supervision to transfer knowledge from a planning-oriented teacher (UniAD) to a lightweight BEV representation.", "result": "Achieves 39.0 mAP for detection, 1.08 minADE for motion forecasting, 0.32 collision rate on nuScenes, running 5x faster (11 FPS) with 78% parameter reduction compared to UniAD.", "conclusion": "Demonstrates that full-stack driving intelligence can be effectively retained in resource-constrained settings using camera-only input, enabling real-time autonomous driving deployment."}}
{"id": "2509.18387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18387", "abs": "https://arxiv.org/abs/2509.18387", "authors": ["Thomas Gossard", "Filip Radovic", "Andreas Ziegler", "Andrea Zell"], "title": "BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking", "comment": null, "summary": "Motion blur reduces the clarity of fast-moving objects, posing challenges for\ndetection systems, especially in racket sports, where balls often appear as\nstreaks rather than distinct points. Existing labeling conventions mark the\nball at the leading edge of the blur, introducing asymmetry and ignoring\nvaluable motion cues correlated with velocity. This paper introduces a new\nlabeling strategy that places the ball at the center of the blur streak and\nexplicitly annotates blur attributes. Using this convention, we release a new\ntable tennis ball detection dataset. We demonstrate that this labeling approach\nconsistently enhances detection performance across various models. Furthermore,\nwe introduce BlurBall, a model that jointly estimates ball position and motion\nblur attributes. By incorporating attention mechanisms such as\nSqueeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art\nresults in ball detection. Leveraging blur not only improves detection accuracy\nbut also enables more reliable trajectory prediction, benefiting real-time\nsports analytics.", "AI": {"tldr": "This paper introduces a new ball labeling strategy that places the ball at the center of motion blur streaks instead of the leading edge, and presents BlurBall - a model that jointly estimates ball position and motion blur attributes using attention mechanisms over multi-frame inputs.", "motivation": "Current ball detection systems struggle with motion blur in fast-moving sports like table tennis, where balls appear as streaks. Existing labeling conventions mark balls at the leading edge of blur, which introduces asymmetry and ignores valuable motion cues correlated with velocity.", "method": "Proposed a new labeling strategy placing the ball at the center of blur streaks with explicit blur attribute annotation. Developed BlurBall model using Squeeze-and-Excitation attention mechanisms over multi-frame inputs to jointly estimate ball position and motion blur attributes.", "result": "The new labeling approach consistently enhances detection performance across various models. BlurBall achieves state-of-the-art results in ball detection and enables more reliable trajectory prediction.", "conclusion": "Leveraging motion blur information not only improves ball detection accuracy but also benefits real-time sports analytics through better trajectory prediction capabilities."}}
{"id": "2509.18388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18388", "abs": "https://arxiv.org/abs/2509.18388", "authors": ["Binhua Huang", "Ni Wang", "Wendong Yao", "Soumyabrata Dev"], "title": "MVP: Motion Vector Propagation for Zero-Shot Video Object Detection", "comment": "5 pages, 1 figure", "summary": "Running a large open-vocabulary (Open-vocab) detector on every video frame is\naccurate but expensive. We introduce a training-free pipeline that invokes\nOWLv2 only on fixed-interval keyframes and propagates detections to\nintermediate frames using compressed-domain motion vectors (MV). A simple 3x3\ngrid aggregation of motion vectors provides translation and uniform-scale\nupdates, augmented with an area-growth check and an optional single-class\nswitch. The method requires no labels, no fine-tuning, and uses the same prompt\nlist for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),\nour approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose\nintersection-over-union (IoU) thresholds it remains close to framewise\nOWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse\nlocalization is largely preserved. Under the same keyframe schedule, MVP\noutperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A\nsupervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled\ntraining, whereas our method remains label-free and open-vocabulary. These\nresults indicate that compressed-domain propagation is a practical way to\nreduce detector invocations while keeping strong zero-shot coverage in videos.\nOur code and models are available at https://github.com/microa/MVP.", "AI": {"tldr": "MVP is a training-free pipeline that reduces computational cost by running OWLv2 detector only on keyframes and propagating detections to intermediate frames using compressed-domain motion vectors, achieving competitive performance while remaining label-free and open-vocabulary.", "motivation": "Running large open-vocabulary detectors on every video frame is accurate but computationally expensive. The goal is to reduce detector invocations while maintaining strong zero-shot coverage in videos.", "method": "Invokes OWLv2 only on fixed-interval keyframes and propagates detections to intermediate frames using compressed-domain motion vectors. Uses 3x3 grid aggregation of motion vectors for translation and uniform-scale updates, with area-growth check and optional single-class switch. Requires no labels, no fine-tuning, and uses same prompt list for all open-vocabulary methods.", "result": "On ILSVRC2015-VID: mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose IoU thresholds, remains close to framewise OWLv2-Large (0.747/0.721 at 0.2/0.3 vs 0.784/0.780). Outperforms tracker-based propagation methods under same keyframe schedule. While supervised YOLOv12x reaches 0.631 mAP@0.5, MVP remains label-free.", "conclusion": "Compressed-domain propagation is a practical way to reduce detector invocations while keeping strong zero-shot coverage in videos, offering a label-free and open-vocabulary alternative to supervised methods."}}
{"id": "2509.18390", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18390", "abs": "https://arxiv.org/abs/2509.18390", "authors": ["Zitian Zhang", "Joshua Urban Davis", "Jeanne Phuong Anh Vu", "Jiangtao Kuang", "Jean-Fran\u00e7ois Lalonde"], "title": "Improving the color accuracy of lighting estimation models", "comment": "Project page: https://lvsn.github.io/coloraccuracy", "summary": "Advances in high dynamic range (HDR) lighting estimation from a single image\nhave opened new possibilities for augmented reality (AR) applications.\nPredicting complex lighting environments from a single input image allows for\nthe realistic rendering and compositing of virtual objects. In this work, we\ninvestigate the color robustness of such methods -- an often overlooked yet\ncritical factor for achieving visual realism. While most evaluations conflate\ncolor with other lighting attributes (e.g., intensity, direction), we isolate\ncolor as the primary variable of interest. Rather than introducing a new\nlighting estimation algorithm, we explore whether simple adaptation techniques\ncan enhance the color accuracy of existing models. Using a novel HDR dataset\nfeaturing diverse lighting colors, we systematically evaluate several\nadaptation strategies. Our results show that preprocessing the input image with\na pre-trained white balance network improves color robustness, outperforming\nother strategies across all tested scenarios. Notably, this approach requires\nno retraining of the lighting estimation model. We further validate the\ngenerality of this finding by applying the technique to three state-of-the-art\nlighting estimation methods from recent literature.", "AI": {"tldr": "This paper investigates color robustness in HDR lighting estimation methods for AR applications, showing that simple preprocessing with a white balance network improves color accuracy without retraining existing models.", "motivation": "To address the often-overlooked issue of color robustness in HDR lighting estimation, which is critical for achieving visual realism in augmented reality applications when rendering virtual objects.", "method": "Systematically evaluated several adaptation strategies using a novel HDR dataset with diverse lighting colors, focusing on preprocessing input images with a pre-trained white balance network rather than developing new lighting estimation algorithms.", "result": "Preprocessing with a white balance network significantly improves color robustness across all tested scenarios and works effectively with three state-of-the-art lighting estimation methods without requiring retraining.", "conclusion": "Simple adaptation techniques, particularly white balance preprocessing, can effectively enhance the color accuracy of existing HDR lighting estimation models, providing a practical solution for improving visual realism in AR applications."}}
{"id": "2509.18405", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18405", "abs": "https://arxiv.org/abs/2509.18405", "authors": ["Sourav Halder", "Jinjun Tong", "Xinyu Wu"], "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models", "comment": "12 pages, 5 figures, 2 tables", "summary": "Checks remain a foundational instrument in the financial ecosystem,\nfacilitating substantial transaction volumes across institutions. However,\ntheir continued use also renders them a persistent target for fraud,\nunderscoring the importance of robust check fraud detection mechanisms. At the\ncore of such systems lies the accurate identification and localization of\ncritical fields, such as the signature, magnetic ink character recognition\n(MICR) line, courtesy amount, legal amount, payee, and payer, which are\nessential for subsequent verification against reference checks belonging to the\nsame customer. This field-level detection is traditionally dependent on object\ndetection models trained on large, diverse, and meticulously labeled datasets,\na resource that is scarce due to proprietary and privacy concerns. In this\npaper, we introduce a novel, training-free framework for automated check field\ndetection, leveraging the power of a vision language model (VLM) in conjunction\nwith a multimodal large language model (MLLM). Our approach enables zero-shot\ndetection of check components, significantly lowering the barrier to deployment\nin real-world financial settings. Quantitative evaluation of our model on a\nhand-curated dataset of 110 checks spanning multiple formats and layouts\ndemonstrates strong performance and generalization capability. Furthermore,\nthis framework can serve as a bootstrap mechanism for generating high-quality\nlabeled datasets, enabling the development of specialized real-time object\ndetection models tailored to institutional needs.", "AI": {"tldr": "A training-free framework using vision language models for zero-shot detection of check fields like signatures and MICR lines, eliminating need for large labeled datasets.", "motivation": "Check fraud detection requires accurate field identification but traditional methods depend on large labeled datasets which are scarce due to privacy concerns.", "method": "Leverages vision language model (VLM) with multimodal large language model (MLLM) for zero-shot detection of check components without training.", "result": "Strong performance on 110 diverse checks, demonstrating generalization capability across different formats and layouts.", "conclusion": "Framework enables deployment in financial settings and can bootstrap labeled datasets for specialized real-time detection models."}}
{"id": "2509.18425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18425", "abs": "https://arxiv.org/abs/2509.18425", "authors": ["Philip Wootaek Shin", "Jack Sampson", "Vijaykrishnan Narayanan", "Andres Marquez", "Mahantesh Halappanavar"], "title": "Losing the Plot: How VLM responses degrade on imperfect charts", "comment": null, "summary": "Vision language models (VLMs) show strong results on chart understanding, yet\nexisting benchmarks assume clean figures and fact based queries. Real world\ncharts often contain distortions and demand reasoning beyond simple matching.\nWe evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp\nperformance drops under corruption or occlusion, with hallucinations such as\nvalue fabrication, trend misinterpretation, and entity confusion becoming more\nfrequent. Models remain overconfident in degraded settings, generating\nplausible but unsupported explanations.\n  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,\nand Reasoning Testing on Noisy and Occluded Input Selections), a dataset\ncombining chart corruptions, occlusions, and exam style multiple choice\nquestions inspired by Korea's CSAT English section. A key innovation is prompt\nreverse inconsistency, where models contradict themselves when asked to confirm\nversus deny the same statement. Our contributions are threefold: (1)\nbenchmarking state of the art VLMs, exposing systematic vulnerabilities in\nchart reasoning; (2) releasing CHART NOISe, the first dataset unifying\ncorruption, occlusion, and reverse inconsistency; and (3) proposing baseline\nmitigation strategies such as quality filtering and occlusion detection.\nTogether, these efforts establish a rigorous testbed for advancing robustness\nand reliability in chart understanding.", "AI": {"tldr": "This paper introduces CHART NOISe, a benchmark for testing vision language models on noisy and occluded charts, revealing significant performance drops and hallucinations in current VLMs under degraded conditions.", "motivation": "Existing chart understanding benchmarks assume clean figures and fact-based queries, but real-world charts often contain distortions and require complex reasoning beyond simple matching.", "method": "The authors evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro on chart corruptions and occlusions, and introduce CHART NOISe dataset with multiple-choice questions and prompt reverse inconsistency testing.", "result": "VLMs show sharp performance drops under corruption/occlusion, with increased hallucinations including value fabrication, trend misinterpretation, and entity confusion. Models remain overconfident in degraded settings.", "conclusion": "The paper establishes a rigorous testbed for advancing robustness in chart understanding and proposes baseline mitigation strategies like quality filtering and occlusion detection."}}
{"id": "2509.18427", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.18427", "abs": "https://arxiv.org/abs/2509.18427", "authors": ["Xinyang Wu", "Muheng Li", "Xia Li", "Orso Pusterla", "Sairos Safai", "Philippe C. Cattin", "Antony J. Lomax", "Ye Zhang"], "title": "CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction", "comment": null, "summary": "Four-dimensional MRI (4D-MRI) is an promising technique for capturing\nrespiratory-induced motion in radiation therapy planning and delivery.\nConventional 4D reconstruction methods, which typically rely on phase binning\nor separate template scans, struggle to capture temporal variability,\ncomplicate workflows, and impose heavy computational loads. We introduce a\nneural representation framework that considers respiratory motion as a smooth,\ncontinuous deformation steered by a 1D surrogate signal, completely replacing\nthe conventional discrete sorting approach. The new method fuses motion\nmodeling with image reconstruction through two synergistic networks: the\nSpatial Anatomy Network (SAN) encodes a continuous 3D anatomical\nrepresentation, while a Temporal Motion Network (TMN), guided by\nTransformer-derived respiratory signals, produces temporally consistent\ndeformation fields. Evaluation using a free-breathing dataset of 19 volunteers\ndemonstrates that our template- and phase-free method accurately captures both\nregular and irregular respiratory patterns, while preserving vessel and\nbronchial continuity with high anatomical fidelity. The proposed method\nsignificantly improves efficiency, reducing the total processing time from\napproximately five hours required by conventional discrete sorting methods to\njust 15 minutes of training. Furthermore, it enables inference of each 3D\nvolume in under one second. The framework accurately reconstructs 3D images at\nany respiratory state, achieves superior performance compared to conventional\nmethods, and demonstrates strong potential for application in 4D radiation\ntherapy planning and real-time adaptive treatment.", "AI": {"tldr": "A neural representation framework for 4D-MRI reconstruction that replaces conventional phase binning with continuous motion modeling using two networks: Spatial Anatomy Network for 3D anatomy and Temporal Motion Network for deformation fields guided by respiratory signals.", "motivation": "Conventional 4D-MRI reconstruction methods struggle with temporal variability, complicate workflows, and have heavy computational loads. They rely on phase binning or separate template scans which limit their effectiveness.", "method": "Uses two synergistic networks: Spatial Anatomy Network (SAN) encodes continuous 3D anatomical representation, and Temporal Motion Network (TMN) produces temporally consistent deformation fields guided by Transformer-derived respiratory signals. The framework treats respiratory motion as smooth, continuous deformation steered by a 1D surrogate signal.", "result": "Evaluation on 19 volunteers shows the method accurately captures regular and irregular respiratory patterns while preserving vessel and bronchial continuity. Reduces processing time from ~5 hours to 15 minutes training, with inference of each 3D volume in under 1 second. Achieves superior performance compared to conventional methods.", "conclusion": "The framework enables accurate reconstruction of 3D images at any respiratory state and demonstrates strong potential for 4D radiation therapy planning and real-time adaptive treatment applications."}}
{"id": "2509.18451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18451", "abs": "https://arxiv.org/abs/2509.18451", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony Maida"], "title": "An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects", "comment": null, "summary": "Unpredictable movement patterns and small visual mark make precise tracking\nof fast-moving tiny objects like a racquetball one of the challenging problems\nin computer vision. This challenge is particularly relevant for sport robotics\napplications, where lightweight and accurate tracking systems can improve robot\nperception and planning capabilities. While Kalman filter-based tracking\nmethods have shown success in general object tracking scenarios, their\nperformance degrades substantially when dealing with rapidly moving objects\nthat exhibit irregular bouncing behavior. In this study, we evaluate the\nperformance of five state-of-the-art Kalman filter-based tracking\nmethods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom\ndataset containing 10,000 annotated racquetball frames captured at 720p-1280p\nresolution. We focus our analysis on two critical performance factors:\ninference speed and update frequency per image, examining how these parameters\naffect tracking accuracy and reliability for fast-moving tiny objects. Our\nexperimental evaluation across four distinct scenarios reveals that DeepOCSORT\nachieves the lowest tracking error with an average ADE of 31.15 pixels compared\nto ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest\nprocessing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.\nHowever, our results show that all Kalman filter-based trackers exhibit\nsignificant tracking drift with spatial errors ranging from 3-11cm (ADE values:\n31-114 pixels), indicating fundamental limitations in handling the\nunpredictable motion patterns of fast-moving tiny objects like racquetballs.\nOur analysis demonstrates that current tracking approaches require substantial\nimprovements, with error rates 3-4x higher than standard object tracking\nbenchmarks, highlighting the need for specialized methodologies for fast-moving\ntiny object tracking applications.", "AI": {"tldr": "Evaluation of five Kalman filter-based tracking methods (OCSORT, DeepOCSORT, ByteTrack, BoTSORT, StrongSORT) for fast-moving tiny objects like racquetballs, revealing significant tracking drift and limitations in handling unpredictable motion patterns.", "motivation": "Fast-moving tiny objects like racquetballs present challenges for computer vision tracking due to unpredictable movement patterns and small visual marks, particularly relevant for sport robotics applications where lightweight and accurate tracking systems are needed.", "method": "Used a custom dataset of 10,000 annotated racquetball frames at 720p-1280p resolution to evaluate five state-of-the-art Kalman filter-based tracking methods, analyzing inference speed, update frequency, and tracking accuracy across four distinct scenarios.", "result": "DeepOCSORT achieved lowest tracking error (ADE: 31.15 pixels) while ByteTrack was fastest (26.6ms inference time), but all trackers showed significant drift with spatial errors of 3-11cm (ADE: 31-114 pixels), 3-4x higher than standard benchmarks.", "conclusion": "Current Kalman filter-based trackers have fundamental limitations for fast-moving tiny objects, requiring specialized methodologies as error rates are substantially higher than standard object tracking benchmarks."}}
{"id": "2509.18473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18473", "abs": "https://arxiv.org/abs/2509.18473", "authors": ["Binhua Huang", "Wendong Yao", "Shaowu Chen", "Guoxin Wang", "Qingyuan Wang", "Soumyabrata Dev"], "title": "MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition", "comment": "5 pages, 2 figures", "summary": "We introduce MoCrop, a motion-aware adaptive cropping module for efficient\nvideo action recognition in the compressed domain. MoCrop uses motion vectors\nthat are available in H.264 video to locate motion-dense regions and produces a\nsingle clip-level crop that is applied to all I-frames at inference. The module\nis training free, adds no parameters, and can be plugged into diverse\nbackbones. A lightweight pipeline that includes denoising & merge (DM), Monte\nCarlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix\nsearch yields robust crops with negligible overhead. On UCF101, MoCrop improves\naccuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy\nat equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer\nFLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy\nat the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6\nto 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B\nindicate strong generality and make MoCrop practical for real-time deployment\nin the compressed domain. Our code and models are available at\nhttps://github.com/microa/MoCrop.", "AI": {"tldr": "MoCrop is a motion-aware adaptive cropping module for efficient video action recognition that uses motion vectors from compressed video to identify motion-dense regions and apply training-free crops to reduce computation while maintaining accuracy.", "motivation": "To enable efficient video action recognition in the compressed domain by leveraging readily available motion vectors to reduce computational overhead without sacrificing accuracy, making it suitable for real-time deployment.", "method": "A lightweight pipeline with denoising & merge (DM), Monte Carlo sampling (MCS), and adaptive cropping (AC) via motion-density submatrix search. The module is training-free, parameter-free, and can be integrated into various backbones.", "result": "On UCF101, MoCrop improves accuracy or reduces compute: +3.5% Top-1 accuracy at equal FLOPs, or +2.4% Top-1 accuracy with 26.5% fewer FLOPs. Applied to CoViAR, it achieves 89.2% accuracy at original cost and 88.5% accuracy while reducing compute from 11.6 to 8.5 GFLOPs.", "conclusion": "MoCrop demonstrates strong generality across multiple architectures (ResNet-50, MobileNet-V3, EfficientNet-B1, Swin-B) and provides practical benefits for real-time compressed domain video analysis with consistent performance gains."}}
{"id": "2509.18481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18481", "abs": "https://arxiv.org/abs/2509.18481", "authors": ["Xinyu Wang", "Zikun Zhou", "Yingjian Li", "Xin An", "Hongpeng Wang"], "title": "Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems", "comment": null, "summary": "Coding images for machines with minimal bitrate and strong analysis\nperformance is key to effective edge-cloud systems. Several approaches deploy\nan image codec and perform analysis on the reconstructed image. Other methods\ncompress intermediate features using entropy models and subsequently perform\nanalysis on the decoded features. Nevertheless, these methods both perform\npoorly under low-bitrate conditions, as they retain many redundant details or\nlearn over-concentrated symbol distributions. In this paper, we propose a\nCodebook-based Adaptive Feature Compression framework with Semantic\nEnhancement, named CAFC-SE. It maps continuous visual features to discrete\nindices with a codebook at the edge via Vector Quantization (VQ) and\nselectively transmits them to the cloud. The VQ operation that projects feature\nvectors onto the nearest visual primitives enables us to preserve more\ninformative visual patterns under low-bitrate conditions. Hence, CAFC-SE is\nless vulnerable to low-bitrate conditions. Extensive experiments demonstrate\nthe superiority of our method in terms of rate and accuracy.", "AI": {"tldr": "CAFC-SE is a codebook-based adaptive feature compression framework with semantic enhancement that uses vector quantization to map visual features to discrete indices for efficient transmission under low-bitrate conditions.", "motivation": "Existing image compression methods perform poorly under low-bitrate conditions because they retain redundant details or learn over-concentrated symbol distributions, limiting analysis performance in edge-cloud systems.", "method": "The framework uses Vector Quantization (VQ) to map continuous visual features to discrete indices via a codebook, selectively transmitting them to the cloud while preserving informative visual patterns.", "result": "Extensive experiments show that CAFC-SE achieves superior performance in terms of both rate (compression efficiency) and accuracy (analysis performance) compared to existing methods.", "conclusion": "CAFC-SE is less vulnerable to low-bitrate conditions and provides an effective solution for coding images for machines with minimal bitrate and strong analysis performance in edge-cloud systems."}}
{"id": "2509.18493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18493", "abs": "https://arxiv.org/abs/2509.18493", "authors": ["Md Mostafijur Rahman", "Radu Marculescu"], "title": "MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation", "comment": "11 pages, 3 figures, Accepted at ICCV 2025 Workshop CVAMD", "summary": "In this paper, we introduce MK-UNet, a paradigm shift towards\nultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image\nsegmentation. Central to MK-UNet is the multi-kernel depth-wise convolution\nblock (MKDC) we design to adeptly process images through multiple kernels,\nwhile capturing complex multi-resolution spatial relationships. MK-UNet also\nemphasizes the images salient features through sophisticated attention\nmechanisms, including channel, spatial, and grouped gated attention. Our\nMK-UNet network, with a modest computational footprint of only 0.316M\nparameters and 0.314G FLOPs, represents not only a remarkably lightweight, but\nalso significantly improved segmentation solution that provides higher accuracy\nover state-of-the-art (SOTA) methods across six binary medical imaging\nbenchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with\nnearly 333$\\times$ and 123$\\times$ fewer parameters and FLOPs, respectively.\nSimilarly, when compared against UNeXt, MK-UNet exhibits superior segmentation\nperformance, improving the DICE score up to 6.7% margins while operating with\n4.7$\\times$ fewer #Params. Our MK-UNet also outperforms other recent\nlightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with\nmuch lower computational resources. This leap in performance, coupled with\ndrastic computational gains, positions MK-UNet as an unparalleled solution for\nreal-time, high-fidelity medical diagnostics in resource-limited settings, such\nas point-of-care devices. Our implementation is available at\nhttps://github.com/SLDGroup/MK-UNet.", "AI": {"tldr": "MK-UNet is an ultra-lightweight CNN architecture for medical image segmentation that achieves state-of-the-art performance with only 0.316M parameters and 0.314G FLOPs, outperforming existing methods while being significantly more computationally efficient.", "motivation": "To develop a highly efficient and accurate medical image segmentation solution suitable for resource-limited settings like point-of-care devices, addressing the need for real-time, high-fidelity diagnostics without heavy computational requirements.", "method": "Uses multi-kernel depth-wise convolution blocks (MKDC) to process images through multiple kernels while capturing complex multi-resolution spatial relationships, combined with sophisticated attention mechanisms including channel, spatial, and grouped gated attention.", "result": "MK-UNet outperforms TransUNet in DICE score with 333\u00d7 fewer parameters and 123\u00d7 fewer FLOPs, beats UNeXt with up to 6.7% higher DICE score and 4.7\u00d7 fewer parameters, and surpasses other lightweight networks like MedT, CMUNeXt, EGE-UNet, and Rolling-UNet across six binary medical imaging benchmarks.", "conclusion": "MK-UNet represents a paradigm shift in medical image segmentation, offering unparalleled performance with drastic computational efficiency, making it ideal for real-time applications in resource-constrained environments."}}
{"id": "2509.18501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18501", "abs": "https://arxiv.org/abs/2509.18501", "authors": ["Maximilian Fehrentz", "Alexander Winkler", "Thomas Heiliger", "Nazim Haouchine", "Christian Heiliger", "Nassir Navab"], "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation", "comment": "Accepted at MICCAI 2025", "summary": "We introduce BridgeSplat, a novel approach for deformable surgical navigation\nthat couples intraoperative 3D reconstruction with preoperative CT data to\nbridge the gap between surgical video and volumetric patient data. Our method\nrigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian\nparameters and mesh deformation through photometric supervision. By\nparametrizing each Gaussian relative to its parent mesh triangle, we enforce\nalignment between Gaussians and mesh and obtain deformations that can be\npropagated back to update the CT. We demonstrate BridgeSplat's effectiveness on\nvisceral pig surgeries and synthetic data of a human liver under simulation,\nshowing sensible deformations of the preoperative CT on monocular RGB data.\nCode, data, and additional resources can be found at\nhttps://maxfehrentz.github.io/ct-informed-splatting/ .", "AI": {"tldr": "BridgeSplat is a novel approach for deformable surgical navigation that couples intraoperative 3D reconstruction with preoperative CT data using 3D Gaussians rigged to CT meshes.", "motivation": "To bridge the gap between surgical video and volumetric patient data by enabling real-time deformation tracking during surgery.", "method": "Rigs 3D Gaussians to CT mesh and jointly optimizes Gaussian parameters and mesh deformation through photometric supervision, parametrizing each Gaussian relative to its parent mesh triangle.", "result": "Demonstrated effectiveness on visceral pig surgeries and synthetic human liver data, showing sensible deformations of preoperative CT on monocular RGB data.", "conclusion": "BridgeSplat successfully enables deformable surgical navigation by coupling intraoperative reconstruction with preoperative CT data through Gaussian-based deformation modeling."}}
{"id": "2509.18502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18502", "abs": "https://arxiv.org/abs/2509.18502", "authors": ["Wenjie Liu", "Hongmin Liu", "Lixin Zhang", "Bin Fan"], "title": "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment", "comment": null, "summary": "Research on unsupervised domain adaptation (UDA) for semantic segmentation of\nremote sensing images has been extensively conducted. However, research on how\nto achieve domain adaptation in practical scenarios where source domain data is\ninaccessible namely, source-free domain adaptation (SFDA) remains limited.\nSelf-training has been widely used in SFDA, which requires obtaining as many\nhigh-quality pseudo-labels as possible to train models on target domain data.\nMost existing methods optimize the entire pseudo-label set to obtain more\nsupervisory information. However, as pseudo-label sets often contain\nsubstantial noise, simultaneously optimizing all labels is challenging. This\nlimitation undermines the effectiveness of optimization approaches and thus\nrestricts the performance of self-training. To address this, we propose a novel\npseudo-label optimization framework called Diffusion-Guided Label Enrichment\n(DGLE), which starts from a few easily obtained high-quality pseudo-labels and\npropagates them to a complete set of pseudo-labels while ensuring the quality\nof newly generated labels. Firstly, a pseudo-label fusion method based on\nconfidence filtering and super-resolution enhancement is proposed, which\nutilizes cross-validation of details and contextual information to obtain a\nsmall number of high-quality pseudo-labels as initial seeds. Then, we leverage\nthe diffusion model to propagate incomplete seed pseudo-labels with irregular\ndistributions due to its strong denoising capability for randomly distributed\nnoise and powerful modeling capacity for complex distributions, thereby\ngenerating complete and high-quality pseudo-labels. This method effectively\navoids the difficulty of directly optimizing the complete set of pseudo-labels,\nsignificantly improves the quality of pseudo-labels, and thus enhances the\nmodel's performance in the target domain.", "AI": {"tldr": "DGLE is a novel pseudo-label optimization framework for source-free domain adaptation in semantic segmentation of remote sensing images, using diffusion models to propagate high-quality seed labels to generate complete, high-quality pseudo-labels.", "motivation": "Current source-free domain adaptation methods struggle with noisy pseudo-labels when optimizing entire label sets simultaneously, limiting self-training effectiveness in practical scenarios where source domain data is inaccessible.", "method": "Proposes Diffusion-Guided Label Enrichment (DGLE): 1) uses confidence filtering and super-resolution enhancement to obtain high-quality seed pseudo-labels, 2) leverages diffusion models to propagate these seeds to generate complete pseudo-labels while maintaining quality.", "result": "The method effectively avoids the difficulty of directly optimizing complete pseudo-label sets and significantly improves pseudo-label quality.", "conclusion": "DGLE enhances model performance in target domain by generating high-quality pseudo-labels through diffusion-based propagation from carefully selected seed labels."}}
{"id": "2509.18504", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18504", "abs": "https://arxiv.org/abs/2509.18504", "authors": ["Jiaxin Dai", "Xiang Xiang"], "title": "Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning", "comment": null, "summary": "In the field of machine learning, hyperbolic space demonstrates superior\nrepresentation capabilities for hierarchical data compared to conventional\nEuclidean space. This work focuses on the Coarse-To-Fine Few-Shot\nClass-Incremental Learning (C2FSCIL) task. Our study follows the Knowe\napproach, which contrastively learns coarse class labels and subsequently\nnormalizes and freezes the classifier weights of learned fine classes in the\nembedding space. To better interpret the \"coarse-to-fine\" paradigm, we propose\nembedding the feature extractor into hyperbolic space. Specifically, we employ\nthe Poincar\\'e ball model of hyperbolic space, enabling the feature extractor\nto transform input images into feature vectors within the Poincar\\'e ball\ninstead of Euclidean space. We further introduce hyperbolic contrastive loss\nand hyperbolic fully-connected layers to facilitate model optimization and\nclassification in hyperbolic space. Additionally, to enhance performance under\nfew-shot conditions, we implement maximum entropy distribution in hyperbolic\nspace to estimate the probability distribution of fine-class feature vectors.\nThis allows generation of augmented features from the distribution to mitigate\noverfitting during training with limited samples. Experiments on C2FSCIL\nbenchmarks show that our method effectively improves both coarse and fine class\naccuracies.", "AI": {"tldr": "This paper proposes using hyperbolic space embeddings for Coarse-To-Fine Few-Shot Class-Incremental Learning (C2FSCIL), showing superior performance for hierarchical data representation compared to Euclidean space.", "motivation": "Hyperbolic space offers better representation capabilities for hierarchical data than Euclidean space, making it suitable for the C2FSCIL task which involves learning coarse-to-fine class relationships with limited samples.", "method": "The approach embeds feature extractors into hyperbolic space using the Poincar\u00e9 ball model, introduces hyperbolic contrastive loss and fully-connected layers, and implements maximum entropy distribution in hyperbolic space for feature augmentation to prevent overfitting in few-shot scenarios.", "result": "Experiments on C2FSCIL benchmarks demonstrate that the proposed method effectively improves both coarse and fine class accuracies compared to traditional approaches.", "conclusion": "Hyperbolic space embedding provides a powerful framework for C2FSCIL tasks, enabling better hierarchical representation learning and improved performance in few-shot incremental learning scenarios."}}
{"id": "2509.18538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18538", "abs": "https://arxiv.org/abs/2509.18538", "authors": ["Zixin Zhu", "Haoxiang Li", "Xuelu Feng", "He Wu", "Chunming Qiao", "Junsong Yuan"], "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts", "comment": "Accepted as Spotlight at NeurIPS 2025", "summary": "Towards intelligent image editing, object removal should eliminate both the\ntarget object and its causal visual artifacts, such as shadows and reflections.\nHowever, existing image appearance-based methods either follow strictly\nmask-aligned training and fail to remove these causal effects which are not\nexplicitly masked, or adopt loosely mask-aligned strategies that lack\ncontrollability and may unintentionally over-erase other objects. We identify\nthat these limitations stem from ignoring the causal relationship between an\nobject's geometry presence and its visual effects. To address this limitation,\nwe propose a geometry-aware two-stage framework that decouples object removal\ninto (1) geometry removal and (2) appearance rendering. In the first stage, we\nremove the object directly from the geometry (e.g., depth) using strictly\nmask-aligned supervision, enabling structure-aware editing with strong\ngeometric constraints. In the second stage, we render a photorealistic RGB\nimage conditioned on the updated geometry, where causal visual effects are\nconsidered implicitly as a result of the modified 3D geometry. To guide\nlearning in the geometry removal stage, we introduce a preference-driven\nobjective based on positive and negative sample pairs, encouraging the model to\nremove objects as well as their causal visual artifacts while avoiding new\nstructural insertions. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in removing both objects and their\nassociated artifacts on two popular benchmarks. The code is available at\nhttps://github.com/buxiangzhiren/GeoRemover.", "AI": {"tldr": "A geometry-aware two-stage framework for intelligent object removal that eliminates both target objects and their causal visual artifacts (shadows, reflections) by decoupling geometry removal and appearance rendering.", "motivation": "Existing image editing methods either fail to remove causal visual artifacts not explicitly masked (strict mask-aligned) or lack controllability and may over-erase other objects (loose mask-aligned). The key limitation is ignoring the causal relationship between object geometry and visual effects.", "method": "Two-stage framework: (1) Geometry removal stage - remove object from geometry (depth) using strictly mask-aligned supervision with preference-driven objective; (2) Appearance rendering stage - generate photorealistic RGB image conditioned on updated geometry, where causal effects are implicitly handled.", "result": "Extensive experiments show state-of-the-art performance in removing both objects and associated artifacts on two popular benchmarks.", "conclusion": "The proposed geometry-aware approach effectively addresses the limitations of appearance-based methods by leveraging the causal relationship between object geometry and visual effects, achieving superior object removal with artifact elimination."}}
{"id": "2509.18546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18546", "abs": "https://arxiv.org/abs/2509.18546", "authors": ["Yujia Liu", "Dingquan Li", "Tiejun Huang"], "title": "SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models", "comment": null, "summary": "No-Reference Image Quality Assessment (NR-IQA) models play an important role\nin various real-world applications. Recently, adversarial attacks against\nNR-IQA models have attracted increasing attention, as they provide valuable\ninsights for revealing model vulnerabilities and guiding robust system design.\nSome effective attacks have been proposed against NR-IQA models in white-box\nsettings, where the attacker has full access to the target model. However,\nthese attacks often suffer from poor transferability to unknown target models\nin more realistic black-box scenarios, where the target model is inaccessible.\nThis work makes the first attempt to address the challenge of low\ntransferability in attacking NR-IQA models by proposing a transferable Signed\nEnsemble Gaussian black-box Attack (SEGA). The main idea is to approximate the\ngradient of the target model by applying Gaussian smoothing to source models\nand ensembling their smoothed gradients. To ensure the imperceptibility of\nadversarial perturbations, SEGA further removes inappropriate perturbations\nusing a specially designed perturbation filter mask. Experimental results on\nthe CLIVE dataset demonstrate the superior transferability of SEGA, validating\nits effectiveness in enabling successful transfer-based black-box attacks\nagainst NR-IQA models.", "AI": {"tldr": "This paper proposes SEGA, a transferable black-box attack method for NR-IQA models that uses Gaussian smoothing and ensemble techniques to improve attack transferability across different models.", "motivation": "Existing adversarial attacks against NR-IQA models work well in white-box settings but have poor transferability to unknown target models in realistic black-box scenarios, limiting their practical applicability.", "method": "SEGA approximates the target model's gradient by applying Gaussian smoothing to source models and ensembling their smoothed gradients, with a perturbation filter mask to ensure imperceptibility of adversarial perturbations.", "result": "Experimental results on the CLIVE dataset demonstrate SEGA's superior transferability compared to existing methods, enabling successful black-box attacks against NR-IQA models.", "conclusion": "SEGA represents the first effective solution for transferable black-box attacks on NR-IQA models, providing valuable insights for robust system design by revealing model vulnerabilities."}}
{"id": "2509.18550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18550", "abs": "https://arxiv.org/abs/2509.18550", "authors": ["Mohammad Junayed Hasan", "Nabeel Mohammed", "Shafin Rahman", "Philipp Koehn"], "title": "HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025.\n  Final version to appear in the conference proceedings", "summary": "The distinction between genuine and posed emotions represents a fundamental\npattern recognition challenge with significant implications for data mining\napplications in social sciences, healthcare, and human-computer interaction.\nWhile recent multi-task learning frameworks have shown promise in combining\ndeep learning architectures with handcrafted D-Marker features for smile facial\nemotion recognition, these approaches exhibit computational inefficiencies due\nto auxiliary task supervision and complex loss balancing requirements. This\npaper introduces HadaSmileNet, a novel feature fusion framework that directly\nintegrates transformer-based representations with physiologically grounded\nD-Markers through parameter-free multiplicative interactions. Through\nsystematic evaluation of 15 fusion strategies, we demonstrate that Hadamard\nmultiplicative fusion achieves optimal performance by enabling direct feature\ninteractions while maintaining computational efficiency. The proposed approach\nestablishes new state-of-the-art results for deep learning methods across four\nbenchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS\n(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational\nanalysis reveals 26 percent parameter reduction and simplified training\ncompared to multi-task alternatives, while feature visualization demonstrates\nenhanced discriminative power through direct domain knowledge integration. The\nframework's efficiency and effectiveness make it particularly suitable for\npractical deployment in multimedia data mining applications that require\nreal-time affective computing capabilities.", "AI": {"tldr": "HadaSmileNet introduces a parameter-free Hadamard multiplicative fusion framework that combines transformer representations with D-Marker features for smile emotion recognition, achieving state-of-the-art results with 26% parameter reduction.", "motivation": "Existing multi-task learning approaches for smile emotion recognition suffer from computational inefficiencies due to auxiliary task supervision and complex loss balancing requirements.", "method": "Direct integration of transformer-based representations with physiologically grounded D-Markers through parameter-free multiplicative interactions, with systematic evaluation of 15 fusion strategies.", "result": "Achieved new state-of-the-art results: UvA-NEMO (88.7%, +0.8), MMI (99.7%), SPOS (98.5%, +0.7), BBC (100%, +5.0), with 26% parameter reduction and simplified training.", "conclusion": "The framework's efficiency and effectiveness make it suitable for practical deployment in multimedia data mining applications requiring real-time affective computing capabilities."}}
{"id": "2509.18566", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18566", "abs": "https://arxiv.org/abs/2509.18566", "authors": ["Xiaoting Yin", "Hao Shi", "Kailun Yang", "Jiajun Zhai", "Shangwei Guo", "Lin Wang", "Kaiwei Wang"], "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction", "comment": null, "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "AI": {"tldr": "A novel event-guided framework for joint human-scene reconstruction from monocular event cameras using 3D Gaussian Splatting, addressing motion blur through event-stream matching.", "motivation": "Reconstructing dynamic humans with static scenes from monocular videos is challenging under fast motion due to motion blur in RGB frames. Event cameras offer superior temporal resolution for dynamic human reconstruction.", "method": "Uses unified 3D Gaussians with learnable semantic attributes; human Gaussians undergo deformation while scene Gaussians remain static. Introduces event-guided loss matching simulated brightness changes with event streams to combat blur.", "result": "Achieves state-of-the-art human-scene reconstruction on ZJU-MoCap-Blur and MMHPSD-Blur datasets, with significant improvements in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "conclusion": "The approach eliminates need for external human masks and simplifies Gaussian set management, demonstrating superior performance for dynamic human reconstruction under fast motion conditions."}}
{"id": "2509.18571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18571", "abs": "https://arxiv.org/abs/2509.18571", "authors": ["Yuhan Wang", "Cheng Liu", "Zihan Zhao", "Weichao Wu"], "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought", "comment": null, "summary": "Real-time threat monitoring identifies threatening behaviors in video streams\nand provides reasoning and assessment of threat events through explanatory\ntext. However, prevailing methodologies, whether based on supervised learning\nor generative models, struggle to concurrently satisfy the demanding\nrequirements of real-time performance and decision explainability. To bridge\nthis gap, we introduce Live-E2T, a novel framework that unifies these two\nobjectives through three synergistic mechanisms. First, we deconstruct video\nframes into structured Human-Object-Interaction-Place semantic tuples. This\napproach creates a compact, semantically focused representation, circumventing\nthe information degradation common in conventional feature compression. Second,\nan efficient online event deduplication and updating mechanism is proposed to\nfilter spatio-temporal redundancies, ensuring the system's real time\nresponsiveness. Finally, we fine-tune a Large Language Model using a\nChain-of-Thought strategy, endow it with the capability for transparent and\nlogical reasoning over event sequences to produce coherent threat assessment\nreports. Extensive experiments on benchmark datasets, including XD-Violence and\nUCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art\nmethods in terms of threat detection accuracy, real-time efficiency, and the\ncrucial dimension of explainability.", "AI": {"tldr": "Live-E2T is a novel framework for real-time threat monitoring that addresses the trade-off between performance and explainability through semantic tuple decomposition, online event deduplication, and LLM-based reasoning.", "motivation": "Existing threat monitoring methods struggle to simultaneously achieve real-time performance and decision explainability, creating a gap that needs to be bridged for practical applications.", "method": "The framework uses three mechanisms: 1) Deconstructing video frames into Human-Object-Interaction-Place semantic tuples for compact representation, 2) Online event deduplication to filter redundancies, and 3) Fine-tuning LLMs with Chain-of-Thought for transparent reasoning over event sequences.", "result": "Extensive experiments on XD-Violence and UCF-Crime datasets show Live-E2T significantly outperforms state-of-the-art methods in threat detection accuracy, real-time efficiency, and explainability.", "conclusion": "Live-E2T successfully unifies real-time performance and explainability requirements through its synergistic mechanisms, providing a practical solution for real-time threat monitoring with transparent reasoning."}}
{"id": "2509.18582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18582", "abs": "https://arxiv.org/abs/2509.18582", "authors": ["Daiqing Qi", "Handong Zhao", "Jing Shi", "Simon Jenni", "Yifei Fan", "Franck Dernoncourt", "Scott Cohen", "Sheng Li"], "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers", "comment": null, "summary": "While editing directly from life, photographers have found it too difficult\nto see simultaneously both the blue and the sky. Photographer and curator,\nSzarkowski insightfully revealed one of the notable gaps between general and\naesthetic visual understanding: while the former focuses on identifying the\nfactual element in an image (sky), the latter transcends such object\nidentification, viewing it instead as an aesthetic component--a pure color\nblock (blue). Such fundamental distinctions between general (detection,\nlocalization, etc.) and aesthetic (color, lighting, composition, etc.) visual\nunderstanding present a significant challenge for Multimodal Large Language\nModels (MLLMs). Although some recent works have made initial explorations, they\nare often limited to general and basic aesthetic commonsense. As a result, they\nfrequently fall short in real-world scenarios (Fig. 1), which require extensive\nexpertise--including photographic techniques, photo pre/post-processing\nknowledge, and more, to provide a detailed analysis and description. To\nfundamentally enhance the aesthetics understanding of MLLMs, we first introduce\na novel dataset, PhotoCritique, derived from extensive discussions among\nprofessional photographers and enthusiasts, and characterized by the large\nscale, expertise, and diversity. Then, to better learn visual aesthetics from\nPhotoCritique, we furthur propose a novel model, PhotoEye, featuring a\nlanguageguided multi-view vision fusion mechanism to understand image\naesthetics from multiple perspectives. Finally, we present a novel benchmark,\nPhotoBench, a comprehensive and professional benchmark for aesthetic visual\nunderstanding. On existing benchmarks and PhotoBench, our model demonstrates\nclear advantages over existing models.", "AI": {"tldr": "This paper addresses the gap between general and aesthetic visual understanding in MLLMs by introducing PhotoCritique dataset, PhotoEye model with multi-view vision fusion, and PhotoBench benchmark for professional aesthetic evaluation.", "motivation": "Current MLLMs struggle with aesthetic visual understanding beyond basic object identification, lacking expertise in photographic techniques and professional analysis required for real-world scenarios.", "method": "Created PhotoCritique dataset from professional photographer discussions, developed PhotoEye model with language-guided multi-view vision fusion, and established PhotoBench benchmark for comprehensive aesthetic evaluation.", "result": "The proposed model demonstrates clear advantages over existing models on both existing benchmarks and the new PhotoBench benchmark.", "conclusion": "The work fundamentally enhances MLLMs' aesthetic understanding through expert-curated data, specialized model architecture, and professional evaluation framework."}}
{"id": "2509.18591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18591", "abs": "https://arxiv.org/abs/2509.18591", "authors": ["Pengchao Deng", "Shengqi Chen"], "title": "Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network", "comment": null, "summary": "This paper presents an advanced tumor segmentation framework for real-time\nMRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method\nleverages the XMem model, a memory-augmented architecture, to segment tumors\nacross long cine-MRI sequences. The proposed system efficiently integrates\nmemory mechanisms to track tumor motion in real-time, achieving high\nsegmentation accuracy even under challenging conditions with limited annotated\ndata. Unfortunately, the detailed experimental records have been lost,\npreventing us from reporting precise quantitative results at this stage.\nNevertheless, From our preliminary impressions during development, the\nXMem-based framework demonstrated reasonable segmentation performance and\nsatisfied the clinical real-time requirement. Our work contributes to improving\nthe precision of tumor tracking during MRI-guided radiotherapy, which is\ncrucial for enhancing the accuracy and safety of cancer treatments.", "AI": {"tldr": "An XMem-based tumor segmentation framework for real-time MRI-guided radiotherapy that achieves reasonable performance and meets clinical real-time requirements, though detailed experimental results are unavailable.", "motivation": "To improve precision of tumor tracking during MRI-guided radiotherapy for enhanced accuracy and safety of cancer treatments, addressing the TrackRAD2025 challenge.", "method": "Leverages XMem model with memory-augmented architecture to segment tumors across long cine-MRI sequences, integrating memory mechanisms for real-time tumor motion tracking.", "result": "Preliminary impressions indicate reasonable segmentation performance and satisfaction of clinical real-time requirements, though precise quantitative results are unavailable due to lost experimental records.", "conclusion": "The XMem-based framework contributes to improving tumor tracking precision in MRI-guided radiotherapy, demonstrating potential for clinical applications despite incomplete quantitative validation."}}
{"id": "2509.18593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18593", "abs": "https://arxiv.org/abs/2509.18593", "authors": ["Xiaoman Wu", "Lubin Gan", "Siying Wu", "Jing Zhang", "Yunwei Ou", "Xiaoyan Sun"], "title": "SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution", "comment": null, "summary": "Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims\nto enhance low-resolution (LR) contrasts leveraging high-resolution (HR)\nreferences, shortening acquisition time and improving imaging efficiency while\npreserving anatomical details. The main challenge lies in maintaining\nspatial-semantic consistency, ensuring anatomical structures remain\nwell-aligned and coherent despite structural discrepancies and motion between\nthe target and reference images. Conventional methods insufficiently model\nspatial-semantic consistency and underuse frequency-domain information, which\nleads to poor fine-grained alignment and inadequate recovery of high-frequency\ndetails. In this paper, we propose the Spatial-Semantic Consistent Model\n(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast\nspatial alignment, a Semantic-Aware Token Aggregation Block for long-range\nsemantic consistency, and a Spatial-Frequency Fusion Block for fine structure\nrestoration. Experiments on public and private datasets show that SSCM achieves\nstate-of-the-art performance with fewer parameters while ensuring spatially and\nsemantically consistent reconstructions.", "AI": {"tldr": "SSCM is a novel model for multi-contrast MRI super-resolution that enhances low-resolution images using high-resolution references while maintaining spatial-semantic consistency through dynamic warping, semantic token aggregation, and spatial-frequency fusion.", "motivation": "Current MC-MRI SR methods insufficiently model spatial-semantic consistency and underutilize frequency-domain information, leading to poor alignment and inadequate recovery of high-frequency details.", "method": "Proposes Spatial-Semantic Consistent Model (SSCM) with three key components: Dynamic Spatial Warping Module for inter-contrast alignment, Semantic-Aware Token Aggregation Block for long-range consistency, and Spatial-Frequency Fusion Block for fine structure restoration.", "result": "Experiments on public and private datasets show SSCM achieves state-of-the-art performance with fewer parameters while ensuring spatially and semantically consistent reconstructions.", "conclusion": "SSCM effectively addresses the challenges of MC-MRI SR by integrating spatial alignment, semantic consistency, and frequency-domain information for superior super-resolution results."}}
{"id": "2509.18600", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18600", "abs": "https://arxiv.org/abs/2509.18600", "authors": ["Zhuoxiao Chen", "Hongyang Yu", "Ying Xu", "Yadan Luo", "Long Duong", "Yuan-Fang Li"], "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation", "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce clinically\nfaithful reports from chest X-ray images. Prevailing work typically follows a\nscale-driven paradigm, by multi-stage training over large paired corpora and\noversized backbones, making pipelines highly data- and compute-intensive. In\nthis paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based\nreward (FactS) to tackle the RRG task under constrained budgets. OraPO enables\nsingle-stage, RL-only training by converting failed GRPO explorations on rare\nor difficult studies into direct preference supervision via a lightweight\noracle step. FactS grounds learning in diagnostic evidence by extracting atomic\nclinical facts and checking entailment against ground-truth labels, yielding\ndense, interpretable sentence-level rewards. Together, OraPO and FactS create a\ncompact and powerful framework that significantly improves learning efficiency\non clinically challenging cases, setting the new SOTA performance on the\nCheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training\ndata using a small base VLM on modest hardware.", "AI": {"tldr": "OraPO with FactScore-based reward enables efficient radiology report generation using single-stage RL training and lightweight oracle supervision, achieving SOTA performance with significantly reduced data and computational requirements.", "motivation": "To address the data- and compute-intensive nature of prevailing radiology report generation methods that rely on multi-stage training over large corpora and oversized backbones, making them impractical under constrained budgets.", "method": "Proposes Oracle-educated GRPO (OraPO) with FactScore-based reward (FactS) for single-stage RL-only training, converting failed explorations into direct preference supervision via lightweight oracle, and using atomic clinical fact extraction with entailment checking for dense sentence-level rewards.", "result": "Achieves new SOTA performance on CheXpert Plus dataset (0.341 F1) with 2-3 orders of magnitude less training data using a small base VLM on modest hardware.", "conclusion": "OraPO and FactS create a compact and powerful framework that significantly improves learning efficiency on clinically challenging cases while maintaining high performance with minimal resource requirements."}}
{"id": "2509.18602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18602", "abs": "https://arxiv.org/abs/2509.18602", "authors": ["Xu Liu", "Yibo Lu", "Xinxian Wang", "Xinyu Wu"], "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation", "comment": "Accepted at ACPR 2025 (oral)", "summary": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based\ntraining-free framework that enables controllable fusion of multiple reference\nstyles in diffusion models. Most of the existing reference-based methods are\nlimited by (a) acceptance of only one style image, thus prohibiting hybrid\naesthetics and scalability to more styles, and (b) lack of a principled\nmechanism to balance several stylistic influences. AMSF mitigates these\nchallenges by encoding all style images and textual hints with a semantic token\ndecomposition module that is adaptively injected into every cross-attention\nlayer of an frozen diffusion model. A similarity-aware re-weighting module then\nrecalibrates, at each denoising step, the attention allocated to every style\ncomponent, yielding balanced and user-controllable blends without any\nfine-tuning or external adapters. Both qualitative and quantitative evaluations\nshow that AMSF produces multi-style fusion results that consistently outperform\nthe state-of-the-art approaches, while its fusion design scales seamlessly to\ntwo or more styles. These capabilities position AMSF as a practical step toward\nexpressive multi-style generation in diffusion models.", "AI": {"tldr": "AMSF is a training-free framework for controllable fusion of multiple reference styles in diffusion models, enabling hybrid aesthetics without fine-tuning.", "motivation": "Existing reference-based methods are limited to single style images and lack principled mechanisms to balance multiple stylistic influences, preventing hybrid aesthetics and scalability.", "method": "Encodes all style images and textual hints with semantic token decomposition, adaptively injected into cross-attention layers. Uses similarity-aware re-weighting to recalibrate attention to each style component at every denoising step.", "result": "Qualitative and quantitative evaluations show AMSF consistently outperforms state-of-the-art approaches and scales seamlessly to two or more styles.", "conclusion": "AMSF represents a practical step toward expressive multi-style generation in diffusion models with balanced and user-controllable blends."}}
{"id": "2509.18613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18613", "abs": "https://arxiv.org/abs/2509.18613", "authors": ["Yuzhi Wu", "Li Xiao", "Jun Liu", "Guangfeng Jiang", "XiangGen Xia"], "title": "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving", "comment": null, "summary": "The emerging 4D millimeter-wave radar, measuring the range, azimuth,\nelevation, and Doppler velocity of objects, is recognized for its\ncost-effectiveness and robustness in autonomous driving. Nevertheless, its\npoint clouds exhibit significant sparsity and noise, restricting its standalone\napplication in 3D object detection. Recent 4D radar-camera fusion methods have\nprovided effective perception. Most existing approaches, however, adopt\nexplicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera\nfusion, neglecting radar's inherent drawbacks. Specifically, they overlook the\nsparse and incomplete geometry of radar point clouds and restrict fusion to\ncoarse scene-level integration. To address these problems, we propose\nMLF-4DRCNet, a novel two-stage framework for 3D object detection via\nmulti-level fusion of 4D radar and camera images. Our model incorporates the\npoint-, scene-, and proposal-level multi-modal information, enabling\ncomprehensive feature representation. It comprises three crucial components:\nthe Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion\nPooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.\nOperating at the point-level, ERPE densities radar point clouds with 2D image\ninstances and encodes them into voxels via the proposed Triple-Attention Voxel\nFeature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D\nimage features using deformable attention to capture scene context and adopts\npooling to the fused features. PLFE refines region proposals by fusing image\nfeatures, and further integrates with the pooled features from HSFP.\nExperimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets\ndemonstrate that MLF-4DRCNet achieves the state-of-the-art performance.\nNotably, it attains performance comparable to LiDAR-based models on the VoD\ndataset.", "AI": {"tldr": "MLF-4DRCNet is a novel two-stage framework for 3D object detection that performs multi-level fusion of 4D radar and camera data, addressing limitations of existing radar-camera fusion methods.", "motivation": "Existing 4D radar-camera fusion methods adopt LiDAR-camera fusion paradigms that overlook radar's sparse and noisy point clouds, restricting fusion to coarse scene-level integration.", "method": "A two-stage framework with three modules: Enhanced Radar Point Encoder (ERPE) for point-level fusion, Hierarchical Scene Fusion Pooling (HSFP) for scene-level fusion, and Proposal-Level Fusion Enhancement (PLFE) for proposal refinement.", "result": "Achieves state-of-the-art performance on View-of-Delft and TJ4DRadSet datasets, with performance comparable to LiDAR-based models on VoD dataset.", "conclusion": "The multi-level fusion approach effectively addresses radar's inherent drawbacks and enables comprehensive feature representation for improved 3D object detection."}}
{"id": "2509.18619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18619", "abs": "https://arxiv.org/abs/2509.18619", "authors": ["Yichen Wu", "Xu Liu", "Chenxuan Zhao", "Xinyu Wu"], "title": "Prompt-Guided Dual Latent Steering for Inversion Problems", "comment": "Accepted at DICTA 2025 (oral)", "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.", "AI": {"tldr": "PDLS is a training-free framework that uses dual latent steering to improve image inversion in diffusion models, balancing structural fidelity and semantic accuracy without per-image optimization.", "motivation": "Current single-latent vector methods struggle to balance structural fidelity with semantic accuracy, leading to semantic drift issues like blurred details or incorrect attributes in reconstructed images.", "method": "PDLS decomposes inversion into structural and semantic paths, formulates dual guidance as an optimal control problem, and uses Linear Quadratic Regulator (LQR) to dynamically steer the generative trajectory at each step.", "result": "Extensive experiments on FFHQ-1K and ImageNet-1K show PDLS produces more faithful reconstructions that better preserve original image details while aligning with semantic information compared to single-latent baselines.", "conclusion": "PDLS effectively prevents semantic drift while preserving fine details across various inversion tasks including deblurring, super-resolution, and inpainting, without requiring costly per-image optimization."}}
{"id": "2509.18638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18638", "abs": "https://arxiv.org/abs/2509.18638", "authors": ["Yiwei Lyu", "Samir Harake", "Asadur Chowdury", "Soumyanil Banerjee", "Rachel Gologorsky", "Shixuan Liu", "Anna-Katharina Meissner", "Akshay Rao", "Chenhui Zhao", "Akhil Kondepudi", "Cheng Jiang", "Xinhai Hou", "Rushikesh S. Joshi", "Volker Neuschmelting", "Ashok Srinivasan", "Dawn Kleindorfer", "Brian Athey", "Vikas Gulani", "Aditya Pandey", "Honglak Lee", "Todd Hollon"], "title": "Learning neuroimaging models from health system-scale data", "comment": null, "summary": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological\ndiseases. The global demand for magnetic resonance imaging (MRI) studies has\nrisen steadily, placing significant strain on health systems, prolonging\nturnaround times, and intensifying physician burnout \\cite{Chen2017-bt,\nRula2024-qp-1}. These challenges disproportionately impact patients in\nlow-resource and rural settings. Here, we utilized a large academic health\nsystem as a data engine to develop Prima, the first vision language model (VLM)\nserving as an AI foundation for neuroimaging that supports real-world, clinical\nMRI studies as input. Trained on over 220,000 MRI studies, Prima uses a\nhierarchical vision architecture that provides general and transferable MRI\nfeatures. Prima was tested in a 1-year health system-wide study that included\n30K MRI studies. Across 52 radiologic diagnoses from the major neurologic\ndisorders, including neoplastic, inflammatory, infectious, and developmental\nlesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,\noutperforming other state-of-the-art general and medical AI models. Prima\noffers explainable differential diagnoses, worklist priority for radiologists,\nand clinical referral recommendations across diverse patient demographics and\nMRI systems. Prima demonstrates algorithmic fairness across sensitive groups\nand can help mitigate health system biases, such as prolonged turnaround times\nfor low-resource populations. These findings highlight the transformative\npotential of health system-scale VLMs and Prima's role in advancing AI-driven\nhealthcare.", "AI": {"tldr": "Prima is a vision language model for neuroimaging that uses hierarchical vision architecture trained on 220,000 MRI studies, achieving 92.0 mean AUC across 52 neurological diagnoses and outperforming state-of-the-art models.", "motivation": "Addressing the growing demand for MRI studies that strains health systems, prolongs turnaround times, and disproportionately affects low-resource populations.", "method": "Developed Prima using a large academic health system as data engine, trained on over 220,000 MRI studies with hierarchical vision architecture for general and transferable MRI features.", "result": "In a 1-year health system-wide study with 30K MRI studies, Prima achieved mean diagnostic AUC of 92.0 across 52 radiologic diagnoses, outperforming other AI models while demonstrating algorithmic fairness across diverse demographics.", "conclusion": "Prima demonstrates transformative potential for health system-scale VLMs in advancing AI-driven healthcare by providing explainable diagnoses, worklist prioritization, and mitigating health system biases."}}
{"id": "2509.18639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18639", "abs": "https://arxiv.org/abs/2509.18639", "authors": ["Yuanhuiyi Lyu", "Chi Kit Wong", "Chenfei Liao", "Lutao Jiang", "Xu Zheng", "Zexin Lu", "Linfeng Zhang", "Xuming Hu"], "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation", "comment": null, "summary": "Recent works have made notable advancements in enhancing unified models for\ntext-to-image generation through the Chain-of-Thought (CoT). However, these\nreasoning methods separate the processes of understanding and generation, which\nlimits their ability to guide the reasoning of unified models in addressing the\ndeficiencies of their generative capabilities. To this end, we propose a novel\nreasoning framework for unified models, Understanding-in-Generation (UiG),\nwhich harnesses the robust understanding capabilities of unified models to\nreinforce their performance in image generation. The core insight of our UiG is\nto integrate generative guidance by the strong understanding capabilities\nduring the reasoning process, thereby mitigating the limitations of generative\nabilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse\nunderstanding into the generation process. Initially, we verify the generated\nimage and incorporate the understanding of unified models into the editing\ninstructions. Subsequently, we enhance the generated image step by step,\ngradually infusing the understanding into the generation process. Our UiG\nframework demonstrates a significant performance improvement in text-to-image\ngeneration over existing text-to-image reasoning methods, e.g., a 3.92% gain on\nthe long prompt setting of the TIIF benchmark. The project code:\nhttps://github.com/QC-LY/UiG", "AI": {"tldr": "UiG is a novel reasoning framework that integrates understanding capabilities into the generation process for text-to-image models, using image editing as a bridge to enhance generation quality step by step.", "motivation": "Current Chain-of-Thought methods separate understanding and generation processes, limiting their ability to guide unified models in addressing generative deficiencies. There's a need to leverage strong understanding capabilities to reinforce image generation performance.", "method": "Proposes Understanding-in-Generation (UiG) framework that uses image editing as a bridge. It verifies generated images, incorporates model understanding into editing instructions, and enhances images step by step to infuse understanding into generation.", "result": "Significant performance improvement in text-to-image generation, achieving 3.92% gain on the long prompt setting of TIIF benchmark compared to existing reasoning methods.", "conclusion": "UiG effectively integrates understanding capabilities into the generation process, demonstrating superior performance over traditional separated reasoning approaches for unified text-to-image models."}}
{"id": "2509.18642", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18642", "abs": "https://arxiv.org/abs/2509.18642", "authors": ["Nicolas Toussaint", "Emanuele Colleoni", "Ricardo Sanchez-Matilla", "Joshua Sutcliffe", "Vanessa Thompson", "Muhammad Asad", "Imanol Luengo", "Danail Stoyanov"], "title": "Zero-shot Monocular Metric Depth for Endoscopic Images", "comment": "Accepted at MICCAI 2025 DEMI Workshop", "summary": "Monocular relative and metric depth estimation has seen a tremendous boost in\nthe last few years due to the sharp advancements in foundation models and in\nparticular transformer based networks. As we start to see applications to the\ndomain of endoscopic images, there is still a lack of robust benchmarks and\nhigh-quality datasets in that area. This paper addresses these limitations by\npresenting a comprehensive benchmark of state-of-the-art (metric and relative)\ndepth estimation models evaluated on real, unseen endoscopic images, providing\ncritical insights into their generalisation and performance in clinical\nscenarios. Additionally, we introduce and publish a novel synthetic dataset\n(EndoSynth) of endoscopic surgical instruments paired with ground truth metric\ndepth and segmentation masks, designed to bridge the gap between synthetic and\nreal-world data. We demonstrate that fine-tuning depth foundation models using\nour synthetic dataset boosts accuracy on most unseen real data by a significant\nmargin. By providing both a benchmark and a synthetic dataset, this work\nadvances the field of depth estimation for endoscopic images and serves as an\nimportant resource for future research. Project page, EndoSynth dataset and\ntrained weights are available at https://github.com/TouchSurgery/EndoSynth.", "AI": {"tldr": "This paper presents a benchmark for depth estimation models on endoscopic images and introduces EndoSynth, a synthetic dataset that improves model performance when used for fine-tuning.", "motivation": "There is a lack of robust benchmarks and high-quality datasets for depth estimation in endoscopic images, limiting the application of advanced foundation models in clinical scenarios.", "method": "The authors created a comprehensive benchmark of state-of-the-art depth estimation models evaluated on real endoscopic images, and developed EndoSynth - a synthetic dataset with ground truth metric depth and segmentation masks for surgical instruments.", "result": "Fine-tuning depth foundation models using the EndoSynth synthetic dataset significantly boosts accuracy on most unseen real endoscopic data.", "conclusion": "This work advances depth estimation for endoscopic images by providing both a benchmark and synthetic dataset, serving as an important resource for future research in this domain."}}
{"id": "2509.18683", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18683", "abs": "https://arxiv.org/abs/2509.18683", "authors": ["Lanhu Wu", "Zilin Gao", "Hao Fei", "Mong-Li Lee", "Wynne Hsu"], "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection", "comment": "Accepted to ACM MM 2025", "summary": "RGB-D salient object detection (SOD) aims to identify the most conspicuous\nobjects in a scene with the incorporation of depth cues. Existing methods\nmainly rely on CNNs, limited by the local receptive fields, or Vision\nTransformers that suffer from the cost of quadratic complexity, posing a\nchallenge in balancing performance and computational efficiency. Recently,\nstate space models (SSM), Mamba, have shown great potential for modeling\nlong-range dependency with linear complexity. However, directly applying SSM to\nRGB-D SOD may lead to deficient local semantics as well as the inadequate\ncross-modality fusion. To address these issues, we propose a Local Emphatic and\nAdaptive Fusion state space model (LEAF-Mamba) that contains two novel\ncomponents: 1) a local emphatic state space module (LE-SSM) to capture\nmulti-scale local dependencies for both modalities. 2) an SSM-based adaptive\nfusion module (AFM) for complementary cross-modality interaction and reliable\ncross-modality integration. Extensive experiments demonstrate that the\nLEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in\nboth efficacy and efficiency. Moreover, our method can achieve excellent\nperformance on the RGB-T SOD task, proving a powerful generalization ability.", "AI": {"tldr": "LEAF-Mamba: A state space model for RGB-D salient object detection that combines local emphatic SSM for multi-scale local dependencies and adaptive fusion module for cross-modality integration, achieving superior performance and efficiency.", "motivation": "Existing RGB-D SOD methods using CNNs have limited receptive fields, while Vision Transformers suffer from quadratic complexity. State space models like Mamba show promise for long-range dependency modeling with linear complexity, but direct application leads to deficient local semantics and inadequate cross-modality fusion.", "method": "Proposes LEAF-Mamba with two components: 1) Local Emphatic State Space Module (LE-SSM) to capture multi-scale local dependencies for both RGB and depth modalities, 2) SSM-based Adaptive Fusion Module (AFM) for complementary cross-modality interaction and reliable integration.", "result": "Extensive experiments show LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in both efficacy and efficiency. The method also achieves excellent performance on RGB-T SOD task, demonstrating strong generalization ability.", "conclusion": "The proposed LEAF-Mamba effectively addresses the limitations of existing methods by combining the strengths of state space models for efficient long-range dependency modeling with specialized modules for local semantics and cross-modality fusion, achieving superior performance across multiple SOD tasks."}}
{"id": "2509.18692", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18692", "abs": "https://arxiv.org/abs/2509.18692", "authors": ["Xinle Gao", "Linghui Ye", "Zhiyong Xiao"], "title": "Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification", "comment": null, "summary": "With the rapid development of society and continuous advances in science and\ntechnology, the food industry increasingly demands higher production quality\nand efficiency. Food image classification plays a vital role in enabling\nautomated quality control on production lines, supporting food safety\nsupervision, and promoting intelligent agricultural production. However, this\ntask faces challenges due to the large number of parameters and high\ncomputational complexity of Vision Transformer models. To address these issues,\nwe propose a lightweight food image classification algorithm that integrates a\nWindow Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism\n(SAM). The WMHAM reduces computational cost by capturing local and global\ncontextual features through efficient window partitioning, while the SAM\nadaptively emphasizes key spatial regions to improve discriminative feature\nrepresentation. Experiments conducted on the Food-101 and Vireo Food-172\ndatasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,\nrespectively, while significantly reducing parameters and FLOPs compared with\nbaseline methods. These results confirm that the proposed approach achieves an\neffective balance between computational efficiency and classification\nperformance, making it well-suited for deployment in resource-constrained\nenvironments.", "AI": {"tldr": "A lightweight food image classification algorithm combining Window Multi-Head Attention Mechanism and Spatial Attention Mechanism to reduce computational complexity while maintaining high accuracy.", "motivation": "The food industry demands higher production quality and efficiency, requiring automated quality control. However, Vision Transformer models face challenges due to large parameters and high computational complexity, limiting deployment in resource-constrained environments.", "method": "Proposed algorithm integrates Window Multi-Head Attention Mechanism (WMHAM) to capture local and global contextual features through efficient window partitioning, and Spatial Attention Mechanism (SAM) to adaptively emphasize key spatial regions for improved discriminative feature representation.", "result": "Achieved accuracies of 95.24% on Food-101 and 94.33% on Vireo Food-172 datasets, while significantly reducing parameters and FLOPs compared to baseline methods.", "conclusion": "The approach effectively balances computational efficiency and classification performance, making it suitable for deployment in resource-constrained environments in the food industry."}}
{"id": "2509.18693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18693", "abs": "https://arxiv.org/abs/2509.18693", "authors": ["Siyi Chen", "Kai Wang", "Weicong Pang", "Ruiming Yang", "Ziru Chen", "Renjun Gao", "Alexis Kai Hon Lau", "Dasa Gu", "Chenchen Zhang", "Cheng Li"], "title": "OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery", "comment": "Project is available at\n  https://anonymous.4open.science/r/openset_remotesensing_tagging-2B5F/README.md", "summary": "Open-set land-cover analysis in remote sensing requires the ability to\nachieve fine-grained spatial localization and semantically open categorization.\nThis involves not only detecting and segmenting novel objects without\ncategorical supervision but also assigning them interpretable semantic labels\nthrough multimodal reasoning. In this study, we introduce OSDA, an integrated\nthree-stage framework for annotation-free open-set land-cover discovery,\nsegmentation, and description. The pipeline consists of: (1) precise discovery\nand mask extraction with a promptable fine-tuned segmentation model (SAM), (2)\nsemantic attribution and contextual description via a two-phase fine-tuned\nmultimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring\nof the MLLMs evaluation. By combining pixel-level accuracy with high-level\nsemantic understanding, OSDA addresses key challenges in open-world remote\nsensing interpretation. Designed to be architecture-agnostic and label-free,\nthe framework supports robust evaluation across diverse satellite imagery\nwithout requiring manual annotation. Our work provides a scalable and\ninterpretable solution for dynamic land-cover monitoring, showing strong\npotential for automated cartographic updating and large-scale earth observation\nanalysis.", "AI": {"tldr": "OSDA is a three-stage framework for open-set land-cover analysis in remote sensing that enables annotation-free discovery, segmentation, and description of novel objects using SAM and multimodal LLMs.", "motivation": "Open-set land-cover analysis requires fine-grained spatial localization and semantically open categorization to detect and segment novel objects without categorical supervision, while assigning interpretable semantic labels through multimodal reasoning.", "method": "A three-stage pipeline: (1) precise discovery and mask extraction with fine-tuned SAM, (2) semantic attribution via fine-tuned MLLM, (3) LLM-as-judge and manual scoring for evaluation. The framework is architecture-agnostic and label-free.", "result": "The framework provides pixel-level accuracy with high-level semantic understanding, addressing key challenges in open-world remote sensing interpretation without requiring manual annotation.", "conclusion": "OSDA offers a scalable and interpretable solution for dynamic land-cover monitoring, showing strong potential for automated cartographic updating and large-scale earth observation analysis."}}
{"id": "2509.18697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18697", "abs": "https://arxiv.org/abs/2509.18697", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2021: cross-domain plant identification", "comment": "15 pages, 6 figures, CLEF 2021 Conference and Labs of the Evaluation\n  Forum, September 21 to 24, 2021, Bucharest, Romania", "summary": "Automated plant identification has improved considerably thanks to recent\nadvances in deep learning and the availability of training data with more and\nmore field photos. However, this profusion of data concerns only a few tens of\nthousands of species, mainly located in North America and Western Europe, much\nless in the richest regions in terms of biodiversity such as tropical\ncountries. On the other hand, for several centuries, botanists have\nsystematically collected, catalogued and stored plant specimens in herbaria,\nespecially in tropical regions, and recent efforts by the biodiversity\ninformatics community have made it possible to put millions of digitised\nrecords online. The LifeCLEF 2021 plant identification challenge (or \"PlantCLEF\n2021\") was designed to assess the extent to which automated identification of\nflora in data-poor regions can be improved by using herbarium collections. It\nis based on a dataset of about 1,000 species mainly focused on the Guiana\nShield of South America, a region known to have one of the highest plant\ndiversities in the world. The challenge was evaluated as a cross-domain\nclassification task where the training set consisted of several hundred\nthousand herbarium sheets and a few thousand photos to allow learning a\ncorrespondence between the two domains. In addition to the usual metadata\n(location, date, author, taxonomy), the training data also includes the values\nof 5 morphological and functional traits for each species. The test set\nconsisted exclusively of photos taken in the field. This article presents the\nresources and evaluations of the assessment carried out, summarises the\napproaches and systems used by the participating research groups and provides\nan analysis of the main results.", "AI": {"tldr": "The paper presents PlantCLEF 2021 challenge focusing on automated plant identification in biodiversity-rich but data-poor tropical regions using herbarium collections to supplement limited field photos.", "motivation": "Current automated plant identification systems are biased toward North America and Europe due to abundant field photos, while biodiversity-rich tropical regions lack sufficient training data despite having extensive herbarium collections.", "method": "Cross-domain classification task using herbarium sheets (hundreds of thousands) and limited field photos (thousands) to learn correspondence between domains, with additional morphological and functional trait data for each species.", "result": "The challenge evaluated automated identification systems on a dataset of ~1,000 species from the Guiana Shield region, testing exclusively on field photos after training on herbarium collections.", "conclusion": "The study demonstrates the potential of leveraging historical herbarium collections to improve automated plant identification in data-poor biodiversity hotspots, addressing geographical biases in current AI systems."}}
{"id": "2509.18699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18699", "abs": "https://arxiv.org/abs/2509.18699", "authors": ["Zedong Zhang", "Ying Tai", "Jianjun Qian", "Jian Yang", "Jun Li"], "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping", "comment": null, "summary": "Fusing cross-category objects to a single coherent object has gained\nincreasing attention in text-to-image (T2I) generation due to its broad\napplications in virtual reality, digital media, film, and gaming. However,\nexisting methods often produce biased, visually chaotic, or semantically\ninconsistent results due to overlapping artifacts and poor integration.\nMoreover, progress in this field has been limited by the absence of a\ncomprehensive benchmark dataset. To address these problems, we propose\n\\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective\napproach comprising two key components: (1) Group-wise Embedding Swapping,\nwhich fuses semantic attributes from different concepts through feature\nmanipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism\nguided by a balance evaluation score to ensure coherent synthesis.\nAdditionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a\nlarge-scale, hierarchically structured dataset built upon ImageNet-1K and\nWordNet. COF includes 95 superclasses, each with 10 subclasses, enabling\n451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap\noutperforms state-of-the-art compositional T2I methods, including GPT-Image-1\nusing simple and complex prompts.", "AI": {"tldr": "AGSwap is a novel method for fusing cross-category objects in text-to-image generation, addressing issues of bias and visual chaos through adaptive feature manipulation and optimization.", "motivation": "Existing methods for cross-category object fusion produce biased, visually chaotic, or semantically inconsistent results due to overlapping artifacts and poor integration. The field also lacks a comprehensive benchmark dataset.", "method": "AGSwap consists of two components: (1) Group-wise Embedding Swapping for fusing semantic attributes through feature manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism guided by a balance evaluation score. The authors also introduce COF, a large-scale dataset with 451,250 unique fusion pairs.", "result": "Extensive experiments show that AGSwap outperforms state-of-the-art compositional T2I methods, including GPT-Image-1, using both simple and complex prompts.", "conclusion": "AGSwap effectively addresses the challenges in cross-category object fusion, providing a robust solution for coherent synthesis and advancing the field with a comprehensive benchmark dataset."}}
{"id": "2509.18705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18705", "abs": "https://arxiv.org/abs/2509.18705", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries", "comment": "13 pages, 5 figures, CLEF 2019 Conference and Labs of the Evaluation\n  Forum, September 09 to 12, 2019, Lugano, Switzerland", "summary": "Automated identification of plants has improved considerably thanks to the\nrecent progress in deep learning and the availability of training data.\nHowever, this profusion of data only concerns a few tens of thousands of\nspecies, while the planet has nearly 369K. The LifeCLEF 2019 Plant\nIdentification challenge (or \"PlantCLEF 2019\") was designed to evaluate\nautomated identification on the flora of data deficient regions. It is based on\na dataset of 10K species mainly focused on the Guiana shield and the Northern\nAmazon rainforest, an area known to have one of the greatest diversity of\nplants and animals in the world. As in the previous edition, a comparison of\nthe performance of the systems evaluated with the best tropical flora experts\nwas carried out. This paper presents the resources and assessments of the\nchallenge, summarizes the approaches and systems employed by the participating\nresearch groups, and provides an analysis of the main outcomes.", "AI": {"tldr": "The LifeCLEF 2019 Plant Identification challenge focused on automated plant identification for data-deficient regions, specifically the Guiana shield and Northern Amazon rainforest, using a dataset of 10K species and comparing system performance with tropical flora experts.", "motivation": "Current automated plant identification systems are limited to a few tens of thousands of species, while the planet has nearly 369K species. The challenge aimed to address this gap by focusing on data-deficient regions with high biodiversity.", "method": "The challenge used a dataset of 10K species primarily from the Guiana shield and Northern Amazon rainforest. It evaluated automated identification systems and compared their performance with the best tropical flora experts.", "result": "The paper presents the outcomes of the challenge, including the performance of various systems and their comparison with expert identification capabilities.", "conclusion": "The LifeCLEF 2019 Plant Identification challenge successfully evaluated automated plant identification systems for data-deficient regions, providing valuable insights into the capabilities and limitations of current approaches in handling high biodiversity areas."}}
{"id": "2509.18711", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18711", "abs": "https://arxiv.org/abs/2509.18711", "authors": ["Ke Li", "Di Wang", "Ting Wang", "Fuyu Dong", "Yiming Zhang", "Luyao Zhang", "Xiangyu Wang", "Shaofeng Li", "Quan Wang"], "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images", "comment": null, "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.", "AI": {"tldr": "RSVG-ZeroOV is a training-free framework for zero-shot open-vocabulary remote sensing visual grounding that uses frozen foundation models to localize objects in satellite images based on natural language queries without requiring task-specific training.", "motivation": "Existing remote sensing visual grounding methods are limited to closed-set vocabularies and require expensive datasets and fine-tuning. The authors aim to create a more efficient, scalable solution that works in open-world scenarios without training.", "method": "The framework has three stages: (1) Overview: Uses a vision-language model to get cross-attention maps for semantic correlations; (2) Focus: Leverages diffusion models to capture structural/shape information missed by VLMs; (3) Evolve: Uses attention evolution to suppress irrelevant activations and produce purified segmentation masks.", "result": "Extensive experiments show RSVG-ZeroOV consistently outperforms existing weakly-supervised and zero-shot methods, demonstrating the effectiveness of the training-free approach.", "conclusion": "The proposed framework offers an efficient and scalable solution for open-vocabulary remote sensing visual grounding by effectively combining frozen foundation models without requiring task-specific training."}}
{"id": "2509.18715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18715", "abs": "https://arxiv.org/abs/2509.18715", "authors": ["Yingquan Wang", "Pingping Zhang", "Chong Sun", "Dong Wang", "Huchuan Lu"], "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification", "comment": "Accepted by TCSVT2025", "summary": "Object Re-IDentification (ReID) aims to recognize individuals across\nnon-overlapping camera views. While recent advances have achieved remarkable\nprogress, most existing models are constrained to either single-domain or\ncross-domain scenarios, limiting their real-world applicability. Single-domain\nmodels tend to overfit to domain-specific features, whereas cross-domain models\noften rely on diverse normalization strategies that may inadvertently suppress\nidentity-specific discriminative cues. To address these limitations, we propose\nan Attribute Prompt Composition (APC) framework, which exploits textual\nsemantics to jointly enhance discrimination and generalization. Specifically,\nwe design an Attribute Prompt Generator (APG) consisting of a Semantic\nAttribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an\nover-complete attribute dictionary to provide rich semantic descriptions, while\nPCM adaptively composes relevant attributes from SAD to generate discriminative\nattribute-aware features. In addition, motivated by the strong generalization\nability of Vision-Language Models (VLM), we propose a Fast-Slow Training\nStrategy (FSTS) to balance ReID-specific discrimination and generalizable\nrepresentation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)\nto rapidly acquire ReID-specific discriminative knowledge and a Slow Update\nStream (SUS) to retain the generalizable knowledge inherited from the\npre-trained VLM. Through a mutual interaction, the framework effectively\nfocuses on ReID-relevant features while mitigating overfitting. Extensive\nexperiments on both conventional and Domain Generalized (DG) ReID datasets\ndemonstrate that our framework surpasses state-of-the-art methods, exhibiting\nsuperior performances in terms of both discrimination and generalization. The\nsource code is available at https://github.com/AWangYQ/APC.", "AI": {"tldr": "The paper proposes an Attribute Prompt Composition (APC) framework that uses textual semantics to enhance both discrimination and generalization in Object Re-Identification (ReID). It introduces an Attribute Prompt Generator with semantic dictionary and composition module, plus a Fast-Slow Training Strategy to balance ReID-specific learning with generalizable knowledge from Vision-Language Models.", "motivation": "Existing ReID models are limited to either single-domain (overfitting to domain-specific features) or cross-domain scenarios (suppressing identity-specific cues through normalization). There's a need for models that can maintain discrimination while achieving better generalization across domains.", "method": "APC framework with Attribute Prompt Generator (Semantic Attribute Dictionary + Prompt Composition Module) and Fast-Slow Training Strategy (Fast Update Stream for ReID-specific learning + Slow Update Stream to retain VLM generalizable knowledge).", "result": "Extensive experiments on conventional and Domain Generalized ReID datasets show the framework surpasses state-of-the-art methods in both discrimination and generalization performance.", "conclusion": "The proposed APC framework effectively addresses limitations of existing ReID models by leveraging textual semantics and balanced training strategy, achieving superior performance in both discrimination and generalization across domains."}}
{"id": "2509.18717", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18717", "abs": "https://arxiv.org/abs/2509.18717", "authors": ["Tong Zhang", "Kuofeng Gao", "Jiawang Bai", "Leo Yu Zhang", "Xin Yin", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment", "comment": null, "summary": "Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)\nmodels are threatened by targeted data poisoning and backdoor attacks due to\nmassive training image-caption pairs crawled from the Internet. Previous\ndefense methods correct poisoned image-caption pairs by matching a new caption\nfor each image. However, the matching process relies solely on the global\nrepresentations of images and captions, overlooking fine-grained features of\nvisual and textual features. It may introduce incorrect image-caption pairs and\nharm the CLIP pre-training. To address their limitations, we propose an Optimal\nTransport-based framework to reconstruct image-caption pairs, named OTCCLIP. We\npropose a new optimal transport-based distance measure between fine-grained\nvisual and textual feature sets and re-assign new captions based on the\nproposed optimal transport distance. Additionally, to further reduce the\nnegative impact of mismatched pairs, we encourage the inter- and intra-modality\nfine-grained alignment by employing optimal transport-based objective\nfunctions. Our experiments demonstrate that OTCCLIP can successfully decrease\nthe attack success rates of poisoning attacks. Also, compared to previous\nmethods, OTCCLIP significantly improves CLIP's zero-shot and linear probing\nperformance trained on poisoned datasets.", "AI": {"tldr": "OTCCLIP is an Optimal Transport-based framework that defends CLIP models against data poisoning attacks by reconstructing image-caption pairs using fine-grained feature alignment.", "motivation": "Previous defense methods for CLIP models rely solely on global representations, overlooking fine-grained features which can introduce incorrect image-caption pairs and harm pre-training.", "method": "Proposes an optimal transport-based distance measure between fine-grained visual and textual feature sets, reassigns captions based on this distance, and uses optimal transport-based objective functions for inter- and intra-modality alignment.", "result": "OTCCLIP successfully decreases attack success rates of poisoning attacks and significantly improves CLIP's zero-shot and linear probing performance on poisoned datasets compared to previous methods.", "conclusion": "The optimal transport-based approach effectively addresses limitations of previous defense methods by leveraging fine-grained feature alignment to reconstruct poisoned image-caption pairs."}}
{"id": "2509.18733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18733", "abs": "https://arxiv.org/abs/2509.18733", "authors": ["Yilin Gao", "Kangyi Chen", "Zhongxing Peng", "Hengjie Lu", "Shugong Xu"], "title": "Knowledge Transfer from Interaction Learning", "comment": "Accepted by ICCV2025", "summary": "Current visual foundation models (VFMs) face a fundamental limitation in\ntransferring knowledge from vision language models (VLMs), while VLMs excel at\nmodeling cross-modal interactions through unified representation spaces,\nexisting VFMs predominantly adopt result-oriented paradigms that neglect the\nunderlying interaction processes. This representational discrepancy hinders\neffective knowledge transfer and limits generalization across diverse vision\ntasks. We propose Learning from Interactions (LFI), a cognitive-inspired\nframework that addresses this gap by explicitly modeling visual understanding\nas an interactive process. Our key insight is that capturing the dynamic\ninteraction patterns encoded in pre-trained VLMs enables more faithful and\nefficient knowledge transfer to VFMs. The approach centers on two technical\ninnovations, Interaction Queries, which maintain persistent relational\nstructures across network layers, and interaction-based supervision, derived\nfrom the cross-modal attention mechanisms of VLMs. Comprehensive experiments\ndemonstrate consistent improvements across multiple benchmarks, achieving 3.3\nand 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO\ndetection/segmentation respectively, with minimal parameter overhead and faster\nconvergence. The framework particularly excels in cross-domain settings,\ndelivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human\nevaluations further confirm its cognitive alignment, outperforming\nresult-oriented methods by 2.7 times in semantic consistency metrics.", "AI": {"tldr": "LFI is a cognitive-inspired framework that enables better knowledge transfer from vision language models to visual foundation models by modeling visual understanding as an interactive process rather than just using result-oriented approaches.", "motivation": "Current visual foundation models struggle to effectively transfer knowledge from vision language models because they focus on results rather than the underlying interaction processes that VLMs excel at modeling through cross-modal representations.", "method": "The approach uses Interaction Queries to maintain persistent relational structures across network layers and interaction-based supervision derived from cross-modal attention mechanisms of pre-trained VLMs.", "result": "Achieved significant improvements: 3.3 and 1.6 mAP/2.4 AP gains on TinyImageNet classification and COCO detection/segmentation, with 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Also showed 2.7x better semantic consistency in human evaluations.", "conclusion": "LFI framework enables more faithful and efficient knowledge transfer from VLMs to VFMs by explicitly modeling interaction processes, leading to better generalization, faster convergence, and cognitive alignment with minimal parameter overhead."}}
{"id": "2509.18738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18738", "abs": "https://arxiv.org/abs/2509.18738", "authors": ["Ruichao Hou", "Xingyuan Li", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection", "comment": null, "summary": "RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent\nobjects by integrating complementary information from RGB and thermal\nmodalities. However, learning the precise boundaries and complete objects\nremains challenging due to the intrinsic insufficient feature fusion and the\nextrinsic limitations of data scarcity. In this paper, we propose a novel\nhybrid prompt-driven segment anything model (HyPSAM), which leverages the\nzero-shot generalization capabilities of the segment anything model (SAM) for\nRGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that\ngenerates high-quality initial saliency maps as visual prompts. DFNet employs\ndynamic convolution and multi-branch decoding to facilitate adaptive\ncross-modality interaction, overcoming the limitations of fixed-parameter\nkernels and enhancing multi-modal feature representation. Moreover, we propose\na plug-and-play refinement network (P2RNet), which serves as a general\noptimization strategy to guide SAM in refining saliency maps by using hybrid\nprompts. The text prompt ensures reliable modality input, while the mask and\nbox prompts enable precise salient object localization. Extensive experiments\non three public datasets demonstrate that our method achieves state-of-the-art\nperformance. Notably, HyPSAM has remarkable versatility, seamlessly integrating\nwith different RGB-T SOD methods to achieve significant performance gains,\nthereby highlighting the potential of prompt engineering in this field. The\ncode and results of our method are available at:\nhttps://github.com/milotic233/HyPSAM.", "AI": {"tldr": "HyPSAM is a novel hybrid prompt-driven segment anything model for RGB-thermal salient object detection that leverages SAM's zero-shot capabilities through dynamic fusion and refinement networks using visual, text, mask, and box prompts.", "motivation": "RGB-T SOD faces challenges in learning precise boundaries and complete objects due to insufficient feature fusion between modalities and data scarcity. The paper aims to overcome these limitations by leveraging SAM's generalization capabilities.", "method": "Proposes HyPSAM with two main components: DFNet (Dynamic Fusion Network) for generating initial saliency maps as visual prompts using dynamic convolution and multi-branch decoding, and P2RNet (Plug-and-Play Refinement Network) that guides SAM using hybrid prompts (text, mask, box) to refine saliency maps.", "result": "Extensive experiments on three public datasets show state-of-the-art performance. The method demonstrates remarkable versatility by integrating with different RGB-T SOD methods to achieve significant performance gains.", "conclusion": "HyPSAM highlights the potential of prompt engineering in RGB-T SOD, providing a flexible framework that can be seamlessly integrated with existing methods to improve performance through adaptive cross-modality interaction and precise object localization."}}
{"id": "2509.18743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18743", "abs": "https://arxiv.org/abs/2509.18743", "authors": ["Susmit Neogi"], "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop", "summary": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning.", "AI": {"tldr": "TriFusion-AE is a multimodal cross-attention autoencoder that integrates text, depth maps, and LiDAR point clouds to improve robustness against noise and adversarial attacks in autonomous driving perception.", "motivation": "Raw LiDAR point clouds are vulnerable to noise, occlusion, and adversarial corruptions, and existing autoencoders degrade under challenging real-world conditions.", "method": "Uses multimodal cross-attention to align semantic cues from text, geometric features from depth maps, and spatial structure from LiDAR. The framework is model-agnostic and integrates with CNN-based point cloud autoencoders.", "result": "Shows limited gains under mild perturbations but achieves significantly more robust reconstruction under strong adversarial attacks and heavy noise, where CNN-based autoencoders collapse. Evaluated on nuScenes-mini dataset.", "conclusion": "The multimodal fusion framework enables resilient representation learning for LiDAR-based perception in autonomous driving applications."}}
{"id": "2509.18754", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18754", "abs": "https://arxiv.org/abs/2509.18754", "authors": ["Yuyang Liu", "Xinyuan Shi", "Bang Yang", "Peilin Zhou", "Jiahua Dong", "Long Chen", "Ian Reid", "Xiaondan Liang"], "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage", "comment": "16 pages", "summary": "The success of Large Language Models (LLMs) has significantly propelled the\nresearch of video understanding. To harvest the benefits of well-trained expert\nmodels (i.e., tools), video LLMs prioritize the exploration of tool usage\ncapabilities. Existing methods either prompt closed-source LLMs or employ the\ninstruction tuning paradigm for tool-use fine-tuning. These methods, however,\nassume an established repository of fixed tools and struggle to generalize to\nreal-world environments where tool data is perpetually evolving and streaming\nin. To this end, we propose to enhance open-source video LLMs with COntinuaL\nTool usage (termed COLT), which automatically acquires tool-use ability in a\nsuccessive tool stream without suffering 'catastrophic forgetting' of the past\nlearned tools. Specifically, our COLT incorporates a learnable tool codebook as\na tool-specific memory system. Then relevant tools are dynamically selected\nbased on the similarity between user instruction and tool features within the\ncodebook. To unleash the tool usage potential of video LLMs, we collect a\nvideo-centric tool-use instruction tuning dataset VideoToolBench. Extensive\nexperiments on both previous video LLM benchmarks and the tool-use-specific\nVideoToolBench dataset demonstrate the state-of-the-art performance of our\nproposed COLT.", "AI": {"tldr": "COLT enhances video LLMs with continuous tool usage capability, enabling automatic acquisition of tool-use abilities in evolving tool streams without catastrophic forgetting of previously learned tools.", "motivation": "Existing video LLM methods assume fixed tool repositories and struggle with real-world environments where tool data is perpetually evolving and streaming in.", "method": "COLT incorporates a learnable tool codebook as a tool-specific memory system, dynamically selecting relevant tools based on similarity between user instruction and tool features. Uses VideoToolBench dataset for instruction tuning.", "result": "Extensive experiments on video LLM benchmarks and VideoToolBench demonstrate state-of-the-art performance.", "conclusion": "COLT successfully enables continuous tool usage in video LLMs, overcoming limitations of existing methods in dynamic tool environments."}}
{"id": "2509.18759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18759", "abs": "https://arxiv.org/abs/2509.18759", "authors": ["Zhaorui Wang", "Yi Gu", "Deming Zhou", "Renjing Xu"], "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation", "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in\n3D reconstruction and novel view synthesis. However, reconstructing 3D scenes\nfrom sparse viewpoints remains highly challenging due to insufficient visual\ninformation, which results in noticeable artifacts persisting across the 3D\nrepresentation. To address this limitation, recent methods have resorted to\ngenerative priors to remove artifacts and complete missing content in\nunder-constrained areas. Despite their effectiveness, these approaches struggle\nto ensure multi-view consistency, resulting in blurred structures and\nimplausible details. In this work, we propose FixingGS, a training-free method\nthat fully exploits the capabilities of the existing diffusion model for\nsparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our\ndistillation approach, which delivers more accurate and cross-view coherent\ndiffusion priors, thereby enabling effective artifact removal and inpainting.\nIn addition, we propose an adaptive progressive enhancement scheme that further\nrefines reconstructions in under-constrained regions. Extensive experiments\ndemonstrate that FixingGS surpasses existing state-of-the-art methods with\nsuperior visual quality and reconstruction performance. Our code will be\nreleased publicly.", "AI": {"tldr": "FixingGS is a training-free method that enhances sparse-view 3D Gaussian Splatting reconstruction by using diffusion model priors to remove artifacts and complete missing content while ensuring multi-view consistency.", "motivation": "Current methods for sparse-view 3DGS reconstruction struggle with multi-view consistency, resulting in blurred structures and implausible details when using generative priors to address insufficient visual information from sparse viewpoints.", "method": "Proposes a distillation approach that delivers accurate and cross-view coherent diffusion priors, combined with an adaptive progressive enhancement scheme to refine reconstructions in under-constrained regions.", "result": "Extensive experiments show FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance.", "conclusion": "FixingGS effectively addresses the limitations of sparse-view 3DGS reconstruction by leveraging diffusion models for artifact removal and inpainting while maintaining multi-view consistency."}}
{"id": "2509.18763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18763", "abs": "https://arxiv.org/abs/2509.18763", "authors": ["Xijun Wang", "Junyun Huang", "Rayyan Abdalla", "Chengyuan Zhang", "Ruiqi Xian", "Dinesh Manocha"], "title": "Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models", "comment": null, "summary": "We address the critical gap between the computational demands of\nvision-language models and the possible ultra-low-bit weight precision\n(bitwidth $\\leq2$ bits) we can use for higher efficiency. Our work is motivated\nby the substantial computational cost and memory requirements of VLMs, which\nrestrict their applicability in hardware-constrained environments. We propose\nBi-VLM, which separates model weights non-uniformly based on the Gaussian\nquantiles. Our formulation groups the model weights into outlier (salient) and\nmultiple inlier (unsalient) subsets, ensuring that each subset contains a\nproportion of weights corresponding to its quantile in the distribution. We\npropose a saliency-aware hybrid quantization algorithm and use it to quantize\nweights by imposing different constraints on the scaler and binary matrices\nbased on the saliency metric and compression objective. We have evaluated our\napproach on different VLMs. For the language model part of the VLM, our Bi-VLM\noutperforms the SOTA by 3%-47% on the visual question answering task in terms\nof four different benchmarks and three different models. For the overall VLM,\nour Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the\nquantized models and observe that there is redundancy of image tokens 90% - 99%\nin the quantized models. This helps us to further prune the visual tokens to\nimprove efficiency.", "AI": {"tldr": "Bi-VLM addresses the computational demands of vision-language models by proposing a non-uniform weight separation method based on Gaussian quantiles, enabling ultra-low-bit weight precision (\u22642 bits) for higher efficiency.", "motivation": "The substantial computational cost and memory requirements of VLMs restrict their applicability in hardware-constrained environments, creating a critical gap between model demands and achievable efficiency.", "method": "Proposes Bi-VLM which separates model weights non-uniformly based on Gaussian quantiles, grouping weights into outlier (salient) and multiple inlier (unsalient) subsets. Uses a saliency-aware hybrid quantization algorithm with different constraints on scaler and binary matrices based on saliency metric and compression objective.", "result": "For language model part: outperforms SOTA by 3%-47% on visual question answering across four benchmarks and three models. For overall VLM: outperforms SOTA by 4%-45%. Also identifies 90%-99% redundancy in image tokens, enabling further token pruning.", "conclusion": "Bi-VLM successfully bridges the gap between VLM computational demands and ultra-low-bit precision, achieving significant performance improvements while identifying substantial token redundancy for further efficiency gains."}}
{"id": "2509.18765", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18765", "abs": "https://arxiv.org/abs/2509.18765", "authors": ["Azad Singh", "Deepak Mishra"], "title": "DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for medical\nimage representation learning, particularly in settings with limited labeled\ndata. However, existing SSL methods often rely on complex architectures,\nanatomy-specific priors, or heavily tuned augmentations, which limit their\nscalability and generalizability. More critically, these models are prone to\nshortcut learning, especially in modalities like chest X-rays, where anatomical\nsimilarity is high and pathology is subtle. In this work, we introduce DiSSECT\n-- Discrete Self-Supervision for Efficient Clinical Transferable\nRepresentations, a framework that integrates multi-scale vector quantization\ninto the SSL pipeline to impose a discrete representational bottleneck. This\nconstrains the model to learn repeatable, structure-aware features while\nsuppressing view-specific or low-utility patterns, improving representation\ntransfer across tasks and domains. DiSSECT achieves strong performance on both\nclassification and segmentation tasks, requiring minimal or no fine-tuning, and\nshows particularly high label efficiency in low-label regimes. We validate\nDiSSECT across multiple public medical imaging datasets, demonstrating its\nrobustness and generalizability compared to existing state-of-the-art\napproaches.", "AI": {"tldr": "DiSSECT is a self-supervised learning framework that uses multi-scale vector quantization to create discrete representational bottlenecks, improving feature learning for medical imaging by suppressing shortcut learning and enhancing transferability.", "motivation": "Existing SSL methods for medical imaging rely on complex architectures, anatomy-specific priors, or heavily tuned augmentations, which limit scalability and generalizability. They are prone to shortcut learning, especially in modalities like chest X-rays where anatomical similarity is high and pathology is subtle.", "method": "DiSSECT integrates multi-scale vector quantization into the SSL pipeline to impose a discrete representational bottleneck, constraining the model to learn repeatable, structure-aware features while suppressing view-specific or low-utility patterns.", "result": "DiSSECT achieves strong performance on both classification and segmentation tasks, requiring minimal or no fine-tuning, and shows particularly high label efficiency in low-label regimes. It demonstrates robustness and generalizability across multiple public medical imaging datasets.", "conclusion": "The discrete representational bottleneck approach effectively improves representation transfer across tasks and domains, making DiSSECT a scalable and generalizable solution for medical image representation learning in limited labeled data settings."}}
{"id": "2509.18779", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18779", "abs": "https://arxiv.org/abs/2509.18779", "authors": ["Hemanth Puppala", "Wayne Sarasua", "Srinivas Biyaguda", "Farhad Farzinpour", "Mashrur Chowdhury"], "title": "Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning", "comment": "Preprint under review in TRR, 20 pages, 9 figures, 4 tables", "summary": "Deer-vehicle collisions represent a critical safety challenge in the United\nStates, causing nearly 2.1 million incidents annually and resulting in\napproximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic\ndamages. These collisions also contribute significantly to declining deer\npopulations. This paper presents a real-time detection and driver warning\nsystem that integrates thermal imaging, deep learning, and\nvehicle-to-everything communication to help mitigate deer-vehicle collisions.\nOur system was trained and validated on a custom dataset of over 12,000 thermal\ndeer images collected in Mars Hill, North Carolina. Experimental evaluation\ndemonstrates exceptional performance with 98.84 percent mean average precision,\n95.44 percent precision, and 95.96 percent recall. The system was field tested\nduring a follow-up visit to Mars Hill and readily sensed deer providing the\ndriver with advanced warning. Field testing validates robust operation across\ndiverse weather conditions, with thermal imaging maintaining between 88 and 92\npercent detection accuracy in challenging scenarios where conventional visible\nlight based cameras achieve less than 60 percent effectiveness. When a high\nprobability threshold is reached sensor data sharing messages are broadcast to\nsurrounding vehicles and roadside units via cellular vehicle to everything\n(CV2X) communication devices. Overall, our system achieves end to end latency\nconsistently under 100 milliseconds from detection to driver alert. This\nresearch establishes a viable technological pathway for reducing deer-vehicle\ncollisions through thermal imaging and connected vehicles.", "AI": {"tldr": "A real-time deer detection system using thermal imaging, deep learning, and vehicle-to-everything communication that achieves 98.84% mAP and under 100ms latency to prevent deer-vehicle collisions.", "motivation": "Deer-vehicle collisions cause 2.1 million incidents annually in the US, resulting in 440 fatalities, 59,000 injuries, $10 billion in damages, and contribute to declining deer populations.", "method": "Integration of thermal imaging, deep learning on a custom dataset of 12,000 thermal deer images, and CV2X communication for real-time detection and driver warnings.", "result": "System achieved 98.84% mAP, 95.44% precision, 95.96% recall, maintained 88-92% detection accuracy in challenging conditions where visible light cameras achieved <60%, with end-to-end latency under 100ms.", "conclusion": "The research establishes a viable technological pathway for reducing deer-vehicle collisions through thermal imaging and connected vehicles, validated by successful field testing."}}
{"id": "2509.18796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18796", "abs": "https://arxiv.org/abs/2509.18796", "authors": ["Danush Kumar Venkatesh", "Stefanie Speidel"], "title": "Towards Application Aligned Synthetic Surgical Image Synthesis", "comment": null, "summary": "The scarcity of annotated surgical data poses a significant challenge for\ndeveloping deep learning systems in computer-assisted interventions. While\ndiffusion models can synthesize realistic images, they often suffer from data\nmemorization, resulting in inconsistent or non-diverse samples that may fail to\nimprove, or even harm, downstream performance. We introduce \\emph{Surgical\nApplication-Aligned Diffusion} (SAADi), a new framework that aligns diffusion\nmodels with samples preferred by downstream models. Our method constructs pairs\nof \\emph{preferred} and \\emph{non-preferred} synthetic images and employs\nlightweight fine-tuning of diffusion models to align the image generation\nprocess with downstream objectives explicitly. Experiments on three surgical\ndatasets demonstrate consistent gains of $7$--$9\\%$ in classification and\n$2$--$10\\%$ in segmentation tasks, with the considerable improvements observed\nfor underrepresented classes. Iterative refinement of synthetic samples further\nboosts performance by $4$--$10\\%$. Unlike baseline approaches, our method\novercomes sample degradation and establishes task-aware alignment as a key\nprinciple for mitigating data scarcity and advancing surgical vision\napplications.", "AI": {"tldr": "SAADi is a surgical vision framework that aligns diffusion models with downstream task objectives by fine-tuning on preferred vs non-preferred synthetic image pairs, achieving significant performance gains in classification and segmentation tasks.", "motivation": "Address data scarcity in surgical deep learning by overcoming diffusion model memorization issues that produce inconsistent/non-diverse samples which can harm downstream performance.", "method": "Constructs preferred/non-preferred synthetic image pairs and performs lightweight fine-tuning of diffusion models to align generation with downstream objectives. Uses iterative refinement of synthetic samples.", "result": "Consistent gains of 7-9% in classification and 2-10% in segmentation across three surgical datasets, with largest improvements for underrepresented classes. Iterative refinement boosts performance by 4-10%.", "conclusion": "SAADi overcomes sample degradation and establishes task-aware alignment as a key principle for mitigating data scarcity in surgical vision applications."}}
{"id": "2509.18801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18801", "abs": "https://arxiv.org/abs/2509.18801", "authors": ["Kuang Xiaodong", "Li Bingxuan", "Li Yuan", "Rao Fan", "Ma Gege", "Xie Qingguo", "Mok Greta S P", "Liu Huafeng", "Zhu Wentao"], "title": "A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising", "comment": null, "summary": "Achieving high image quality for temporal frames in dynamic positron emission\ntomography (PET) is challenging due to the limited statistic especially for the\nshort frames. Recent studies have shown that deep learning (DL) is useful in a\nwide range of medical image denoising tasks. In this paper, we propose a\nmodel-based neural network for dynamic PET image denoising. The inter-frame\nspatial correlation and intra-frame structural consistency in dynamic PET are\nused to establish the kernel space-based multidimensional sparse (KMDS) model.\nWe then substitute the inherent forms of the parameter estimation with neural\nnetworks to enable adaptive parameters optimization, forming the end-to-end\nneural KMDS-Net. Extensive experimental results from simulated and real data\ndemonstrate that the neural KMDS-Net exhibits strong denoising performance for\ndynamic PET, outperforming previous baseline methods. The proposed method may\nbe used to effectively achieve high temporal and spatial resolution for dynamic\nPET. Our source code is available at\nhttps://github.com/Kuangxd/Neural-KMDS-Net/tree/main.", "AI": {"tldr": "Proposes KMDS-Net, a model-based neural network for dynamic PET image denoising that combines kernel space-based multidimensional sparse modeling with neural networks for adaptive parameter optimization.", "motivation": "Dynamic PET imaging faces challenges in achieving high image quality due to limited statistics in short temporal frames, requiring effective denoising methods.", "method": "Establishes KMDS model using inter-frame spatial correlation and intra-frame structural consistency, then substitutes parameter estimation with neural networks to create end-to-end KMDS-Net for adaptive optimization.", "result": "Extensive experiments on simulated and real data show KMDS-Net outperforms baseline methods in denoising performance for dynamic PET.", "conclusion": "The proposed method effectively achieves high temporal and spatial resolution for dynamic PET, with source code made publicly available."}}
{"id": "2509.18802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18802", "abs": "https://arxiv.org/abs/2509.18802", "authors": ["Garam Kim", "Tae Kyeong Jeong", "Juyoun Park"], "title": "Surgical Video Understanding with Label Interpolation", "comment": "8 pages, 10 figures", "summary": "Robot-assisted surgery (RAS) has become a critical paradigm in modern\nsurgery, promoting patient recovery and reducing the burden on surgeons through\nminimally invasive approaches. To fully realize its potential, however, a\nprecise understanding of the visual data generated during surgical procedures\nis essential. Previous studies have predominantly focused on single-task\napproaches, but real surgical scenes involve complex temporal dynamics and\ndiverse instrument interactions that limit comprehensive understanding.\nMoreover, the effective application of multi-task learning (MTL) requires\nsufficient pixel-level segmentation data, which are difficult to obtain due to\nthe high cost and expertise required for annotation. In particular, long-term\nannotations such as phases and steps are available for every frame, whereas\nshort-term annotations such as surgical instrument segmentation and action\ndetection are provided only for key frames, resulting in a significant\ntemporal-spatial imbalance. To address these challenges, we propose a novel\nframework that combines optical flow-based segmentation label interpolation\nwith multi-task learning. optical flow estimated from annotated key frames is\nused to propagate labels to adjacent unlabeled frames, thereby enriching sparse\nspatial supervision and balancing temporal and spatial information for\ntraining. This integration improves both the accuracy and efficiency of\nsurgical scene understanding and, in turn, enhances the utility of RAS.", "AI": {"tldr": "A novel framework combining optical flow-based label interpolation with multi-task learning to address temporal-spatial imbalance in robot-assisted surgery data annotation.", "motivation": "Robot-assisted surgery requires comprehensive visual understanding, but existing approaches face challenges with complex temporal dynamics and limited pixel-level segmentation data due to high annotation costs and temporal-spatial imbalance in available annotations.", "method": "Proposes optical flow-based segmentation label interpolation that propagates labels from annotated key frames to adjacent unlabeled frames, combined with multi-task learning to balance temporal and spatial information for training.", "result": "The integration improves both accuracy and efficiency of surgical scene understanding by enriching sparse spatial supervision and balancing temporal-spatial information.", "conclusion": "The proposed framework enhances the utility of robot-assisted surgery by addressing annotation challenges and improving comprehensive surgical scene understanding."}}
{"id": "2509.18824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18824", "abs": "https://arxiv.org/abs/2509.18824", "authors": ["Yanzuo Lu", "Xin Xia", "Manlin Zhang", "Huafeng Kuang", "Jianbin Zheng", "Yuxi Ren", "Xuefeng Xiao"], "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation", "comment": "Technical Report", "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.", "AI": {"tldr": "Hyper-Bagel is a unified acceleration framework that speeds up multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving 2x+ speedup in understanding and 16.67x-22x speedup in generation while maintaining quality.", "motivation": "Unified multimodal models face significant computational overhead from iterative diffusion denoising and autoregressive decoding as contexts integrate increasingly numerous interleaved multimodal tokens.", "method": "Uses divide-and-conquer strategy with speculative decoding for next-token prediction and multi-stage distillation for diffusion denoising. Develops both lossless 6-NFE model and highly efficient 1-NFE model with adversarial distillation and human feedback learning.", "result": "Achieves over 2x speedup in multimodal understanding, 16.67x speedup in text-to-image generation, and 22x speedup in image editing while preserving original model quality. The 1-NFE model enables near real-time interactive editing.", "conclusion": "Hyper-Bagel provides substantial performance gains through advanced acceleration techniques, making complex multimodal interactions seamless and instantaneous while maintaining cost-effectiveness and responsiveness."}}
{"id": "2509.18839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18839", "abs": "https://arxiv.org/abs/2509.18839", "authors": ["Gianmarco Spinaci", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography", "comment": "11 pages, 2 figures", "summary": "This study evaluates the capabilities of Multimodal Large Language Models\n(LLMs) and Vision Language Models (VLMs) in the task of single-label\nclassification of Christian Iconography. The goal was to assess whether\ngeneral-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,\ncan interpret the Iconography, typically addressed by supervised classifiers,\nand evaluate their performance. Two research questions guided the analysis:\n(RQ1) How do multimodal LLMs perform on image classification of Christian\nsaints? And (RQ2), how does performance vary when enriching input with\ncontextual information or few-shot exemplars? We conducted a benchmarking study\nusing three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and\nWikidata, filtered to include the top 10 most frequent classes. Models were\ntested under three conditions: (1) classification using class labels, (2)\nclassification with Iconclass descriptions, and (3) few-shot learning with five\nexemplars. Results were compared against ResNet50 baselines fine-tuned on the\nsame datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed\nthe ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,\nwhere Siglip reached the highest accuracy score, suggesting model sensitivity\nto image size and metadata alignment. Enriching prompts with class descriptions\ngenerally improved zero-shot performance, while few-shot learning produced\nlower results, with only occasional and minimal increments in accuracy. We\nconclude that general-purpose multimodal LLMs are capable of classification in\nvisually complex cultural heritage domains. These results support the\napplication of LLMs as metadata curation tools in digital humanities workflows,\nsuggesting future research on prompt optimization and the expansion of the\nstudy to other classification strategies and models.", "AI": {"tldr": "This study evaluates multimodal LLMs and VLMs for Christian iconography classification, finding that Gemini-2.5 Pro and GPT-4o outperform ResNet50 baselines, with performance varying based on dataset characteristics and prompt enrichment strategies.", "motivation": "To assess whether general-purpose multimodal models can interpret Christian iconography typically handled by supervised classifiers, and evaluate their potential as metadata curation tools in digital humanities.", "method": "Benchmarking study using three datasets (ArtDL, ICONCLASS, Wikidata) with top 10 classes, testing models under three conditions: class labels only, Iconclass descriptions, and few-shot learning with 5 exemplars, compared against ResNet50 baselines.", "result": "Gemini-2.5 Pro and GPT-4o outperformed ResNet50 baselines. Accuracy dropped significantly on Wikidata dataset where Siglip performed best. Prompt enrichment with descriptions improved zero-shot performance, while few-shot learning showed minimal improvements.", "conclusion": "General-purpose multimodal LLMs are capable of classification in complex cultural heritage domains, supporting their use as metadata curation tools in digital humanities workflows."}}
{"id": "2509.18840", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18840", "abs": "https://arxiv.org/abs/2509.18840", "authors": ["Ismael Elsharkawi", "Hossam Sharara", "Ahmed Rafea"], "title": "ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction", "comment": "Under Review", "summary": "Image Representation Learning is an important problem in Computer Vision.\nTraditionally, images were processed as grids, using Convolutional Neural\nNetworks or as a sequence of visual tokens, using Vision Transformers.\nRecently, Vision Graph Neural Networks (ViG) have proposed the treatment of\nimages as a graph of nodes; which provides a more intuitive image\nrepresentation. The challenge is to construct a graph of nodes in each layer\nthat best represents the relations between nodes and does not need a\nhyper-parameter search. ViG models in the literature depend on\nnon-parameterized and non-learnable statistical methods that operate on the\nlatent features of nodes to create a graph. This might not select the best\nneighborhood for each node. Starting from k-NN graph construction to HyperGraph\nConstruction and Similarity-Thresholded graph construction, these methods lack\nthe ability to provide a learnable hyper-parameter-free graph construction\nmethod. To overcome those challenges, we present the Learnable Reparameterized\nGraph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies\nkey-query attention between every pair of nodes; then uses soft-threshold\nreparameterization for edge selection, which allows the use of a differentiable\nmathematical model for training. Using learnable parameters to select the\nneighborhood removes the bias that is induced by any clustering or thresholding\nmethods previously introduced in the literature. In addition, LRGC allows\ntuning the threshold in each layer to the training data since the thresholds\nare learnable through training and are not provided as hyper-parameters to the\nmodel. We demonstrate that the proposed ViG-LRGC approach outperforms\nstate-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark\ndataset.", "AI": {"tldr": "LRGC introduces a learnable graph construction method for Vision GNNs using attention and soft-threshold reparameterization, outperforming existing ViG models on ImageNet-1k.", "motivation": "Traditional ViG models use non-learnable statistical methods for graph construction that may not select optimal neighborhoods and require hyper-parameter tuning. There's a need for parameter-free, learnable graph construction.", "method": "LRGC applies key-query attention between node pairs and uses soft-threshold reparameterization for differentiable edge selection, making thresholds learnable during training rather than fixed hyper-parameters.", "result": "ViG-LRGC outperforms state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark dataset.", "conclusion": "LRGC provides an effective learnable graph construction method that removes bias from previous clustering/thresholding approaches and adapts thresholds to training data through learning."}}
{"id": "2509.18847", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18847", "abs": "https://arxiv.org/abs/2509.18847", "authors": ["Junhao Su", "Yuanliang Wan", "Junwei Yang", "Hengyu Shi", "Tianyang Han", "Junfeng Luo", "Yurui Qiu"], "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions", "comment": "9pages", "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.", "AI": {"tldr": "The paper proposes structured reflection for tool-augmented LLMs, turning error diagnosis and repair into explicit, trainable actions to improve multi-turn tool-call success and error recovery.", "motivation": "Current self-reflection practices for tool-augmented LLMs rely on heuristic prompts or one-way reasoning, which are fragile in multi-turn interactions and often lead to repeated mistakes after failures.", "method": "The method introduces structured reflection with Reflect-Call-Final strategy, combining DAPO and GSPO objectives with a tailored reward scheme. Training uses Tool-Reflection-Bench, a benchmark that programmatically checks structural validity, executability, parameter correctness, and result consistency.", "result": "Experiments on BFCL v3 and Tool-Reflection-Bench show significant improvements in multi-turn tool-call success, error recovery, and reduction of redundant calls.", "conclusion": "Making reflection explicit and optimizing it directly improves tool interaction reliability and provides a reproducible path for agents to learn from failure."}}
{"id": "2509.18891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18891", "abs": "https://arxiv.org/abs/2509.18891", "authors": ["Xueyu Liu", "Xiaoyi Zhang", "Guangze Shi", "Meilin Liu", "Yexin Lai", "Yongfei Wu", "Mingqiang Wei"], "title": "Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model", "comment": null, "summary": "Prompt quality plays a critical role in the performance of the Segment\nAnything Model (SAM), yet existing approaches often rely on heuristic or\nmanually crafted prompts, limiting scalability and generalization. In this\npaper, we propose Point Prompt Defender, an adversarial reinforcement learning\nframework that adopts an attack-for-defense paradigm to automatically optimize\npoint prompts. We construct a task-agnostic point prompt environment by\nrepresenting image patches as nodes in a dual-space graph, where edges encode\nboth physical and semantic distances. Within this environment, an attacker\nagent learns to activate a subset of prompts that maximally degrade SAM's\nsegmentation performance, while a defender agent learns to suppress these\ndisruptive prompts and restore accuracy. Both agents are trained using Deep\nQ-Networks with a reward signal based on segmentation quality variation. During\ninference, only the defender is deployed to refine arbitrary coarse prompt\nsets, enabling enhanced SAM segmentation performance across diverse tasks\nwithout retraining. Extensive experiments show that Point Prompt Defender\neffectively improves SAM's robustness and generalization, establishing a\nflexible, interpretable, and plug-and-play framework for prompt-based\nsegmentation.", "AI": {"tldr": "Point Prompt Defender is an adversarial reinforcement learning framework that automatically optimizes point prompts for SAM using an attack-for-defense paradigm to improve segmentation performance.", "motivation": "Existing approaches rely on heuristic or manually crafted prompts, limiting scalability and generalization for SAM's performance.", "method": "Uses adversarial RL with attacker and defender agents in a dual-space graph environment. Attacker degrades SAM performance while defender suppresses disruptive prompts. Both trained with Deep Q-Networks based on segmentation quality.", "result": "Extensive experiments show improved SAM robustness and generalization. Defender effectively refines arbitrary coarse prompt sets during inference without retraining.", "conclusion": "Establishes a flexible, interpretable, and plug-and-play framework for prompt-based segmentation that enhances SAM performance across diverse tasks."}}
{"id": "2509.18894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18894", "abs": "https://arxiv.org/abs/2509.18894", "authors": ["Jenna Kline", "Anirudh Potlapally", "Bharath Pillai", "Tanishka Wani", "Rugved Katole", "Vedant Patil", "Penelope Covey", "Hari Subramoni", "Tanya Berger-Wolf", "Christopher Stewart"], "title": "SmartWilds: Multimodal Wildlife Monitoring Dataset", "comment": "8 pages", "summary": "We present the first release of SmartWilds, a multimodal wildlife monitoring\ndataset. SmartWilds is a synchronized collection of drone imagery, camera trap\nphotographs and videos, and bioacoustic recordings collected during summer 2025\nat The Wilds safari park in Ohio. This dataset supports multimodal AI research\nfor comprehensive environmental monitoring, addressing critical needs in\nendangered species research, conservation ecology, and habitat management. Our\npilot deployment captured four days of synchronized monitoring across three\nmodalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,\nPrzewalski's horses, as well as species native to Ohio, including bald eagles,\nwhite-tailed deer, and coyotes. We provide a comparative analysis of sensor\nmodality performance, demonstrating complementary strengths for landuse\npatterns, species detection, behavioral analysis, and habitat monitoring. This\nwork establishes reproducible protocols for multimodal wildlife monitoring\nwhile contributing open datasets to advance conservation computer vision\nresearch. Future releases will include synchronized GPS tracking data from\ntagged individuals, citizen science data, and expanded temporal coverage across\nmultiple seasons.", "AI": {"tldr": "SmartWilds is the first multimodal wildlife monitoring dataset combining synchronized drone imagery, camera trap photos/videos, and bioacoustic recordings from a 220-acre safari park, supporting AI research for conservation and habitat management.", "motivation": "To address critical needs in endangered species research, conservation ecology, and habitat management by providing comprehensive multimodal data for environmental monitoring.", "method": "Pilot deployment involving four days of synchronized monitoring across three modalities (drone imagery, camera traps, bioacoustic recordings) in a 220-acre pasture containing various species including endangered and native animals.", "result": "The dataset enables comparative analysis of sensor modality performance, demonstrating complementary strengths for landuse patterns, species detection, behavioral analysis, and habitat monitoring.", "conclusion": "This work establishes reproducible protocols for multimodal wildlife monitoring and contributes open datasets to advance conservation computer vision research, with future releases planned to include GPS tracking and expanded temporal coverage."}}
{"id": "2509.18897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18897", "abs": "https://arxiv.org/abs/2509.18897", "authors": ["Jiayu Wang", "Ruizhi Wang", "Jie Song", "Haofei Zhang", "Mingli Song", "Zunlei Feng", "Li Sun"], "title": "RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing", "comment": "26 pages, 4 figures", "summary": "In this paper, we introduce a novel benchmark designed to propel the\nadvancement of general-purpose, large-scale 3D vision models for remote sensing\nimagery. While several datasets have been proposed within the realm of remote\nsensing, many existing collections either lack comprehensive depth information\nor fail to establish precise alignment between depth data and remote sensing\nimages. To address this deficiency, we present a visual Benchmark for 3D\nunderstanding of Remotely Sensed images, dubbed RS3DBench. This dataset\nencompasses 54,951 pairs of remote sensing images and pixel-level aligned depth\nmaps, accompanied by corresponding textual descriptions, spanning a broad array\nof geographical contexts. It serves as a tool for training and assessing 3D\nvisual perception models within remote sensing image spatial understanding\ntasks. Furthermore, we introduce a remotely sensed depth estimation model\nderived from stable diffusion, harnessing its multimodal fusion capabilities,\nthereby delivering state-of-the-art performance on our dataset. Our endeavor\nseeks to make a profound contribution to the evolution of 3D visual perception\nmodels and the advancement of geographic artificial intelligence within the\nremote sensing domain. The dataset, models and code will be accessed on the\nhttps://rs3dbench.github.io.", "AI": {"tldr": "RS3DBench is a novel benchmark dataset for 3D understanding of remote sensing imagery, containing 54,951 pairs of remote sensing images with pixel-level aligned depth maps and textual descriptions, along with a state-of-the-art depth estimation model based on stable diffusion.", "motivation": "Existing remote sensing datasets lack comprehensive depth information or precise alignment between depth data and remote sensing images, limiting the development of general-purpose 3D vision models for remote sensing applications.", "method": "Created RS3DBench dataset with 54,951 aligned image-depth pairs across diverse geographical contexts, and developed a remotely sensed depth estimation model using stable diffusion's multimodal fusion capabilities.", "result": "The proposed depth estimation model achieves state-of-the-art performance on the RS3DBench dataset, demonstrating effective 3D visual perception for remote sensing imagery.", "conclusion": "RS3DBench contributes significantly to advancing 3D visual perception models and geographic AI in remote sensing, with the dataset, models, and code made publicly available to facilitate further research."}}
{"id": "2509.18898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18898", "abs": "https://arxiv.org/abs/2509.18898", "authors": ["Pengteng Li", "Yunfan Lu", "Pinhao Song", "Weiyu Guo", "Huizai Yao", "F. Richard Yu", "Hui Xiong"], "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring", "comment": null, "summary": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS.", "AI": {"tldr": "DeblurSplat is the first SfM-free deblurring 3D Gaussian Splatting method that uses event cameras to address motion blur without requiring Structure-from-Motion, achieving high-fidelity novel view synthesis with efficient rendering.", "motivation": "To solve motion blur in 3D scene reconstruction without relying on traditional Structure-from-Motion pipelines, which can introduce cumulative errors from inaccurate camera pose estimation.", "method": "Uses DUSt3R for direct point cloud initialization from blurred images, avoiding SfM, and incorporates event streams for fine-grained supervision by decoding latent sharp images from events and blurred images.", "result": "Extensive experiments show DeblurSplat generates high-fidelity novel views and achieves significant rendering efficiency improvements over state-of-the-art deblur 3D-GS methods.", "conclusion": "The method successfully demonstrates SfM-free deblurring with event cameras, providing accurate scene reconstruction while maintaining rendering efficiency across diverse scenes."}}
{"id": "2509.18910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18910", "abs": "https://arxiv.org/abs/2509.18910", "authors": ["Shuwei Guo", "Simin Luan", "Yan Ke", "Zeyd Boukhers", "John See", "Cong Yang"], "title": "Moir\u00e9Net: A Compact Dual-Domain Network for Image Demoir\u00e9ing", "comment": null, "summary": "Moir\\'e patterns arise from spectral aliasing between display pixel lattices\nand camera sensor grids, manifesting as anisotropic, multi-scale artifacts that\npose significant challenges for digital image demoir\\'eing. We propose\nMoir\\'eNet, a convolutional neural U-Net-based framework that synergistically\nintegrates frequency and spatial domain features for effective artifact\nremoval. Moir\\'eNet introduces two key components: a Directional\nFrequency-Spatial Encoder (DFSE) that discerns moir\\'e orientation via\ndirectional difference convolution, and a Frequency-Spatial Adaptive Selector\n(FSAS) that enables precise, feature-adaptive suppression. Extensive\nexperiments demonstrate that Moir\\'eNet achieves state-of-the-art performance\non public and actively used datasets while being highly parameter-efficient.\nWith only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,\nMoir\\'eNet combines superior restoration quality with parameter efficiency,\nmaking it well-suited for resource-constrained applications including\nsmartphone photography, industrial imaging, and augmented reality.", "AI": {"tldr": "Moir\u00e9Net is a U-Net-based CNN framework that integrates frequency and spatial domain features to remove moir\u00e9 patterns from digital images, achieving state-of-the-art performance with high parameter efficiency.", "motivation": "Moir\u00e9 patterns from aliasing between display pixels and camera sensors create anisotropic, multi-scale artifacts that are challenging to remove from digital images.", "method": "Proposes Moir\u00e9Net with two key components: Directional Frequency-Spatial Encoder (DFSE) that identifies moir\u00e9 orientation using directional difference convolution, and Frequency-Spatial Adaptive Selector (FSAS) for feature-adaptive suppression.", "result": "Achieves state-of-the-art performance on public datasets with only 5.513M parameters (48% reduction vs ESDNet-L), combining superior restoration quality with parameter efficiency.", "conclusion": "Moir\u00e9Net is well-suited for resource-constrained applications like smartphone photography, industrial imaging, and augmented reality due to its efficient performance."}}
{"id": "2509.18912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18912", "abs": "https://arxiv.org/abs/2509.18912", "authors": ["Yunzhe Shen", "Kai Peng", "Leiye Liu", "Wei Ji", "Jingjing Li", "Miao Zhang", "Yongri Piao", "Huchuan Lu"], "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation", "comment": null, "summary": "Audio-visual segmentation (AVS) plays a critical role in multimodal machine\nlearning by effectively integrating audio and visual cues to precisely segment\nobjects or regions within visual scenes. Recent AVS methods have demonstrated\nsignificant improvements. However, they overlook the inherent frequency-domain\ncontradictions between audio and visual modalities--the pervasively interfering\nnoise in audio high-frequency signals vs. the structurally rich details in\nvisual high-frequency signals. Ignoring these differences can result in\nsuboptimal performance. In this paper, we rethink the AVS task from a deeper\nperspective by reformulating AVS task as a frequency-domain decomposition and\nrecomposition problem. To this end, we introduce a novel Frequency-Aware\nAudio-Visual Segmentation (FAVS) framework consisting of two key modules:\nFrequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal\nConsistency (SCMC) module. FDED module employs a residual-based iterative\nfrequency decomposition to discriminate modality-specific semantics and\nstructural features, and SCMC module leverages a mixture-of-experts\narchitecture to reinforce semantic consistency and modality-specific feature\npreservation through dynamic expert routing. Extensive experiments demonstrate\nthat our FAVS framework achieves state-of-the-art performance on three\nbenchmark datasets, and abundant qualitative visualizations further verify the\neffectiveness of the proposed FDED and SCMC modules. The code will be released\nas open source upon acceptance of the paper.", "AI": {"tldr": "FAVS is a novel audio-visual segmentation framework that addresses frequency-domain contradictions between audio and visual modalities through frequency decomposition and cross-modal consistency modules.", "motivation": "Current AVS methods overlook the inherent frequency-domain contradictions - audio high-frequency signals contain interfering noise while visual high-frequency signals contain rich structural details, leading to suboptimal performance.", "method": "Proposes Frequency-Aware Audio-Visual Segmentation (FAVS) with two modules: Frequency-Domain Enhanced Decomposer (FDED) for modality-specific feature discrimination using residual-based iterative frequency decomposition, and Synergistic Cross-Modal Consistency (SCMC) using mixture-of-experts architecture for semantic consistency and feature preservation.", "result": "Achieves state-of-the-art performance on three benchmark datasets, with qualitative visualizations verifying the effectiveness of FDED and SCMC modules.", "conclusion": "Reformulating AVS as a frequency-domain decomposition and recomposition problem effectively addresses modality-specific frequency characteristics, leading to superior segmentation performance."}}
{"id": "2509.18913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18913", "abs": "https://arxiv.org/abs/2509.18913", "authors": ["Nguyen Van Tu", "Pham Nguyen Hai Long", "Vo Hoai Viet"], "title": "xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision", "comment": null, "summary": "Deep learning has become the de facto standard and dominant paradigm in image\nanalysis tasks, achieving state-of-the-art performance. However, this approach\noften results in \"black-box\" models, whose decision-making processes are\ndifficult to interpret, raising concerns about reliability in critical\napplications. To address this challenge and provide human a method to\nunderstand how AI model process and make decision, the field of xAI has\nemerged. This paper surveys four representative approaches in xAI for visual\nperception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),\n(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their\nunderlying mechanisms, strengths and limitations, as well as evaluation\nmetrics, thereby providing a comprehensive overview to guide future research\nand applications.", "AI": {"tldr": "This paper surveys four representative xAI approaches for visual perception tasks: Saliency Maps, Concept Bottleneck Models, Prototype-based methods, and Hybrid approaches, analyzing their mechanisms, strengths, limitations, and evaluation metrics.", "motivation": "Deep learning models are often \"black-box\" systems whose decision-making processes are difficult to interpret, raising reliability concerns in critical applications. The field of Explainable AI (xAI) has emerged to address this challenge and provide human-understandable explanations for AI model decisions.", "method": "The paper conducts a comprehensive survey and analysis of four key xAI approaches for visual perception: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM), (iii) Prototype-based methods, and (iv) Hybrid approaches. The analysis covers their underlying mechanisms, strengths, limitations, and evaluation metrics.", "result": "The survey provides a comprehensive overview of current xAI methods for visual perception, highlighting the trade-offs and characteristics of different approaches to help researchers and practitioners understand the state of the art in explainable AI for image analysis.", "conclusion": "This survey serves as a guide for future research and applications in xAI for visual perception tasks, helping bridge the gap between high-performing deep learning models and human-interpretable decision-making processes."}}
{"id": "2509.18917", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18917", "abs": "https://arxiv.org/abs/2509.18917", "authors": ["Amirhesam Aghanouri", "Cristina Olaverri-Monreal"], "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models", "comment": null, "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.", "AI": {"tldr": "This paper proposes a denoising diffusion probabilistic model (DDPM) with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic LiDAR data for autonomous vehicle perception, addressing challenges of real-world data collection.", "motivation": "Real-world LiDAR data collection is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations, which hinders autonomous vehicle perception systems.", "method": "The authors apply a DDPM enhanced with novel noise scheduling and time-step embedding techniques to generate synthetic LiDAR point clouds. These modifications improve the denoising process and temporal awareness for more realistic data generation.", "result": "Extensive evaluation on IAMCV and KITTI-360 datasets using four performance metrics shows superior performance over state-of-the-art methods, effectively mitigating effects of noisy/sparse LiDAR data and producing diverse point clouds with rich spatial relationships.", "conclusion": "The proposed diffusion model generates high-quality synthetic LiDAR data that improves autonomous vehicle perception systems by providing realistic augmentation data to overcome limitations of real-world data collection."}}
{"id": "2509.18919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18919", "abs": "https://arxiv.org/abs/2509.18919", "authors": ["Chuni Liu", "Hongjie Li", "Jiaqi Du", "Yangyang Hou", "Qian Sun", "Lei Jin", "Ke Xu"], "title": "Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset", "comment": null, "summary": "The pretraining-finetuning paradigm is a crucial strategy in metallic surface\ndefect detection for mitigating the challenges posed by data scarcity. However,\nits implementation presents a critical dilemma. Pretraining on natural image\ndatasets such as ImageNet, faces a significant domain gap. Meanwhile, naive\nself-supervised pretraining on in-domain industrial data is often ineffective\ndue to the inability of existing learning objectives to distinguish subtle\ndefect patterns from complex background noise and textures. To resolve this, we\nintroduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm\nthat explicitly guides representation learning through anomaly priors. AGSSP\nemploys a two-stage framework: (1) it first pretrains the model's backbone by\ndistilling knowledge from anomaly maps, encouraging the network to capture\ndefect-salient features; (2) it then pretrains the detector using pseudo-defect\nboxes derived from these maps, aligning it with localization tasks. To enable\nthis, we develop a knowledge-enhanced method to generate high-quality anomaly\nmaps and collect a large-scale industrial dataset of 120,000 images.\nAdditionally, we present two small-scale, pixel-level labeled metallic surface\ndefect datasets for validation. Extensive experiments demonstrate that AGSSP\nconsistently enhances performance across various settings, achieving up to a\n10\\% improvement in mAP@0.5 and 11.4\\% in mAP@0.5:0.95 compared to\nImageNet-based models. All code, pretrained models, and datasets are publicly\navailable at https://clovermini.github.io/AGSSP-Dev/.", "AI": {"tldr": "AGSSP introduces anomaly-guided self-supervised pretraining to bridge the domain gap in metallic surface defect detection by using anomaly maps to guide representation learning, achieving significant performance improvements over ImageNet-based models.", "motivation": "Traditional pretraining approaches face challenges: ImageNet pretraining has domain gap issues with industrial data, while naive self-supervised pretraining on industrial data fails to distinguish subtle defects from complex background noise.", "method": "Two-stage framework: (1) pretrain backbone by distilling knowledge from anomaly maps to capture defect-salient features, (2) pretrain detector using pseudo-defect boxes from anomaly maps. Uses knowledge-enhanced method to generate high-quality anomaly maps and collects 120,000-image industrial dataset.", "result": "AGSSP consistently enhances performance across various settings, achieving up to 10% improvement in mAP@0.5 and 11.4% in mAP@0.5:0.95 compared to ImageNet-based models.", "conclusion": "AGSSP effectively resolves the pretraining dilemma in metallic surface defect detection by leveraging anomaly priors to guide representation learning, demonstrating superior performance and providing publicly available resources."}}
{"id": "2509.18924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18924", "abs": "https://arxiv.org/abs/2509.18924", "authors": ["Kartik Teotia", "Helge Rhodin", "Mohit Mendiratta", "Hyeongwoo Kim", "Marc Habermann", "Christian Theobalt"], "title": "Audio-Driven Universal Gaussian Head Avatars", "comment": "(SIGGRAPH Asia 2025) Project page:\n  https://kartik-teotia.github.io/UniGAHA/", "summary": "We introduce the first method for audio-driven universal photorealistic\navatar synthesis, combining a person-agnostic speech model with our novel\nUniversal Head Avatar Prior (UHAP). UHAP is trained on cross-identity\nmulti-view videos. In particular, our UHAP is supervised with neutral scan\ndata, enabling it to capture the identity-specific details at high fidelity. In\ncontrast to previous approaches, which predominantly map audio features to\ngeometric deformations only while ignoring audio-dependent appearance\nvariations, our universal speech model directly maps raw audio inputs into the\nUHAP latent expression space. This expression space inherently encodes, both,\ngeometric and appearance variations. For efficient personalization to new\nsubjects, we employ a monocular encoder, which enables lightweight regression\nof dynamic expression variations across video frames. By accounting for these\nexpression-dependent changes, it enables the subsequent model fine-tuning stage\nto focus exclusively on capturing the subject's global appearance and geometry.\nDecoding these audio-driven expression codes via UHAP generates highly\nrealistic avatars with precise lip synchronization and nuanced expressive\ndetails, such as eyebrow movement, gaze shifts, and realistic mouth interior\nappearance as well as motion. Extensive evaluations demonstrate that our method\nis not only the first generalizable audio-driven avatar model that can account\nfor detailed appearance modeling and rendering, but it also outperforms\ncompeting (geometry-only) methods across metrics measuring lip-sync accuracy,\nquantitative image quality, and perceptual realism.", "AI": {"tldr": "First method for audio-driven universal photorealistic avatar synthesis using Universal Head Avatar Prior (UHAP) that captures both geometric and appearance variations from audio inputs, outperforming geometry-only approaches.", "motivation": "Previous avatar synthesis methods primarily map audio to geometric deformations while ignoring appearance variations. There's a need for a universal approach that can generate photorealistic avatars with both accurate lip synchronization and nuanced expressive details.", "method": "Combines person-agnostic speech model with UHAP trained on cross-identity multi-view videos. Uses monocular encoder for efficient personalization and maps raw audio to UHAP latent expression space that encodes both geometry and appearance variations.", "result": "Generates highly realistic avatars with precise lip synchronization, eyebrow movement, gaze shifts, and realistic mouth interior appearance. Outperforms competing geometry-only methods in lip-sync accuracy, image quality, and perceptual realism.", "conclusion": "This is the first generalizable audio-driven avatar model that accounts for detailed appearance modeling and rendering, demonstrating superior performance over existing approaches."}}
{"id": "2509.18926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18926", "abs": "https://arxiv.org/abs/2509.18926", "authors": ["Pamela Osuna-Vargas", "Altug Kamacioglu", "Dominik F. Aschauer", "Petros E. Vlachos", "Sercan Alipek", "Jochen Triesch", "Simon Rumpel", "Matthias Kaschube"], "title": "SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines", "comment": null, "summary": "Dendritic spines are key structural components of excitatory synapses in the\nbrain. Given the size of dendritic spines provides a proxy for synaptic\nefficacy, their detection and tracking across time is important for studies of\nthe neural basis of learning and memory. Despite their relevance, large-scale\nanalyses of the structural dynamics of dendritic spines in 3D+time microscopy\ndata remain challenging and labor-intense. Here, we present a modular machine\nlearning-based pipeline designed to automate the detection, time-tracking, and\nfeature extraction of dendritic spines in volumes chronically recorded with\ntwo-photon microscopy. Our approach tackles the challenges posed by biological\ndata by combining a transformer-based detection module, a depth-tracking\ncomponent that integrates spatial features, a time-tracking module to associate\n3D spines across time by leveraging spatial consistency, and a feature\nextraction unit that quantifies biologically relevant spine properties. We\nvalidate our method on open-source labeled spine data, and on two complementary\nannotated datasets that we publish alongside this work: one for detection and\ndepth-tracking, and one for time-tracking, which, to the best of our knowledge,\nis the first data of this kind. To encourage future research, we release our\ndata, code, and pre-trained weights at\nhttps://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,\nend-to-end analysis of dendritic spine dynamics.", "AI": {"tldr": "A machine learning pipeline for automated detection, tracking, and analysis of dendritic spines in 3D+time microscopy data to study synaptic structural dynamics.", "motivation": "Large-scale analysis of dendritic spine dynamics is challenging and labor-intensive, but crucial for understanding neural basis of learning and memory since spine size correlates with synaptic efficacy.", "method": "Modular pipeline combining transformer-based detection, depth-tracking with spatial features, time-tracking using spatial consistency, and feature extraction for biological properties. Validated on open-source and new annotated datasets.", "result": "Method validated on published datasets and two new complementary annotated datasets (detection/depth-tracking and time-tracking), with code, data, and pre-trained weights released publicly.", "conclusion": "Establishes a scalable, end-to-end baseline for analyzing dendritic spine dynamics, addressing challenges in biological data analysis and enabling future research in synaptic plasticity studies."}}
{"id": "2509.18938", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18938", "abs": "https://arxiv.org/abs/2509.18938", "authors": ["Matheus Vin\u00edcius Todescato", "Joel Lu\u00eds Carbonera"], "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning", "comment": "This paper was accepted at International Conference on Tools with\n  Artificial Intelligence (ICTAI) 2025", "summary": "While deep learning, including Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs), has significantly advanced classification\nperformance, its typical reliance on extensive annotated datasets presents a\nmajor obstacle in many practical scenarios where such data is scarce.\nVision-language models (VLMs) and transfer learning with pre-trained visual\nmodels appear as promising techniques to deal with this problem. This paper\nproposes a novel zero-shot image classification framework that combines a VLM\nand a pre-trained visual model within a self-learning cycle. Requiring only the\nset of class names and no labeled training data, our method utilizes a\nconfidence-based pseudo-labeling strategy to train a lightweight classifier\ndirectly on the test data, enabling dynamic adaptation. The VLM identifies\nhigh-confidence samples, and the pre-trained visual model enhances their visual\nrepresentations. These enhanced features then iteratively train the classifier,\nallowing the system to capture complementary semantic and visual cues without\nsupervision. Notably, our approach avoids VLM fine-tuning and the use of large\nlanguage models, relying on the visual-only model to reduce the dependence on\nsemantic representation. Experimental evaluations on ten diverse datasets\ndemonstrate that our approach outperforms the baseline zero-shot method.", "AI": {"tldr": "A zero-shot image classification framework combining vision-language models and pre-trained visual models in a self-learning cycle, requiring only class names and no labeled data.", "motivation": "Deep learning's reliance on extensive annotated datasets is problematic in scenarios with scarce data. Vision-language models and transfer learning offer promising solutions to address this limitation.", "method": "Uses confidence-based pseudo-labeling to train a lightweight classifier directly on test data. A VLM identifies high-confidence samples, while a pre-trained visual model enhances their representations. These enhanced features iteratively train the classifier to capture complementary semantic and visual cues without supervision.", "result": "Experimental evaluations on ten diverse datasets demonstrate that the approach outperforms baseline zero-shot methods.", "conclusion": "The proposed framework successfully enables dynamic adaptation for image classification without requiring labeled training data, VLM fine-tuning, or large language models, reducing dependence on semantic representation."}}
{"id": "2509.18956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18956", "abs": "https://arxiv.org/abs/2509.18956", "authors": ["Zijing Guo", "Yunyang Zhao", "Lin Wang"], "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting", "comment": null, "summary": "Mirror-containing environments pose unique challenges for 3D reconstruction\nand novel view synthesis (NVS), as reflective surfaces introduce view-dependent\ndistortions and inconsistencies. While cutting-edge methods such as Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical\nscenes, their performance deteriorates in the presence of mirrors. Existing\nsolutions mainly focus on handling mirror surfaces through symmetry mapping but\noften overlook the rich information carried by mirror reflections. These\nreflections offer complementary perspectives that can fill in absent details\nand significantly enhance reconstruction quality. To advance 3D reconstruction\nin mirror-rich environments, we present MirrorScene3D, a comprehensive dataset\nfeaturing diverse indoor scenes, 1256 high-quality images, and annotated mirror\nmasks, providing a benchmark for evaluating reconstruction methods in\nreflective settings. Building on this, we propose ReflectiveGS, an extension of\n3D Gaussian Splatting that utilizes mirror reflections as complementary\nviewpoints rather than simple symmetry artifacts, enhancing scene geometry and\nrecovering absent details. Experiments on MirrorScene3D show that\nReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and\ntraining speed, setting a new benchmark for 3D reconstruction in mirror-rich\nenvironments.", "AI": {"tldr": "MirrorScene3D dataset and ReflectiveGS method address 3D reconstruction challenges in mirror-rich environments by leveraging mirror reflections as complementary viewpoints rather than treating them as artifacts.", "motivation": "Existing 3D reconstruction methods like NeRF and 3DGS perform poorly in mirror-containing environments because they treat reflections as distortions rather than valuable information sources that can enhance scene understanding and fill in missing details.", "method": "Proposes ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints to improve scene geometry and recover absent details, along with creating the MirrorScene3D dataset for benchmarking.", "result": "ReflectiveGS outperforms existing methods on SSIM, PSNR, LPIPS metrics and training speed on the MirrorScene3D dataset, demonstrating superior 3D reconstruction quality in mirror-rich environments.", "conclusion": "The approach successfully transforms mirror reflections from problematic artifacts into valuable information sources, setting a new benchmark for 3D reconstruction in reflective environments and opening new possibilities for scene understanding."}}
{"id": "2509.18958", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18958", "abs": "https://arxiv.org/abs/2509.18958", "authors": ["Cristina Iacono", "Mariarosaria Meola", "Federica Conte", "Laura Mecozzi", "Umberto Bracale", "Pietro Falco", "Fanny Ficuciello"], "title": "Generative data augmentation for biliary tract detection on intraoperative images", "comment": null, "summary": "Cholecystectomy is one of the most frequently performed procedures in\ngastrointestinal surgery, and the laparoscopic approach is the gold standard\nfor symptomatic cholecystolithiasis and acute cholecystitis. In addition to the\nadvantages of a significantly faster recovery and better cosmetic results, the\nlaparoscopic approach bears a higher risk of bile duct injury, which has a\nsignificant impact on quality of life and survival. To avoid bile duct injury,\nit is essential to improve the intraoperative visualization of the bile duct.\nThis work aims to address this problem by leveraging a deep-learning approach\nfor the localization of the biliary tract from white-light images acquired\nduring the surgical procedures. To this end, the construction and annotation of\nan image database to train the Yolo detection algorithm has been employed.\nBesides classical data augmentation techniques, the paper proposes Generative\nAdversarial Network (GAN) for the generation of a synthetic portion of the\ntraining dataset. Experimental results have been discussed along with ethical\nconsiderations.", "AI": {"tldr": "This paper presents a deep-learning approach using Yolo detection algorithm and GAN-based data augmentation to localize the biliary tract from white-light surgical images during laparoscopic cholecystectomy, aiming to reduce bile duct injuries.", "motivation": "Laparoscopic cholecystectomy, while advantageous for recovery and cosmetic results, carries a higher risk of bile duct injury which significantly impacts patient quality of life and survival. Improving intraoperative visualization of the bile duct is essential to prevent these injuries.", "method": "The authors constructed and annotated an image database to train the Yolo detection algorithm for bile duct localization. They employed classical data augmentation techniques and proposed using Generative Adversarial Networks (GANs) to generate synthetic training data.", "result": "Experimental results were discussed, though specific performance metrics are not provided in the abstract. The approach demonstrates the feasibility of using deep learning for bile duct localization in surgical settings.", "conclusion": "The paper presents a promising deep-learning solution for biliary tract localization during laparoscopic surgery, with ethical considerations addressed alongside the technical implementation."}}
{"id": "2509.18973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18973", "abs": "https://arxiv.org/abs/2509.18973", "authors": ["Jiabao Chen", "Shan Xiong", "Jialin Peng"], "title": "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images", "comment": "MICCAI2025", "summary": "Domain adaptive segmentation (DAS) of numerous organelle instances from\nlarge-scale electron microscopy (EM) is a promising way to enable\nannotation-efficient learning. Inspired by SAM, we propose a promptable\nmultitask framework, namely Prompt-DAS, which is flexible enough to utilize any\nnumber of point prompts during the adaptation training stage and testing stage.\nThus, with varying prompt configurations, Prompt-DAS can perform unsupervised\ndomain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well\nas interactive segmentation during testing. Unlike the foundation model SAM,\nwhich necessitates a prompt for each individual object instance, Prompt-DAS is\nonly trained on a small dataset and can utilize full points on all instances,\nsparse points on partial instances, or even no points at all, facilitated by\nthe incorporation of an auxiliary center-point detection task. Moreover, a\nnovel prompt-guided contrastive learning is proposed to enhance discriminative\nfeature learning. Comprehensive experiments conducted on challenging benchmarks\ndemonstrate the effectiveness of the proposed approach over existing UDA, WDA,\nand SAM-based approaches.", "AI": {"tldr": "Prompt-DAS is a promptable multitask framework for domain adaptive segmentation of organelle instances from EM images, enabling flexible prompt usage for UDA, WDA, and interactive segmentation with improved efficiency over SAM.", "motivation": "To enable annotation-efficient learning for domain adaptive segmentation of organelle instances from large-scale electron microscopy by developing a flexible framework that can work with varying prompt configurations.", "method": "Proposes Prompt-DAS, a promptable multitask framework that incorporates an auxiliary center-point detection task and novel prompt-guided contrastive learning to enhance discriminative feature learning. It can utilize full points, sparse points, or no points during training and testing.", "result": "Comprehensive experiments on challenging benchmarks demonstrate effectiveness over existing UDA, WDA, and SAM-based approaches.", "conclusion": "Prompt-DAS provides a flexible and efficient solution for domain adaptive segmentation that outperforms existing methods while requiring less annotation effort compared to foundation models like SAM."}}
{"id": "2509.19002", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19002", "abs": "https://arxiv.org/abs/2509.19002", "authors": ["Hao Wang", "Eiki Murata", "Lingfang Zhang", "Ayako Sato", "So Fukuda", "Ziqi Yin", "Wentao Hu", "Keisuke Nakao", "Yusuke Nakamura", "Sebastian Zwirner", "Yi-Chia Chen", "Hiroyuki Otomo", "Hiroki Ouchi", "Daisuke Kawahara"], "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.", "AI": {"tldr": "VIR-Bench is a new benchmark for evaluating multimodal large language models' geospatial-temporal intelligence using 200 travel videos, focusing on itinerary reconstruction as a challenging task that current MLLMs struggle with.", "motivation": "Current video benchmarks mainly cover indoor scenes or short-range outdoor activities, leaving long-distance travel challenges unexplored. Mastering extended geospatial-temporal trajectories is crucial for real-world applications like embodied-AI planning and navigation.", "method": "The authors present VIR-Bench, consisting of 200 travel videos that frame itinerary reconstruction as an evaluation task. They test state-of-the-art MLLMs and conduct an in-depth case study developing a prototype travel-planning agent.", "result": "Experimental results show that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores on VIR-Bench, highlighting the difficulty of handling videos spanning extended spatial and temporal scales.", "conclusion": "The benchmark effectively evaluates MLLMs' geospatial-temporal intelligence, and the prototype travel-planning agent demonstrates that insights from VIR-Bench translate into concrete performance gains in user-facing applications."}}
{"id": "2509.19003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19003", "abs": "https://arxiv.org/abs/2509.19003", "authors": ["Honghao Chen", "Xingzhou Lou", "Xiaokun Feng", "Kaiqi Huang", "Xinlong Wang"], "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards", "comment": "Accepted by NeurIPS 2025", "summary": "Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS.", "AI": {"tldr": "This paper proposes a fine-grained chain of step reasoning framework for vision-language models, enabling accurate assessment of intermediate reasoning steps and effective reinforcement learning with fine-grained rewards.", "motivation": "Existing chain of thought reasoning in vision-language models operates at coarse-grained levels, struggling with fine-grained structured reasoning and lacking proper evaluation of intermediate reasoning quality.", "method": "The authors present a simple, transparent framework including step-level reasoning data, process reward model (PRM), and reinforcement learning training to enable fine-grained reasoning assessment.", "result": "The proposed models achieve strong baselines with consistent improvements on challenging vision-language benchmarks, and thorough analysis reveals intriguing properties of inference-time scaling.", "conclusion": "This work establishes a baseline for vision-language models and provides insights into more complex multimodal reasoning, with the framework's components (dataset, PRM, code) being made publicly available."}}
{"id": "2509.19028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19028", "abs": "https://arxiv.org/abs/2509.19028", "authors": ["Ioannis Sarafis", "Alexandros Papadopoulos", "Anastasios Delopoulos"], "title": "Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model", "comment": "Submitted to the 20th International Workshop on Semantic and Social\n  Media Adaptation & Personalization", "summary": "In this paper, we propose a weakly supervised semantic segmentation approach\nfor food images which takes advantage of the zero-shot capabilities and\npromptability of the Segment Anything Model (SAM) along with the attention\nmechanisms of Vision Transformers (ViTs). Specifically, we use class activation\nmaps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable\nfor food image segmentation. The ViT model, a Swin Transformer, is trained\nexclusively using image-level annotations, eliminating the need for pixel-level\nannotations during training. Additionally, to enhance the quality of the\nSAM-generated masks, we examine the use of image preprocessing techniques in\ncombination with single-mask and multi-mask SAM generation strategies. The\nmethodology is evaluated on the FoodSeg103 dataset, generating an average of\n2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for\nthe multi-mask scenario. We envision the proposed approach as a tool to\naccelerate food image annotation tasks or as an integrated component in food\nand nutrition tracking applications.", "AI": {"tldr": "A weakly supervised semantic segmentation method for food images using ViT-generated CAMs as prompts for SAM, achieving mIoU of 0.54 on FoodSeg103 dataset with only image-level annotations.", "motivation": "To develop a food image segmentation approach that eliminates the need for pixel-level annotations during training, leveraging zero-shot capabilities of SAM and attention mechanisms of ViTs to accelerate food image annotation and support nutrition tracking applications.", "method": "Uses class activation maps (CAMs) from Swin Transformer ViT trained with image-level annotations as prompts for Segment Anything Model (SAM). Explores image preprocessing techniques and single-mask/multi-mask SAM generation strategies to enhance mask quality.", "result": "Achieved mIoU of 0.54 on FoodSeg103 dataset with multi-mask scenario, generating average of 2.4 masks per image (excluding background).", "conclusion": "The proposed approach serves as an effective tool for accelerating food image annotation and can be integrated into food and nutrition tracking applications, demonstrating the viability of weakly supervised segmentation using SAM and ViT attention mechanisms."}}
{"id": "2509.19052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19052", "abs": "https://arxiv.org/abs/2509.19052", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation", "comment": null, "summary": "Accurate segmentation of cardiac anatomy in echocardiography is essential for\ncardiovascular diagnosis and treatment. Yet echocardiography is prone to\ndeformation and speckle noise, causing frame-to-frame segmentation jitter. Even\nwith high accuracy in single-frame segmentation, temporal instability can\nweaken functional estimates and impair clinical interpretability. To address\nthese issues, we propose DyL-UNet, a dynamic learning-based temporal\nconsistency U-Net segmentation architecture designed to achieve temporally\nstable and precise echocardiographic segmentation. The framework constructs an\nEcho-Dynamics Graph (EDG) through dynamic learning to extract dynamic\ninformation from videos. DyL-UNet incorporates multiple Swin-Transformer-based\nencoder-decoder branches for processing single-frame images. It further\nintroduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,\nwhich uses EDG-encoded dynamic features and cardiac-phase cues to enforce\ntemporal consistency during segmentation. Extensive experiments on the CAMUS\nand EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation\naccuracy comparable to existing methods while achieving superior temporal\nconsistency, providing a reliable solution for automated clinical\nechocardiography.", "AI": {"tldr": "DyL-UNet is a dynamic learning-based U-Net architecture that achieves temporally stable and precise echocardiographic segmentation by incorporating Echo-Dynamics Graph and Cardiac Phase-Dynamics Attention mechanisms.", "motivation": "Echocardiography suffers from deformation and speckle noise causing frame-to-frame segmentation jitter, which weakens functional estimates and impairs clinical interpretability despite high single-frame accuracy.", "method": "Proposes DyL-UNet with Echo-Dynamics Graph (EDG) for dynamic information extraction, multiple Swin-Transformer-based encoder-decoder branches, and Cardiac Phase-Dynamics Attention (CPDA) at skip connections to enforce temporal consistency.", "result": "Extensive experiments on CAMUS and EchoNet-Dynamic datasets show DyL-UNet maintains comparable segmentation accuracy to existing methods while achieving superior temporal consistency.", "conclusion": "DyL-UNet provides a reliable solution for automated clinical echocardiography by addressing temporal instability issues in cardiac segmentation."}}
{"id": "2509.19070", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19070", "abs": "https://arxiv.org/abs/2509.19070", "authors": ["Zijian Ling", "Han Zhang", "Yazhuo Zhou", "Jiahao Cui"], "title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?", "comment": "Accepted at the Open Science for Foundation Models (SCI-FM) Workshop\n  at ICLR 2025", "summary": "This paper presents ColorBlindnessEval, a novel benchmark designed to\nevaluate the robustness of Vision-Language Models (VLMs) in visually\nadversarial scenarios inspired by the Ishihara color blindness test. Our\ndataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with\nvarying color combinations, challenging VLMs to accurately recognize numerical\ninformation embedded in complex visual patterns. We assess 9 VLMs using Yes/No\nand open-ended prompts and compare their performance with human participants.\nOur experiments reveal limitations in the models' ability to interpret numbers\nin adversarial contexts, highlighting prevalent hallucination issues. These\nfindings underscore the need to improve the robustness of VLMs in complex\nvisual environments. ColorBlindnessEval serves as a valuable tool for\nbenchmarking and improving the reliability of VLMs in real-world applications\nwhere accuracy is critical.", "AI": {"tldr": "ColorBlindnessEval is a benchmark for evaluating Vision-Language Models' robustness using Ishihara-like color blindness test images, revealing limitations in numerical recognition and hallucination issues.", "motivation": "To assess VLMs' performance in visually adversarial scenarios inspired by color blindness tests, addressing the need for robustness in complex visual environments.", "method": "Created a dataset of 500 Ishihara-like images with numbers 0-99, tested 9 VLMs using Yes/No and open-ended prompts, and compared results with human participants.", "result": "VLMs showed limitations in interpreting numbers in adversarial contexts, with prevalent hallucination issues, performing worse than humans.", "conclusion": "The benchmark highlights the need to improve VLM robustness and serves as a valuable tool for benchmarking reliability in real-world applications."}}
{"id": "2509.19073", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19073", "abs": "https://arxiv.org/abs/2509.19073", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become a powerful representation for\nimage-based object reconstruction, yet its performance drops sharply in\nsparse-view settings. Prior works address this limitation by employing\ndiffusion models to repair corrupted renders, subsequently using them as pseudo\nground truths for later optimization. While effective, such approaches incur\nheavy computation from the diffusion fine-tuning and repair steps. We present\nWaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object\nreconstruction. Our key idea is to shift diffusion into the wavelet domain:\ndiffusion is applied only to the low-resolution LL subband, while\nhigh-frequency subbands are refined with a lightweight network. We further\npropose an efficient online random masking strategy to curate training pairs\nfor diffusion fine-tuning, replacing the commonly used, but inefficient,\nleave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360\nand OmniObject3D, show WaveletGaussian achieves competitive rendering quality\nwhile substantially reducing training time.", "AI": {"tldr": "WaveletGaussian is an efficient framework for sparse-view 3D Gaussian object reconstruction that shifts diffusion to the wavelet domain, using diffusion only on low-resolution subbands and lightweight network refinement for high-frequency subbands, with an online random masking strategy to reduce training time.", "motivation": "3D Gaussian Splatting (3DGS) performs poorly in sparse-view settings, and existing diffusion-based repair methods are computationally expensive due to fine-tuning and repair steps.", "method": "Shift diffusion into wavelet domain: apply diffusion only to low-resolution LL subband, refine high-frequency subbands with lightweight network. Use efficient online random masking strategy instead of leave-one-out for training pair curation.", "result": "Achieves competitive rendering quality on Mip-NeRF 360 and OmniObject3D datasets while substantially reducing training time compared to previous methods.", "conclusion": "WaveletGaussian provides an efficient alternative for sparse-view 3D Gaussian reconstruction by leveraging wavelet domain processing and optimized training strategies, maintaining quality while significantly improving computational efficiency."}}
{"id": "2509.19082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19082", "abs": "https://arxiv.org/abs/2509.19082", "authors": ["Alexey Nekrasov", "Ali Athar", "Daan de Geus", "Alexander Hermans", "Bastian Leibe"], "title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference", "comment": null, "summary": "Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i", "AI": {"tldr": "Sa2VA-i is an improved version of Sa2VA that fixes inconsistencies between training and inference procedures, achieving state-of-the-art results on multiple video segmentation benchmarks with significant performance improvements.", "motivation": "The original Sa2VA model underperforms on referring video object segmentation tasks due to inconsistencies between training and inference procedures, despite its strong performance on other segmentation benchmarks.", "method": "The authors propose Sa2VA-i, which rectifies the identified inconsistencies in the original Sa2VA model while using the same checkpoints.", "result": "Sa2VA-i achieves substantial improvements: +11.6 J&F on MeViS, +1.4 on Ref-YT-VOS, +3.3 on Ref-DAVIS, and +4.1 on ReVOS. The Sa2VA-i-1B model performs on par with the original Sa2VA-26B model on MeViS.", "conclusion": "The work demonstrates the importance of seemingly trivial implementation details and provides valuable insights for the referring video segmentation field."}}
{"id": "2509.19087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19087", "abs": "https://arxiv.org/abs/2509.19087", "authors": ["Ganesh Mallya", "Yotam Gigi", "Dahun Kim", "Maxim Neumann", "Genady Beryozkin", "Tomer Shekel", "Anelia Angelova"], "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications", "comment": null, "summary": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.", "AI": {"tldr": "A training-free approach to adapt generalist multimodal models (like Gemini2.5) for multi-spectral imagery analysis without retraining, enabling zero-shot performance on remote sensing tasks.", "motivation": "Multi-spectral imagery is valuable for remote sensing but requires specialized ML models that are costly to train. Generalist multimodal models can't handle multi-spectral inputs despite their powerful capabilities.", "method": "Proposes adapting inputs to the model's visual space and injecting domain-specific information as instructions, allowing generalist models to process multi-spectral data in zero-shot mode without retraining.", "result": "Achieves strong zero-shot performance gains on popular remote sensing benchmarks for land cover and land use classification using Gemini2.5.", "conclusion": "Enables geospatial professionals to easily leverage powerful multimodal models for specialized sensor data, accelerating work with rich reasoning capabilities."}}
{"id": "2509.19090", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19090", "abs": "https://arxiv.org/abs/2509.19090", "authors": ["Guoxin Wang", "Jun Zhao", "Xinyi Liu", "Yanbo Liu", "Xuyang Cao", "Chao Li", "Zhuoyun Liu", "Qintian Sun", "Fangru Zhou", "Haoqiang Xing", "Zhenhong Yang"], "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning", "comment": null, "summary": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.", "AI": {"tldr": "Citrus-V is a multimodal medical foundation model that integrates detection, segmentation, and chain-of-thought reasoning for unified clinical imaging analysis.", "motivation": "Existing medical imaging models are narrowly focused and require multiple specialized networks, limiting generalization. Clinical applications demand precise visual grounding, multimodal integration, and reasoning capabilities.", "method": "Proposes a novel multimodal training approach combining image analysis with textual reasoning, enabling pixel-level lesion localization, structured report generation, and diagnostic inference in a single framework.", "result": "Outperforms existing open-source medical models and expert-level imaging systems across multiple benchmarks, supporting precise lesion quantification, automated reporting, and reliable second opinions.", "conclusion": "Citrus-V delivers a unified pipeline from visual grounding to clinical reasoning, addressing the limitations of specialized medical imaging models through multimodal integration."}}
{"id": "2509.19096", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19096", "abs": "https://arxiv.org/abs/2509.19096", "authors": ["Ilhan Skender", "Kailin Tong", "Selim Solmaz", "Daniel Watzenig"], "title": "Investigating Traffic Accident Detection Using Multimodal Large Language Models", "comment": "Accepted for presentation at the 2025 IEEE International Automated\n  Vehicle Validation Conference (IAVVC 2025). Final version to appear in IEEE\n  Xplore", "summary": "Traffic safety remains a critical global concern, with timely and accurate\naccident detection essential for hazard reduction and rapid emergency response.\nInfrastructure-based vision sensors offer scalable and efficient solutions for\ncontinuous real-time monitoring, facilitating automated detection of acci-\ndents directly from captured images. This research investigates the zero-shot\ncapabilities of multimodal large language models (MLLMs) for detecting and\ndescribing traffic accidents using images from infrastructure cameras, thus\nminimizing reliance on extensive labeled datasets. Main contributions include:\n(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,\nexplicitly addressing the scarcity of diverse, realistic, infrastructure-based\naccident data through controlled simulations; (2) Comparative performance\nanalysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent\nidentification and descriptive capabilities without prior fine-tuning; and (3)\nIntegration of advanced visual analytics, specifically YOLO for object\ndetection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for\ninstance segmentation, into enhanced prompts to improve model accuracy and\nexplainability. Key numerical results show Pixtral as the top performer with an\nF1-score of 0.71 and 83% recall, while Gemini models gained precision with\nenhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and\nrecall losses. Gemma 3 offered the most balanced performance with minimal\nmetric fluctuation. These findings demonstrate the substantial potential of\nintegrating MLLMs with advanced visual analytics techniques, enhancing their\napplicability in real-world automated traffic monitoring systems.", "AI": {"tldr": "This paper investigates zero-shot capabilities of multimodal large language models (MLLMs) for traffic accident detection using infrastructure camera images, evaluating models like Gemini, Gemma 3, and Pixtral on simulated accident data.", "motivation": "Traffic safety requires timely accident detection, and infrastructure-based vision sensors offer scalable solutions. The research aims to minimize reliance on extensive labeled datasets by exploring MLLMs' zero-shot capabilities.", "method": "Used simulated DeepAccident dataset from CARLA, evaluated MLLMs (Gemini 1.5/2.0, Gemma 3, Pixtral) without fine-tuning, and integrated visual analytics (YOLO, Deep SORT, SAM) into enhanced prompts for improved accuracy.", "result": "Pixtral performed best with F1-score of 0.71 and 83% recall. Gemini models gained precision (up to 90%) but suffered F1/recall losses. Gemma 3 showed most balanced performance with minimal metric fluctuation.", "conclusion": "MLLMs integrated with advanced visual analytics show substantial potential for real-world automated traffic monitoring systems, demonstrating effective zero-shot accident detection capabilities."}}
{"id": "2509.19115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19115", "abs": "https://arxiv.org/abs/2509.19115", "authors": ["G\u00f6rkay Aydemir", "Weidi Xie", "Fatma G\u00fcney"], "title": "Track-On2: Enhancing Online Point Tracking with Memory", "comment": null, "summary": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2", "AI": {"tldr": "Track-On2 is a transformer-based model for online long-term point tracking that improves performance and efficiency through architectural refinements, better memory usage, and enhanced synthetic training strategies.", "motivation": "Address the problem of long-term point tracking under significant appearance changes, motion, and occlusion in online settings suitable for real-time applications.", "method": "Causal transformer-based model with memory mechanism for temporal coherence, using coarse patch-level classification followed by refinement at inference, trained purely on synthetic data.", "result": "Achieves state-of-the-art results across five synthetic and real-world benchmarks, surpassing prior online trackers and even strong offline methods that use bidirectional context.", "conclusion": "Demonstrates the effectiveness of causal, memory-based architectures trained on synthetic data as scalable solutions for real-world point tracking."}}
{"id": "2509.19129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19129", "abs": "https://arxiv.org/abs/2509.19129", "authors": ["Adam Romlein", "Benjamin X. Hou", "Yuval Boss", "Cynthia L. Christman", "Stacie Koslovsky", "Erin E. Moreland", "Jason Parham", "Anthony Hoogs"], "title": "KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments", "comment": "Accepted to the IEEE/CVF International Conference on Computer Vision\n  (ICCV 2025)", "summary": "We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral\nsynchronization and real-time detection of seals and polar bears. Utilized in\naerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort\nseas around Alaska, KAMERA provides up to an 80% reduction in dataset\nprocessing time over previous methods. Our rigorous calibration and hardware\nsynchronization enable using multiple spectra for object detection. All\ncollected data are annotated with metadata so they can be easily referenced\nlater. All imagery and animal detections from a survey are mapped onto a world\nplane for accurate surveyed area estimates and quick assessment of survey\nresults. We hope KAMERA will inspire other mapping and detection efforts in the\nscientific community, with all software, models, and schematics fully\nopen-sourced.", "AI": {"tldr": "KAMERA is a multi-camera, multi-spectral system for real-time detection of seals and polar bears during aerial surveys, reducing processing time by 80% compared to previous methods.", "motivation": "To improve efficiency and accuracy in aerial surveys for ice-associated seals in Arctic regions (Bering, Chukchi, and Beaufort seas), enabling better wildlife monitoring and conservation efforts.", "method": "Uses rigorous calibration and hardware synchronization of multiple cameras and spectra for object detection, with all data annotated with metadata and mapped onto a world plane for accurate area estimation.", "result": "Achieves up to 80% reduction in dataset processing time over previous methods, with all imagery and animal detections accurately mapped for quick assessment of survey results.", "conclusion": "KAMERA provides an efficient solution for wildlife detection and mapping, with all software, models, and schematics open-sourced to inspire similar scientific mapping and detection efforts."}}
{"id": "2509.19156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19156", "abs": "https://arxiv.org/abs/2509.19156", "authors": ["Maurf Hassan", "Steven Davy", "Muhammad Zawish", "Owais Bin Zuber", "Nouman Ashraf"], "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit", "comment": "This paper was accepted at ICMLA 2025. The official version will\n  appear in IEEE Xplore", "summary": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.", "AI": {"tldr": "NeuCODEX is a neuromorphic co-inference architecture that reduces data transmission and energy consumption for Spiking Neural Networks by optimizing spatial and temporal redundancy through spike-driven compression and dynamic early-exit mechanisms.", "motivation": "Full SNN inference at the edge faces latency and energy constraints from fixed timestep overheads, while existing edge-cloud co-inference systems suffer from high latency and feature transmission costs.", "method": "NeuCODEX incorporates a learned spike-driven compression module to reduce data transmission and employs a dynamic early-exit mechanism to adaptively terminate inference based on output confidence.", "result": "The system reduces data transfer by up to 2048x, edge energy consumption by over 90%, and end-to-end latency by up to 3x compared to edge-only inference, with less than 2% accuracy drop on CIFAR10, Caltech, CIFAR10-DVS, and N-Caltech datasets.", "conclusion": "NeuCODEX enables practical, high-performance SNN deployment in resource-constrained environments by significantly improving efficiency while maintaining accuracy."}}
{"id": "2509.19165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19165", "abs": "https://arxiv.org/abs/2509.19165", "authors": ["Yun Wang", "Junjie Hu", "Junhui Hou", "Chenghao Zhang", "Renwei Yang", "Dapeng Oliver Wu"], "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions", "comment": null, "summary": "Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.", "AI": {"tldr": "A robust self-supervised stereo matching method that addresses performance degradation under adverse weather conditions by injecting visual foundation model priors and using scene correspondence learning.", "motivation": "Current self-supervised stereo matching methods perform poorly under adverse weather conditions (night, rain, fog) due to CNN feature extractor struggles with degraded regions and disrupted pixel correspondences from photometric consistency assumptions.", "method": "Inject robust priors from visual foundation models into CNN feature extractors, create synthetic stereo datasets with realistic weather degradations, and use a two-step training paradigm: robust self-supervised scene correspondence learning and adverse weather distillation.", "result": "Extensive experiments show the method outperforms existing state-of-the-art self-supervised methods under adverse weather conditions.", "conclusion": "The proposed solution effectively improves stereo matching performance in adverse weather by leveraging robust priors and scene correspondence learning, demonstrating versatility and superior performance compared to existing methods."}}
{"id": "2509.19166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19166", "abs": "https://arxiv.org/abs/2509.19166", "authors": ["Siddharth Gupta", "Jitin Singla"], "title": "YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives", "comment": null, "summary": "Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal\nmucosal cell proliferation called polyps in the inner wall of the colon. When\nleft undetected, polyps can become malignant tumors. Colonoscopy is the\nstandard procedure for detecting polyps, as it enables direct visualization and\nremoval of suspicious lesions. Manual detection by colonoscopy can be\ninconsistent and is subject to oversight. Therefore, object detection based on\ndeep learning offers a better solution for a more accurate and real-time\ndiagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based\npolyp detection pipeline, trained using M2IoU loss, versatile data\naugmentations and negative data to replicate real clinical situations. Our\npipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp\ndatasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12\nand mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg\ndataset. The significant increase is achieved in mAP$_{50:95}$ score, showing\nthe precision of polyp detection. We show robustness based on polyp size and\nprecise location detection, making it clinically relevant in AI-assisted\ncolorectal screening.", "AI": {"tldr": "YOLO-LAN is a YOLO-based polyp detection pipeline that outperforms existing methods on colorectal cancer screening datasets, achieving high precision in real-time detection.", "motivation": "Manual polyp detection during colonoscopy is inconsistent and prone to oversight, so deep learning-based object detection offers a more accurate and real-time solution for colorectal cancer screening.", "method": "Proposed YOLO-LAN pipeline using YOLO-based architecture trained with M2IoU loss, versatile data augmentations, and negative data to replicate real clinical situations.", "result": "Achieved mAP$_{50}$ of 0.9619 and mAP$_{50:95}$ of 0.8599 with YOLOv12, and mAP$_{50}$ of 0.9540 and mAP$_{50:95}$ of 0.8487 with YOLOv8 on Kvasir-seg dataset, showing significant improvement in precision.", "conclusion": "The pipeline demonstrates robustness in polyp size detection and precise location detection, making it clinically relevant for AI-assisted colorectal screening."}}
{"id": "2509.19183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19183", "abs": "https://arxiv.org/abs/2509.19183", "authors": ["Mingqi Gao", "Jingkun Chen", "Yunqi Miao", "Gengshen Wu", "Zhijin Qin", "Jungong Han"], "title": "The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC", "comment": null, "summary": "This technical report explores the MOSEv2 track of the LSVOS Challenge, which\ntargets complex semi-supervised video object segmentation. By analysing and\nadapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its\nlong-term memory and concept-aware memory, showing that long-term memory\npreserves temporal continuity under occlusion and reappearance, while\nconcept-aware memory supplies semantic priors that suppress distractors;\ntogether, these traits directly benefit several MOSEv2's core challenges. Our\nsolution achieves a JF score of 39.89% on the test set, ranking 1st in the\nMOSEv2 track of the LSVOS Challenge.", "AI": {"tldr": "The paper presents a winning solution for the MOSEv2 track of LSVOS Challenge using an enhanced SAM-2 framework (SeC) with long-term and concept-aware memory mechanisms.", "motivation": "To address complex semi-supervised video object segmentation challenges, particularly handling temporal continuity under occlusion/reappearance and suppressing distractors in the MOSEv2 track.", "method": "Analyzed and adapted SeC (enhanced SAM-2 framework) with long-term memory for temporal continuity and concept-aware memory for semantic priors to suppress distractors.", "result": "Achieved a JF score of 39.89% on the test set, ranking 1st in the MOSEv2 track of the LSVOS Challenge.", "conclusion": "The combination of long-term memory (preserving temporal continuity) and concept-aware memory (providing semantic priors) effectively addresses core challenges in semi-supervised video object segmentation."}}
{"id": "2509.19191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19191", "abs": "https://arxiv.org/abs/2509.19191", "authors": ["Yueyan Li", "Chenggong Zhao", "Zeyuan Zang", "Caixia Yuan", "Xiaojie Wang"], "title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable performance across\na variety of real-world tasks. However, existing VLMs typically process visual\ninformation by serializing images, a method that diverges significantly from\nthe parallel nature of human vision. Moreover, their opaque internal mechanisms\nhinder both deeper understanding and architectural innovation. Inspired by the\ndual-stream hypothesis of human vision, which distinguishes the \"what\" and\n\"where\" pathways, we deconstruct the visual processing in VLMs into object\nrecognition and spatial perception for separate study. For object recognition,\nwe convert images into text token maps and find that the model's perception of\nimage content unfolds as a two-stage process from shallow to deep layers,\nbeginning with attribute recognition and culminating in semantic\ndisambiguation. For spatial perception, we theoretically derive and empirically\nverify the geometric structure underlying the positional representation in\nVLMs. Based on these findings, we introduce an instruction-agnostic token\ncompression algorithm based on a plug-and-play visual decoder to improve\ndecoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.\nThrough rigorous experiments, our work validates these analyses, offering a\ndeeper understanding of VLM internals and providing clear principles for\ndesigning more capable future architectures.", "AI": {"tldr": "This paper analyzes vision-language models' visual processing by deconstructing it into object recognition and spatial perception pathways, revealing a two-stage recognition process and geometric positional structure, then proposes efficiency improvements.", "motivation": "Existing VLMs process images serially unlike human parallel vision, and their opaque mechanisms hinder understanding and innovation. The research aims to bridge this gap using human vision principles.", "method": "Deconstructs VLM visual processing using dual-stream hypothesis: object recognition via text token maps revealing two-stage process, and spatial perception via theoretical derivation of geometric positional structure. Proposes token compression algorithm and RoPE scaling technique.", "result": "Validated analyses show VLM perception unfolds from attribute recognition to semantic disambiguation, and identified geometric structure in positional representations. Proposed methods improve decoding efficiency and spatial reasoning.", "conclusion": "The work provides deeper understanding of VLM internals and clear design principles for more capable architectures, bridging the gap between serial VLM processing and human parallel vision."}}
{"id": "2509.19203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19203", "abs": "https://arxiv.org/abs/2509.19203", "authors": ["Ioanna Ntinou", "Alexandros Xenos", "Yassine Ouali", "Adrian Bulat", "Georgios Tzimiropoulos"], "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions", "comment": "Accepted at EMNLP 2025", "summary": "Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have\nbecome the standard approach for learning discriminative vision-language\nrepresentations. However, these models often exhibit shallow language\nunderstanding, manifesting bag-of-words behaviour. These limitations are\nreinforced by their dual-encoder design, which induces a modality gap.\nAdditionally, the reliance on vast web-collected data corpora for training\nmakes the process computationally expensive and introduces significant privacy\nconcerns. To address these limitations, in this work, we challenge the\nnecessity of vision encoders for retrieval tasks by introducing a vision-free,\nsingle-encoder retrieval pipeline. Departing from the traditional text-to-image\nretrieval paradigm, we migrate to a text-to-text paradigm with the assistance\nof VLLM-generated structured image descriptions. We demonstrate that this\nparadigm shift has significant advantages, including a substantial reduction of\nthe modality gap, improved compositionality, and better performance on short\nand long caption queries, all attainable with only a few hours of calibration\non two GPUs. Additionally, substituting raw images with textual descriptions\nintroduces a more privacy-friendly alternative for retrieval. To further assess\ngeneralisation and address some of the shortcomings of prior compositionality\nbenchmarks, we release two benchmarks derived from Flickr30k and COCO,\ncontaining diverse compositional queries made of short captions, which we coin\nsubFlickr and subCOCO. Our vision-free retriever matches and often surpasses\ntraditional multimodal models. Importantly, our approach achieves\nstate-of-the-art zero-shot performance on multiple retrieval and\ncompositionality benchmarks, with models as small as 0.3B parameters. Code is\navailable at: https://github.com/IoannaNti/LexiCLIP", "AI": {"tldr": "A vision-free text-to-text retrieval approach using VLLM-generated image descriptions that matches or surpasses traditional multimodal models while being more efficient and privacy-friendly.", "motivation": "Address limitations of contrastively-trained VLMs like CLIP, including shallow language understanding, modality gap, computational expense, and privacy concerns from web-collected training data.", "method": "Replace traditional text-to-image retrieval with text-to-text paradigm using structured image descriptions generated by VLLMs, requiring only few hours of calibration on two GPUs.", "result": "Achieves state-of-the-art zero-shot performance on multiple retrieval benchmarks, reduces modality gap, improves compositionality, and works well with models as small as 0.3B parameters.", "conclusion": "Vision-free retrieval with textual image descriptions is a viable alternative that outperforms traditional multimodal approaches while being more efficient and privacy-preserving."}}
{"id": "2509.19207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19207", "abs": "https://arxiv.org/abs/2509.19207", "authors": ["Israfel Salazar", "Desmond Elliott", "Yova Kementchedjhieva"], "title": "Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs", "comment": null, "summary": "Contrastive vision-language models (VLMs) have made significant progress in\nbinding visual and textual information, but understanding long, dense captions\nremains an open challenge. We hypothesize that compositionality, the capacity\nto reason about object-attribute bindings and inter-object relationships, is\nkey to understanding longer captions. In this paper, we investigate the\ninteraction between compositionality and long-caption understanding, asking\nwhether training for one property enhances the other. We train and evaluate a\nrange of models that target each of these capabilities. Our results reveal a\nbidirectional relationship: compositional training improves performance on\nlong-caption retrieval, and training on long captions promotes\ncompositionality. However, these gains are sensitive to data quality and model\ndesign. We find that training on poorly structured captions, or with limited\nparameter updates, fails to support generalization. Likewise, strategies that\naim at retaining general alignment, such as freezing positional embeddings, do\nnot improve compositional understanding. Overall, we find that compositional\nunderstanding and long-caption understanding are intertwined capabilities that\ncan be jointly learned through training on dense, grounded descriptions.\nDespite these challenges, we show that models trained on high-quality,\nlong-caption data can achieve strong performance in both tasks, offering\npractical guidance for improving VLM generalization.", "AI": {"tldr": "Training for compositionality improves long-caption understanding and vice versa, but gains depend on data quality and model design.", "motivation": "Understanding long, dense captions remains challenging for contrastive vision-language models, and compositionality may be key to addressing this limitation.", "method": "Train and evaluate models targeting compositionality and long-caption understanding, examining bidirectional relationships and sensitivity to data quality and architectural choices.", "result": "Bidirectional relationship found: compositional training improves long-caption retrieval, and long-caption training promotes compositionality, but gains are sensitive to data quality and model design.", "conclusion": "Compositional understanding and long-caption understanding are intertwined capabilities that can be jointly learned through training on dense, grounded descriptions, with high-quality data enabling strong performance in both tasks."}}
{"id": "2509.19208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19208", "abs": "https://arxiv.org/abs/2509.19208", "authors": ["Earl Ranario", "Ismael Mayanja", "Heesup Yun", "Brian N. Bailey", "J. Mason Earles"], "title": "Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data", "comment": null, "summary": "Accurate plant segmentation in thermal imagery remains a significant\nchallenge for high throughput field phenotyping, particularly in outdoor\nenvironments where low contrast between plants and weeds and frequent\nocclusions hinder performance. To address this, we present a framework that\nleverages synthetic RGB imagery, a limited set of real annotations, and\nGAN-based cross-modality alignment to enhance semantic segmentation in thermal\nimages. We trained models on 1,128 synthetic images containing complex mixtures\nof crop and weed plants in order to generate image segmentation masks for crop\nand weed plants. We additionally evaluated the benefit of integrating as few as\nfive real, manually segmented field images within the training process using\nvarious sampling strategies. When combining all the synthetic images with a few\nlabeled real images, we observed a maximum relative improvement of 22% for the\nweed class and 17% for the plant class compared to the full real-data baseline.\nCross-modal alignment was enabled by translating RGB to thermal using\nCycleGAN-turbo, allowing robust template matching without calibration. Results\ndemonstrated that combining synthetic data with limited manual annotations and\ncross-domain translation via generative models can significantly boost\nsegmentation performance in complex field environments for multi-model imagery.", "AI": {"tldr": "A framework using synthetic RGB imagery, limited real annotations, and GAN-based cross-modality alignment to improve plant segmentation in thermal images for field phenotyping.", "motivation": "Accurate plant segmentation in thermal imagery is challenging due to low contrast between plants and weeds and frequent occlusions in outdoor environments.", "method": "Trained models on 1,128 synthetic images with crop/weed mixtures, integrated 5 real segmented field images using various sampling strategies, and used CycleGAN-turbo for RGB-to-thermal translation to enable cross-modal alignment.", "result": "Maximum relative improvement of 22% for weed class and 17% for plant class compared to full real-data baseline when combining synthetic data with limited real annotations.", "conclusion": "Combining synthetic data with limited manual annotations and cross-domain translation via generative models can significantly boost segmentation performance in complex field environments for multi-modal imagery."}}
{"id": "2509.19218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19218", "abs": "https://arxiv.org/abs/2509.19218", "authors": ["Yunzhi Xu", "Yushuang Ding", "Hu Sun", "Hongxi Zhang", "Li Zhao"], "title": "HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus", "comment": "10 pages, 7 figures", "summary": "Evaluation of hydrocephalus in children is challenging, and the related\nresearch is limited by a lack of publicly available, expert-annotated datasets,\nparticularly those with segmentation of the choroid plexus. To address this, we\npresent HyKid, an open-source dataset from 48 pediatric patients with\nhydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was\nreconstructed from routine low-resolution images using a slice-to-volume\nalgorithm. Manually corrected segmentations of brain tissues, including white\nmatter, grey matter, lateral ventricle, external CSF, and the choroid plexus,\nwere provided by an experienced neurologist. Additionally, structured data was\nextracted from clinical radiology reports using a Retrieval-Augmented\nGeneration framework. The strong correlation between choroid plexus volume and\ntotal CSF volume provided a potential biomarker for hydrocephalus evaluation,\nachieving excellent performance in a predictive model (AUC = 0.87). The\nproposed HyKid dataset provided a high-quality benchmark for neuroimaging\nalgorithms development, and it revealed the choroid plexus-related features in\nhydrocephalus assessments. Our datasets are publicly available at\nhttps://www.synapse.org/Synapse:syn68544889.", "AI": {"tldr": "HyKid is an open-source pediatric hydrocephalus dataset with 3D MRIs and expert-annotated segmentations, showing choroid plexus volume as a potential biomarker for hydrocephalus evaluation.", "motivation": "Addressing the lack of publicly available, expert-annotated datasets for pediatric hydrocephalus evaluation, particularly those with choroid plexus segmentation.", "method": "Created HyKid dataset from 48 pediatric patients with 3D MRIs reconstructed using slice-to-volume algorithm, with manual segmentations by neurologist and structured data extraction using Retrieval-Augmented Generation framework.", "result": "Found strong correlation between choroid plexus volume and total CSF volume, achieving AUC = 0.87 in predictive model for hydrocephalus evaluation.", "conclusion": "HyKid provides a high-quality benchmark for neuroimaging algorithms and reveals choroid plexus-related features in hydrocephalus assessments."}}
{"id": "2509.19227", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19227", "abs": "https://arxiv.org/abs/2509.19227", "authors": ["Tongshuai Wu", "Chao Lu", "Ze Song", "Yunlong Lin", "Sizhe Fan", "Xuemei Chen"], "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation", "comment": null, "summary": "With the widespread deployment of dashcams and advancements in computer\nvision, developing accident prediction models from the dashcam perspective has\nbecome critical for proactive safety interventions. However, two key challenges\npersist: modeling feature-level interactions among traffic participants (often\noccluded in dashcam views) and capturing complex, asynchronous multi-temporal\nbehavioral cues preceding accidents. To deal with these two challenges, a\nMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stage\naccident anticipation from dashcam videos. MsFIN has three layers for\nmulti-scale feature aggregation, temporal feature processing and multi-scale\nfeature post fusion, respectively. For multi-scale feature aggregation, a\nMulti-scale Module is designed to extract scene representations at short-term,\nmid-term and long-term temporal scales. Meanwhile, the Transformer architecture\nis leveraged to facilitate comprehensive feature interactions. Temporal feature\nprocessing captures the sequential evolution of scene and object features under\ncausal constraints. In the multi-scale feature post fusion stage, the network\nfuses scene and object features across multiple temporal scales to generate a\ncomprehensive risk representation. Experiments on DAD and DADA datasets show\nthat MsFIN significantly outperforms state-of-the-art models with single-scale\nfeature extraction in both prediction correctness and earliness. Ablation\nstudies validate the effectiveness of each module in MsFIN, highlighting how\nthe network achieves superior performance through multi-scale feature fusion\nand contextual interaction modeling.", "AI": {"tldr": "MsFIN is a multi-scale feature interaction network for early accident anticipation from dashcam videos, addressing challenges of occluded traffic participants and complex multi-temporal behavioral cues through multi-scale feature aggregation, temporal processing, and fusion.", "motivation": "To enable proactive safety interventions by developing accident prediction models from dashcam perspective, overcoming challenges of modeling feature-level interactions among often-occluded traffic participants and capturing complex asynchronous multi-temporal behavioral cues preceding accidents.", "method": "Proposes MsFIN with three layers: 1) Multi-scale Module for scene representations at short-term, mid-term, and long-term temporal scales using Transformer for feature interactions; 2) Temporal feature processing under causal constraints; 3) Multi-scale feature post fusion of scene and object features across temporal scales for comprehensive risk representation.", "result": "Experiments on DAD and DADA datasets show MsFIN significantly outperforms state-of-the-art single-scale models in both prediction correctness and earliness. Ablation studies validate effectiveness of each module.", "conclusion": "The network achieves superior performance through multi-scale feature fusion and contextual interaction modeling, demonstrating the importance of multi-scale temporal analysis for early accident anticipation."}}
{"id": "2509.19230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19230", "abs": "https://arxiv.org/abs/2509.19230", "authors": ["Tianshuo Zhang", "Li Gao", "Siran Peng", "Xiangyu Zhu", "Zhen Lei"], "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces", "comment": "Accepted by NeurIPS 2025", "summary": "The rise of realistic digital face generation and manipulation poses\nsignificant social risks. The primary challenge lies in the rapid and diverse\nevolution of generation techniques, which often outstrip the detection\ncapabilities of existing models. To defend against the ever-evolving new types\nof forgery, we need to enable our model to quickly adapt to new domains with\nlimited computation and data while avoiding forgetting previously learned\nforgery types. In this work, we posit that genuine facial samples are abundant\nand relatively stable in acquisition methods, while forgery faces continuously\nevolve with the iteration of manipulation techniques. Given the practical\ninfeasibility of exhaustively collecting all forgery variants, we frame face\nforgery detection as a continual learning problem and allow the model to\ndevelop as new forgery types emerge. Specifically, we employ a Developmental\nMixture of Experts (MoE) architecture that uses LoRA models as its individual\nexperts. These experts are organized into two groups: a Real-LoRA to learn and\nrefine knowledge of real faces, and multiple Fake-LoRAs to capture incremental\ninformation from different forgery types. To prevent catastrophic forgetting,\nwe ensure that the learning direction of Fake-LoRAs is orthogonal to the\nestablished subspace. Moreover, we integrate orthogonal gradients into the\northogonal loss of Fake-LoRAs, preventing gradient interference throughout the\ntraining process of each task. Experimental results under both the datasets and\nmanipulation types incremental protocols demonstrate the effectiveness of our\nmethod.", "AI": {"tldr": "This paper proposes a continual learning approach for face forgery detection using a Developmental Mixture of Experts (MoE) architecture with LoRA models to adapt to evolving forgery techniques while preventing catastrophic forgetting.", "motivation": "The rapid evolution of digital face generation and manipulation techniques outpaces existing detection models, requiring systems that can quickly adapt to new forgery types while retaining knowledge of previous ones without exhaustive data collection.", "method": "Uses a Developmental MoE architecture with LoRA models organized into Real-LoRA for stable real face knowledge and multiple Fake-LoRAs for incremental forgery learning. Employs orthogonal gradients and subspace learning to prevent catastrophic forgetting and gradient interference.", "result": "Experimental results show effectiveness under both dataset and manipulation type incremental protocols, demonstrating the method's ability to adapt to new forgery types while maintaining detection capabilities for previously learned types.", "conclusion": "The proposed continual learning framework successfully addresses the challenge of evolving face forgery techniques by enabling models to incrementally learn new forgery types while preserving existing knowledge, providing a practical solution for real-world forgery detection scenarios."}}
{"id": "2509.19244", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19244", "abs": "https://arxiv.org/abs/2509.19244", "authors": ["Shufan Li", "Jiuxiang Gu", "Kangning Liu", "Zhe Lin", "Zijun Wei", "Aditya Grover", "Jason Kuen"], "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation", "comment": "32 pages, 15 figures", "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)\ncapable of image understanding and generation tasks. Unlike existing multimodal\ndiffsion language models such as MMaDa and Muddit which only support simple\nimage-level understanding tasks and low-resolution image generation, Lavida-O\nexhibits many new capabilities such as object grounding, image-editing, and\nhigh-resolution (1024px) image synthesis. It is also the first unified MDM that\nuses its understanding capabilities to improve image generation and editing\nresults through planning and iterative self-reflection. To allow effective and\nefficient training and sampling, Lavida-O ntroduces many novel techniques such\nas Elastic Mixture-of-Transformer architecture, universal text conditioning,\nand stratified sampling. \\ours~achieves state-of-the-art performance on a wide\nrange of benchmarks such as RefCOCO object grounding, GenEval text-to-image\ngeneration, and ImgEdit image editing, outperforming existing autoregressive\nand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while\noffering considerable speedup at inference.", "AI": {"tldr": "Lavida-O is a unified multi-modal Masked Diffusion Model that supports advanced image understanding and generation tasks including object grounding, image editing, and high-resolution synthesis, outperforming existing models while being faster.", "motivation": "To overcome limitations of existing multimodal diffusion models that only support simple image-level tasks and low-resolution generation, by creating a unified model with enhanced capabilities.", "method": "Uses Masked Diffusion Model with novel techniques including Elastic Mixture-of-Transformer architecture, universal text conditioning, and stratified sampling for efficient training and inference.", "result": "Achieves state-of-the-art performance on benchmarks like RefCOCO, GenEval, and ImgEdit, outperforming models like Qwen2.5-VL and FluxKontext-dev with faster inference.", "conclusion": "Lavida-O demonstrates superior capabilities in unified image understanding and generation through innovative architectural and sampling techniques, setting new benchmarks in multimodal AI."}}
{"id": "2509.19245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19245", "abs": "https://arxiv.org/abs/2509.19245", "authors": ["Benedetta Liberatori", "Alessandro Conti", "Lorenzo Vaquero", "Yiming Wang", "Elisa Ricci", "Paolo Rota"], "title": "ConViS-Bench: Estimating Video Similarity Through Semantic Concepts", "comment": "Accepted to NeurIPS 2025", "summary": "What does it mean for two videos to be similar? Videos may appear similar\nwhen judged by the actions they depict, yet entirely different if evaluated\nbased on the locations where they were filmed. While humans naturally compare\nvideos by taking different aspects into account, this ability has not been\nthoroughly studied and presents a challenge for models that often depend on\nbroad global similarity scores. Large Multimodal Models (LMMs) with video\nunderstanding capabilities open new opportunities for leveraging natural\nlanguage in comparative video tasks. We introduce Concept-based Video\nSimilarity estimation (ConViS), a novel task that compares pairs of videos by\ncomputing interpretable similarity scores across a predefined set of key\nsemantic concepts. ConViS allows for human-like reasoning about video\nsimilarity and enables new applications such as concept-conditioned video\nretrieval. To support this task, we also introduce ConViS-Bench, a new\nbenchmark comprising carefully annotated video pairs spanning multiple domains.\nEach pair comes with concept-level similarity scores and textual descriptions\nof both differences and similarities. Additionally, we benchmark several\nstate-of-the-art models on ConViS, providing insights into their alignment with\nhuman judgments. Our results reveal significant performance differences on\nConViS, indicating that some concepts present greater challenges for estimating\nvideo similarity. We believe that ConViS-Bench will serve as a valuable\nresource for advancing research in language-driven video understanding.", "AI": {"tldr": "ConViS introduces a new task for concept-based video similarity estimation using natural language, enabling human-like reasoning about video comparisons across different semantic aspects.", "motivation": "Current video similarity models rely on broad global scores and lack the ability to compare videos based on specific semantic concepts like humans do, which limits their interpretability and practical applications.", "method": "Proposed Concept-based Video Similarity (ConViS) framework that computes interpretable similarity scores across predefined semantic concepts using Large Multimodal Models. Created ConViS-Bench benchmark with annotated video pairs and concept-level similarity scores.", "result": "Benchmarking revealed significant performance differences among state-of-the-art models, showing that some concepts are more challenging for video similarity estimation. The approach enables concept-conditioned video retrieval.", "conclusion": "ConViS-Bench provides a valuable resource for advancing language-driven video understanding research, addressing the need for more nuanced and interpretable video similarity assessment."}}
{"id": "2509.19252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19252", "abs": "https://arxiv.org/abs/2509.19252", "authors": ["Gabriel Maldonado", "Narges Rashvand", "Armin Danesh Pazho", "Ghazal Alinezhad Noghre", "Vinit Katariya", "Hamed Tabkhi"], "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps", "comment": null, "summary": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization.", "AI": {"tldr": "An adversarially-refined VQ-GAN framework with dense motion tokenization for compressing human motion heatmaps while preserving fine-grained details, outperforming baselines significantly.", "motivation": "Continuous human motion understanding is challenging due to high dimensionality and redundancy, requiring efficient compression and representation for analyzing complex motion dynamics.", "method": "Combines dense motion tokenization with adversarial refinement to eliminate reconstruction artifacts like motion smearing and temporal misalignment in spatio-temporal heatmaps.", "result": "Outperforms dVAE baseline by 9.31% SSIM and reduces temporal instability by 37.1% on CMU Panoptic dataset. Shows 2D motion requires 128-token vocabulary while 3D motion needs 1024-token codebook.", "conclusion": "The method establishes practical deployment feasibility for diverse motion analysis applications and provides insights into motion complexity representation."}}
{"id": "2509.19258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19258", "abs": "https://arxiv.org/abs/2509.19258", "authors": ["Dheerendranath Battalapalli", "Apoorva Safai", "Maria Jaramillo", "Hyemin Um", "Gustavo Adalfo Pineda Ortiz", "Ulas Bagci", "Manmeet Singh Ahluwalia", "Marwa Ismail", "Pallavi Tiwari"], "title": "Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies", "comment": "Under Review: npj Digital Medicine", "summary": "A significant challenge in solid tumors is reliably distinguishing\nconfounding pathologies from malignant neoplasms on routine imaging. While\nradiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,\nmany aggregate features across the region of interest (ROI) and miss complex\nspatial relationships among varying intensity compositions. We present a new\nGraph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional\nheterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of\nsub-regions using per-voxel radiomic measurements, then (2) computes\ngraph-theoretic metrics to quantify spatial associations among clusters. The\nresulting weighted graphs encode higher-order spatial relationships within the\nROI, aiming to reliably capture ILH and disambiguate confounding pathologies\nfrom malignancy. To assess efficacy and clinical feasibility, GrRAiL was\nevaluated in n=947 subjects spanning three use cases: differentiating tumor\nrecurrence from radiation effects in glioblastoma (GBM; n=106) and brain\nmetastasis (n=233), and stratifying pancreatic intraductal papillary mucinous\nneoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional\nsetting, GrRAiL consistently outperformed state-of-the-art baselines - Graph\nNeural Networks (GNNs), textural radiomics, and intensity-graph analysis. In\nGBM, cross-validation (CV) and test accuracies for recurrence vs\npseudo-progression were 89% and 78% with >10% test-accuracy gains over\ncomparators. In brain metastasis, CV and test accuracies for recurrence vs\nradiation necrosis were 84% and 74% (>13% improvement). For IPMN risk\nstratification, CV and test accuracies were 84% and 75%, showing >10%\nimprovement.", "AI": {"tldr": "GrRAiL is a graph-based radiomic learning method that captures intralesional heterogeneity on MRI by analyzing spatial relationships among sub-region clusters, outperforming existing methods in distinguishing tumor recurrence from radiation effects and stratifying pancreatic neoplasms.", "motivation": "Current radiomics methods aggregate features across lesion regions and miss complex spatial relationships, making it difficult to reliably distinguish confounding pathologies from malignant neoplasms on clinical MRI scans.", "method": "GrRAiL identifies clusters of sub-regions using per-voxel radiomic measurements, then computes graph-theoretic metrics to quantify spatial associations among these clusters, creating weighted graphs that encode higher-order spatial relationships within lesions.", "result": "In multi-institutional evaluations across three clinical use cases (glioblastoma, brain metastasis, pancreatic IPMNs), GrRAiL consistently outperformed state-of-the-art methods with test accuracies of 78%, 74%, and 75% respectively, showing >10% improvement over comparators.", "conclusion": "GrRAiL provides a clinically feasible approach for characterizing intralesional heterogeneity that reliably disambiguates confounding pathologies from malignancy, demonstrating significant performance gains over existing radiomics and graph-based methods."}}
{"id": "2509.19259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19259", "abs": "https://arxiv.org/abs/2509.19259", "authors": ["Markos Diomataris", "Berat Mert Albaba", "Giorgio Becherini", "Partha Ghosh", "Omid Taheri", "Michael J. Black"], "title": "Moving by Looking: Towards Vision-Driven Avatar Motion Generation", "comment": null, "summary": "The way we perceive the world fundamentally shapes how we move, whether it is\nhow we navigate in a room or how we interact with other humans. Current human\nmotion generation methods, neglect this interdependency and use task-specific\n``perception'' that differs radically from that of humans. We argue that the\ngeneration of human-like avatar behavior requires human-like perception.\nConsequently, in this work we present CLOPS, the first human avatar that solely\nuses egocentric vision to perceive its surroundings and navigate. Using vision\nas the primary driver of motion however, gives rise to a significant challenge\nfor training avatars: existing datasets have either isolated human motion,\nwithout the context of a scene, or lack scale. We overcome this challenge by\ndecoupling the learning of low-level motion skills from learning of high-level\ncontrol that maps visual input to motion. First, we train a motion prior model\non a large motion capture dataset. Then, a policy is trained using Q-learning\nto map egocentric visual inputs to high-level control commands for the motion\nprior. Our experiments empirically demonstrate that egocentric vision can give\nrise to human-like motion characteristics in our avatars. For example, the\navatars walk such that they avoid obstacles present in their visual field.\nThese findings suggest that equipping avatars with human-like sensors,\nparticularly egocentric vision, holds promise for training avatars that behave\nlike humans.", "AI": {"tldr": "CLOPS is the first human avatar system that uses egocentric vision as the primary perception mechanism to generate human-like motion, overcoming dataset limitations through a two-stage training approach that separates motion skills learning from visual control mapping.", "motivation": "Current human motion generation methods use task-specific perception that differs from human perception. The authors argue that generating human-like avatar behavior requires human-like perception, specifically egocentric vision.", "method": "Two-stage approach: 1) Train a motion prior model on large motion capture dataset for low-level motion skills, 2) Train a policy using Q-learning to map egocentric visual inputs to high-level control commands for the motion prior.", "result": "Avatars demonstrate human-like motion characteristics, such as walking to avoid obstacles in their visual field, showing that egocentric vision can produce human-like behavior.", "conclusion": "Equipping avatars with human-like sensors, particularly egocentric vision, is promising for training avatars that behave like humans."}}
{"id": "2509.19282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19282", "abs": "https://arxiv.org/abs/2509.19282", "authors": ["Bingnan Li", "Chen-Yu Wang", "Haiyang Xu", "Xiang Zhang", "Ethan Armand", "Divyansh Srivastava", "Xiaojun Shan", "Zeyuan Chen", "Jianwen Xie", "Zhuowen Tu"], "title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps", "comment": "Accepted to NeurIPS 2025 Dataset&Benchmark Track", "summary": "Despite steady progress in layout-to-image generation, current methods still\nstruggle with layouts containing significant overlap between bounding boxes. We\nidentify two primary challenges: (1) large overlapping regions and (2)\noverlapping instances with minimal semantic distinction. Through both\nqualitative examples and quantitative analysis, we demonstrate how these\nfactors degrade generation quality. To systematically assess this issue, we\nintroduce OverLayScore, a novel metric that quantifies the complexity of\noverlapping bounding boxes. Our analysis reveals that existing benchmarks are\nbiased toward simpler cases with low OverLayScore values, limiting their\neffectiveness in evaluating model performance under more challenging\nconditions. To bridge this gap, we present OverLayBench, a new benchmark\nfeaturing high-quality annotations and a balanced distribution across different\nlevels of OverLayScore. As an initial step toward improving performance on\ncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a\ncurated amodal mask dataset. Together, our contributions lay the groundwork for\nmore robust layout-to-image generation under realistic and challenging\nscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.", "AI": {"tldr": "The paper addresses layout-to-image generation challenges with overlapping bounding boxes, introduces OverLayScore metric and OverLayBench benchmark, and proposes CreatiLayout-AM model for improved performance on complex overlaps.", "motivation": "Current layout-to-image generation methods struggle with layouts containing significant overlap between bounding boxes, particularly with large overlapping regions and instances with minimal semantic distinction, which degrades generation quality.", "method": "The authors introduce OverLayScore to quantify overlapping complexity, create OverLayBench benchmark with balanced distribution across overlap levels, and propose CreatiLayout-AM model fine-tuned on amodal mask dataset.", "result": "Analysis shows existing benchmarks are biased toward simpler cases with low OverLayScore values. The proposed benchmark provides better evaluation under challenging conditions.", "conclusion": "The contributions establish groundwork for more robust layout-to-image generation in realistic scenarios with complex overlaps, addressing current limitations in the field."}}
{"id": "2509.19296", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.19296", "abs": "https://arxiv.org/abs/2509.19296", "authors": ["Sherwin Bahmani", "Tianchang Shen", "Jiawei Ren", "Jiahui Huang", "Yifeng Jiang", "Haithem Turki", "Andrea Tagliasacchi", "David B. Lindell", "Zan Gojcic", "Sanja Fidler", "Huan Ling", "Jun Gao", "Xuanchi Ren"], "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation", "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/", "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.", "AI": {"tldr": "A self-distillation framework that distills 3D knowledge from video diffusion models into explicit 3D Gaussian Splatting representations, enabling 3D scene generation without multi-view training data.", "motivation": "Current 3D reconstruction methods require multi-view real-world data, which is not always available. Video diffusion models have strong imagination capabilities but are limited to 2D, restricting their use in robotics and simulation applications that require 3D environments.", "method": "Proposes a self-distillation framework with an augmented RGB decoder and 3DGS decoder. The 3DGS decoder is supervised by the RGB decoder's output, allowing training with synthetic data from video diffusion models. Supports text-to-3D and image-to-3D generation, and extends to dynamic scenes from monocular videos.", "result": "Achieves state-of-the-art performance in both static and dynamic 3D scene generation, enabling real-time rendering from text prompts or single images.", "conclusion": "The framework successfully bridges 2D video diffusion models with explicit 3D representations, eliminating the need for multi-view training data and enabling practical applications in robotics, autonomous driving, and industrial AI."}}
{"id": "2509.19297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19297", "abs": "https://arxiv.org/abs/2509.19297", "authors": ["Weijie Wang", "Yeqing Chen", "Zeyu Zhang", "Hengyu Liu", "Haoxiao Wang", "Zhiyuan Feng", "Wenkang Qin", "Zheng Zhu", "Donny Y. Chen", "Bohan Zhuang"], "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction", "comment": "Project Page: https://lhmd.top/volsplat, Code:\n  https://github.com/ziplab/VolSplat", "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\nsolution for novel view synthesis. Existing methods predominantly rely on a\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\n3D Gaussian. We rethink this widely adopted formulation and identify several\ninherent limitations: it renders the reconstructed 3D models heavily dependent\non the number of input views, leads to view-biased density distributions, and\nintroduces alignment errors, particularly when source views contain occlusions\nor low texture. To address these challenges, we introduce VolSplat, a new\nmulti-view feed-forward paradigm that replaces pixel alignment with\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\nadaptive control over Gaussian density based on 3D scene complexity, yielding\nmore faithful Gaussian point clouds, improved geometric consistency, and\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\nstate-of-the-art performance while producing more plausible and view-consistent\nGaussian reconstructions. In addition to superior results, our approach\nestablishes a more scalable framework for feed-forward 3D reconstruction with\ndenser and more robust representations, paving the way for further research in\nwider communities. The video results, code and trained models are available on\nour project page: https://lhmd.top/volsplat.", "AI": {"tldr": "VolSplat introduces a voxel-aligned Gaussian prediction paradigm that overcomes limitations of pixel-aligned 3D Gaussian Splatting, achieving state-of-the-art novel view synthesis with improved geometric consistency.", "motivation": "Current pixel-aligned 3DGS methods have inherent limitations including view dependency, biased density distributions, and alignment errors from occlusions or low texture. These issues make reconstructions unreliable.", "method": "Replaces pixel alignment with voxel-aligned Gaussians by predicting Gaussians directly from a 3D voxel grid, enabling adaptive density control based on 3D scene complexity.", "result": "Achieves SOTA performance on RealEstate10K and ScanNet benchmarks, producing more plausible and view-consistent Gaussian reconstructions with enhanced rendering quality.", "conclusion": "VolSplat establishes a more scalable framework for feed-forward 3D reconstruction with denser, more robust representations, paving the way for broader research applications."}}
{"id": "2509.19300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19300", "abs": "https://arxiv.org/abs/2509.19300", "authors": ["Chen Chen", "Pengsheng Guo", "Liangchen Song", "Jiasen Lu", "Rui Qian", "Xinze Wang", "Tsu-Jui Fu", "Wei Liu", "Yinfei Yang", "Alex Schwing"], "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching", "comment": null, "summary": "Conditional generative modeling aims to learn a conditional data distribution\nfrom samples containing data-condition pairs. For this, diffusion and\nflow-based methods have attained compelling results. These methods use a\nlearned (flow) model to transport an initial standard Gaussian noise that\nignores the condition to the conditional data distribution. The model is hence\nrequired to learn both mass transport and conditional injection. To ease the\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\nthe target, or both distributions. By relocating these distributions, CAR-Flow\nshortens the probability path the model must learn, leading to faster training\nin practice. On low-dimensional synthetic data, we visualize and quantify the\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\nintroducing less than 0.6% additional parameters.", "AI": {"tldr": "CAR-Flow is a lightweight method that conditions source/target distributions to shorten probability paths in flow matching, improving training efficiency and performance with minimal parameter overhead.", "motivation": "Existing flow-based methods require models to learn both mass transport and conditional injection, which is demanding. The paper aims to ease this burden by conditioning the distributions themselves.", "method": "Proposes Condition-Aware Reparameterization (CAR-Flow) - a learned shift that conditions source, target, or both distributions to shorten the probability path that the model must learn.", "result": "On ImageNet-256, CAR-Flow reduces FID from 2.07 to 1.68 for SiT-XL/2 while adding less than 0.6% parameters. Low-dimensional synthetic experiments show visual and quantitative improvements.", "conclusion": "CAR-Flow effectively reduces the learning burden on flow models by conditioning distributions, leading to faster training and better performance with minimal computational overhead."}}
