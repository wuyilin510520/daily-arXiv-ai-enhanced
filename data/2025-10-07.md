<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 169]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: SoC-DT is a differentiable framework that combines reaction-diffusion tumor growth models with standard-of-care interventions and patient personalization to predict post-treatment tumor structure on imaging, outperforming traditional PDE models and neural networks.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of tumor trajectories under standard-of-care therapies is a major unmet need in oncology, as conventional reaction-diffusion models fail to capture tumor dynamics under heterogeneous therapeutic paradigms.

Method: SoC-DT unifies reaction-diffusion tumor growth models with discrete SoC interventions (surgery, chemotherapy, radiotherapy) and genomic/demographic personalization. It uses an implicit-explicit exponential time-differencing solver (IMEX-SoC) for stability, positivity, and scalability.

Result: Evaluated on synthetic and real-world glioma data, SoC-DT consistently outperforms classical PDE baselines and purely data-driven neural models in predicting tumor dynamics.

Conclusion: SoC-DT establishes a principled foundation for patient-specific digital twins in oncology by bridging mechanistic interpretability with modern differentiable solvers, enabling biologically consistent tumor dynamics estimation.

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: A hybrid framework combining distributed multi-GPU inference with interactive visualization for analyzing celebrity dynamics in video content, enabling scalable processing and multi-dimensional insights.


<details>
  <summary>Details</summary>
Motivation: To understand video structure and dynamics in an era dominated by video content, particularly focusing on celebrity appearances and relationships in episodes.

Method: Uses distributed multi-GPU inference system with optimized ONNX models, heterogeneous batch inference, and high-throughput parallelism for scalable video processing, coupled with interactive visualization platform.

Result: Generates comprehensive visualizations including appearance frequency charts, duration analyses, co-appearance matrices, network graphs, and heatmaps that reveal patterns in celebrity prominence, screen-time distribution, and temporal dynamics.

Conclusion: The framework bridges distributed recognition with structured visual analytics, enabling new possibilities for entertainment analytics, content creation strategies, and audience engagement studies.

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: This paper benchmarks cross-domain underwater plastic detection models, finding lightweight CNNs like MobileNetV2 outperform larger models and zero-shot approaches, with supervised training providing better generalization than pretrained vision-language models.


<details>
  <summary>Details</summary>
Motivation: Marine plastic pollution requires reliable automated detection, but domain shift causes performance degradation when models trained on one dataset are applied to new underwater imagery.

Method: Benchmarked CNN models (MobileNetV2, ResNet-18, EfficientNet-B0) and vision transformers (DeiT-Tiny, ViT-B16) trained on labeled underwater data, then evaluated on cross-domain test set. Also assessed zero-shot models CLIP ViT-L14 and Gemini 2.0 Flash.

Result: MobileNetV2 achieved strongest cross-domain performance (F1 0.97). All fine-tuned models had high Precision (~99%) but varying Recall. Zero-shot CLIP had Recall ~80% but low Precision (~56%), while Gemini had high Precision (~99%) but lower Recall (~81%).

Conclusion: Compact CNNs with supervised training generalize effectively for cross-domain underwater detection, while large pretrained vision-language models offer complementary strengths but different performance trade-offs.

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP is an Arabic image captioning framework that combines CLIP-based visual label retrieval with multimodal text generation, achieving state-of-the-art results through interpretable Arabic visual concepts.


<details>
  <summary>Details</summary>
Motivation: To create an Arabic image captioning system that grounds generation in interpretable visual concepts rather than relying solely on end-to-end approaches, enabling culturally coherent and contextually accurate captions.

Method: Uses three multilingual encoders (mCLIP, AraCLIP, Jina V4) for visual label retrieval from a hybrid vocabulary of training captions and 21K translated Visual Genome labels. Top-k retrieved labels are transformed into Arabic prompts and passed with images to vision-language models (Qwen-VL and Gemini Pro Vision) for caption generation.

Result: mCLIP + Gemini Pro Vision achieved best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained highest LLM-judge score (36.33%). Six encoder-decoder configurations were tested.

Conclusion: The interpretable pipeline successfully enables culturally coherent and contextually accurate Arabic captions, with different encoder-decoder combinations excelling in different evaluation metrics.

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [5] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: Comparison of EfficientNet-B0 (CNN) and ViT-Base (Vision Transformer) on SpaceNet dataset under imbalanced and balanced label distributions, showing CNN's efficiency advantages while both achieve high accuracy.


<details>
  <summary>Details</summary>
Motivation: To conduct a controlled comparison between convolutional neural networks and vision transformers on satellite imagery, specifically examining performance under different label distribution regimes.

Method: Used EfficientNet-B0 and ViT-Base models on SpaceNet dataset with two label regimes: naturally imbalanced five-class split and balanced-resampled split (700 images per class). Applied matched preprocessing, lightweight augmentations, and 40-epoch training on single NVIDIA P100 GPU.

Result: On imbalanced split: EfficientNet-B0 achieved 93% test accuracy with strong macro-F1 and lower latency; ViT-Base was competitive at 93% but with larger parameter count and runtime. On balanced split: EfficientNet-B0 reached 99% accuracy while ViT-Base remained competitive, showing that balancing narrows architecture gaps.

Conclusion: CNNs retain efficiency advantages over Vision Transformers, though both architectures perform strongly, especially on balanced datasets. Balancing label distributions reduces performance gaps between architectures while CNNs maintain deployment efficiency benefits.

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [6] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: This paper provides a comprehensive review of camera-based AI sensing systems for vulnerable road user (VRU) safety, covering detection, tracking, trajectory prediction, and intent recognition tasks from the past five years.


<details>
  <summary>Details</summary>
Motivation: Existing surveys on AI for VRU safety focus mainly on detection, leaving gaps in coverage of other essential vision-based tasks needed for comprehensive VRU protection in dynamic urban environments.

Method: Systematic examination of four core AI tasks: detection and classification, tracking and reidentification, trajectory prediction, and intent recognition and prediction, with emphasis on developments from the past five years.

Result: The review identifies emerging research trends and provides a state-of-the-art overview of camera-based AI sensing systems for VRU safety, highlighting their potential for proactive protection solutions.

Conclusion: The survey highlights four major open challenges from data, model, and deployment perspectives and aims to serve as a foundational reference for developing next-generation sensing systems to enhance VRU safety.

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [7] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: Earth Observation Foundation Models (EOFMs) are sensitive to sensor architecture differences, which affects their representation space and highlights current design pitfalls.


<details>
  <summary>Details</summary>
Motivation: To understand how diverse sensor architectures impact the internal representations of EOFMs, as current models are typically trained on single modalities and applied across different ones.

Method: Analysis of EOFM representation spaces to examine sensitivity to sensor architecture differences.

Result: The representation space of EOFMs is highly sensitive to sensor architecture, revealing significant impacts on model performance.

Conclusion: Understanding sensor architecture sensitivity provides crucial insights for improving EOFM design and guiding robust remote-sensing science.

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [8] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: An inpainting-guided perturbation method generates photorealistic explanations for ecological vision models, revealing fine-grained morphological cues while preserving scene context.


<details>
  <summary>Details</summary>
Motivation: To address the opacity of automated ecological monitoring models and improve trust/field adoption by providing interpretable, photorealistic explanations.

Method: Uses inpainting-guided perturbation with SAM-refined masks for object removal/replacement and background replacement, applied to YOLOv9 seal detection in drone imagery.

Result: Produces explanations that localize diagnostic structures, avoid deletion artifacts, and provide domain-relevant insights validated by expert review and confidence metrics.

Conclusion: The approach enables trustworthy AI deployment in ecology by generating ecologically plausible explanations that support expert validation.

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [9] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: This paper provides a comprehensive survey of Medical Image Segmentation (MIS) methodologies, covering both traditional techniques and modern deep learning approaches, with a special focus on emerging trends and a case study on lumbar spine segmentation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between traditional image processing techniques and modern deep learning approaches in medical image segmentation, and to address persistent challenges in the field.

Method: Systematic survey methodology covering thresholding, edge detection, region-based segmentation, clustering algorithms, model-based techniques, CNNs, FCNs, U-Net variants, attention mechanisms, semi-supervised learning, GANs, Transformer-based models, and emerging trends like hybrid architectures and federated learning.

Result: The survey comprehensively maps the evolution of MIS methodologies and identifies emerging trends including cross-modality learning, active learning strategies, and specialized applications like lumbar spine segmentation.

Conclusion: Despite significant progress, critical challenges persist in MIS including dataset bias, domain adaptation, model interpretability, and integration into clinical workflows, requiring continued research and development.

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [10] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: DECOR is a deep clustering framework for wafer defect detection that handles orientation variations and imperfect data conditions without manual tuning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Early detection of wafer defects is critical for semiconductor manufacturing yield optimization, but raw wafer data is complex, unlabeled, imbalanced, and can contain multiple defects on single wafers.

Method: DECOR uses deep clustering with orientation robustness to group complex defect patterns from wafer maps into consistent clusters, explicitly accounting for orientation variations to ensure spatially similar defects are clustered regardless of rotation or alignment.

Result: Experiments on the MixedWM38 dataset show DECOR outperforms existing clustering baseline methods and can discover clusters without manual tuning.

Conclusion: DECOR provides a reliable and scalable solution for automated visual inspection systems in semiconductor manufacturing.

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [11] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: This paper proposes an error correction method for multi-class facial emotion classification on imbalanced datasets using LSTM with attention mechanism to focus on key facial areas.


<details>
  <summary>Details</summary>
Motivation: To address class imbalance in facial emotion recognition where some emotions significantly outnumber others, which is common in real-world applications like anti-fraud systems.

Method: Uses LSTM neural network with attention mechanism focusing on informative facial areas. Trains on subsets of six classes and performs error correction for the excluded seventh class.

Result: Error correction is possible for all classes with varying success rates. Some classes are better restored than others. Test results show improved quality metrics for small classes.

Conclusion: The proposed method is effective for facial expression analysis and stable classification under skewed class distributions, particularly promising for detecting rare events in applications like anti-fraud systems.

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [12] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: The paper introduces DCG-Bench, the first benchmark for dynamic chart generation, and proposes a two-stage training method with Joint-Code-Visual Reward that achieves state-of-the-art performance with a 3B parameter model.


<details>
  <summary>Details</summary>
Motivation: While MLLMs have improved static chart generation, their potential for dynamic chart generation remains underexplored, creating a research gap that needs to be addressed.

Method: Created DCG-8K dataset with instruction-code-video triplets and QA pairs, then used a two-stage training recipe with Joint-Code-Visual Reward for group relative policy optimization to train Qwen2.5-VL-DCG-3B model.

Result: The model beats the best open-sourced MLLM by 8.31% average performance gain across three tasks and shows on-par performance against proprietary models despite having only 3B parameters.

Conclusion: The proposed training recipe is effective for dynamic chart generation, and the DCG-Bench benchmark reveals shortcomings of existing MLLMs in visual-to-chart tasks.

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [13] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: VoT (Visual odometry Transformer) is an end-to-end monocular visual odometry method that eliminates traditional components like bundle adjustment and feature matching, using temporal and spatial attention to directly predict camera motion from frame sequences.


<details>
  <summary>Details</summary>
Motivation: Traditional monocular visual odometry methods are complex pipelines that rely heavily on camera calibration and hyperparameter tuning, and struggle in unseen real-world scenarios. Current large-scale 3D models have limitations in handling long videos and providing accurate per-frame estimates.

Method: VoT processes sequences of monocular frames by extracting features and modeling global relationships through temporal and spatial attention. It directly predicts camera motion without estimating dense geometry and relies solely on camera poses for supervision. The framework is modular and allows integration of various pre-trained encoders.

Result: VoT scales effectively with larger datasets, benefits substantially from stronger pre-trained backbones, generalizes across diverse camera motions and calibration settings, and outperforms traditional methods while running more than 3 times faster.

Conclusion: Monocular visual odometry can be effectively addressed in an end-to-end manner, eliminating the need for handcrafted components and providing a more efficient and generalizable solution.

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [14] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: A novel inference-time search algorithm that uses side information to guide diffusion models for solving inverse problems, improving reconstruction quality without gradient-based guidance artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based inverse problem solvers overlook side information that could significantly improve reconstruction, especially in severely ill-posed settings.

Method: Proposed an inference-time search algorithm that guides sampling using side information while balancing exploration and exploitation, avoiding reward-hacking artifacts common in gradient-based guidance.

Result: Consistently improved qualitative and quantitative performance across various inverse problems (box inpainting, super-resolution, motion/Gaussian/nonlinear/blind deblurring), outperforming reward gradient-based guidance baselines.

Conclusion: The approach enables more accurate and reliable reconstructions and can be seamlessly integrated into existing diffusion-based image reconstruction pipelines.

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [15] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: This paper provides a comprehensive review of publicly available sonar image datasets across various modalities, analyzing their applications and synthesizing findings into a master table and timeline to guide researchers in underwater acoustic data analysis.


<details>
  <summary>Details</summary>
Motivation: The scarcity of publicly available, well-annotated sonar image datasets creates a significant bottleneck for developing robust machine learning models in underwater exploration, autonomous navigation, and ecosystem monitoring.

Method: The authors mapped publicly accessible datasets across various sonar modalities (SSS, FLS, SAS, MBES, DIDSON) and conducted analysis on applications including classification, detection, segmentation, and 3D reconstruction. They synthesized findings into a master table and chronological timeline.

Result: The review catalogs existing sonar image datasets, contextualizes them, identifies gaps, and provides a clear roadmap for researchers. It offers accessible comparison of dataset characteristics, sizes, and annotation details.

Conclusion: This work serves as a base guide for researchers of any kind who wish to start or advance in the field of underwater acoustic data analysis by providing a comprehensive overview of available sonar image datasets and their applications.

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [16] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: A lensless camera and neural algorithm co-design for display calibration without specialized hardware, enabling light field reconstruction from multiple viewpoints.


<details>
  <summary>Details</summary>
Motivation: Display calibration is essential but difficult for most users due to requirements for specialized equipment and dark rooms, making it inaccessible.

Method: Co-design of a lensless camera with Implicit Neural Representation algorithm to capture display characteristics from various viewpoints, reconstructing light fields from a 46.6° × 37.6° viewing cone.

Result: The pipeline enables efficient reconstruction of light fields emitted from displays across a wide viewing angle.

Conclusion: This emerging pipeline represents initial progress toward effortless display calibration and characterization without specialized hardware requirements.

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [17] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: Provenance networks are neural models that provide end-to-end explainability by linking predictions directly to supporting training examples, embedding interpretability into the architecture itself.


<details>
  <summary>Details</summary>
Motivation: Address model opaqueness, hallucination, and credit assignment in deep learning by providing training-data-driven explainability and improving transparency, robustness, and trustworthiness.

Method: Learn to link each prediction to supporting training examples through joint optimization of primary task and explainability objective, operating similarly to a learned KNN with relevance-weighted exemplars.

Result: Enables systematic investigation of memorization vs generalization trade-off, verification of training set inclusion, detection of mislabeled/anomalous data, enhanced resilience to input perturbations, and identification of similar inputs.

Conclusion: Provenance networks offer complementary explainability approach with insights traditional deep networks cannot provide, though they introduce computational costs and currently scale to moderately sized datasets.

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [18] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: UCF is a unified post-hoc framework that refines anomaly detection by filtering matching noise in cost volumes, working for both unimodal and multimodal UAD settings.


<details>
  <summary>Details</summary>
Motivation: Existing UAD methods suffer from matching noise and lack a unified approach for both unimodal and multimodal scenarios, limiting detection ability and knowledge transfer.

Method: Construct cost volume by matching test samples against normal samples, then apply learnable filtering with multi-layer attention guidance to reduce matching noise and highlight anomalies.

Result: Achieves SOTA results on 22 benchmarks, enhancing various UAD methods in both unimodal (RGB) and multimodal (RGB-3D, RGB-Text) scenarios.

Conclusion: UCF provides a generic and effective solution for improving UAD performance across diverse settings by addressing the fundamental matching noise problem.

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [19] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: A framework using Visual Language Models (VLMs) to automatically evaluate and refine object detection results in industrial diagrams like P&IDs, addressing the lack of quality assessment methods in digitalization processes.


<details>
  <summary>Details</summary>
Motivation: Industrial diagrams are crucial for plant operations but lack automated quality assessment methods for object detection outputs during digitalization, which is essential for building digital twins and intelligent automation.

Method: Employ Visual Language Models (VLMs) to assess object detection results by leveraging their multimodal capabilities to identify missing or inconsistent detections and guide refinement.

Result: The framework enables automated quality assessment and improves overall detection performance on complex industrial diagrams by identifying detection errors.

Conclusion: VLMs provide an effective solution for automated quality evaluation of object detection in industrial diagram digitalization, enhancing detection accuracy and supporting digital twin development.

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [20] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: SpatialViLT enhances vision-language models by integrating spatial features like depth maps and 3D coordinates through multi-task learning, improving spatial reasoning for 3D scenes and complex object configurations.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models face challenges in spatial reasoning for 3D scenes and complex object configurations, limiting their ability to understand spatial relationships in multimodal data.

Method: Introduces SpatialViLT with two variants: SpatialViLT (full object regions) and MaskedSpatialViLT (masked object regions), using multi-task learning to integrate spatial features like depth maps, 3D coordinates, and edge maps. SpatialEnsemble combines both approaches.

Result: Achieves state-of-the-art accuracy on the Visual Spatial Reasoning (VSR) dataset, with strong performance in directional, topological, and proximity relations.

Conclusion: This work represents a significant advancement in enhancing spatial intelligence for AI systems, crucial for advanced multimodal understanding and real-world applications.

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [21] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: Deep learning encoder-decoder networks trained on synthetic data effectively reduce artifacts in two-phase optical-sectioning structured illumination microscopy, improving image clarity without requiring clean ground-truth data.


<details>
  <summary>Details</summary>
Motivation: Two-phase optical-sectioning SI suffers from residual artifacts due to reduced acquisition time, and conventional denoising methods struggle to suppress them. Supervised deep learning is limited by the lack of clean ground-truth data.

Method: Used encoder-decoder networks (asymmetrical denoising autoencoder and U-Net) trained on synthetic data created by applying real artifact fields to synthetic images, then evaluated on real OS-SI images.

Result: Both networks improved image clarity, with each network excelling against different types of artifacts. The approach successfully enabled supervised denoising of OS-SI images.

Conclusion: Synthetic training enables effective supervised denoising of OS-SI images, and encoder-decoder networks show potential to streamline reconstruction workflows in structured illumination microscopy.

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [22] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL is a multimodal framework that integrates histopathology with spatial transcriptomics using pathway activation scores instead of individual genes, improving prediction accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches rely on a small set of highly variable genes, which limits predictive scope and overlooks coordinated biological programs that shape tissue phenotypes.

Method: PEaRL represents transcriptomics through pathway activation scores computed with ssGSEA, encodes biologically coherent pathway signals with a transformer, and aligns them with histology features via contrastive learning.

Result: Across three cancer ST datasets (breast, skin, and lymph node), PEaRL consistently outperforms SOTA methods, yielding up to 58.9% and 20.4% increase in Pearson correlation coefficient for gene- and pathway-level expression prediction.

Conclusion: Grounding transcriptomic representation in pathways produces more biologically faithful and interpretable multimodal models, advancing computational pathology beyond gene-level embeddings.

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [23] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS is a vision-language framework for multi-modal medical image analysis that uses hierarchical semantic prompts and dual-prompt mechanism to achieve superior performance across multiple imaging modalities and medical tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of task-specific models that lack generalizability and existing universal approaches that suffer from simplistic conditioning and poor medical semantic understanding in medical imaging.

Method: Introduces hierarchical semantic prompts for fine-grained control, dual-prompt mechanism for text-controlled architecture, and parameter-efficient fine-tuning for rapid adaptation to new tasks and modalities.

Result: Outperforms state-of-the-art models on 8 out of 10 datasets across three imaging modalities, 30+ organs and tumor types. Achieves CI of 0.69 for prognosis prediction with EHR integration on head and neck cancer dataset.

Conclusion: DuPLUS establishes a versatile and clinically relevant solution for medical image analysis with strong generalization capabilities and extensibility to various medical tasks.

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [24] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: A mobile-optimized two-stage framework combining YOLOv10 for detection and MobileSAM for segmentation using threading to enable real-time animal monitoring in conservation settings.


<details>
  <summary>Details</summary>
Motivation: Real-time animal detection and segmentation are crucial for wildlife conservation through remote monitoring, but challenging due to limited computational resources and cryptic species appearances.

Method: Two-stage deep learning framework with Threading Detection Model (TDM) that parallelizes YOLOv10-based detection and MobileSAM-based segmentation to reduce latency and improve real-time performance.

Result: Achieved mAP50 of 0.9627, mAP75 of 0.7731, mAP95 of 0.7178, and MobileSAM mIoU of 0.7421 on Houbara Bustard dataset. YOLOv10 operates at 43.7 ms per frame, enabling real-time performance.

Conclusion: The proposed threaded YOLOv10+MobileSAM framework successfully enables real-time animal detection and segmentation for conservation applications, with publicly available code and curated dataset of 40,000 annotated images.

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [25] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: The Platonic Transformer introduces geometric inductive biases into Transformers using Platonic solid symmetry groups, enabling equivariance to translations and Platonic symmetries without increasing computational cost.


<details>
  <summary>Details</summary>
Motivation: Transformers lack geometric symmetry biases common in science and computer vision, while existing equivariant methods sacrifice efficiency and flexibility.

Method: Define attention relative to reference frames from Platonic solid symmetry groups, creating a principled weight-sharing scheme that enables equivariance while preserving standard Transformer architecture.

Result: Achieves competitive performance on computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular property prediction (QM9, OMol25) benchmarks by leveraging geometric constraints at no additional cost.

Conclusion: The Platonic Transformer resolves the trade-off between geometric equivariance and Transformer efficiency, with formal equivalence to dynamic group convolution enabling adaptive geometric filters and scalable variants.

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [26] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: This survey paper provides a comprehensive overview of domain generalization in semantic segmentation, highlighting the paradigm shift towards foundation-model-based approaches and offering extensive performance comparisons.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks struggle with generalization to unknown domains despite recent progress, and domain generalization is particularly important for semantic segmentation tasks in critical applications like biomedicine and automated driving.

Method: The authors cluster and review existing domain generalization approaches for semantic segmentation, identifying key trends and paradigm shifts in the field.

Result: The survey reveals a significant paradigm shift towards foundation-model-based domain generalization and provides extensive performance comparisons that highlight the substantial influence of foundation models on domain generalization capabilities.

Conclusion: This comprehensive survey aims to advance domain generalization research and inspire scientists to explore new research directions in this rapidly evolving field.

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [27] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: A transformer-based model for automated endoscopic report generation to reduce documentation burden on gastroenterologists.


<details>
  <summary>Details</summary>
Motivation: Endoscopic procedures create significant documentation burden, contributing to physician burnout and workflow inefficiencies in gastroenterology.

Method: Two-stage training framework: pre-training transformer vision encoder and text decoder on image/caption pairs, then fine-tuning on endoscopic image/report pairs for clinical findings generation.

Result: The model successfully generates clinically meaningful findings from endoscopic images, streamlining documentation processes.

Conclusion: This automated report generation approach can reduce physician workload and improve patient care by addressing documentation challenges in gastroenterology.

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [28] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan is a diffusion-based planner that converts 2D hand-drawn sketches over depth images into 3D drone flight paths, achieving zero-shot sim-to-real transfer with high success rates in real-world navigation.


<details>
  <summary>Details</summary>
Motivation: To enable intuitive drone navigation through hand-drawn sketches while addressing the gap between ideal 2D projections and real human sketches, allowing for natural human-drone interaction.

Method: Uses a two-component system: SketchAdapter maps human sketches to 2D paths, and DiffPath (diffusion model) generates 3D trajectories from 2D projections and depth images. Trained on 32k synthetic flight paths with 872 human-annotated sketches.

Result: Achieved 100% success in low/medium clutter and 40% in high-clutter real-world environments, outperforming ablations by 20-60% in task completion. Demonstrated effective zero-shot sim-to-real transfer.

Conclusion: The modular design combining human-labeled and auto-labeled data significantly improves human intent interpretation and 3D path inference, enabling robust sketch-based drone navigation in unseen environments.

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [29] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: A novel biometric defense method for AI-based talking-head videoconferencing that detects identity hijacking by analyzing pose-expression latents rather than reconstructed RGB video, using contrastive learning to isolate persistent identity cues.


<details>
  <summary>Details</summary>
Motivation: AI talking-head systems use compact pose-expression latents that can be puppeteered to hijack identities, and existing deepfake detectors fail because every frame is synthetic.

Method: Pose-conditioned, large-margin contrastive encoder that disentangles persistent identity cues from transient pose and expression in the transmitted latent, with cosine similarity testing for identity swap detection.

Result: Consistently outperforms existing puppeteering defenses, operates in real-time, and shows strong generalization to out-of-distribution scenarios across multiple talking-head generation models.

Conclusion: The proposed biometric leakage defense effectively detects identity hijacking in real-time by analyzing latent representations rather than RGB video, providing robust security for talking-head videoconferencing systems.

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [30] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: REVEL enables interactive drag-based video manipulation anytime on anything, addressing challenges of latent distribution drift and context interference through DragStream's training-free approach with adaptive self-rectification and spatial-frequency optimization.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive video diffusion models lack fine-grained streaming control over outputs, making it difficult to ensure consistent alignment with user expectations through interactive manipulation.

Method: Proposes DragStream with two key components: 1) adaptive distribution self-rectification using neighboring frames' statistics to constrain latent embedding drift, and 2) spatial-frequency selective optimization to exploit contextual information while mitigating interference through selective visual cue propagation.

Result: Extensive experiments demonstrate the effectiveness of DragStream, which can be seamlessly integrated into existing autoregressive video diffusion models.

Conclusion: REVEL enables versatile drag-style video manipulation (translation, deformation, rotation) and DragStream effectively addresses the challenges of latent distribution drift and context interference in streaming video manipulation.

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [31] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL is an ensemble framework that integrates multiple foundation models for computational pathology without requiring manual feature selection or fine-tuning, achieving superior performance across three cancer datasets.


<details>
  <summary>Details</summary>
Motivation: Adapting and benchmarking individual foundation models for specific diagnostic tasks in computational pathology is time-consuming and resource-intensive due to their scale and diversity.

Method: Group-Aggregative Selection Multi-Instance Learning (GAS-MIL) - a flexible ensemble framework that seamlessly integrates features from multiple foundation models while preserving their complementary strengths.

Result: Across prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa) cancer datasets, GAS-MIL consistently achieves superior or on-par performance relative to individual foundation models and established MIL methods.

Conclusion: GAS-MIL enables efficient integration of heterogeneous foundation models, streamlining model deployment for pathology and providing a scalable foundation for future multimodal and precision oncology applications.

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [32] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: A real-time situational awareness assessment framework for drone-assisted naloxone delivery in opioid overdose emergencies, using graph embeddings and transformers to analyze bystander behavior.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for real-time situational awareness assessment in human-autonomy teaming for opioid overdose emergencies, enabling drones to effectively guide medically untrained bystanders before EMS arrival.

Method: Proposed video-based real-time SA assessment framework using graph embeddings and transformer models, integrating visual perception and comprehension cues including geometric, kinematic, and interaction graph features.

Result: Achieved high-performance SA prediction with strong temporal segmentation accuracy, outperforming FINCH baseline by 9% in Mean over Frames (MoF) and 5% in Intersection over Union (IoU).

Conclusion: The framework supports development of adaptive drone systems that can effectively guide bystanders, improving emergency response outcomes and saving lives in opioid overdose situations.

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [33] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: Evaluation of four OCR systems (Tesseract, EasyOCR, PaddleOCR, TrOCR) on food packaging images shows Tesseract has best accuracy, EasyOCR offers good multilingual balance, PaddleOCR has high coverage but is slow, and TrOCR performs worst despite GPU acceleration.


<details>
  <summary>Details</summary>
Motivation: Accurate OCR for food packaging is important for compliance and nutrition monitoring, but challenging due to multilingual text, dense layouts, varied fonts, glare, and curved surfaces.

Method: Used dataset of 231 products (1,628 images) processed by all four OCR models. Created ground truth subset of 113 images (60 products) for accuracy evaluation using metrics including CER, WER, BLEU, ROUGE-L, F1, coverage, and execution time.

Result: Tesseract achieved lowest CER (0.912) and highest BLEU (0.245). EasyOCR provided good balance between accuracy and multilingual support. PaddleOCR achieved near complete coverage but was slower. TrOCR produced weakest results despite GPU acceleration.

Conclusion: Results provide packaging-specific benchmark, establish baseline, and highlight directions for layout-aware methods and text localization in OCR systems.

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [34] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle is a plug-and-play module that predicts relevant frames and optimal frame count for video understanding, reducing input frames by 35-78% while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current frame sampling methods (uniform/fixed-budget) fail to adapt to information density and task complexity variations, causing inefficiency and information loss in video understanding.

Method: Lightweight module trained with 4-stage curriculum using weak proxy signals initially, then strong supervision from FrameOracle-41K dataset with keyframe annotations. Predicts both relevant frames and optimal frame count.

Result: Reduces 16-frame inputs to 10.4 frames (35% reduction) without accuracy loss. Reduces 64-frame candidates to 13.9 frames (78% reduction) while improving accuracy by 1.4%. Achieves SOTA efficiency-accuracy trade-offs.

Conclusion: FrameOracle enables scalable video understanding by adaptively selecting optimal frames, significantly reducing computational costs while maintaining or improving performance across multiple VLMs and benchmarks.

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [35] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: Proposes a hybrid Co-FineTuning method for visual bug detection in video games that combines labeled and unlabeled data from target and co-domain games to reduce dependency on extensive labeled datasets.


<details>
  <summary>Details</summary>
Motivation: Manual visual bug detection in games is resource-intensive and requires specialized knowledge, while supervised models need large labeled datasets which are challenging due to infrequent occurrence of visual bugs.

Method: Hybrid Co-FineTuning method that integrates labeled samples from target game and co-domain games with unlabeled data to enhance feature representation learning.

Result: Superior performance compared to conventional baselines across multiple gaming environments, maintains competitive performance with only 50% of labeled data from target game.

Conclusion: The CFT framework demonstrates enhanced scalability and adaptability for efficient visual bug detection across various game titles while reducing dependency on labeled examples.

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [36] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: The Hierarchical Reasoning Model (HRM) with Transformer-style modules is evaluated as an image classifier on MNIST, CIFAR-10, and CIFAR-100. While it performs well on MNIST (98% accuracy), it overfits and generalizes poorly on natural images compared to simple CNN baselines.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the HRM with Transformer-style modules can serve as a practical image classifier, particularly testing its performance under raw training conditions without data augmentation.

Method: HRM uses two Transformer-style modules (f_L, f_H) with DEQ-style training, deep supervision, Rotary Position Embeddings, and RMSNorm. Evaluated on MNIST, CIFAR-10, and CIFAR-100 with no data augmentation, identical optimizer family with one-epoch warmup then cosine-floor decay, and label smoothing.

Result: HRM achieves 98% test accuracy on MNIST but performs poorly on natural images: 65.0% on CIFAR-10 vs 77.2% for CNN baseline, and 29.7% on CIFAR-100 vs 45.3% for CNN. HRM trains ~30x slower per epoch and shows significant overfitting (91.5% train vs 29.7% test on CIFAR-100).

Conclusion: HRM is not competitive with simple convolutional architectures for small-resolution image classification without augmentation due to insufficient image-specific inductive bias, though modifications could potentially improve its performance.

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [37] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: Survey paper analyzing DINOv2's self-supervised learning approach, comparing it with other methods, and discussing its emergent properties and limitations.


<details>
  <summary>Details</summary>
Motivation: Recent advances in SSL have enabled learning general-purpose visual features that capture both high-level semantics and fine-grained spatial structure, with DINOv2 establishing new state-of-the-art performance surpassing weakly supervised methods.

Method: Examines DINOv2's core approach including multi-crop view augmentation and self-distillation with mean teacher, traces their development in previous work, and compares performance with other SSL and WSL methods across various downstream tasks.

Result: DINOv2 surpasses weakly supervised methods like OpenCLIP on most benchmarks and demonstrates remarkable emergent properties in learned features with transformer backbones.

Conclusion: Discusses DINOv2's limitations, its impact on the field, and suggests future research directions for self-supervised learning.

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [38] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: DCS is a novel framework that creates a mutual boosting loop between diffusion models and FSCIL classifiers using reward-aligned learning to address few-shot class-incremental learning challenges.


<details>
  <summary>Details</summary>
Motivation: Current FSCIL methods struggle with generalization due to limited datasets, and direct use of diffusion models for augmentation can cause semantic misalignment or ineffective guidance.

Method: DCS establishes a co-evolutionary process where a dynamic, multi-faceted reward function derived from classifier state guides diffusion model at feature level (semantic coherence/diversity) and logits level (exploratory generation/inter-class discriminability).

Result: Demonstrably achieves state-of-the-art performance on FSCIL benchmarks, significantly enhancing both knowledge retention and new class learning.

Conclusion: The mutual boosting loop between diffusion model and classifier through reward-aligned learning effectively addresses FSCIL challenges and outperforms existing methods.

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [39] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM is a vision-language framework for detecting safety violations in mining surveillance videos, featuring a domain-specific dataset, clause filtering for efficiency, and behavior magnification for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional manual safety inspection in mining is labor-intensive, error-prone, and insufficient for large-scale dynamic environments, creating urgent need for automated safety monitoring.

Method: Proposes MonitorVLM with three innovations: domain-specific VQA dataset (9,000 samples across 40 regulations), clause filter module for efficient inference, and behavior magnifier module for enhanced action recognition.

Result: Significantly outperforms baseline models with 22.01% precision, 34.22% recall, and 28.37% F1 score improvements over 72B unfine-tuned baseline. Also reduces inference latency by 13.56% and improves precision by 3.45% and recall by 8.62%.

Conclusion: Demonstrates the potential of multimodal large models to enhance occupational safety monitoring in mining and other high-risk industries through automated violation detection and reporting.

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [40] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: A hybrid model combining guidance classification with diffusion techniques for accident detection in ITS, achieving 97.32% accuracy through cloud-based implementation and conditional modules.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional classification approaches in intelligent transportation systems by leveraging diffusion models' ability to understand complex data distributions for improved accident detection.

Method: Hybrid model integrating fine-tuned ExceptionNet outputs as input to diffusion model, using image tensors as conditioning, with multiple conditional modules that modulate linear projections using time and image embeddings.

Result: The proposed diffusion model achieves 97.32% accuracy in image-based accident detection, outperforming baseline models on publicly available dataset.

Conclusion: The integration of diffusion models with guidance classification provides a robust framework for accident detection in ITS, with cloud-based implementation enabling scalable and efficient processing.

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [41] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: SAMSOD model improves RGB-T salient object detection by addressing modality imbalance and gradient conflicts through unimodal supervision and gradient deconfliction, with decoupled adapters for better foreground-background learning.


<details>
  <summary>Details</summary>
Motivation: Current RGB-T SOD methods using fine-tuned Segment Anything Model ignore modality imbalance convergence and gradient differences between high/low activations, limiting performance.

Method: Proposes SAMSOD with unimodal supervision for non-dominant modality learning, gradient deconfliction to reduce conflicting gradients, and two decoupled adapters to separately mask high/low-activation neurons for enhanced background learning.

Result: Demonstrates effectiveness on RGB-T SOD benchmark datasets, scribble supervised RGB-T SOD, fully supervised RGB-D SOD datasets, and RGB-D rail surface defect detection.

Conclusion: The proposed method effectively addresses modality imbalance and gradient conflicts, showing strong performance across multiple SOD tasks and datasets.

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [42] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: The paper introduces SOREC dataset for small object referring expression comprehension and proposes PIZA adapter for parameter-efficient fine-tuning to improve small object localization.


<details>
  <summary>Details</summary>
Motivation: Localizing extremely small objects in referring expression comprehension remains challenging despite its importance in real-world applications like autonomous driving.

Method: Created SOREC dataset with 100,000 expression-bounding box pairs for small objects in driving scenarios, and proposed PIZA adapter module for progressive-iterative zooming to localize small objects.

Result: Applied PIZA to GroundingDINO and demonstrated significant improvement in accuracy on the SOREC dataset.

Conclusion: The proposed dataset and method effectively address the challenge of small object localization in REC tasks, with publicly available resources.

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [43] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: The paper proposes Attention-WNet, a deep learning model that incorporates attention mechanisms into WNet for improved retinal artery-vein segmentation to help diagnose vasculature diseases.


<details>
  <summary>Details</summary>
Motivation: Retinal artery-vein segmentation is crucial for analyzing retinal vessels to identify biomarkers for eye diseases and systemic vasculature conditions like stroke and myocardial infarction.

Method: Developed Attention-WNet by incorporating attention mechanisms into the WNet deep learning model for enhanced retinal vessel segmentation.

Result: The proposed approach outperformed other state-of-the-art models when tested on HRF and DRIVE datasets.

Conclusion: Attention-WNet demonstrates superior performance in retinal artery-vein segmentation, providing an effective tool for clinical analysis of vasculature health.

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [44] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: This paper creates demographic annotations for LAION-400M dataset and shows direct correlation between training data imbalances and downstream model biases in vision-language models.


<details>
  <summary>Details</summary>
Motivation: To understand how training data composition contributes to demographic biases in vision-language models, given the lack of demographic annotations in web-scale datasets like LAION-400M.

Method: Created person-centric annotations for the full LAION-400M dataset using validated automatic labeling pipelines combining object detection, multimodal captioning, and finetuned classifiers, producing over 276 million bounding boxes with perceived gender and race/ethnicity labels.

Result: Uncovered demographic imbalances and harmful associations in training data, showing disproportionate linking of men and individuals perceived as Black or Middle Eastern with crime-related and negative content. Found that 60-70% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the data.

Conclusion: Established the first large-scale empirical link between dataset composition and downstream model bias, demonstrating that training data imbalances directly contribute to demographic biases in vision-language models.

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [45] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: Comparison of generic vs specialized pretrained networks for favela detection, examining whether task specificity or data volume yields better performance.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for informal settlement detection haven't fully utilized recent pretrained networks' potential, creating a gap in understanding whether task-specific pretraining or large-scale generic pretraining works better.

Method: Compare two types of pretrained neural networks: generic networks trained on large diverse image datasets, and specialized networks pretrained specifically on satellite imagery.

Result: The paper investigates the performance comparison but doesn't specify the actual results in the abstract.

Conclusion: The research aims to determine whether task specificity or data volume yields superior performance in urban informal settlement detection, though the conclusion isn't explicitly stated in the abstract.

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [46] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: LoRA patching bypasses proactive Deepfake defenses by injecting plug-and-play patches into generators, using adaptive gating and multi-modal feature alignment to defeat existing protections with minimal training.


<details>
  <summary>Details</summary>
Motivation: Current proactive Deepfake defenses that embed adversarial perturbations in facial images lack robustness and reliability, creating a security vulnerability that needs to be exposed and addressed.

Method: Proposes Low-Rank Adaptation (LoRA) patching with learnable gating mechanism to prevent gradient explosions, and Multi-Modal Feature Alignment (MMFA) loss for semantic-level feature alignment between adversarial and desired outputs.

Result: Successfully defeats multiple state-of-the-art proactive defenses with only 1,000 facial examples and a single epoch of fine-tuning, revealing critical weaknesses in current defense paradigms.

Conclusion: Current Deepfake defense strategies have significant vulnerabilities, and the proposed LoRA patching approach both exposes these weaknesses and offers defensive patching as a complementary solution to mitigate the security threat.

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [47] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: Proposes Reference-Set-Finetuning (RSF) to improve Visual Place Recognition by finetuning models on test-time reference sets, boosting performance on challenging benchmarks by ~2.3% on average.


<details>
  <summary>Details</summary>
Motivation: Some VPR benchmarks remain challenging when test environments differ significantly from training datasets, creating a domain gap that current methods struggle with.

Method: Reference-Set-Finetuning (RSF) - finetuning VPR models on the test-time reference set (the "map") before receiving test queries, leveraging domain-specific information from the target environment.

Result: RSF boosts State-of-the-Art VPR methods by ~2.3% average increase in Recall@1 on challenging datasets, while maintaining generalization across diverse test datasets.

Conclusion: Test-time reference set finetuning is an effective complementary approach to bridge train-test domain gaps in VPR, improving performance on challenging benchmarks without sacrificing generalization.

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [48] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: ARSAM accelerates Sharpness-Aware Minimization (SAM) by decomposing gradients and adaptively reusing components, achieving comparable accuracy to SAM with ~40% speedup.


<details>
  <summary>Details</summary>
Motivation: SAM improves generalization but doubles computational cost compared to SGD due to requiring twice the gradient calculations per step.

Method: Decomposes SAM gradient into SGD gradient and Projection of Second-order gradient onto First-order gradient (PSF), then adaptively reuses PSF and timely updates it to maintain performance.

Result: Achieves state-of-the-art accuracies comparable to SAM across diverse architectures, with ~40% speedup on CIFAR-10/100. Also accelerates challenging tasks like human pose estimation and model quantization without performance loss.

Conclusion: ARSAM provides an efficient alternative to SAM that maintains generalization benefits while significantly reducing computational overhead, demonstrating broad practicality across various tasks.

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [49] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: CoPA framework improves concept bottleneck models by extracting multilayer concepts under prompt guidance, enhancing concept capture and disease prediction performance.


<details>
  <summary>Details</summary>
Motivation: Existing concept-based methods have limited concept capture capabilities - they only use final layer features, neglect shallow/multiscale features, and lack guidance for fine-grained concept extraction, hindering clinical diagnostic transparency.

Method: Proposes Concept Prompting and Aggregating (CoPA) with Concept-aware Embedding Generator (CEG) to extract concept representations from each layer, and Concept Prompt Tuning (CPT) to amplify critical concept-related visual cues through prompt guidance.

Result: CoPA outperforms state-of-the-art methods on three public datasets, effectively capturing valuable concept-wise information and improving both concept and disease prediction performance.

Conclusion: The CoPA framework successfully addresses limitations of existing concept-based methods by leveraging multilayer concept extraction under prompt guidance, enhancing model transparency and diagnostic performance in clinical applications.

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [50] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: ZFP compression achieves 22.89:1 data reduction for 3D medical imaging while maintaining cerebrovascular segmentation quality (Dice 0.87656 vs 0.8774 baseline).


<details>
  <summary>Details</summary>
Motivation: Address challenges of large 3D medical imaging datasets that hinder collaborative research and transferability.

Method: Apply ZFP compression in error tolerance and fixed-rate modes to 3D medical dataset with vascular segmentations, compare segmentation quality on compressed vs uncompressed volumes.

Result: ZFP achieved up to 22.89:1 compression ratio with minimal impact on segmentation performance (mean Dice 0.87656 vs baseline 0.8774).

Conclusion: ZFP is a viable tool for enabling efficient and accessible research on large-scale medical datasets, fostering broader collaboration.

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [51] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: Proposes a hybrid medical image segmentation architecture combining CNNs, Transformers, and Mamba-based attention to capture local, global, and long-range dependencies, achieving state-of-the-art performance with balanced efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of task-specific models with varying performance across modalities and anatomical regions, and the challenge of balancing model complexity with clinical requirements for both accuracy and efficiency.

Method: Three-branch encoder integrating CNNs, Transformers, and Mamba-based Attention Fusion (MAF), multi-scale attention-based CNN decoder, and co-attention gate for enhanced feature selection and cross-scale communication.

Result: Outperforms state-of-the-art methods in accuracy and generalization across multiple benchmark datasets while maintaining comparable computational complexity.

Conclusion: The architecture effectively balances efficiency and effectiveness, offering a practical and scalable solution for diverse medical imaging tasks.

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [52] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: This paper presents a YOLOv9-based deep learning system for automated road damage and manhole detection using polygonal annotations, achieving 78.1% overall accuracy with strong performance on road damage classes but challenges in manhole detection due to class imbalance.


<details>
  <summary>Details</summary>
Motivation: Manual monitoring of road damages is time-consuming, costly, and error-prone, especially in developing countries. There's a need for automated solutions to improve urban safety and infrastructure maintenance in smart city development.

Method: Used YOLOv9 algorithm with polygonal annotations (instead of traditional bounding boxes) for more precise localization. Developed a novel dataset of over 1000 images from Dhaka, Bangladesh, training the model for three classes: Broken, Not Broken, and Manhole.

Result: Achieved 78.1% overall image-level accuracy. Strong performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score) classes, but poor performance for Manhole detection (18.2% F1-score) due to class imbalance in the dataset.

Conclusion: The approach offers an efficient and scalable solution for monitoring urban infrastructure in developing countries, though improvements are needed for better manhole detection through addressing class imbalance issues.

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [53] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: Proposes Contrastive-SDE, combining score-based diffusion models with contrastive learning for unpaired image-to-image translation without requiring aligned samples or classifier training.


<details>
  <summary>Details</summary>
Motivation: Leverage the strengths of diffusion models for high-fidelity generation and contrastive learning for semantic consistency in unpaired settings where aligned samples are unavailable.

Method: Time-dependent contrastive learning using SimCLR with image-domain invariant feature pairs, then using the learned contrastive model to guide inference of a pretrained SDE for translation.

Result: Achieves comparable performance to state-of-the-art on several metrics across three unpaired I2I tasks, with significantly faster convergence and no label supervision requirements.

Conclusion: Contrastive-SDE provides an efficient alternative for unpaired image-to-image translation that preserves domain-invariant features while discarding domain-specific ones without needing classifier training.

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [54] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO is an extended benchmark that reveals current VLA models achieve 90% accuracy in standard LIBERO evaluation but collapse to 0% under realistic perturbations, exposing their reliance on memorization rather than genuine understanding.


<details>
  <summary>Details</summary>
Motivation: Current LIBERO benchmark settings lead to inflated performance estimates and prevent fair model comparison, as models memorize training data rather than developing true task comprehension.

Method: Extended LIBERO benchmark with systematic perturbations across four dimensions: manipulated objects, initial states, task instructions, and environments to test model robustness.

Result: Models that achieve over 90% accuracy in standard evaluation collapse to 0.0% under generalized perturbations, showing they rely on rote memorization of action sequences and environment layouts.

Conclusion: Current evaluation practices are severely flawed, and the community should adopt robust assessments that test genuine model generalization and comprehension rather than memorization.

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [55] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: The paper introduces Mirage, a dataset of AI-generated images with visible artifacts that current detectors fail on, and investigates using Large Vision-Language Models for explainable AI image detection.


<details>
  <summary>Details</summary>
Motivation: There's a growing gap where AI-generated images are becoming undetectable by standard AI detectors but still distinguishable by humans, creating a need for better detection methods.

Method: Created the Mirage dataset with diverse AI-generated images containing visible artifacts, then tested Large Vision-Language Models (LVLMs) for explainable AI image detection across Mirage and existing benchmarks.

Result: LVLMs are highly effective at detecting AI-generated images with visible artifacts but their performance declines when images lack such visual cues.

Conclusion: LVLMs show promise for explainable AI image detection, particularly for images with visible artifacts, but struggle with more sophisticated AI-generated content lacking obvious cues.

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [56] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround introduces a unified visual grounding paradigm that dynamically selects intermediate transformer layers as "mask as prompt" instead of using fixed last hidden layers, addressing error propagation and spatial cue limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of current visual grounding methods that rely on fixed last hidden layers (amplifying cumulative errors) and use <SEG> tokens as prompts (lacking explicit spatial cues like coordinates).

Method: Proposes Policy-Prompted Masking with two components: Stochastic Skip Connection (SSC) - a reinforcement learning policy for dynamic layer selection, and Mask as Prompt (MasP) - using similarity maps as soft logit masks to prompt SAM for mask generation with explicit spatial cues.

Result: UGround unifies visual grounding within a single framework spanning traditional refer expression segmentation to reasoning segmentation, single-target to multi-target, and positive query to false premise scenarios.

Conclusion: UGround provides an effective unified paradigm for visual grounding that addresses key limitations of existing methods through dynamic layer selection and explicit spatial cue integration.

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [57] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4 is a framework that optimizes 4D Gaussian Splatting by progressively pruning and merging Gaussians to reduce storage overhead while maintaining reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: 4D Gaussian Splatting faces major storage overhead challenges with millions of Gaussians needed for high-fidelity reconstruction, and existing methods have limitations in compression ratio or visual quality.

Method: Progressive pruning in three stages: Gaussian Sampling to identify critical primitives, Gaussian Pruning to remove redundancies, and Gaussian Merging to fuse similar primitives. Also integrates implicit appearance compression and generalizes Sub-Vector Quantization (SVQ) to 4D representations.

Result: Reduces model sizes by over 60% while maintaining reconstruction quality, significantly outperforming recent state-of-the-art methods on standard benchmark datasets.

Conclusion: OMG4 represents a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications.

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [58] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: A novel framework for adapting open-vocabulary object detection from ground-view to aerial imagery through structured domain alignment, achieving significant performance improvements in zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection models are limited to fixed classes, making it costly to add new categories. Open-vocabulary detection enables identifying unseen classes without explicit training, but domain shifts between ground-view and aerial imagery require specialized adaptation strategies.

Method: Proposes contrastive image-to-image alignment to enhance similarity between aerial and ground-view embeddings, and multi-instance vocabulary associations to align aerial images with text embeddings.

Result: Achieves improvements of +6.32 mAP on DOTAv2, +4.16 mAP on VisDrone, and +3.46 mAP on HRRSD in zero-shot setting compared to finetuned closed-vocabulary models.

Conclusion: The framework paves the way for more flexible and scalable object detection systems in aerial applications by effectively transferring open-vocabulary representations across domains.

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [59] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: This review paper analyzes deep learning approaches for skin cancer diagnosis, discussing challenges like complex features and data imbalance, and presents solutions including data augmentation and hybrid models.


<details>
  <summary>Details</summary>
Motivation: Skin cancer is a prevalent and deadly disease where early detection is crucial. Deep learning shows promise for automated diagnosis but faces challenges that need to be addressed.

Method: The review follows PRISMA framework methodology, synthesizing recent research on DL approaches for skin disease diagnosis.

Result: The review identifies innovative approaches to overcome DL challenges in skin cancer diagnosis, including data augmentation, hybrid models, and feature fusion techniques.

Conclusion: Deep learning has transformative potential in dermatological care but requires continued advancements for full integration into clinical workflows and improved decision-making.

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [60] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: SDAKD is a novel GAN distillation method that introduces a student discriminator to address capacity mismatch issues, achieving better performance than existing methods in super-resolution tasks.


<details>
  <summary>Details</summary>
Motivation: GANs have high computational requirements that limit deployment on resource-constrained devices, and existing knowledge distillation methods struggle due to capacity mismatch between student generators and teacher discriminators.

Method: Proposes Student Discriminator Assisted Knowledge Distillation (SDAKD) with a three-stage training strategy and adapted feature map distillation in the last two stages.

Result: Experiments on GCFSR and Real-ESRGAN show consistent improvements over baselines and state-of-the-art GAN knowledge distillation methods.

Conclusion: SDAKD effectively addresses capacity mismatch in GAN distillation and enables more efficient deployment of GANs on resource-constrained devices.

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [61] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: Created PoseGaze-AHP, the first 3D dataset synchronizing head pose and gaze movements for ocular-induced abnormal head posture diagnosis, using LLMs for clinical data extraction and achieving 91.92% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing datasets analyze head pose and ocular movements separately, limiting integrated diagnostic approaches and AI-driven advancements in AHP analysis.

Method: Used Claude 3.5 Sonnet LLM with iterative prompting strategies to extract clinical data from literature, then transformed records into 3D representations using Neural Head Avatar framework, generating 7,920 images from two head textures.

Result: Successfully created PoseGaze-AHP dataset covering broad spectrum of ocular conditions with 91.92% extraction accuracy, making it the first publicly available resource for AI-driven ocular-induced AHP diagnosis.

Conclusion: PoseGaze-AHP enables development of accurate, privacy-compliant diagnostic tools for ocular-induced abnormal head posture by providing integrated head pose and gaze movement data.

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [62] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: This paper introduces DHQA-4D, a large-scale dataset for quality assessment of dynamic 4D digital human avatars, and proposes DynaMesh-Rater, a novel LMM-based method that extracts multi-dimensional features to predict quality scores.


<details>
  <summary>Details</summary>
Motivation: Dynamic 4D human avatars are prone to noise degradation during collection, compression, and transmission, which affects user viewing experience, making quality assessment increasingly important.

Method: Proposed DynaMesh-Rater, a large multimodal model that extracts visual features from projected 2D video, motion features from cropped video clips, and geometry features from 4D human mesh, then uses LoRA-based instruction tuning to predict quality scores.

Result: Extensive experiments on the DHQA-4D dataset demonstrate the superiority of DynaMesh-Rater over previous quality assessment methods.

Conclusion: The proposed DHQA-4D dataset and DynaMesh-Rater method provide effective solutions for quality assessment of dynamic 4D digital human avatars, addressing the challenge of noise degradation in various applications.

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [63] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: Improved ResNet-50 with Adaptive Spatial Feature Fusion (ASFF) for skin cancer classification, achieving 93.18% accuracy and superior performance metrics on ISIC 2020 dataset.


<details>
  <summary>Details</summary>
Motivation: Address challenges in skin cancer classification including high inter-class similarity, intra-class variability, and image noise in dermoscopic images.

Method: Enhanced ResNet-50 with ASFF mechanism that adaptively integrates multi-scale semantic and surface features through dual-branch design with global average pooling and fully connected layers for weighted fusion.

Result: Achieved 93.18% accuracy, AUC of 0.9670 (P-R curve) and 0.9717 (ROC curve), outperforming 5 classic CNN models. Grad-CAM validation showed improved lesion focus and background suppression.

Conclusion: The proposed approach provides a more effective and efficient solution for computer-aided skin cancer diagnosis with enhanced feature learning capabilities.

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [64] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: Developed a multimodal deep learning framework using DenseNet-121 CNNs to improve early detection of Oral Squamous Cell Carcinoma by integrating clinical, radiological, and histopathological images.


<details>
  <summary>Details</summary>
Motivation: Late diagnosis of OSCC contributes to high mortality rates, with over 50% of cases detected at advanced stages and 5-year survival below 50%. Need for improved early detection methods.

Method: Used retrospective study with public datasets across three imaging modalities. Trained DenseNet-121 CNNs via transfer learning for each modality, applied augmentation and preprocessing, and fused predictions using validation-weighted ensemble strategy.

Result: High validation accuracy for radiological (100%) and histopathological (95.12%) modalities, lower for clinical images (63.10%). Ensemble model achieved 84.58% overall accuracy on multimodal validation dataset of 55 samples.

Conclusion: The multimodal ensemble framework provides a non-invasive AI-assisted triage tool that enhances early identification of high-risk lesions, supports clinical decision-making, and aims to reduce diagnostic delays and improve patient outcomes.

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [65] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper challenges the scaling law in explainable image quality assessment by showing that data quality matters more than quantity. It proposes IQA-Select, a clustering-based data selection method that achieves better performance using only 10% of data.


<details>
  <summary>Details</summary>
Motivation: Large-scale instruction tuning datasets for MLLMs in explainable IQA cause high computational costs and redundant data that can harm model performance, challenging the conventional scaling law approach.

Method: Proposes a three-stage clustering-based data selection framework: clustering feature extraction, cluster quota allocation, and cluster sampling strategy. Uses a pre-trained MLLM to investigate performance with different data sizes and develops IQA-Select method.

Result: IQA-Select achieves 102.1% and 103.7% performance of full fine-tuning using only 10% selected data on Q-Bench and AesBench datasets respectively, reducing computational costs while improving performance.

Conclusion: Data quality is more important than quantity for explainable IQA, and the proposed IQA-Select method effectively addresses data redundancy while achieving superior performance with minimal computational resources.

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [66] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: The paper introduces a framework that separates high-level planning from low-level action execution using sparse 3D trajectories as an intermediate representation, addressing limitations in current Vision-Language-Action models.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models generalize poorly due to monolithic architectures constrained by scarce data, while dual-system approaches suffer from semantic ambiguities that prevent cross-task training and require fine-tuning for new environments.

Method: Uses sparse 3D trajectories as intermediate representation between VLM planning and action execution. VLMs generate coarse 3D waypoints, which are refined by a generalizable action expert into dense action sequences using real-time point cloud observations. Implements "Action Pre-training, Pointcloud Fine-tuning" paradigm.

Result: The framework combines VLM's broad generalization in visual understanding and planning with fine-grained action-level generalization of the action expert, enabling more efficient training and robust generalization.

Conclusion: The proposed approach effectively bridges high-level planning with low-level physical action execution, overcoming limitations of conventional VLA models and dual-system approaches by using 3D trajectories as an intermediate representation.

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [67] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: A novel method that transforms zero-shot fine-grained image classification into a visual question-answering framework using Large Vision-Language Models, with attention intervention and improved class descriptions, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The potential of Large Vision-Language Models for zero-shot fine-grained image classification remains underexplored, despite their impressive performance on vision-language reasoning tasks.

Method: Transform zero-shot fine-grained image classification into visual question-answering framework using LVLMs, enhanced with novel attention intervention technique and comprehensive class description benchmarks.

Result: Consistently outperforms current state-of-the-art approach across multiple fine-grained image classification benchmarks.

Conclusion: Demonstrates both the effectiveness of the proposed method and the broader potential of LVLMs for zero-shot fine-grained classification tasks.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [68] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: This paper presents a comprehensive benchmark study comparing various defogging methods for autonomous driving perception, including classical filters, modern networks, chained approaches, and VLM-based editing, evaluating both image quality and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving perception systems struggle in foggy conditions, and existing defogging methods show inconsistent improvements in downstream detection/segmentation tasks. There's a gap in understanding real-world transferability and when preprocessing actually helps autonomous perception.

Method: Structured empirical study benchmarking multiple pipelines: classical filters, modern defogging networks, chained variants (filter→model, model→filter), and prompt-driven VLM image editing models. Evaluation uses Foggy Cityscapes dataset with object detection (mAP) and segmentation (PQ, RQ, SQ) metrics.

Result: Analysis reveals when defogging helps, when chaining yields synergy or degradation, and how VLM-based editors compare to dedicated approaches. VLM judge qualitative scores show strong correlation with mAP. The study establishes conditions under which preprocessing genuinely improves autonomous perception.

Conclusion: The research provides a transparent, task-oriented benchmark for defogging methods and identifies specific conditions where preprocessing actually enhances autonomous driving perception in adverse weather conditions.

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [69] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO is a cascaded framework that bridges Text-to-Motion models with conditional Video Diffusion Models for general human motion video generation, addressing alignment issues and introducing camera-aware conditioning.


<details>
  <summary>Details</summary>
Motivation: Human video generation has broad applications but current video diffusion models are underexplored for general-purpose human video generation, mostly limited to image-to-video setups or narrow domains like dance videos.

Method: A cascaded framework that connects Text-to-Motion models with conditional Video Diffusion Models through carefully designed components, including text/visual prompt preparation and a camera-aware conditioning module that automatically selects viewpoints aligned with input text.

Result: Demonstrated effectiveness on both MovieGen benchmark and a new benchmark for T2M-VDM combination, showing versatility across diverse use cases.

Conclusion: CAMEO successfully bridges T2M models and VDMs for general human motion video generation, improving alignment and reducing manual intervention through its cascaded framework and camera-aware conditioning.

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [70] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: OpenFLAME is a federated Visual Positioning System that enables distributed 6DoF localization for AR applications across multiple organizations without centralized data sharing.


<details>
  <summary>Details</summary>
Motivation: Centralized VPS solutions from large companies fail to cover private indoor spaces due to privacy concerns, regulations, and maintenance bottlenecks. There's a need for distributed VPS that allows organizations to maintain their own spaces.

Method: Federated VPS backend where independent organizations 3D scan and maintain separate VPS services for their own spaces. Uses federated image-based localization with reference solutions for managing and merging data across maps without sharing private data.

Result: Enables access control of indoor 3D scans, distributed maintenance of VPS backend, and encourages larger coverage while addressing challenges like coherency across spaces, quality control, and service selection.

Conclusion: OpenFLAME provides a viable federated approach to VPS that overcomes limitations of centralized systems while maintaining privacy and enabling broader spatial coverage for AR applications.

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [71] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: A framework that combines CNN-LSTM models for biomechanical feature extraction with LLMs to generate actionable feedback for tennis stroke analysis, bridging biomechanics and explainable AI.


<details>
  <summary>Details</summary>
Motivation: Existing tennis analysis systems fail to connect biomechanical insights with accessible, meaningful feedback for players and coaches.

Method: Extracts biomechanical features (joint angles, limb velocities, kinetic chain patterns) using CNN-LSTM models from motion data, then uses LLMs to generate actionable feedback based on stroke effectiveness and injury risk analysis.

Result: Developed a novel framework that produces technically accurate, biomechanically grounded, and actionable feedback for end-users using the THETIS dataset.

Conclusion: The framework successfully bridges the gap between explainable AI and sports biomechanics, providing interpretable and actionable feedback for tennis stroke analysis.

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [72] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: TimeWarp is a method to create synthetic temporal datasets that improve Video-LLMs' fine-grained temporal understanding, achieving significant performance gains across seven benchmarks.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs underperform on tasks requiring fine-grained temporal understanding due to lack of visual complexity and temporal nuance in current datasets, causing them to rely on language-based reasoning rather than understanding video dynamics.

Method: Proposed TimeWarp - a systematic method to create targeted synthetic temporal datasets for fine-tuning Video-LLMs, introducing a large-scale preference dataset that captures intricate temporal dynamics and grounds model responses to visual and temporal information.

Result: Applied to existing models, TimeWarp significantly improves performance on temporal understanding benchmarks, achieving absolute performance improvements across seven benchmarks.

Conclusion: TimeWarp effectively advances temporal understanding in Video-LLMs through targeted synthetic datasets, demonstrating the importance of temporal nuance in video understanding tasks.

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [73] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: This paper introduces BMC-LongCLIP, a biomedical vision-language model with extended text context length (512 tokens) to handle long-format biomedical captions, reducing token waste from 55% to 2.2% and achieving significant performance gains in retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Standard vision-language models use short text windows (<77 tokens), forcing truncation of long biomedical captions. Analysis shows many biomedical captions exceed this limit, leading to loss of valuable information.

Method: Extend context length of text encoders in VLMs to 512 tokens, pretrain on BIOMEDICA-LongCAP dataset containing 1M image-caption pairs with context-aware descriptions from full-text articles.

Result: BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 for long-caption retrieval and +2% average improvements in classification, while converging faster than short-context models.

Conclusion: Long-context modeling is a promising direction for advancing biomedical VLMs, enabling better utilization of rich information in long-format biomedical captions.

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [74] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: CPG is a framework for long-tailed semi-supervised learning that handles unknown unlabeled data distributions by progressively adding reliable pseudo-labels to labeled data with a known distribution, using a controllable self-reinforcing optimization cycle.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume unlabeled data follows predefined distributions, but in reality the distribution is unknown and arbitrary. This creates challenges for reliable pseudo-label generation and model training.

Method: Uses a controllable self-reinforcing optimization cycle: (1) dynamic controllable filtering to add reliable pseudo-labels while maintaining known distribution, (2) Bayes-optimal classifier with logit adjustment, (3) improved classifier helps identify more pseudo-labels. Also includes class-aware adaptive augmentation and auxiliary branch for data utilization.

Result: Achieves consistent improvements across benchmark datasets, surpassing state-of-the-art methods by up to 15.97% in accuracy.

Conclusion: CPG effectively handles unknown unlabeled data distributions in long-tailed semi-supervised learning through its controllable pseudo-label generation framework and optimization cycle, with theoretical guarantees on generalization error reduction.

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [75] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: Fine-tuning PaddleOCRv5 improves Classical Chinese (Han-Nom) text recognition from 37.5% to 50.0% accuracy, especially for degraded historical documents.


<details>
  <summary>Details</summary>
Motivation: Existing OCR systems struggle with degraded scans, non-standard glyphs, and handwriting variations in ancient Vietnamese Chinese manuscripts, hindering digitization and cross-lingual semantic research.

Method: Fine-tuned PaddleOCRv5's text recognition module using curated Han-Nom manuscripts, with full training pipeline including preprocessing, LMDB conversion, evaluation, and visualization.

Result: Significant improvement over base model: exact accuracy increased from 37.5% to 50.0%, particularly effective under noisy image conditions.

Conclusion: The fine-tuned model enables better Han-Nom text recognition, supporting downstream applications like semantic alignment, machine translation, and historical linguistics research, with an interactive demo available.

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [76] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: MetaSeg is a meta-learning framework that trains implicit neural representations (INRs) for medical image segmentation, achieving comparable performance to U-Nets with 90% fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Implicit neural representations are effective for signal representation but not naturally suited for predictive tasks like segmentation that require learning semantic structures across signal distributions.

Method: MetaSeg uses an INR that simultaneously predicts pixel intensities and class labels, with a meta-learning procedure to find optimal initial parameters over training data, enabling quick fine-tuning for unseen test images.

Result: MetaSeg achieved Dice scores comparable to U-Net models on 2D and 3D brain MRI segmentation tasks, while using 90% fewer parameters.

Conclusion: MetaSeg provides a scalable alternative to traditional resource-heavy architectures like U-Nets and vision transformers for medical image segmentation.

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [77] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: Video-in-the-Loop (ViTL) is a two-stage framework for long-video QA that localizes relevant intervals with low-fps skimming and then answers with span-aware token reallocation at higher effective frame rates, achieving better performance with fewer frames.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of long-video QA while maintaining accuracy and providing interpretable results with direct attribution.

Method: Two-stage approach: 1) Localize question-relevant intervals using low-fps skim, 2) Answer via span-aware reallocation of visual tokens at higher effective frame rate with interleaved output of spans and final options.

Result: Achieves up to 8.6% improvement with 50% less frame input on long-video QA and temporal grounding tasks (Charades-STA, ActivityNet-Captions), outperforming uniform sampling approaches.

Conclusion: ViTL provides an interpretable, compute-efficient solution for scalable long-video QA through span-aware token reallocation and end-to-end training with coupled localization and answer objectives.

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [78] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: AgentAug is a data augmentation framework that generates diverse fake news videos by simulating creative fabrication processes using LLM-driven pipelines, improving fake news detection performance.


<details>
  <summary>Details</summary>
Motivation: Current fake news detectors suffer from biased patterns due to limited and undiversified training data, failing to capture the complex many-to-many relationships between video segments and fabricated news events in real-world scenarios.

Method: Proposes AgentAug framework with multiple LLM-driven pipelines for four fabrication categories, combined with active learning based on uncertainty sampling to select useful augmented samples during training.

Result: Experimental results on two benchmark datasets demonstrate that AgentAug consistently improves the performance of short video fake news detectors.

Conclusion: AgentAug effectively addresses the data scarcity and diversity issues in fake news video detection by simulating realistic fabrication processes, leading to improved detection performance.

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [79] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: This paper improves prompt-to-prompt image editing by optimizing hyperparameters and developing new methods to enhance precision and reliability in text-driven image manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the variability and inconsistency in results from current stable diffusion models used for image editing, particularly issues like inconsistent hair color changes and cycle inconsistency.

Method: Conducted comprehensive study of 'word swap' method, developed 'attention re-weight method' for better adaptability, and proposed 'CL P2P' framework to address existing limitations.

Result: Enhanced precision and reliability of prompt-to-prompt image editing frameworks through hyperparameter optimization and improved attention mechanisms.

Conclusion: The research contributes to understanding how hyperparameter settings and architectural choices in neural network attention mechanisms significantly influence image composition and quality in text-driven editing.

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [80] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight improves visual grounding accuracy in multimodal GUI systems by using image-grounded reasoning and specialized tools to iteratively narrow focus to relevant screen regions.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs for GUI systems are limited by unreliable visual grounding, preventing accurate pointer-level actions like clicking or dragging.

Method: Train a model for image-grounded reasoning that dynamically invokes multiple specialized tools to iteratively narrow focus to relevant screen regions.

Result: Achieves 52.8% accuracy on ScreenSpot-Pro benchmark with only 18.5K training samples, outperforming larger models like V2P-7B (50.6% with 9.6M samples) and GTA-1-7B (50.1% with 1.56M samples).

Conclusion: GUI-Spotlight substantially improves visual grounding accuracy with significantly fewer training samples compared to existing approaches.

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [81] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: A range estimation method for post-training quantization that minimizes quantization errors through layer-wise local minima optimization, achieving state-of-the-art performance with minimal accuracy loss in 8-bit and 6-bit settings.


<details>
  <summary>Details</summary>
Motivation: Low-bit quantization while maintaining model accuracy is challenging in post-training quantization for reducing deep neural network storage.

Method: Model range estimation as optimization problem minimizing quantization errors by layer-wise local minima, prove local convexity, develop efficient search algorithm, and apply to transformed weights space.

Result: Outperforms state-of-the-art on top-1 accuracy for ResNet series and Inception-v3, with almost no loss in 8-bit/6-bit settings and significant improvement in 4-bit quantization.

Conclusion: The proposed range estimation method effectively improves quantization performance with minimal accuracy degradation across different bit-width settings.

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [82] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind is a tri-modal retrieval framework for 3D assets that uses text, image, and 3D queries to enhance scene generation in the metaverse by ensuring spatial, semantic, and stylistic consistency.


<details>
  <summary>Details</summary>
Motivation: Address inconsistent 3D asset retrieval that ignores spatial, semantic, and stylistic constraints, and the lack of standardized retrieval methods specifically designed for 3D assets.

Method: Uses a flexible tri-modal retrieval mechanism with ESSGNN layout encoder to capture spatial relationships and object appearance, supporting iterative scene construction.

Result: Empirical evaluations show improved spatial and stylistic consistency compared to baseline methods in various retrieval tasks.

Conclusion: MetaFind provides an effective framework for contextually and stylistically coherent 3D asset retrieval in metaverse scene generation.

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [83] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: Proposes an ordinality-aware loss function that integrates sub-class ordinal relationships into binary cross-entropy loss for solar flare prediction, penalizing misclassifications near the threshold more heavily.


<details>
  <summary>Details</summary>
Motivation: Binary classification for solar flare prediction ignores ordinal relationships among sub-classes, leading to frequent misclassifications near the prediction threshold where models struggle to differentiate similar intensity events.

Method: Modified loss function that integrates ordinal information of sub-classes into conventional binary cross-entropy loss, serving as an ordinality-aware regularization method that penalizes incorrect predictions near the threshold more heavily.

Result: The approach aims to enhance model learning by leveraging ordinal characteristics of the data, though specific performance metrics are not provided in the abstract.

Conclusion: Incorporating ordinal weighting into the loss function can improve solar flare prediction performance by better handling the ordinal nature of flare intensity sub-classes and reducing misclassifications near decision boundaries.

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [84] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantDemoire is a post-training quantization framework for demoireing models that addresses performance degradation issues through outlier-aware quantization and frequency-aware calibration, achieving significant parameter/computation reductions while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based demoireing methods require substantial computational resources, limiting deployment on edge devices. Direct application of standard quantization methods causes severe performance degradation due to distribution outliers and weakened representations in smooth regions.

Method: Proposes two key components: 1) Outlier-aware quantizer using sampling-based range estimation to reduce activation outliers and keeping extreme weights in FP16, 2) Frequency-aware calibration strategy that emphasizes low- and mid-frequency components during fine-tuning to mitigate banding artifacts.

Result: QuantDemoire achieves large reductions in parameters and computation while maintaining quality, outperforming existing quantization methods by over 4 dB on W4A4 (4-bit weight and activation quantization).

Conclusion: The proposed QuantDemoire framework effectively addresses quantization challenges for demoireing models, enabling efficient deployment on edge devices without significant quality degradation.

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [85] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA: A novel method combining diffusion generative priors with multi-regularization constraints (anisotropic TV and nuclear norm LoRA) for low-dose sparse-view CT reconstruction, achieving superior performance in image quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the ill-posedness and texture loss problems in extremely sparse-view CT reconstruction, particularly under low-dose conditions where traditional methods struggle with artifact suppression and detail preservation.

Method: Combines diffusion generative prior (NCSN++ with SDE modeling) with multi-regularization constraints (anisotropic TV and nuclear norm LoRA) in an ADMM framework, using 2D slice-based strategy with FFT acceleration and tensor-parallel optimization.

Result: Consistently surpasses benchmarks on AAPM-2016, CTHD, and LIDC datasets with N_view=8,4,2 in SSIM, texture recovery, edge clarity, and artifact suppression, demonstrating strong robustness and generalizability.

Conclusion: Diffusion + TV-LoRA achieves high-fidelity, efficient 3D CT reconstruction with broad clinical applicability in low-dose, sparse-sampling scenarios, with complementary effects from LoRA regularization and diffusion priors.

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [86] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: The paper addresses standardization gaps in topological mapping evaluation by proposing topological consistency as a fundamental metric and introducing a quantitative measure of dataset ambiguity, supported by a curated benchmark dataset and open-source tools.


<details>
  <summary>Details</summary>
Motivation: Progress in topological mapping is hindered by lack of standardized evaluation metrics, datasets, and protocols, preventing fair comparisons between systems and under-quantifying the challenge of perceptual aliasing.

Method: Formalized topological consistency as the fundamental property and showed localization accuracy provides an efficient surrogate metric; proposed quantitative measure of dataset ambiguity; curated diverse benchmark dataset with calibrated ambiguity levels; implemented deep-learned and classical baseline systems.

Result: Experiments and analysis yielded new insights into limitations of current approaches under perceptual aliasing; all datasets, baselines, and evaluation tools were open-sourced to foster reproducible research.

Conclusion: The proposed evaluation protocol enables fair and reproducible comparisons in topological mapping research, addressing key standardization gaps and providing tools for consistent benchmarking.

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [87] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: This paper introduces a novel event-based meshflow estimation task, proposing the EEMFlow network for efficient motion field prediction and creating the HREM/HREM+ datasets to address the lack of event-based meshflow data and explore event density challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from two key gaps in event-based flow estimation: the lack of meshflow-specific event datasets and methods, and the underexplored challenge of event data density affecting model performance.

Method: The authors propose the EEMFlow network with an encoder-decoder architecture for meshflow estimation, create the HREM dataset with high-resolution event data and meshflow labels, and introduce a Confidence-induced Detail Completion module for dense optical flow. They also develop an Adaptive Density Module to handle varying event densities.

Result: EEMFlow achieves exceptional performance with 30x faster runtime compared to state-of-the-art methods. The Adaptive Density Module improves EEMFlow and EEMFlow+ performance by 8% and 10% respectively, demonstrating enhanced generalization across varying event densities.

Conclusion: The paper successfully addresses event-based meshflow estimation by providing comprehensive datasets (HREM/HREM+) and efficient models (EEMFlow/EEMFlow+) that handle both meshflow and dense optical flow estimation while being robust to event density variations.

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [88] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: A novel pipeline for category-level 6D object pose estimation that combines pretrained encoder with diffusion modeling and sampling guidance to achieve faster training convergence and higher accuracy without needing additional evaluation networks.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods for 6D pose estimation suffer from slow training convergence, end-to-end encoder learning with diffusion networks, and require additional networks to filter low-quality pose candidates.

Method: Pretrains encoder with direct pose regression head, jointly learns networks via regression and diffusion denoising heads, and introduces sampling guidance via time-dependent score scaling to balance exploration-exploitation trade-off.

Result: Achieves state-of-the-art accuracies on REAL275, HouseCat6D, and ROPE benchmarks with single-pose inference, while being more efficient in both training and inference.

Conclusion: The proposed method is simple yet effective, accelerating training convergence while maintaining multi-modal characteristics for symmetric objects and ensuring high-quality pose generation.

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [89] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: The paper addresses concept drift in multimodal LLM knowledge distillation by proposing autonomous preference optimization (APO) that enables student models to learn, compare, and critique multiple drifting teacher models.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLM distillation faces concept drift where multiple teachers' reasoning distributions evolve unpredictably, transmitting biases to student models and compromising performance.

Method: Proposes 'learn, compare, critique' paradigm with autonomous preference optimization (APO) - students learn from multiple teachers, compare their reasoning, and critically reflect on drifting inferences to perform concept alignment.

Result: Extensive experiments show superior performance in consistency, robustness and generalization. Also created CXR-MAX dataset with 170,982 distilled reasoning trajectories from MIMIC-CXR.

Conclusion: The APO framework effectively addresses concept drift in multimodal LLM distillation, producing robust, consistent, and generalizable student models through autonomous preference optimization.

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [90] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: SiteShield is a multi-modal LVLM-based RAG framework that automates construction safety inspection reports by integrating visual and audio inputs, outperforming unimodal LLMs with improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional construction safety inspection methods are inefficient due to large information volumes, and existing LVLM applications face limitations like irrelevant responses, restricted modal inputs, and hallucinations. LLMs are constrained by training data availability and lack real-time adaptability.

Method: Developed SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG) framework that integrates visual and audio inputs for automated safety inspection report generation.

Result: SiteShield outperformed unimodal LLMs without RAG with F1 score of 0.82, hamming loss of 0.04, precision of 0.76, and recall of 0.96 using real-world data.

Conclusion: SiteShield offers a novel pathway to enhance information retrieval and efficiency in generating construction safety inspection reports through multi-modal integration and RAG framework.

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [91] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE is a generative debiasing framework that mitigates neural network biases without requiring prior knowledge of biases or bias-conflicting samples, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Neural networks often learn implicit biases and spurious correlations from training data, relying on superficial patterns rather than task-relevant features. Existing debiasing methods require impractical assumptions like prior knowledge of biases or access to bias-conflicting samples.

Method: BLADE trains a generative model to translate images across bias domains while preserving task-relevant features, then adaptively refines images with synthetic counterparts based on bias susceptibility. It aligns images with bias-translated counterparts (same task features, different bias) and misaligns with same-bias samples.

Result: BLADE significantly outperforms state-of-the-art methods, exceeding the closest baseline by ~18% absolute margin on corrupted CIFAR-10 dataset under worst group setting, establishing new benchmark in bias mitigation.

Conclusion: BLADE demonstrates potential for developing more robust deep learning models without explicit supervision, effectively mitigating biases without requiring impractical assumptions about bias knowledge or bias-conflicting samples.

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [92] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: SEG-MIL-CBM integrates concept-guided segmentation with multiple instance learning to provide spatially grounded concept explanations without requiring concept annotations, improving robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks lack transparency and may exploit unreliable features, limiting trust in safety-critical applications. Existing Concept Bottleneck Models require costly concept annotations and lack spatial grounding.

Method: Integrates concept-guided image segmentation into attention-based multiple instance learning framework, treating segmented regions as instances and aggregating evidence across them to align with high-level concepts.

Result: Achieves robust performance across spurious correlations, input corruptions, and large-scale benchmarks while providing transparent, concept-level explanations without concept annotations.

Conclusion: SEG-MIL-CBM enables spatially grounded concept explanations without annotation costs, improving both interpretability and robustness in computer vision models.

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [93] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: HyCa introduces a hybrid ODE solver-inspired caching framework that applies dimension-wise caching strategies to accelerate Diffusion Transformers, achieving 5-6x speedup across various models without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers have state-of-the-art fidelity but suffer from slow iterative sampling due to expensive transformer forward passes at each timestep. Existing feature caching methods use uniform strategies that ignore heterogeneous dynamic behaviors across feature dimensions.

Method: Model hidden feature evolution as a mixture of ODEs across dimensions and introduce HyCa, a hybrid ODE solver-inspired caching framework that applies dimension-wise caching strategies instead of uniform caching.

Result: Achieves near-lossless acceleration across diverse domains: 5.55x speedup on FLUX, 5.56x on HunyuanVideo, 6.24x on Qwen-Image and Qwen-Image-Edit without retraining.

Conclusion: HyCa's dimension-wise caching approach based on ODE modeling effectively accelerates Diffusion Transformers while maintaining quality, providing significant speedups across multiple models and domains.

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [94] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: World-To-Image is a framework that enhances text-to-image generation by using web-searching agents to retrieve images for unknown concepts, enabling more accurate synthesis of novel entities through multimodal prompt optimization.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with novel or out-of-distribution entities due to knowledge cutoffs, leading to degraded performance when prompted with unfamiliar concepts.

Method: Uses an agent that dynamically searches the web to retrieve images for unknown concepts, then performs multimodal prompt optimization to steer generative models toward accurate synthesis.

Result: Achieves +8.1% improvement in accuracy-to-prompt on the NICE benchmark, outperforming state-of-the-art methods in both semantic alignment and visual aesthetics with high efficiency in less than three iterations.

Conclusion: The framework enables T2I systems to better reflect the ever-changing real world by bridging knowledge gaps through agent-driven world knowledge retrieval and optimization.

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [95] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC is a framework that constructs hierarchical semantic trees from token embeddings to simplify autoregressive image generation, improving training efficiency by up to 57% and generation quality.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive models treat visual tokens as a flat vocabulary, ignoring the intrinsic structure of the embedding space where proximity correlates with semantic similarity, leading to inefficient training and limited generation quality.

Method: MASC uses geometry-aware distance metrics and density-driven agglomerative construction to build hierarchical semantic trees from token embeddings, transforming flat prediction tasks into structured hierarchical ones.

Result: MASC accelerates training by up to 57% and improves generation quality, reducing FID of LlamaGen-XL from 2.87 to 2.58, making AR frameworks competitive with state-of-the-art methods.

Conclusion: Structuring the prediction space is as crucial as architectural innovation for scalable generative modeling, and MASC effectively addresses the fundamental inefficiency in autoregressive image generation.

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [96] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn is a two-stage forensic framework that improves AI-generated image detection accuracy and interpretability by first scanning for suspicious regions and then performing focused analysis on zoomed-in areas.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AI-generated imagery has blurred boundaries between real and synthetic content, raising digital integrity concerns. Current vision-language models often fail to detect subtle artifacts in high-quality synthetic images.

Method: Proposes ZoomIn framework that mimics human visual inspection: first scans images to locate suspicious regions, then performs focused analysis on zoomed-in areas. Uses MagniFake dataset of 20,000 real and synthetic images with bounding boxes and forensic explanations generated through automated VLM-based pipeline.

Result: Achieves 96.39% accuracy with robust generalization while providing human-understandable explanations grounded in visual evidence.

Conclusion: ZoomIn effectively improves both detection accuracy and interpretability for AI-generated image forensics through its two-stage approach and automated explanation generation.

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [97] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: A simple, end-to-end trainable image registration algorithm implemented in few lines of Python code that works with minimal training data and time.


<details>
  <summary>Details</summary>
Motivation: To create a simple and efficient image registration method that requires minimal training data, training time, and code complexity while maintaining accuracy.

Method: End-to-end trainable algorithm implemented in Python with very few lines of code, demonstrated on stereo vision with 74 images on 19x15 input windows.

Result: Achieves accurate image registration results with very little training data and training time, successfully applied to stereo vision applications.

Conclusion: The algorithm excels in brevity and serves as an excellent starting point for scenarios with limitations in training data, training time, or code complexity.

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [98] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: The paper introduces ArConv layers to redesign and optimize convolutional layers, creating a more accessible deep neural network model with only 1.3M parameters that outperforms MobileNetV2 on eye disease detection.


<details>
  <summary>Details</summary>
Motivation: To improve accessibility of deep neural networks for eye disease detection by reducing computational complexity while maintaining high accuracy, enabling deployment on mobile devices.

Method: Redesigned and optimized convolutional layers by creating novel ArConv layers, resulting in a new general model with efficient performance suitable for mobile phones.

Result: The final model achieved 0.9328 accuracy on RfMiD test set with only 1.3M parameters, outperforming MobileNetV2 (2.2M parameters, 0.9266 accuracy) under identical conditions.

Conclusion: The proposed ArConv-based model successfully increases accessibility of neural networks for medical applications while maintaining competitive accuracy with reduced computational complexity.

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [99] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido is a generative model for photorealistic neural rendering that treats 3D as a specialized video domain, using sequence-to-sequence image synthesis to perform view synthesis without explicit 3D representations.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework for object- and scene-level neural rendering that can leverage large-scale video data for pre-training, reducing reliance on scarce camera-labeled 3D datasets.

Method: Uses a masked autoregressive framework with decoder-only rectified flow transformer to generate any number of 6-DoF target views conditioned on reference views, unifying 3D and video modeling.

Result: Sets new state-of-the-art on view synthesis benchmarks, with zero-shot performance outperforming other generative methods in few-view settings and matching per-scene optimization methods in many-view settings.

Conclusion: Kaleido successfully demonstrates that 3D can be treated as a video domain, enabling unified modeling that benefits from video pre-training while achieving high-quality view synthesis without explicit 3D representations.

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [100] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: CoSSeg-TTA is a liver segmentation framework for MRI that combines nnU-Netv2 with semi-supervised learning and domain adaptation to address limited annotated data and cross-center variability.


<details>
  <summary>Details</summary>
Motivation: Liver segmentation from contrast-enhanced MRI is challenging due to limited annotated data, heterogeneous enhancement protocols, and domain shifts across scanners/institutions. Traditional domain adaptation methods like Pix2Pix and cycle-GAN have limitations including structural distortions and unstable training.

Method: Built on nnU-Netv2 with semi-supervised mean teacher scheme to exploit unlabeled data. Includes domain adaptation with randomized histogram-based style transfer and contrast-aware network. Uses continual test-time adaptation for inference robustness.

Result: Consistently outperforms nnU-Netv2 baseline with superior Dice score and Hausdorff Distance. Shows strong generalization to unseen domains under low-annotation conditions.

Conclusion: The proposed CoSSeg-TTA framework effectively addresses domain shift challenges in liver MRI segmentation and demonstrates superior performance compared to baseline methods.

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [101] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: A patch-agnostic defense using concept-based explanations to neutralize adversarial patch attacks without needing prior knowledge of patch size or location, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing defenses against adversarial patch attacks require prior knowledge of patch characteristics, limiting their practical applicability in real-world scenarios.

Method: Leverages concept-based explanations to identify and suppress the most influential concept activation vectors, neutralizing patch effects without explicit detection.

Result: Achieves higher robust and clean accuracy than PatchCleanser on Imagenette with ResNet-50, maintaining strong performance across varying patch sizes and locations.

Conclusion: Concept-driven defenses show promise for scalable security against adversarial patch attacks by combining interpretability with robustness.

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [102] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: Adapt-STformer is a flexible and efficient Seq-VPR method using Recurrent Deformable Transformer Encoder that supports variable sequence lengths, achieves faster inference and lower memory usage while improving recall performance.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based Seq-VPR methods prioritize performance but lack flexibility to handle variable sequence lengths, fast inference, and low memory usage required for real-time applications.

Method: Proposed Adapt-STformer with Recurrent Deformable Transformer Encoder (Recurrent-DTE) that uses iterative recurrent mechanism to fuse information from multiple sequential frames.

Result: Boosts recall by up to 17%, reduces sequence extraction time by 36%, and lowers memory usage by 35% compared to second-best baseline on Nordland, Oxford, and NuScenes datasets.

Conclusion: Adapt-STformer achieves both flexibility and efficiency in Seq-VPR, making it suitable for real-time applications while maintaining superior performance.

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [103] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit reframes image editing as video generation to ensure physical consistency by treating input and edited images as video frames and leveraging pretrained video models to capture temporal physics.


<details>
  <summary>Details</summary>
Motivation: Current image editing methods lack physical consistency, which is crucial for world simulation tasks where edited objects must remain coherent.

Method: Treats input and edited images as first/last video frames, uses pretrained video models for temporal consistency, and introduces temporal reasoning with reasoning tokens that are dropped after few steps to avoid full video rendering costs.

Result: Outperforms state-of-the-art baselines in both visual fidelity and physical plausibility on the new PBench-Edit benchmark.

Conclusion: ChronoEdit effectively bridges the physical consistency gap in image editing by leveraging video generation principles and temporal reasoning.

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [104] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD is the largest publicly available 3D mesh gait dataset for Parkinson's Disease, enabling supervised clinical score prediction and unsupervised motion analysis tasks to improve gait assessment.


<details>
  <summary>Details</summary>
Motivation: Objective gait assessment in Parkinson's Disease is limited by the absence of large, diverse, and clinically annotated motion datasets.

Method: Created CARE-PD dataset from 9 cohorts across 8 clinical centers, converting RGB video/motion capture into anonymized SMPL meshes. Evaluated supervised UPDRS score prediction and unsupervised motion tasks under multiple generalization protocols.

Result: Motion encoders consistently outperformed handcrafted features. Pretraining on CARE-PD reduced MPJPE from 60.8mm to 7.5mm and boosted PD severity macro-F1 by 17 percentage points.

Conclusion: CARE-PD demonstrates the value of clinically curated, diverse training data for improving Parkinson's Disease gait assessment and is released for non-commercial research.

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [105] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR is a multi-scale autoregressive framework that predicts spatial gene expression from H&E stained images by modeling gene co-expression dependencies and treating expression as discrete counts rather than continuous values.


<details>
  <summary>Details</summary>
Motivation: Spatial Transcriptomics is expensive, while H&E stained images are widely available. Current methods predict genes independently and use continuous regression for discrete count data, leading to biologically implausible results.

Method: GenAR clusters genes hierarchically to capture co-expression, models expression as discrete token generation without codebooks, and uses multi-scale decoding conditioned on fused histological and spatial embeddings.

Result: Extensive experiments on four Spatial Transcriptomics datasets across different tissues show GenAR achieves state-of-the-art performance.

Conclusion: GenAR provides a cost-effective alternative for spatial gene expression prediction with potential applications in precision medicine and molecular profiling.

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [106] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: RAP uses lightweight 3D rasterization instead of photorealistic rendering for end-to-end driving training, enabling scalable data augmentation with counterfactual recovery maneuvers and cross-agent view synthesis, achieving state-of-the-art performance on major benchmarks.


<details>
  <summary>Details</summary>
Motivation: Imitation learning for end-to-end driving lacks recovery data, leading to compounding errors. Photorealistic rendering methods are too slow and costly for training. The authors argue that semantic fidelity (geometry and dynamics) matters more than photorealism for driving.

Method: Proposes 3D Rasterization to replace costly rendering with lightweight rasterization of annotated primitives. Introduces Raster-to-Real feature-space alignment to bridge the sim-to-real gap. Together these form RAP (Rasterization Augmented Planning) pipeline.

Result: Achieves state-of-the-art closed-loop robustness and long-tail generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo Open Dataset Vision-based E2E Driving, and Bench2Drive.

Conclusion: Lightweight rasterization with feature alignment suffices to scale E2E training, offering a practical alternative to photorealistic rendering for end-to-end driving planners.

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [107] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: Diffusion^2 is a novel framework for pedestrian trajectory prediction in momentary scenarios where limited observational data is available. It uses two sequential diffusion models for backward prediction of unobserved history and forward prediction of future trajectories.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios often lack sufficient observational data (e.g., pedestrians emerging from blind spots), making accurate trajectory prediction challenging and increasing traffic accident risks. Current methods struggle with momentary trajectory prediction.

Method: Two sequentially connected diffusion models: one for backward prediction to generate unobserved historical trajectories, and another for forward prediction to forecast future trajectories. Includes dual-head parameterization for aleatoric uncertainty estimation and temporally adaptive noise module for dynamic noise scaling.

Result: Sets new state-of-the-art performance in momentary trajectory prediction on ETH/UCY and Stanford Drone datasets.

Conclusion: The proposed Diffusion^2 framework effectively addresses the challenge of momentary trajectory prediction and demonstrates superior performance compared to existing methods.

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [108] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim is a language-guided framework that generates 4D scenes with multi-view consistency and object-level controls, enabling interactive editing of dynamic environments without full re-generation.


<details>
  <summary>Details</summary>
Motivation: World models supporting controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, reproducible evaluation, and flexible task design. Current text-to-video models are constrained to 2D views and offer limited interaction.

Method: Integrates trajectory-guided generation with feature field distillation, allowing edits to be applied interactively without full re-generation. From natural language instructions, produces dynamic environments with object-level controls.

Result: MorphoSim maintains high scene fidelity while enabling controllability and editability. Objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints.

Conclusion: MorphoSim provides a framework for generating 4D scenes with multi-view consistency and object-level controls, addressing limitations of existing 2D text-to-video models and supporting robotics applications.

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [109] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: VLMs struggle with compositional counting of multiple shape types despite performing well with single shapes, revealing a fundamental limitation in current models.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Vision-Language Models (VLMs) can count objects correctly, particularly focusing on compositional counting scenarios that current models may struggle with.

Method: Created VLMCountBench benchmark using basic geometric shapes (triangles, circles) in minimalist settings with strict variable control, systematically studying effects of color, size, and prompt refinement through controlled ablation studies.

Result: VLMs can count reliably with single shape types but exhibit substantial failures when multiple shape types are combined (compositional counting), showing significant performance degradation in complex scenarios.

Conclusion: Current VLMs have a fundamental empirical limitation in compositional counting, highlighting an important research direction for improving model capabilities in handling multiple object types simultaneously.

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [110] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: CodeFormer++ is a novel blind face restoration framework that decomposes the problem into three sub-tasks: identity-preserving restoration, high-quality generation, and dynamic fusion, achieving better balance between visual quality and identity fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing blind face restoration methods using generative priors often suffer from trade-offs between visual quality and identity preservation, leading to either identity distortion or poor degradation removal.

Method: Proposes three key components: learning-based deformable face registration for semantic alignment, texture guided restoration network for dynamic texture transfer, and deep metric learning with informative samples for better feature fusion.

Result: Extensive experiments on real-world and synthetic datasets show superior performance in both visual fidelity and identity consistency compared to existing methods.

Conclusion: CodeFormer++ effectively maximizes generative priors for high-quality face restoration while preserving identity through its decomposed approach and novel components.

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [111] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: A.I.R. is a training-free frame selection method for VideoQA that uses a powerful VLM for deep semantic analysis of queries and processes frames iteratively in small batches, achieving better performance and efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current frame selection methods for VideoQA face a trade-off: lightweight models like CLIP fail to capture query nuances, while VLM-based methods are computationally prohibitive for processing entire videos.

Method: Proposed A.I.R. approach uses a powerful VLM for deep semantic analysis of complex queries, deployed in an iterative loop that processes only small batches of high-potential frames at a time without requiring training.

Result: Extensive experiments on various VideoQA benchmarks show A.I.R. outperforms existing frame selection methods, significantly boosts foundation VLM performance, and achieves substantial computational efficiency gains over other VLM-based techniques.

Conclusion: A.I.R. effectively addresses the computational-accuracy trade-off in VideoQA frame selection by combining deep semantic analysis with efficient iterative processing, delivering superior performance while maintaining computational feasibility.

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [112] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: reAR is a simple training strategy that improves visual autoregressive generation by addressing generator-tokenizer inconsistency through token-wise regularization, achieving performance comparable to larger diffusion models.


<details>
  <summary>Details</summary>
Motivation: Visual autoregressive generation underperforms compared to diffusion models, with prior work attributing this to tokenizer limitations and rasterization ordering. The core issue identified is generator-tokenizer inconsistency where AR-generated tokens may not be well-decoded by the tokenizer.

Method: Proposes reAR training strategy with token-wise regularization: when predicting next token, the causal transformer also recovers visual embedding of current token and predicts embedding of target token under noisy context. No changes needed to tokenizer, generation order, inference pipeline, or external models.

Result: Substantially improves performance: reduces gFID from 3.02 to 1.86 and improves IS to 316.9 on ImageNet with standard rasterization-based tokenizer. With advanced tokenizers, achieves gFID of 1.42 with only 177M parameters, matching performance of larger state-of-the-art diffusion models (675M).

Conclusion: reAR effectively addresses generator-tokenizer inconsistency in visual autoregressive generation through simple token-wise regularization, achieving significant performance improvements and competitive results with diffusion models while maintaining simplicity and requiring no architectural changes.

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [113] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: SPEGNet is a unified architecture for camouflaged object detection that integrates multi-scale features through channel calibration and spatial enhancement, achieving state-of-the-art performance with real-time inference speed.


<details>
  <summary>Details</summary>
Motivation: Current camouflaged object detection methods accumulate complex components like boundary modules and attention mechanisms, creating computational burden without proportional gains and forcing reduced resolution processing that eliminates fine details essential for camouflage detection.

Method: SPEGNet uses a unified design with multi-scale feature integration via channel calibration and spatial enhancement. Boundaries emerge from context-rich representations with semantic-spatial alignment, and progressive refinement implements scale-adaptive edge modulation with peak influence at intermediate resolutions.

Result: SPEGNet achieves 0.887 Sα on CAMO, 0.890 on COD10K, and 0.895 on NC4K datasets with real-time inference speed. It excels across scales from tiny intricate objects to large pattern-similar ones while handling occlusion and ambiguous boundaries.

Conclusion: The unified SPEGNet architecture effectively balances boundary precision and regional consistency, addressing fragmentation in camouflaged object detection while maintaining computational efficiency and fine detail preservation.

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [114] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM is an automated pipeline that converts detection datasets into medical VQA data with Chain-of-Thought reasoning, using an Integrated CoT-Curriculum Strategy to achieve state-of-the-art performance on medical VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging, particularly in developing clinically aligned medical vision-language models.

Method: Automated pipeline converts detection datasets into medical VQA data with Chain-of-Thought reasoning by linking lesion boxes to organ segmentation and structured rationales. Uses Integrated CoT-Curriculum Strategy with Easy (explicit lesion boxes), Medium (implicit localization), and Hard (weakly supervised reasoning) stages.

Result: MedCLM attains state-of-the-art performance on several medical VQA benchmarks.

Conclusion: Provides a scalable framework for developing clinically aligned medical vision-language models.

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [115] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: Proposes VaseVQA-3D dataset and VaseVLM model to address VLMs' limitations in cultural heritage domain, achieving significant improvements in 3D vase artifact analysis.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language Models struggle with specialized cultural heritage domains like 3D vase artifacts due to data scarcity and insufficient domain knowledge, limiting their effectiveness in culturally significant tasks.

Method: Created VaseVQA-3D dataset with 664 ancient Greek vase 3D models and question-answer pairs, then developed VaseVLM model using domain-adaptive training for enhanced vase artifact analysis.

Result: Achieved 12.8% improvement on R@1 metrics and 6.6% improvement on lexical similarity compared to previous state-of-the-art methods on the VaseVQA-3D dataset.

Conclusion: The approach significantly improves recognition and understanding of 3D vase artifacts, providing new technical pathways for digital heritage preservation research.

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [116] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit is a specialized image editing model for e-commerce that addresses consistency limitations of general models through hierarchical architecture and two-stage training.


<details>
  <summary>Details</summary>
Motivation: General image editing models perform poorly in e-commerce scenarios due to consistency limitations in maintaining product appearance and layout integrity.

Method: Uses hierarchical framework (base model + pattern shifting + consistency modules) with two-stage training strategy and comprehensive data engineering pipeline for high-quality e-commerce editing data.

Result: Outperforms existing general-domain editing models in both objective metrics (VIE Score) and subjective user preference on e-commerce benchmark.

Conclusion: TBStar-Edit successfully addresses e-commerce image editing challenges through domain-specific design and achieves superior consistency preservation compared to general models.

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [117] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: Proposes asynchronous diffusion models that allocate different timesteps to different pixels, allowing prompt-related regions to denoise more gradually and leverage clearer context for better text-to-image alignment.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models struggle with faithful text-to-image alignment due to synchronous denoising where all pixels evolve simultaneously, preventing prompt-related regions from referencing clear context from other regions.

Method: Asynchronous diffusion framework that dynamically modulates timestep schedules for individual pixels, allowing prompt-related regions to denoise more gradually than unrelated regions to leverage clearer inter-pixel context.

Result: Extensive experiments show significant improvement in text-to-image alignment across diverse prompts compared to standard synchronous diffusion models.

Conclusion: Asynchronous diffusion models effectively address the alignment limitations of synchronous denoising by enabling prompt-related regions to reference clearer context during generation, leading to better text-to-image fidelity.

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [118] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: TAG is an efficient plug-and-play guidance method that amplifies tangential components of diffusion model scores to correct sampling trajectories and reduce semantic inconsistencies without modifying the base model.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models suffer from semantic inconsistencies and hallucinations, and existing guidance methods often require external signals or architectural modifications with computational overhead.

Method: TAG uses an intermediate sample as projection basis and amplifies tangential components of estimated scores via first-order Taylor expansion to steer sampling toward higher-probability regions.

Result: TAG improves diffusion sampling fidelity with minimal computational addition, operating as an architecture-agnostic module that enhances sample quality.

Conclusion: TAG offers an efficient, direct guidance approach for diffusion models that reduces inconsistencies without model modifications, providing a new perspective on diffusion guidance.

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [119] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: CRL proposes conditional representation learning to extract task-specific features using LLM-generated descriptive texts as semantic basis, avoiding costly supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Universal representations often misalign with customized downstream tasks, while supervised fine-tuning is computationally expensive and requires heavy annotation.

Method: Use LLM to generate descriptive texts for user criteria to construct semantic basis, then project image representations into this conditional space using VLM.

Result: CRL achieves superior performance on classification and retrieval tasks compared to universal representation methods.

Conclusion: CRL provides an effective framework for learning customized representations without expensive fine-tuning, demonstrating strong generalization across tasks.

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [120] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: The paper introduces AI Session Recorder to capture pathologists' viewing behavior from WSI viewers, creates Pathology-CoT dataset for supervision, and builds Pathologist-o3 agent that outperforms state-of-the-art models in lymph-node metastasis detection.


<details>
  <summary>Details</summary>
Motivation: Current pathology foundation models lack practical agentic systems that can decide what to examine next, adjust magnification, and provide explainable diagnoses. The main blocker is the absence of scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based.

Method: Developed AI Session Recorder to unobtrusively record pathologists' navigation in standard WSI viewers, converting logs into behavioral commands. Created Pathology-CoT dataset through human-in-the-loop review of AI-drafted rationales. Built Pathologist-o3, a two-stage agent that proposes regions of interest and performs behavior-guided reasoning.

Result: On gastrointestinal lymph-node metastasis detection, achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding OpenAI o3 model and generalizing across backbones. Reduced labeling time by roughly six times.

Conclusion: The framework makes agentic pathology practical by turning everyday viewer logs into scalable, expert-validated supervision, establishing a path to human-aligned, upgradeable clinical AI.

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [121] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: S²Fin is a spatial-spectral-frequency interaction network that integrates pairwise fusion modules across spatial, spectral, and frequency domains for multimodal remote sensing image classification, achieving superior performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing feature fusion techniques struggle to extract structural and detail features from heterogeneous and redundant multimodal images, motivating the introduction of frequency domain learning to model key and sparse detail features.

Method: Proposes high-frequency sparse enhancement transformer with sparse spatial-spectral attention, two-level spatial-frequency fusion strategy (adaptive frequency channel module and high-frequency resonance mask), and spatial-spectral attention fusion module for enhanced feature extraction.

Result: Experiments on four benchmark multimodal datasets demonstrate superior classification performance, outperforming state-of-the-art methods.

Conclusion: S²Fin effectively integrates frequency domain learning with spatial and spectral features, providing a robust solution for multimodal remote sensing image classification with limited labeled data.

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [122] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: A novel ensemble framework combining transformers and texture-based methods achieves state-of-the-art deepfake detection performance on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Address the pressing issue of manipulated media detection, particularly deepfakes, where existing approaches fail to generalize across diverse datasets and generation techniques.

Method: Hybrid ensemble framework combining transformer-based architectures (Swin Transformers, ViTs) and texture-based methods, with innovative techniques including data-splitting, sequential training, frequency splitting, patch-based attention, and face segmentation.

Result: Achieves state-of-the-art performance on DFWild-Cup dataset (diverse subset of eight deepfake datasets), with transformers excelling in global feature extraction and texture-based methods providing interpretability.

Conclusion: Hybrid models can effectively address evolving challenges of deepfake detection, offering robust solutions for real-world applications through complementary approaches.

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [123] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: This study compares four superpixel segmentation methods against SLIC for deforestation detection in the ForestEyes project, finding that classifier fusion (ensemble methods) significantly improves performance more than the choice of segmentation method alone.


<details>
  <summary>Details</summary>
Motivation: To determine if newer superpixel-based segmentation methods outperform traditional SLIC for deforestation detection in the ForestEyes citizen science project, where image segments are labeled by volunteers and used for ML model training.

Method: Compared four best segmentation methods with SLIC, trained classifiers using PyCaret AutoML library, and applied classifier fusion (ensemble) approaches to evaluate performance.

Result: Initial results showed little variation in performance among segmentation methods, but applying classifier fusion significantly improved balanced accuracy, demonstrating the importance of combining ML models.

Conclusion: Both segmentation method choice and classifier combination are crucial for deforestation detection, with ensemble methods providing substantial performance improvements over individual segmentation method selection.

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [124] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduPersona is a large-scale benchmark for evaluating subjective abilities of virtual student agents in classroom settings, spanning multiple languages, subjects, and persona types.


<details>
  <summary>Details</summary>
Motivation: As large language models are increasingly used in education, there's a need to assess their classroom-oriented subjective abilities to understand model boundaries and enable trustworthy deployment.

Method: Created EduPersona benchmark with 1,308 authentic classroom dialogues (12,814 Q&A turns), expanded through persona stylization to 128k turns. Decomposed subjective performance into three tasks: basic coherence, student realism, and long-term persona consistency. Evaluated three LLMs and their persona-fine-tuned variants.

Result: Persona-fine-tuned models showed significant improvements: TASK1 +33.6%, TASK2 +30.6%, TASK3 +14.9%. The benchmark effectively reveals heterogeneous difficulty in persona modeling.

Conclusion: EduPersona provides the first classroom benchmark for subjective abilities, establishes a verifiable research paradigm, and will be open-sourced to advance trustworthy AI for education.

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [125] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: A hierarchical Multi-Stage Mixture of Movement Experts (MoME) architecture is proposed for multi-task prediction of psychological traits from gait sequences, outperforming state-of-the-art methods on the PsyMo benchmark.


<details>
  <summary>Details</summary>
Motivation: Gait contains rich biometric and behavioral information, but using walking patterns to infer psychological traits remains challenging and underexplored.

Method: MoME processes walking cycles in four stages of movement complexity using lightweight expert models and task-specific gating modules to adaptively weight experts across traits and stages.

Result: Achieved 37.47% weighted F1 score at run level and 44.6% at subject level on PsyMo benchmark covering 17 psychological traits, outperforming state-of-the-art gait analysis models.

Conclusion: Multi-task gait-based learning is viable for psychological trait estimation, and integrating auxiliary tasks improves performance, providing foundation for movement-informed psychological inference.

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [126] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: ConceptSplit is a framework that addresses concept mixing in multi-concept personalization for text-to-image diffusion models through Token-wise Value Adaptation (ToVA) training and Latent Optimization for Disentangled Attention (LODA) inference.


<details>
  <summary>Details</summary>
Motivation: Multi-concept personalization in T2I diffusion models suffers from concept mixing, where multiple learned concepts interfere or blend undesirably in output images.

Method: ConceptSplit uses two components: ToVA (Token-wise Value Adaptation) - a training method that adapts only value projections in cross-attention, and LODA (Latent Optimization for Disentangled Attention) - an inference method that optimizes input latents to reduce attention entanglement.

Result: The framework achieves robust multi-concept personalization and mitigates unintended concept interference, as demonstrated through extensive qualitative and quantitative experiments.

Conclusion: ConceptSplit effectively addresses concept mixing in multi-concept personalization by focusing on value projection adaptation during training and latent optimization during inference.

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [127] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: A label-efficient liver segmentation method for multi-phase MRI that combines foundation model fine-tuning with co-training and cross pseudo supervision to handle limited labeled data, unlabeled sequences, and vendor/modal variations without spatial registration.


<details>
  <summary>Details</summary>
Motivation: Liver segmentation in multi-phase MRI is crucial for fibrosis assessment but faces challenges with scarce labeled data, uneven distribution across modalities/vendors, spatial misalignment, and missing phases in real-world clinical settings.

Method: Integrates foundation-scale 3D segmentation backbone with fine-tuning, co-training using cross pseudo supervision to leverage unlabeled volumes, and standardized preprocessing pipeline without requiring spatial registration.

Result: The model learns to generalize across MRI phases and vendors, demonstrating robust segmentation performance in both labeled and unlabeled domains.

Conclusion: The approach shows effectiveness for label-efficient liver segmentation in multi-phase, multi-vendor MRI and highlights the potential of combining foundation model adaptation with co-training for real-world clinical imaging tasks.

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [128] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: A diffusion-based framework for identity-consistent facial expression generation with fine-grained control using FLAME blendshape parameters and reference-based editing.


<details>
  <summary>Details</summary>
Motivation: Human-centric generative models need both identity consistency and precise control over human performance, but current methods struggle with fine-grained expression control while maintaining facial identity.

Method: Built on an ID-consistent face foundation model with compositional design featuring expression cross-attention module guided by FLAME blendshape parameters, trained on diverse image/video data with expressive variation, plus pluggable Reference Adapter for real image editing.

Result: Outperforms existing methods in tailored and identity-consistent expression generation, generalizing beyond basic emotions to subtle micro-expressions and expressive transitions.

Conclusion: The framework successfully achieves faithful reimagining of subjects under any facial expression while maintaining identity consistency, with code and models publicly available.

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [129] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: ReactDiff is a temporal diffusion framework that generates diverse and realistic facial reactions in dialogue by incorporating spatio-temporal facial kinematics and action unit dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model the stochasticity and dynamics of real human facial reactions, leading to unnatural expressions and artifacts in generated reactions.

Method: Proposes ReactDiff framework that incorporates two priors into diffusion process: temporal facial behavioral kinematics and facial action unit dependencies to guide realistic human reaction manifolds.

Result: Extensive experiments on REACT2024 dataset show state-of-the-art reaction quality, diversity, and appropriateness compared to existing methods.

Conclusion: ReactDiff successfully generates diverse and human-like facial reactions by modeling facial dynamics and anatomical constraints, overcoming limitations of previous approaches.

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [130] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: This paper proposes a novel approach for 3D Semantic Scene Graph prediction that focuses on improving object feature quality through a discriminative encoder and contrastive pretraining, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Previous methods for 3D Semantic Scene Graph prediction have limitations in optimizing object and relationship features, showing excessive reliance on Graph Neural Networks despite insufficient discriminative capability.

Method: Design a highly discriminative object feature encoder with contrastive pretraining that decouples object representation learning from scene graph prediction, and effectively combines geometric and semantic features for relationship prediction.

Result: The approach significantly outperforms previous state-of-the-art methods on the 3DSSG dataset, with substantial performance improvements across all evaluation metrics when plugging the pretrained encoder into existing frameworks.

Conclusion: Object feature quality is critical for 3D scene graph accuracy, and the proposed contrastive pretraining strategy with discriminative object encoding effectively enhances both object classification and relationship prediction performance.

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [131] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: First benchmark for monocular metric depth estimation in wildlife monitoring evaluates 4 state-of-the-art methods, finding Depth Anything V2 performs best with 0.454m MAE and 0.962 correlation, while median-based depth extraction outperforms mean-based approaches.


<details>
  <summary>Details</summary>
Motivation: Camera traps are widely used for wildlife monitoring but lack accurate distance measurements due to missing depth information in monocular images. Current monocular depth estimation methods haven't been systematically evaluated in natural wildlife environments.

Method: Evaluated four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro, ZoeDepth, Metric3D) and a geometric baseline on 93 camera trap images with ground truth distances from calibrated ChARUCO patterns. Compared median vs mean depth extraction approaches.

Result: Depth Anything V2 achieved best performance (MAE: 0.454m, correlation: 0.962). ZoeDepth performed worst outdoors (MAE: 3.087m). Median-based depth extraction consistently outperformed mean-based across all methods. ZoeDepth was fastest (0.17s/image) but least accurate, while Depth Anything V2 offered best accuracy-speed balance (0.22s/image).

Conclusion: Established first performance benchmarks for wildlife depth estimation applications. Depth Anything V2 provides optimal balance of accuracy and speed. Median-based depth extraction is recommended over mean-based approaches for practical implementation in conservation monitoring systems.

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [132] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: ExposureEngine is an automated system for accurate sponsor visibility analytics in sports broadcasts using rotation-aware logo detection with Oriented Bounding Boxes, achieving high precision and recall.


<details>
  <summary>Details</summary>
Motivation: Traditional manual methods for quantifying sponsor visibility in sports broadcasts are subjective and unscalable, while existing automated systems using Horizontal Bounding Boxes fail to handle rotated or skewed logos from dynamic camera angles.

Method: Developed an end-to-end system that predicts Oriented Bounding Boxes for precise logo detection regardless of orientation, trained on a new dataset of 1,103 frames from Swedish elite soccer with 670 unique sponsor logos annotated with OBBs.

Result: Achieved mean Average Precision (mAP@0.5) of 0.859, with precision of 0.96 and recall of 0.87, demonstrating robust performance in localizing logos under diverse broadcast conditions. Integrated detections into analytical pipeline for visibility metrics.

Conclusion: The system provides a comprehensive solution for auditable and interpretable sponsor measurement in sports media, including a language-driven agentic layer for natural language querying and report generation.

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [133] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: AA-YOLO integrates statistical anomaly detection into YOLO's detection head to improve infrared small target detection by treating targets as anomalies against background, effectively reducing false alarms while maintaining competitive performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Conventional object detectors struggle with infrared small target detection due to complex backgrounds and tiny target sizes, leading to high false alarm rates that limit practical deployment in defense applications.

Method: Proposed Anomaly-Aware YOLO (AA-YOLO) that integrates statistical anomaly detection test into YOLO's detection head, treating small targets as unexpected patterns against background to control false alarm rate.

Result: Achieves competitive performance on IRSTD benchmarks, demonstrates remarkable robustness with limited training data, noise, and domain shifts. Successfully applied across various YOLO backbones including lightweight models and instance segmentation YOLO.

Conclusion: AA-YOLO provides a generic, versatile solution for real-world IRSTD deployments with constrained resources, offering effective false alarm control while maintaining detection performance across different scenarios and model architectures.

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [134] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: Transformer-based two-stream framework for person identification using spatial and temporal keypoint features achieves 98.03% accuracy, showing spatial configurations are more discriminative than temporal dynamics.


<details>
  <summary>Details</summary>
Motivation: To investigate transformer architectures for person identification in natural face-to-face conversations using keypoint data.

Method: Two-stream framework with spatial transformer for 133 COCO WholeBody keypoints and multi-scale temporal transformer for motion patterns, evaluated on CANDOR corpus with pre-trained vs from-scratch training and velocity features.

Result: Spatial transformer: 95.74% accuracy, multi-scale temporal transformer: 93.90% accuracy, feature-level fusion: 98.03% accuracy. Domain-specific training outperforms transfer learning.

Conclusion: Transformers are effective for person identification in natural interactions, with spatial and temporal information being complementary for optimal performance.

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [135] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: PG-Occ is a Progressive Gaussian Transformer Framework for open-vocabulary 3D occupancy prediction that addresses the trade-off between sparse Gaussian representation (missing small objects) and dense representation (high computational cost) through progressive online densification and anisotropy-aware sampling.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D occupancy prediction methods are limited to fixed semantic categories, while recent text-aligned approaches face a fundamental trade-off: sparse Gaussian representations struggle with small objects, while dense representations are computationally expensive.

Method: The framework uses progressive online densification to gradually enhance 3D Gaussian representation for fine-grained details, and an anisotropy-aware sampling strategy with spatio-temporal fusion that adaptively assigns receptive fields to Gaussians at different scales and stages.

Result: PG-Occ achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best performing method.

Conclusion: The proposed progressive Gaussian transformer framework effectively balances computational efficiency and scene detail capture, enabling superior open-vocabulary 3D occupancy prediction for autonomous driving systems.

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [136] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: The paper proposes a novel open-vocabulary learning method that generates unseen-class data using a class-domain-wise pipeline guided by hierarchical semantics, enabling better distribution estimation and achieving up to 14% improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods estimate open-environment distributions using only seen-class data, making estimation error inherently unidentifiable due to the absence of unseen classes. Learning beyond seen classes is crucial for bounding this error.

Method: A class-domain-wise data generation pipeline guided by hierarchical semantic tree and domain information from seen-class data, plus a distribution alignment algorithm that estimates and maximizes posterior probability using generated data.

Result: Extensive experiments on 11 datasets show the method outperforms baseline approaches by up to 14%, demonstrating significant effectiveness and superiority.

Conclusion: Theoretical analysis shows distribution can be effectively estimated by generating unseen-class data, and the proposed method successfully implements this approach with substantial performance gains in open-vocabulary learning.

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [137] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: The FedSurg challenge benchmarked federated learning methods for surgical video classification using appendicitis inflammation data, evaluating generalization to unseen centers and local adaptation through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To assess how well current federated learning methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data.

Method: Participants developed classification strategies using the Appendix300 video dataset, with approaches including foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost.

Result: Generalization performance across centers was limited, while all teams improved after fine-tuning in the adaptation task. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and hyperparameter tuning difficulties.

Conclusion: The challenge establishes the first benchmark for FL in surgical video classification, highlighting trade-offs between local personalization and global robustness, and emphasizing the importance of architecture choice, preprocessing, and loss design for future clinical surgical AI development.

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [138] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: Automated two-robot system for high-fidelity 3D scanning of cultural heritage artefacts using coordinated robotic manipulation and optimized trajectory planning.


<details>
  <summary>Details</summary>
Motivation: Conventional 3D scanning methods require specialized expertise and manual intervention, making cultural heritage preservation challenging and inefficient.

Method: Two-robot system with coordinated motion planning: one robot handles scanning equipment while another manages artefact positioning. Uses parameterized scanning space, optimized trajectory planning, and waypoint distribution for comprehensive coverage.

Result: Achieves significantly lower Chamfer Distance and higher F-score compared to baseline methods, offering superior geometric accuracy and improved digitization efficiency.

Conclusion: The automated system provides high-fidelity 3D scanning with reduced reliance on expert operators, making cultural heritage preservation more efficient and accessible.

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [139] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: Vision-transformers (ViTs) outperform CNNs in large-data geometric estimation tasks but CNNs match ViTs in small-data scenarios due to inductive bias and smaller capacity. ViTs show better cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: To compare the efficiency of ViTs and large-scale CNNs as backbone architectures for geometric estimation tasks (2D rigid transformations and fundamental matrix prediction) in low-data regimes, given their different feature focus (high-level semantics vs. local-global balance).

Method: Systematic comparison of large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet) with ViT-based foundation models (CLIP-ViT variants and DINO) in various data size settings including few-shot scenarios, evaluating performance on geometric estimation tasks.

Result: ViTs outperform CNNs during refinement in large downstream-data scenarios, but in small data scenarios, CNNs match ViT performance due to inductive bias and smaller capacity. ViTs exhibit stronger generalization in cross-domain evaluation.

Conclusion: Careful selection of model architectures for refinement is crucial, motivating future research towards hybrid architectures that balance local and global representations for geometric estimation tasks.

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [140] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DiT-VTON is a Virtual Try-On framework using Diffusion Transformers that achieves superior detail preservation and robustness across diverse product categories, enabling Virtual Try-All capabilities with advanced image editing features.


<details>
  <summary>Details</summary>
Motivation: Existing Virtual Try-On models struggle with fine-grained detail preservation, robustness to real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product categories.

Method: Leverages Diffusion Transformer (DiT) adapted for image-conditioned VTO, exploring multiple configurations including in-context token concatenation, channel concatenation, and ControlNet integration. Trained on expanded dataset with varied backgrounds and non-garment categories.

Result: Surpasses state-of-the-art methods on VITON-HD with superior detail preservation and robustness, outperforms models with VTA and image editing capabilities across thousands of product categories.

Conclusion: DiT-VTON redefines VTO beyond garment try-on to a versatile Virtual Try-All solution capable of handling diverse product categories and supporting advanced image editing functionalities.

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [141] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: EgoSurg reconstructs dynamic egocentric views of surgical team members from fixed-camera video using neural rendering and diffusion enhancement, enabling immersive surgical analysis without disrupting clinical workflow.


<details>
  <summary>Details</summary>
Motivation: Traditional surgical observation methods using fixed cameras or recollections cannot capture the actual egocentric visual perspectives that guide clinical decisions, limiting insights into surgical safety, training, and workflow optimization.

Method: EgoSurg combines geometry-driven neural rendering with diffusion-based view enhancement to synthesize arbitrary egocentric viewpoints from wall-mounted fixed-camera video, requiring no intervention in clinical workflow.

Result: Evaluation across multi-site surgical cases shows EgoSurg successfully reconstructs person-specific visual fields and arbitrary viewpoints with high visual quality and fidelity.

Conclusion: EgoSurg transforms existing OR camera infrastructure into navigable dynamic 3D records, establishing a foundation for immersive surgical data science that enables surgical practice to be visualized and analyzed from every angle.

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [142] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: The paper investigates why Multimodal Language Models (MLMs) struggle with perception-heavy tasks by analyzing visual key-value tokens in popular MLMs. It reveals that image value tokens contain sufficient visual information for tasks like segmentation and correspondence, but language models fail to fully utilize this information due to artifacts in key tokens and poor visual information control.


<details>
  <summary>Details</summary>
Motivation: Despite existing interpretability work on VIT encoders and transformer activations, there's limited understanding of why MLMs perform poorly on perception-heavy tasks. The authors aim to fill this gap by examining how MLMs process visual key-value tokens.

Method: The study analyzes popular MLMs (LLaVA-OneVision, Qwen2.5-VL, Llama-3-LLaVA-NeXT) by examining visual information flow through language models, studying key-value token processing, and testing perception capabilities through zero-shot tasks including segmentation, semantic correspondence, temporal correspondence, and referring expression detection.

Result: Image value tokens encode sufficient visual information for perception tasks, but language models contain less visual information than the original visual encoder (SigLIP). Key tokens in later layers contain artifacts that reduce perception capability. Adding text prefixes improves perception, and 33.3% of Art Style questions in BLINK benchmark show unused visual information in language models.

Conclusion: The findings provide insights into key-value token roles in multimodal systems, suggesting that better visual information control in language models could significantly improve perception capabilities. This paves the way for deeper mechanistic interpretability and new training directions for MLM components.

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [143] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON is the first 4D virtual try-on framework that generates realistic try-on results from a single garment image, supporting free pose control, novel-view rendering, and dynamic garment interactions without multi-view captures or physics priors.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on methods lack support for dynamic garment interactions and require multi-view garment captures or physics priors, limiting their practical applications in AR/VR and gaming.

Method: The framework uses two key modules: (1) Reciprocal Flow Rectifier for optical-flow correction to stabilize avatar fitting and ensure temporal coherence, and (2) Non-Linear Deformer that decomposes Gaussian maps into view-pose-invariant and view-pose-specific components for adaptive garment deformations.

Result: AvatarVTON achieves high fidelity, diversity, and dynamic garment realism, establishing a new benchmark for 4D virtual try-on with superior performance in qualitative and quantitative comparisons.

Conclusion: AvatarVTON enables realistic 4D virtual try-on from single garment images, making it well-suited for AR/VR, gaming, and digital-human applications with its dynamic garment interaction capabilities.

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [144] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: A 3D Flow Matching framework generates synthetic CT from MRI or CBCT for radiotherapy, achieving accurate global anatomy reconstruction but limited fine detail preservation due to resolution constraints.


<details>
  <summary>Details</summary>
Motivation: To enable MRI-only and CBCT-based adaptive radiotherapy by generating synthetic CT images, improving treatment precision while reducing patient radiation exposure.

Method: Uses a fully 3D Flow Matching framework where Gaussian noise is transformed into synthetic CT by integrating a learned velocity field conditioned on features from input MRI/CBCT via a lightweight 3D encoder. Separate models trained for MRI→sCT and CBCT→sCT across abdomen, head/neck, and thorax regions.

Result: Method accurately reconstructs global anatomical structures but preservation of fine details was limited due to low training resolution constraints from memory and runtime limitations.

Conclusion: Future work will explore patch-based training and latent-space flow models to improve resolution and local structural fidelity for better fine detail preservation.

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [145] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: AT-BPTT is a novel framework for dataset distillation that dynamically adapts truncation positions and window sizes based on neural network learning dynamics, achieving state-of-the-art performance with significant speed and memory improvements.


<details>
  <summary>Details</summary>
Motivation: Existing inner-loop optimization methods for dataset distillation rely on random truncation strategies that are inflexible and suboptimal, failing to account for distinct learning dynamics across different training stages.

Method: Proposes Automatic Truncated Backpropagation Through Time (AT-BPTT) with three components: probabilistic stage-aware timestep selection, adaptive window sizing based on gradient variation, and low-rank Hessian approximation for computational efficiency.

Result: Achieves 6.16% average accuracy improvement over baselines on multiple datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet-1K), accelerates inner-loop optimization by 3.9x, and saves 63% memory cost.

Conclusion: AT-BPTT effectively addresses the limitations of random truncation in dataset distillation by dynamically adapting to neural network learning dynamics, delivering superior performance with significant computational benefits.

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [146] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: A novel method for automated PV power plant mapping using aerial images, achieving detailed modeling down to individual PV modules through visual segmentation and structural inference.


<details>
  <summary>Details</summary>
Motivation: Accurate and up-to-date PV power plant models are essential for optimal operation and maintenance, but such models are not easily available, requiring automation and independence from third-party data.

Method: Uses aerial overview images with visual segmentation of PV modules, infers structural information to assign modules to benches/rows/columns, identifies visual keypoints for layout, and merges detections from multiple images while maintaining structural integrity.

Result: Experimentally verified on two different power plants, resulting in a compact georeferenced model with 3D positions and semantic structures suitable for maintenance.

Conclusion: The approach successfully automates PV power plant mapping, providing detailed structural modeling without relying on third-party data, making it suitable for power plant maintenance applications.

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [147] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: A kinesics recognition framework that infers psychological states from 3D skeleton data using ST-GCN and CNN, enabling privacy-preserving and scalable modeling of human-environment interactions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional methods (theoretical models, questionnaires) that are static, labor-intensive, and lack privacy preservation in capturing human psychological states for modeling human-built environment interactions.

Method: Combines spatial-temporal graph convolutional network (ST-GCN) with convolutional neural network (CNN) using transfer learning to infer communicative functions (kinesics) directly from 3D skeleton joint data, eliminating need for manual mappings between actions and psychological categories.

Result: Demonstrated on Dyadic User Engagement (DUET) dataset, the method enables scalable, accurate, and human-centered modeling of behavior while preserving user anonymity and uncovering latent structures in bodily movements reflecting cognitive and emotional states.

Conclusion: Provides a new pathway for enhancing reinforcement learning-driven simulations of human-environment interaction through privacy-preserving, generalizable kinesics recognition from 3D skeleton data.

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [148] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: The paper compares five skeleton-based algorithms for recognizing dyadic human interactions using depth sensors as a privacy-preserving alternative to RGB cameras, focusing on 12 types of social interactions.


<details>
  <summary>Details</summary>
Motivation: Cyber-physical systems traditionally focus on economic goals but neglect social benefits. Cyber-physical-social infrastructure systems aim to address this gap by aligning CPS with social objectives, requiring better understanding of human interactions while preserving privacy.

Method: The study compares five skeleton-based interaction recognition algorithms on a dataset of 12 dyadic interactions captured using depth sensors. The interactions are categorized into communication types like emblems and affect displays, analyzing skeletal movements instead of RGB data.

Result: The research demonstrates that depth sensors with skeleton-based algorithms can effectively recognize dyadic human interactions, providing insights into cultural and emotional aspects of human behavior while addressing privacy concerns.

Conclusion: Skeleton-based interaction recognition using depth sensors offers a viable privacy-preserving approach for understanding human social interactions, laying foundation for cyber-physical-social infrastructure systems that can foster positive social outcomes.

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [149] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: A method combining early exits and knowledge distillation to reduce computational costs in CNNs while maintaining accuracy, using an entropy-based loss for misclassified teacher images.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks have high computational costs that make them impractical for real-time and edge applications, requiring efficient compression techniques.

Method: Integrates early exits and knowledge distillation, training a reduced student early-exit model from a complex teacher model with a new entropy-based loss for misclassified teacher images.

Result: Achieves significant reductions in computational complexity without compromising classification performance on CIFAR10, CIFAR100 and SVHN datasets.

Conclusion: The approach effectively optimizes the accuracy-efficiency trade-off and opens new research perspectives for Knowledge Distillation in other contexts.

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [150] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: μDeepIQA is a deep learning-based image quality assessment method for optical microscopy that provides fast, stable quality predictions and patch-wise quality visualization, overcoming limitations of traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional image quality assessment methods for optical microscopy are computationally expensive for large datasets and unstable for images outside ideal domains, requiring more reliable and efficient solutions.

Method: Retrained a deep convolutional neural network originally designed for natural images to predict individual quality metrics and global quality scores for optical microscopy data, enabling patch-wise quality prediction.

Result: The method provides fast and stable quality predictions that generalize well even outside standard method ranges, with the ability to visualize spatially varying quality within single images.

Conclusion: Deep learning models like μDeepIQA benefit optical microscopy studies through stable outlier performance, small patch assessment capability, and rapid predictions.

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [151] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: An IoT-enabled robotic system for real-time mapping of grape yield and quality in vineyards using deep learning and hyperspectral imaging with domain-adversarial framework to handle illumination variations.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of domain shift in hyperspectral imaging caused by variable illumination conditions in vineyards, enabling non-destructive, real-time assessment of grape yield and quality for precision viticulture.

Method: End-to-end system with two modules: 1) High-performance model for grape bunch detection and weight estimation, 2) Light-Invariant Spectral Autoencoder (LISA) - a domain-adversarial framework that learns illumination-invariant features from uncalibrated hyperspectral data across three illumination domains (lab, morning, afternoon sunlight).

Result: System achieves 0.82 recall for bunch detection and R²=0.76 for weight prediction. LISA module improves quality prediction generalization by over 20% compared to baselines, enabling high-resolution georeferenced mapping of both yield and quality.

Conclusion: The complete pipeline successfully generates actionable, data-driven insights for precision viticulture by combining robust detection and quality assessment modules that handle real-world illumination variations.

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [152] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: This paper introduces a comprehensive multi-modal dataset for benthic habitat mapping, including side-scan sonar tiles, bathymetric maps, and optical images, with manual annotations to support machine learning development in marine ecosystem research.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large, annotated datasets limits the development and benchmarking of machine learning models for benthic habitat mapping, which is crucial for understanding marine ecosystems, conservation, and sustainable resource management.

Method: The authors collected about a million side-scan sonar tiles along the coast of Catalonia, complemented by bathymetric maps and co-registered optical images from AUV surveys. They manually annotated approximately 36,000 SSS tiles with segmentation masks and spatially associated optical images with corresponding SSS tiles to enable cross-modal representation learning.

Result: The paper presents a comprehensive multi-modal dataset with raw sensor data, mosaics, and annotations, along with open-source preprocessing and annotation tools to enhance accessibility and encourage research in underwater habitat mapping.

Conclusion: This resource aims to establish a standardized benchmark for underwater habitat mapping, promoting advancements in autonomous seafloor classification and multi-sensor integration for marine ecosystem research.

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [153] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: Comparison of four object detection models (YOLOv5, Faster R-CNN, SSD, RetinaNet) for motorbike detection in Kigali's challenging traffic environment, evaluating their suitability for real-time autonomous navigation in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Motorcycle taxis in Kigali navigate unpredictably and disregard traffic rules, posing significant challenges for autonomous driving systems that need to detect them reliably.

Method: Used four object detection models (YOLOv5, Faster R-CNN, SSD, RetinaNet) implemented in PyTorch with transfer learning on a custom dataset of 198 images collected in Kigali, evaluating accuracy, localization, and inference speed.

Result: Identified implementation challenges including dataset limitations and model complexities, with recommendations for simplified architectures to enhance accessibility for autonomous systems in developing countries.

Conclusion: Simplified architectures are recommended for future work to improve accessibility of autonomous systems in developing countries like Rwanda, addressing the specific challenges of detecting unpredictable motorcycle taxis in real-time navigation scenarios.

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [154] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: SAHC method integrates hierarchy-specific classification heads with trainable hierarchy matrices and hierarchical consensus mechanism for remote sensing image classification, effectively leveraging hierarchical class relationships.


<details>
  <summary>Details</summary>
Motivation: Most deep learning approaches for remote sensing image classification overlook predefined label hierarchies and focus only on fine-grained classification, missing the semantic relationships among classes.

Method: Proposes Semantics-Aware Hierarchical Consensus (SAHC) with hierarchy-specific classification heads, trainable hierarchy matrices for self-supervised hierarchical structure learning, and hierarchical consensus mechanism for consistent probability distributions across levels.

Result: Experimental evaluation on three benchmark datasets with different hierarchical complexities shows effectiveness in guiding network learning and robustness for remote sensing image classification tasks.

Conclusion: SAHC method successfully leverages hierarchical structure in classification tasks and demonstrates adaptability across different backbone architectures and hierarchical complexities.

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [155] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: REN is an anatomically-informed Mixture-of-Experts framework for medical image classification that uses anatomical priors to train specialized experts for different lung regions, achieving superior performance in interstitial lung disease classification.


<details>
  <summary>Details</summary>
Motivation: Traditional MoE systems lack domain-specific constraints essential for medical imaging, where anatomical structure and regional disease heterogeneity strongly influence pathological patterns.

Method: REN leverages anatomical priors to train seven specialized experts for distinct lung lobes and bilateral lung combinations, using multi-modal gating mechanisms that integrate radiomics biomarkers and deep learning features (CNN, ViT, Mamba) to optimally weight expert contributions.

Result: REN achieved average AUC of 0.8646 ± 0.0467, a +12.5% improvement over SwinUNETR baseline (AUC 0.7685, p = 0.031). Region-specific experts showed lower-lobe models achieving AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79) and aligning with known disease progression patterns.

Conclusion: REN demonstrates strong generalizability and clinical interpretability, presenting a scalable, anatomically-guided approach readily extensible to other structured medical imaging applications.

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [156] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: NFPF is a novel Unsupervised Active Learning method that uses Specific Feature Learning Machine to measure sample importance and Reconstruction Difference for selection, achieving state-of-the-art performance comparable to supervised AL methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Active Learning requires expensive iterative human annotation, while existing Unsupervised Active Learning methods struggle with performance due to local gradient-based scoring and shallow selection approaches.

Method: Proposes Natural Feature Progressive Framework (NFPF) with Specific Feature Learning Machine (SFLM) to quantify sample importance and uses Reconstruction Difference metric for sample selection.

Result: NFPF significantly outperforms all established UAL methods and achieves performance comparable to supervised AL methods on vision datasets, with enhanced robustness and better data distribution coverage.

Conclusion: NFPF revolutionizes sample importance measurement in UAL and provides a compelling alternative that reduces annotation burden while maintaining high performance.

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [157] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: CA3D-Diff is a novel bidirectional mammogram view translation framework that uses conditional diffusion models with column-aware cross-attention and implicit 3D structure reconstruction to address cross-view structural misalignment in dual-view mammography.


<details>
  <summary>Details</summary>
Motivation: In real-world clinical workflows, one mammography view (CC or MLO) may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting diagnostic effectiveness. View-to-view translation can help recover missing views and improve lesion alignment, but this is challenging in mammography due to large non-rigid deformations and severe tissue overlap.

Method: Proposes CA3D-Diff framework with two key components: 1) Column-aware cross-attention mechanism that leverages geometric property that anatomically corresponding regions tend to lie in similar column positions across views, using Gaussian-decayed bias to emphasize local correlations; 2) Implicit 3D structure reconstruction module that back-projects noisy 2D latents into coarse 3D feature volume based on breast-view projection geometry, then refines and injects it into denoising UNet.

Result: Extensive experiments show CA3D-Diff achieves superior performance in bidirectional view translation tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. The synthesized views effectively improve single-view malignancy classification in screening settings.

Conclusion: CA3D-Diff demonstrates practical value in real-world diagnostics by enabling effective view-to-view translation in mammography, addressing the challenges of structural misalignment through geometric-aware attention and 3D anatomical guidance.

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [158] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: SSDD introduces a single-step diffusion decoder that replaces KL-VAE tokenizers, achieving better reconstruction quality and faster sampling without adversarial training.


<details>
  <summary>Details</summary>
Motivation: Current diffusion decoders require adversarial losses and have slow iterative sampling, while KL-VAE tokenizers have limitations in performance and efficiency.

Method: Developed a new pixel diffusion decoder architecture with transformer components and GAN-free training, then used distillation to create an efficient single-step decoder (SSDD).

Result: SSDD improves reconstruction FID from 0.87 to 0.50 with 1.4x higher throughput, and preserves DiT generation quality with 3.8x faster sampling.

Conclusion: SSDD can serve as a drop-in replacement for KL-VAE, enabling higher-quality and faster generative models without adversarial training.

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [159] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: Proposes a watermarking method for visual foundation models to verify ownership by embedding detectable watermarks in internal representations that persist through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Protect intellectual property of visual foundation models from illegal redistribution by dishonest users who may profit from unauthorized distribution.

Method: Fine-tune expressive layers of VFM with encoder-decoder network to embed digital watermarks into internal representations of hold-out input images.

Result: Watermarks remain detectable in functional copies obtained through fine-tuning; achieves low false detection and misdetection probabilities.

Conclusion: The proposed method provides reliable ownership verification for visual foundation models with persistent watermark detection capabilities.

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [160] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: Proposes latent uncertainty representation (LUR) and repulsively trained LUR (RLUR) methods for uncertainty estimation in deep neural networks, achieving comparable performance to existing probabilistic deep learning methods while being more efficient.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are used in safety-critical tasks like driver action recognition, but existing last-layer probabilistic deep learning methods for out-of-distribution detection have variable performance and efficiency issues.

Method: Extends pre-trained DNNs with transformation layers to produce multiple latent representations for uncertainty estimation, comparing LUR and RLUR against eight probabilistic deep learning methods across four driver action datasets.

Result: LUR and RLUR achieve comparable in-distribution classification performance to other methods. For OOD detection, LUR matches top-performing methods while being more efficient to train and easier to tune than approaches requiring MCMC sampling or repulsive training.

Conclusion: The proposed LUR approach provides effective uncertainty estimation for safety-critical applications with better efficiency and easier tuning compared to existing probabilistic deep learning methods.

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [161] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: Machine learning approach using hand-drawn spiral and wave images achieves 93.3% accuracy for Parkinson's disease detection through CNN, transfer learning, and ensemble voting.


<details>
  <summary>Details</summary>
Motivation: Early PD diagnosis is crucial but traditional methods are cumbersome and costly. Need for non-invasive, cost-effective diagnostic solutions.

Method: Three-phase architecture: pre-trained CNNs, custom convolutional layers, and ensemble voting with hard voting. Uses data augmentation, transfer learning, and attention mechanisms.

Result: Spiral images: 90% precision/recall/F1-score; Wave images: 96.67% precision/recall/F1-score; Combined ensemble voting: 93.3% overall accuracy.

Conclusion: Machine learning with hand-drawn images shows strong potential for early PD diagnosis as a non-invasive, cost-effective solution.

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [162] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: This survey provides the first comprehensive examination of post-training methodologies for Video-Large Multimodal Models (Video-LMMs), covering supervised fine-tuning, reinforcement learning, and test-time scaling techniques to enhance video understanding capabilities.


<details>
  <summary>Details</summary>
Motivation: Video understanding is challenging due to complex spatiotemporal relationships and long-term dependencies. While Video-LMMs show promise, their transformation from basic perception to sophisticated reasoning through post-training remains fragmented in literature.

Method: The survey examines three fundamental post-training pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. It presents a structured taxonomy addressing video-specific challenges.

Result: The survey synthesizes key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. It curates essential benchmarks, datasets, and metrics for rigorous assessment.

Conclusion: This survey provides researchers and practitioners with a unified framework for advancing Video-LMM capabilities through systematic post-training methodologies, with ongoing resources maintained at the provided GitHub repository.

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [163] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: The paper introduces a segment matching method using 3D foundation models to handle wide-baseline matching with extreme viewpoint changes up to 180 degrees, outperforming state-of-the-art methods by up to 30% on AUPRC metric.


<details>
  <summary>Details</summary>
Motivation: Segment matching provides robustness to occlusions, lighting variations, and viewpoint changes compared to keypoint matching, but wide-baseline scenarios with extreme viewpoint shifts remain challenging.

Method: Proposed architecture leverages the spatial understanding and inductive bias of 3D foundation models to match segments across image pairs with large viewpoint changes.

Result: Outperforms state-of-the-art methods (SAM2 video propagator and local feature matching) by up to 30% on AUPRC metric on ScanNet++ and Replica datasets, and shows benefits on downstream tasks like 3D instance segmentation and image-goal navigation.

Conclusion: The approach successfully addresses wide-baseline segment matching using 3D foundation models, demonstrating significant performance improvements and practical utility in downstream applications.

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [164] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: A no-reference image quality assessment method for contrast-distorted images that transforms the problem into full-reference assessment using pseudo-reference images generated through contrast enhancement algorithms.


<details>
  <summary>Details</summary>
Motivation: Contrast distortion has been largely overlooked in image quality assessment despite being an important factor affecting image quality, especially under unfavorable lighting conditions.

Method: Generate pseudo-reference images using contrast enhancement algorithms, train a classification network to select the most suitable algorithm based on image content and distortion, then perform full-reference assessment between contrast-enhanced and degraded images.

Result: Performance evaluation on three databases (CCID2014, TID2013, and CSIQ) indicates promising performance of the proposed method.

Conclusion: The proposed approach effectively transforms no-reference image quality assessment into a more accurate full-reference assessment problem for contrast-distorted images.

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [165] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: The paper introduces a Neuroplastic Modular Classifier that combines ResNet-50 and Vision Transformer with FAISS-based similarity retrieval for adaptive image classification in waste management and industrial defect detection.


<details>
  <summary>Details</summary>
Motivation: Efficient classification of waste and industrial surface defects is essential for sustainable waste management and quality control in dynamic environments.

Method: Hybrid architecture combining ResNet-50 for localized features, Vision Transformer for global context, FAISS-based similarity retrieval, and neuroplastic modular blocks that dynamically expand during training when performance plateaus.

Result: Outperforms traditional static models in accuracy and adaptability, validated on garbage classification and KolektorSDD2 industrial defect dataset.

Conclusion: The Neuroplastic Modular Classifier provides a scalable, high-performance solution for real-world image classification with strong applicability in environmental and industrial domains.

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [166] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: This paper addresses the challenge of generating and editing structured visuals like charts and diagrams, presenting a comprehensive solution including a large dataset, unified model training, and a new benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Modern visual generation models struggle with structured visuals that require composition planning, text rendering, and factual fidelity, creating a gap in generating charts, diagrams, and mathematical figures.

Method: Constructed a 1.3M structured image dataset from executable programs, trained a unified model combining VLM with FLUX.1 Kontext via lightweight connector, using three-stage curriculum training and inference-time reasoning.

Result: The model achieves strong editing performance, with inference-time reasoning providing consistent gains across architectures. Evaluation of 15 models shows even leading closed-source systems remain unsatisfactory.

Conclusion: The released dataset, model, and benchmark advance unified multimodal foundations for structured visuals, addressing a critical gap in visual generation capabilities.

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [167] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: A framework for generating videos where characters from different worlds interact naturally while preserving their identities and styles, using Cross-Character Embedding and Cross-Character Augmentation techniques.


<details>
  <summary>Details</summary>
Motivation: To enable natural interactions between characters from different contexts (e.g., Mr. Bean with Tom and Jerry) while preserving their original identities and avoiding style delusion issues.

Method: Uses Cross-Character Embedding (CCE) to learn identity and behavioral logic across multimodal sources, and Cross-Character Augmentation (CCA) to enrich training with synthetic co-existence and mixed-style data.

Result: Experiments on cartoons and live-action series with 10 characters show improvements in identity preservation, interaction quality, and robustness to style delusion.

Conclusion: The framework enables new forms of generative storytelling by allowing natural interactions between previously uncoexistent characters without losing stylistic fidelity.

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [168] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain is a novel inference-time chain-of-visual-thought framework that enhances video generation by injecting visual reasoning from multimodal models to guide sparse keyframe generation and tuning.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with complex dynamics and coherent state transitions, while multimodal models have strong visual reasoning capabilities. The goal is to bridge these strengths to improve video generation quality.

Method: VChain uses large multimodal models to generate critical keyframes as snapshots, then performs sparse inference-time tuning of a pre-trained video generator only at these key moments, avoiding dense supervision.

Result: Extensive experiments on complex, multi-step scenarios show that VChain significantly enhances the quality of generated videos compared to existing approaches.

Conclusion: VChain provides an effective, tuning-efficient framework that improves video generation by leveraging multimodal models' visual reasoning capabilities through sparse keyframe guidance.

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


### [169] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker is a multi-agent framework for automated academic presentation video generation that addresses the labor-intensive process of creating research presentation videos by coordinating slide generation, subtitling, speech synthesis, and talking-head rendering.


<details>
  <summary>Details</summary>
Motivation: Academic presentation videos are essential for research communication but highly labor-intensive to produce, requiring hours of work for short videos. Current methods don't address the distinctive challenges of research presentation videos including dense multi-modal information and coordination of multiple aligned channels.

Method: A multi-agent framework that integrates slide generation with layout refinement using novel tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering. It parallelizes slide-wise generation for efficiency and is built on a benchmark of 101 research papers with author-created presentation materials.

Result: Experiments show that the presentation videos produced are more faithful and informative than existing baselines, establishing a practical step toward automated academic video generation.

Conclusion: PaperTalker provides an effective solution for automated academic presentation video generation, addressing the challenges of multi-modal coordination and information density in research communication videos.

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>
