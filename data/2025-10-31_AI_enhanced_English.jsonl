{"id": "2510.25797", "categories": ["cs.CV", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25797", "abs": "https://arxiv.org/abs/2510.25797", "authors": ["Sai Likhith Karri", "Ansh Saxena"], "title": "Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks", "comment": null, "summary": "This study examines the effectiveness of spatio-temporal modeling and the\nintegration of spatial attention mechanisms in deep learning models for\nunderwater object detection. Specifically, in the first phase, the performance\nof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with\nthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is\ndeveloped, through the addition of a Convolutional Block Attention Module\n(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and\nT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the\nresearch highlights how temporal modeling improves detection accuracy in\ndynamic marine environments, particularly under conditions of sudden movements,\npartial occlusions, and gradual motion. The testing results showed that YOLOv5\nachieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM\noutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,\nhighlighting their superior accuracy and generalization in detecting complex\nobjects. The findings demonstrate that T-YOLOv5 significantly enhances\ndetection reliability compared to the standard model, while T-YOLOv5 with CBAM\nfurther improves performance in challenging scenarios, although there is a loss\nof accuracy when it comes to simpler scenarios.", "AI": {"tldr": "This study evaluates spatio-temporal modeling and spatial attention mechanisms in deep learning for underwater object detection, comparing YOLOv5, T-YOLOv5, and T-YOLOv5 with CBAM, showing significant improvements in detection accuracy for complex marine environments.", "motivation": "To improve underwater object detection accuracy in dynamic marine environments with challenges like sudden movements, partial occlusions, and gradual motion where standard models may underperform.", "method": "Two-phase approach: first evaluating temporal-enhanced T-YOLOv5 vs standard YOLOv5, then developing an augmented T-YOLOv5 with Convolutional Block Attention Module (CBAM) to enhance spatial attention mechanisms.", "result": "T-YOLOv5 and T-YOLOv5 with CBAM significantly outperformed standard YOLOv5 with mAP@50-95 scores of 0.813 and 0.811 respectively, compared to 0.563 for YOLOv5. T-YOLOv5 with CBAM showed better performance in challenging scenarios but slightly reduced accuracy in simpler scenarios.", "conclusion": "Temporal modeling significantly enhances detection reliability in underwater environments, and spatial attention mechanisms further improve performance in complex scenarios, though with trade-offs in simpler detection tasks."}}
{"id": "2510.25897", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25897", "abs": "https://arxiv.org/abs/2510.25897", "authors": ["Nicolas Dufour", "Lucas Degeorge", "Arijit Ghosh", "Vicky Kalogeiton", "David Picard"], "title": "MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency", "comment": "Project page: https://nicolas-dufour.github.io/miro", "summary": "Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).", "AI": {"tldr": "MIRO conditions text-to-image models on multiple reward models during training to directly learn user preferences, improving visual quality, training speed, and achieving state-of-the-art performance on benchmarks.", "motivation": "Current text-to-image models trained on uncurated datasets don't align well with user preferences. Post-hoc reward-based selection methods discard data and harm diversity, semantic fidelity, and efficiency.", "method": "Instead of post-processing, MIRO conditions the model on multiple reward models during training to directly learn user preferences.", "result": "MIRO dramatically improves visual quality of generated images and significantly speeds up training. It achieves state-of-the-art performance on GenEval compositional benchmark and user-preference scores (PickAScore, ImageReward, HPSv2).", "conclusion": "Directly conditioning models on multiple rewards during training is more effective than post-hoc selection for aligning text-to-image generation with user preferences."}}
{"id": "2510.25901", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25901", "abs": "https://arxiv.org/abs/2510.25901", "authors": ["Denniz Goren", "Holger Caesar"], "title": "BikeScenes: Online LiDAR Semantic Segmentation for Bicycles", "comment": null, "summary": "The vulnerability of cyclists, exacerbated by the rising popularity of faster\ne-bikes, motivates adapting automotive perception technologies for bicycle\nsafety. We use our multi-sensor 'SenseBike' research platform to develop and\nevaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the\nautomotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg\nDataset, comprising 3021 consecutive LiDAR scans around the university campus\nof the TU Delft, semantically annotated for 29 dynamic and static classes. By\nevaluating model performance, we demonstrate that fine-tuning on our BikeScenes\ndataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly\noutperforming the 13.8% obtained with SemanticKITTI pre-training alone. This\nresult underscores the necessity and effectiveness of domain-specific training.\nWe highlight key challenges specific to bicycle-mounted, hardware-constrained\nperception systems and contribute the BikeScenes dataset as a resource for\nadvancing research in cyclist-centric LiDAR segmentation.", "AI": {"tldr": "This paper adapts automotive LiDAR perception for bicycle safety by creating a domain-specific dataset (BikeScenes-lidarseg) and showing that fine-tuning on this dataset significantly improves 3D LiDAR segmentation performance for bicycle-mounted systems.", "motivation": "Cyclist vulnerability is increasing due to the popularity of faster e-bikes, creating a need to adapt automotive perception technologies specifically for bicycle safety applications.", "method": "Developed a multi-sensor 'SenseBike' research platform and created the BikeScenes-lidarseg dataset with 3021 consecutive LiDAR scans annotated for 29 classes. Used this dataset to fine-tune 3D LiDAR segmentation models.", "result": "Fine-tuning on the BikeScenes dataset achieved 63.6% mIoU, significantly outperforming the 13.8% mIoU obtained with SemanticKITTI pre-training alone, demonstrating the effectiveness of domain-specific training.", "conclusion": "Domain-specific training is essential for bicycle-mounted perception systems, and the BikeScenes dataset provides a valuable resource for advancing cyclist-centric LiDAR segmentation research."}}
{"id": "2510.25921", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.25921", "abs": "https://arxiv.org/abs/2510.25921", "authors": ["Nikola L. Kolev", "Tommaso Rodani", "Neil J. Curson", "Taylor J. Z. Stock", "Alberto Cazzaniga"], "title": "Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy", "comment": null, "summary": "Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and\natom manipulation, but its utility is often limited by tip degradation and slow\nserial data acquisition. Fabrication adds another layer of complexity since the\ntip is often subjected to large voltages, which may alter the shape of its\napex, requiring it to be conditioned. Here, we propose a machine learning (ML)\napproach for image repair and super-resolution to alleviate both challenges.\nUsing a dataset of only 36 pristine experimental images of Si(001):H, we\ndemonstrate that a physics-informed synthetic data generation pipeline can be\nused to train several state-of-the-art flow-matching and diffusion models.\nQuantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy\n(CMMD) score and structural similarity demonstrates that our models are able to\neffectively restore images and offer a two- to fourfold reduction in image\nacquisition time by accurately reconstructing images from sparsely sampled\ndata. Our framework has the potential to significantly increase STM\nexperimental throughput by offering a route to reducing the frequency of\ntip-conditioning procedures and to enhancing frame rates in existing high-speed\nSTM systems.", "AI": {"tldr": "Machine learning approach for STM image repair and super-resolution using physics-informed synthetic data, enabling faster image acquisition and reduced tip conditioning.", "motivation": "STM limitations include tip degradation, slow serial data acquisition, and complexity in tip fabrication due to voltage-induced shape changes requiring conditioning.", "method": "Used physics-informed synthetic data generation pipeline with only 36 pristine experimental images to train flow-matching and diffusion models for image restoration and super-resolution.", "result": "Models effectively restored images with 2-4x reduction in acquisition time by reconstructing from sparse data, validated by CLIP Maximum Mean Discrepancy and structural similarity metrics.", "conclusion": "The framework can significantly increase STM experimental throughput by reducing tip-conditioning frequency and enhancing frame rates in high-speed STM systems."}}
{"id": "2510.25970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25970", "abs": "https://arxiv.org/abs/2510.25970", "authors": ["Sung-Hoon Yoon", "Minghan Li", "Gaspard Beaudouin", "Congcong Wen", "Muhammad Rafay Azhar", "Mengyu Wang"], "title": "SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing", "comment": "Camera-ready version for NeurIPS 2025, 10 pages (main paper)", "summary": "Rectified flow models have become a de facto standard in image generation due\nto their stable sampling trajectories and high-fidelity outputs. Despite their\nstrong generative capabilities, they face critical limitations in image editing\ntasks: inaccurate inversion processes for mapping real images back into the\nlatent space, and gradient entanglement issues during editing often result in\noutputs that do not faithfully reflect the target prompt. Recent efforts have\nattempted to directly map source and target distributions via ODE-based\napproaches without inversion; however,these methods still yield suboptimal\nediting quality. In this work, we propose a flow decomposition-and-aggregation\nframework built upon an inversion-free formulation to address these\nlimitations. Specifically, we semantically decompose the target prompt into\nmultiple sub-prompts, compute an independent flow for each, and aggregate them\nto form a unified editing trajectory. While we empirically observe that\ndecomposing the original flow enhances diversity in the target space,\ngenerating semantically aligned outputs still requires consistent guidance\ntoward the full target prompt. To this end, we design a projection and\nsoft-aggregation mechanism for flow, inspired by gradient conflict resolution\nin multi-task learning. This approach adaptively weights the sub-target\nvelocity fields, suppressing semantic redundancy while emphasizing distinct\ndirections, thereby preserving both diversity and consistency in the final\nedited output. Experimental results demonstrate that our method outperforms\nexisting zero-shot editing approaches in terms of semantic fidelity and\nattribute disentanglement. The code is available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.", "AI": {"tldr": "Proposes a flow decomposition-and-aggregation framework for image editing that semantically decomposes target prompts into sub-prompts, computes independent flows for each, and aggregates them with adaptive weighting to address inversion and gradient entanglement issues in rectified flow models.", "motivation": "Rectified flow models have limitations in image editing tasks including inaccurate inversion processes and gradient entanglement issues that result in outputs not faithfully reflecting target prompts. Existing ODE-based approaches without inversion still yield suboptimal editing quality.", "method": "Semantically decomposes target prompt into multiple sub-prompts, computes independent flow for each, and aggregates them using projection and soft-aggregation mechanism inspired by gradient conflict resolution in multi-task learning. This adaptively weights sub-target velocity fields to suppress semantic redundancy while emphasizing distinct directions.", "result": "Outperforms existing zero-shot editing approaches in terms of semantic fidelity and attribute disentanglement. The method enhances diversity in target space while maintaining consistent guidance toward full target prompt.", "conclusion": "The proposed flow decomposition-and-aggregation framework effectively addresses limitations of rectified flow models in image editing by preserving both diversity and consistency in final edited outputs through adaptive weighting of semantic sub-flows."}}
{"id": "2510.25976", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.25976", "abs": "https://arxiv.org/abs/2510.25976", "authors": ["Roman Beliy", "Amit Zalcher", "Jonathan Kogman", "Navve Wasserman", "Michal Irani"], "title": "Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer", "comment": null, "summary": "Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.", "AI": {"tldr": "Brain-IT is a brain-inspired method that uses a Brain Interaction Transformer to reconstruct images from fMRI data by enabling interactions between functionally-similar brain-voxel clusters and predicting complementary semantic and structural image features.", "motivation": "Current methods for reconstructing images from fMRI brain recordings often lack faithfulness to the actual seen images, despite recent progress with diffusion models.", "method": "Uses a Brain Interaction Transformer (BIT) that allows interactions between clusters of functionally-similar brain-voxels. Predicts two complementary localized patch-level image features: high-level semantic features and low-level structural features to guide diffusion model reconstruction.", "result": "Achieves faithful image reconstructions from fMRI that surpass current state-of-the-art approaches both visually and by standard objective metrics. With only 1-hour of fMRI data from a new subject, achieves results comparable to methods trained on full 40-hour recordings.", "conclusion": "Brain-IT's brain-inspired approach with functional clusters and direct information flow from brain-voxel clusters to image features enables efficient and faithful image reconstruction from limited fMRI data."}}
{"id": "2510.25990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25990", "abs": "https://arxiv.org/abs/2510.25990", "authors": ["Valentin Boussot", "C\u00e9dric H\u00e9mon", "Jean-Claude Nunes", "Jean-Louis Dillenseger"], "title": "Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI", "comment": "Paper for the Trackrad2025 challenge, Team BreizhTrack", "summary": "In this work, we address the TrackRAD2025 challenge of real-time tumor\ntracking in cine-MRI sequences of the thoracic and abdominal regions under\nstrong data scarcity constraints. Two complementary strategies were explored:\n(i) unsupervised registration with the IMPACT similarity metric and (ii)\nfoundation model-based segmentation leveraging SAM 2.1 and its recent variants\nthrough prompt-based interaction. Due to the one-second runtime constraint, the\nSAM-based method was ultimately selected. The final configuration used SAM2.1\nb+ with mask-based prompts from the first annotated slice, fine-tuned solely on\nthe small labeled subset from TrackRAD2025. Training was configured to minimize\noverfitting, using 1024x1024 patches (batch size 1), standard augmentations,\nand a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was\napplied to all modules (prompt encoder, decoder, Hiera backbone) to preserve\ngeneralization while adapting to annotator-specific styles. Training lasted 300\nepochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently\napplied across all anatomical sites and MRI field strengths. Test-time\naugmentation was considered but ultimately discarded due to negligible\nperformance gains. The final model was selected based on the highest Dice\nSimilarity Coefficient achieved on the validation set after fine-tuning. On the\nhidden test set, the model reached a Dice score of 0.8794, ranking 6th overall\nin the TrackRAD2025 challenge. These results highlight the strong potential of\nfoundation models for accurate and real-time tumor tracking in MRI-guided\nradiotherapy.", "AI": {"tldr": "The paper presents a real-time tumor tracking method for cine-MRI using SAM 2.1 foundation model with mask-based prompts, achieving 0.8794 Dice score in TrackRAD2025 challenge while operating under 1-second runtime constraint.", "motivation": "To address the TrackRAD2025 challenge of real-time tumor tracking in thoracic and abdominal cine-MRI sequences under strong data scarcity constraints, requiring methods that can work with limited labeled data.", "method": "Used SAM 2.1 foundation model with mask-based prompts from first annotated slice, fine-tuned only on small labeled subset. Training used 1024x1024 patches, standard augmentations, balanced Dice+IoU loss, and low uniform learning rate (0.0001) across all modules to prevent overfitting.", "result": "Achieved Dice score of 0.8794 on hidden test set, ranking 6th overall in TrackRAD2025 challenge. The method successfully operated within the 1-second runtime constraint and generalized across different anatomical sites and MRI field strengths.", "conclusion": "Foundation models like SAM 2.1 show strong potential for accurate real-time tumor tracking in MRI-guided radiotherapy, particularly effective under data scarcity conditions while maintaining real-time performance requirements."}}
{"id": "2510.26001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26001", "abs": "https://arxiv.org/abs/2510.26001", "authors": ["Xinhua Wang", "Caibo Feng", "Xiangjun Fu", "Chunxiao Liu"], "title": "Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement", "comment": null, "summary": "We propose an innovative enhancement to the Mamba framework by increasing the\nHausdorff dimension of its scanning pattern through a novel Hilbert Selective\nScan mechanism. This mechanism explores the feature space more effectively,\ncapturing intricate fine-scale details and improving overall coverage. As a\nresult, it mitigates information inconsistencies while refining spatial\nlocality to better capture subtle local interactions without sacrificing the\nmodel's ability to handle long-range dependencies. Extensive experiments on\npublicly available benchmarks demonstrate that our approach significantly\nimproves both the quantitative metrics and qualitative visual fidelity of\nexisting Mamba-based low-light image enhancement methods, all while reducing\ncomputational resource consumption and shortening inference time. We believe\nthat this refined strategy not only advances the state-of-the-art in low-light\nimage enhancement but also holds promise for broader applications in fields\nthat leverage Mamba-based techniques.", "AI": {"tldr": "Enhanced Mamba framework with Hilbert Selective Scan mechanism increases Hausdorff dimension for better feature exploration, improving low-light image enhancement with higher quality and efficiency.", "motivation": "To address information inconsistencies and improve spatial locality in Mamba-based methods while maintaining long-range dependency handling capabilities.", "method": "Proposed Hilbert Selective Scan mechanism that increases the Hausdorff dimension of scanning patterns for more effective feature space exploration.", "result": "Significantly improved quantitative metrics and qualitative visual fidelity on benchmarks, with reduced computational resources and shorter inference time.", "conclusion": "The refined strategy advances low-light image enhancement state-of-the-art and shows promise for broader Mamba-based applications."}}
{"id": "2510.26006", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26006", "abs": "https://arxiv.org/abs/2510.26006", "authors": ["Rishika Bhagwatkar", "Syrielle Montariol", "Angelika Romanou", "Beatriz Borges", "Irina Rish", "Antoine Bosselut"], "title": "CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments", "comment": null, "summary": "Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.", "AI": {"tldr": "CAVE is the first benchmark for real-world visual anomalies with three tasks: description, explanation, and justification, using fine-grained annotations inspired by human cognitive processes.", "motivation": "Current anomaly detection in computer vision is limited to industrial defects or synthetic anomalies, failing to capture the richness and unpredictability of real-world anomalies that humans can naturally identify and explain.", "method": "Introduces CAVE benchmark with fine-grained annotations for visual grounding and categorizing anomalies based on visual manifestations, complexity, severity, and commonness, drawing from cognitive science research on human anomaly perception.", "result": "State-of-the-art Vision-Language Models (VLMs) struggle with visual anomaly perception and commonsense reasoning, even with advanced prompting strategies.", "conclusion": "CAVE serves as a valuable resource for advancing research in anomaly detection and commonsense reasoning in VLMs by providing a realistic and cognitively grounded benchmark."}}
{"id": "2510.26017", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26017", "abs": "https://arxiv.org/abs/2510.26017", "authors": ["Bilal Hassan", "Areg Karapetyan", "Aaron Chung Hin Chow", "Samer Madanat"], "title": "Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning", "comment": "Submitted to Hydrology and Earth System Sciences", "summary": "Climate change and sea-level rise (SLR) pose escalating threats to coastal\ncities, intensifying the need for efficient and accurate methods to predict\npotential flood hazards. Traditional physics-based hydrodynamic simulators,\nalthough precise, are computationally expensive and impractical for city-scale\ncoastal planning applications. Deep Learning (DL) techniques offer promising\nalternatives, however, they are often constrained by challenges such as data\nscarcity and high-dimensional output requirements. Leveraging a recently\nproposed vision-based, low-resource DL framework, we develop a novel,\nlightweight Convolutional Neural Network (CNN)-based model designed to predict\ncoastal flooding under variable SLR projections and shoreline adaptation\nscenarios. Furthermore, we demonstrate the ability of the model to generalize\nacross diverse geographical contexts by utilizing datasets from two distinct\nregions: Abu Dhabi and San Francisco. Our findings demonstrate that the\nproposed model significantly outperforms state-of-the-art methods, reducing the\nmean absolute error (MAE) in predicted flood depth maps on average by nearly\n20%. These results highlight the potential of our approach to serve as a\nscalable and practical tool for coastal flood management, empowering\ndecision-makers to develop effective mitigation strategies in response to the\ngrowing impacts of climate change. Project Page: https://caspiannet.github.io/", "AI": {"tldr": "A lightweight CNN model predicts coastal flooding under sea-level rise scenarios, outperforming state-of-the-art methods by reducing MAE by 20% and generalizing across Abu Dhabi and San Francisco.", "motivation": "Climate change and sea-level rise threaten coastal cities, but traditional physics-based simulators are computationally expensive for city-scale planning. Deep learning offers alternatives but faces data scarcity and high-dimensional output challenges.", "method": "Developed a novel lightweight CNN-based model using a vision-based, low-resource DL framework to predict coastal flooding under variable SLR projections and shoreline adaptation scenarios.", "result": "The model significantly outperforms state-of-the-art methods, reducing mean absolute error in predicted flood depth maps by nearly 20% on average, and demonstrates generalization across diverse geographical contexts.", "conclusion": "The approach serves as a scalable and practical tool for coastal flood management, empowering decision-makers to develop effective mitigation strategies against climate change impacts."}}
{"id": "2510.26027", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26027", "abs": "https://arxiv.org/abs/2510.26027", "authors": ["Ali Rasekh", "Erfan Bagheri Soula", "Omid Daliran", "Simon Gottschalk", "Mohsen Fayyaz"], "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders", "comment": "Accepted to NeurIPS 2025", "summary": "Despite significant advances in Multimodal Large Language Models (MLLMs),\nunderstanding complex temporal dynamics in videos remains a major challenge.\nOur experiments show that current Video Large Language Model (Video-LLM)\narchitectures have critical limitations in temporal understanding, struggling\nwith tasks that require detailed comprehension of action sequences and temporal\nprogression. In this work, we propose a Video-LLM architecture that introduces\nstacked temporal attention modules directly within the vision encoder. This\ndesign incorporates a temporal attention in vision encoder, enabling the model\nto better capture the progression of actions and the relationships between\nframes before passing visual tokens to the LLM. Our results show that this\napproach significantly improves temporal reasoning and outperforms existing\nmodels in video question answering tasks, specifically in action recognition.\nWe improve on benchmarks including VITATECS, MVBench, and Video-MME by up to\n+5.5%. By enhancing the vision encoder with temporal structure, we address a\ncritical gap in video understanding for Video-LLMs. Project page and code are\navailable at: https://alirasekh.github.io/STAVEQ2/.", "AI": {"tldr": "Proposes STAVEQ2, a Video-LLM architecture with stacked temporal attention modules in the vision encoder to improve temporal understanding in videos, achieving up to +5.5% improvement on benchmarks.", "motivation": "Current Video-LLMs struggle with complex temporal dynamics and understanding action sequences in videos, limiting their performance in video question answering tasks.", "method": "Introduces stacked temporal attention modules directly within the vision encoder to capture temporal progression and relationships between frames before passing visual tokens to the LLM.", "result": "Significantly improves temporal reasoning and outperforms existing models on VITATECS, MVBench, and Video-MME benchmarks by up to +5.5%, particularly in action recognition tasks.", "conclusion": "Enhancing the vision encoder with temporal structure addresses a critical gap in video understanding for Video-LLMs, enabling better comprehension of temporal dynamics."}}
{"id": "2510.26049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26049", "abs": "https://arxiv.org/abs/2510.26049", "authors": ["Yuyue Zhou", "Jessica Knight", "Shrimanti Ghosh", "Banafshe Felfeliyan", "Jacob L. Jaremko", "Abhilash R. Hareendranathan"], "title": "FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation", "comment": null, "summary": "Elbow and wrist fractures are the most common fractures in pediatric\npopulations. Automatic segmentation of musculoskeletal structures in ultrasound\n(US) can improve diagnostic accuracy and treatment planning. Fractures appear\nas cortical defects but require expert interpretation. Deep learning (DL) can\nprovide real-time feedback and highlight key structures, helping lightly\ntrained users perform exams more confidently. However, pixel-wise expert\nannotations for training remain time-consuming and costly. To address this\nchallenge, we propose FlexICL, a novel and flexible in-context learning (ICL)\nframework for segmenting bony regions in US images. We apply it to an\nintra-video segmentation setting, where experts annotate only a small subset of\nframes, and the model segments unseen frames. We systematically investigate\nvarious image concatenation techniques and training strategies for visual ICL\nand introduce novel concatenation methods that significantly enhance model\nperformance with limited labeled data. By integrating multiple augmentation\nstrategies, FlexICL achieves robust segmentation performance across four wrist\nand elbow US datasets while requiring only 5% of the training images. It\noutperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and\nconventional segmentation models like U-Net and TransUNet by 1-27% Dice\ncoefficient on 1,252 US sweeps. These initial results highlight the potential\nof FlexICL as an efficient and scalable solution for US image segmentation well\nsuited for medical imaging use cases where labeled data is scarce.", "AI": {"tldr": "FlexICL is a flexible in-context learning framework for segmenting bony regions in pediatric elbow and wrist ultrasound images, achieving robust performance with only 5% labeled training data.", "motivation": "Automatic segmentation of musculoskeletal structures in ultrasound can improve diagnostic accuracy for pediatric fractures, but pixel-wise expert annotations are time-consuming and costly.", "method": "Proposed FlexICL framework using in-context learning for intra-video segmentation, with novel image concatenation techniques and training strategies that enhance performance with limited labeled data.", "result": "Outperforms state-of-the-art visual ICL models (Painter, MAE-VQGAN) and conventional segmentation models (U-Net, TransUNet) by 1-27% Dice coefficient on 1,252 US sweeps.", "conclusion": "FlexICL shows potential as an efficient and scalable solution for ultrasound image segmentation in medical imaging where labeled data is scarce."}}
{"id": "2510.26052", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26052", "abs": "https://arxiv.org/abs/2510.26052", "authors": ["Hoyeon Chang", "Seungjin Kim", "Yoonseok Choi"], "title": "Dynamic VLM-Guided Negative Prompting for Diffusion Models", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: The First Workshop on Generative and Protective AI for\n  Content Creation", "summary": "We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.", "AI": {"tldr": "Dynamic negative prompting using VLMs to generate adaptive negative prompts during denoising process", "motivation": "Traditional negative prompting uses fixed prompts, which may not be contextually appropriate throughout the denoising process", "method": "Generate intermediate image predictions at specific denoising steps and query VLM to produce context-aware negative prompts", "result": "Evaluated on benchmark datasets, showing trade-offs between negative guidance strength and text-image alignment", "conclusion": "Proposed approach enables adaptive negative prompting that responds to image context during generation"}}
{"id": "2510.26105", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26105", "abs": "https://arxiv.org/abs/2510.26105", "authors": ["Xiaosen Wang", "Zhijin Ge", "Shaokang Wang"], "title": "Security Risk of Misalignment between Text and Image in Multi-modal Model", "comment": null, "summary": "Despite the notable advancements and versatility of multi-modal diffusion\nmodels, such as text-to-image models, their susceptibility to adversarial\ninputs remains underexplored. Contrary to expectations, our investigations\nreveal that the alignment between textual and Image modalities in existing\ndiffusion models is inadequate. This misalignment presents significant risks,\nespecially in the generation of inappropriate or Not-Safe-For-Work (NSFW)\ncontent. To this end, we propose a novel attack called Prompt-Restricted\nMulti-modal Attack (PReMA) to manipulate the generated content by modifying the\ninput image in conjunction with any specified prompt, without altering the\nprompt itself. PReMA is the first attack that manipulates model outputs by\nsolely creating adversarial images, distinguishing itself from prior methods\nthat primarily generate adversarial prompts to produce NSFW content.\nConsequently, PReMA poses a novel threat to the integrity of multi-modal\ndiffusion models, particularly in image-editing applications that operate with\nfixed prompts. Comprehensive evaluations conducted on image inpainting and\nstyle transfer tasks across various models confirm the potent efficacy of\nPReMA.", "AI": {"tldr": "PReMA is a novel adversarial attack that manipulates multi-modal diffusion models by modifying input images while keeping prompts fixed, enabling generation of inappropriate content without changing text inputs.", "motivation": "Existing multi-modal diffusion models have inadequate alignment between text and image modalities, creating security risks for generating NSFW content. Current attacks mainly focus on adversarial prompts, leaving image-based manipulation underexplored.", "method": "Proposes Prompt-Restricted Multi-modal Attack (PReMA) that creates adversarial images to manipulate model outputs while keeping prompts unchanged. It targets image-editing applications with fixed prompts.", "result": "Comprehensive evaluations on image inpainting and style transfer tasks across various models demonstrate PReMA's potent efficacy in manipulating generated content.", "conclusion": "PReMA poses a novel threat to multi-modal diffusion model integrity, particularly in applications with fixed prompts, highlighting the need for better text-image alignment and security measures."}}
{"id": "2510.26113", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26113", "abs": "https://arxiv.org/abs/2510.26113", "authors": ["Minjoon Jung", "Junbin Xiao", "Junghyun Kim", "Byoung-Tak Zhang", "Angela Yao"], "title": "EgoExo-Con: Exploring View-Invariant Video Temporal Understanding", "comment": "project page:\n  \\url{https://minjoong507.github.io/projects/EgoExo-Con/}", "summary": "Can Video-LLMs achieve consistent temporal understanding when videos capture\nthe same event from different viewpoints? To study this, we introduce\nEgoExo-Con (Consistency), a benchmark of comprehensively synchronized\negocentric and exocentric video pairs with human-refined queries in natural\nlanguage. EgoExo-Con emphasizes two temporal understanding tasks: Temporal\nVerification and Temporal Grounding. It evaluates not only correctness but\nconsistency across viewpoints. Our analysis reveals two critical limitations of\nexisting Video-LLMs: (1) models often fail to maintain consistency, with\nresults far worse than their single-view performances. (2) When naively\nfinetuned with synchronized videos of both viewpoints, the models show improved\nconsistency but often underperform those trained on a single view. For\nimprovements, we propose View-GRPO, a novel reinforcement learning framework\nthat effectively strengthens view-specific temporal reasoning while encouraging\nconsistent comprehension across viewpoints. Our method demonstrates its\nsuperiority over naive SFT and GRPO, especially for improving cross-view\nconsistency. All resources will be made publicly available.", "AI": {"tldr": "The paper introduces EgoExo-Con, a benchmark for evaluating Video-LLMs' temporal understanding consistency across egocentric and exocentric viewpoints, and proposes View-GRPO to improve cross-view consistency.", "motivation": "To address the limitation that existing Video-LLMs fail to maintain consistent temporal understanding when videos capture the same event from different viewpoints (egocentric vs exocentric).", "method": "Proposes View-GRPO, a novel reinforcement learning framework that strengthens view-specific temporal reasoning while encouraging consistent comprehension across viewpoints.", "result": "View-GRPO demonstrates superiority over naive supervised fine-tuning (SFT) and GRPO, especially for improving cross-view consistency in temporal understanding tasks.", "conclusion": "The proposed View-GRPO effectively addresses the consistency limitations of Video-LLMs across different viewpoints, outperforming existing methods in maintaining temporal understanding consistency."}}
{"id": "2510.26114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26114", "abs": "https://arxiv.org/abs/2510.26114", "authors": ["Caoshuo Li", "Zengmao Ding", "Xiaobin Hu", "Bang Li", "Donghao Luo", "Xu Peng", "Taisong Jin", "Yongge Liu", "Shengwei Han", "Jing Yang", "Xiaoping He", "Feng Gao", "AndyPian Wu", "SevenShu", "Chaoyang Wang", "Chengjie Wang"], "title": "OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research", "comment": null, "summary": "As one of the earliest writing systems, Oracle Bone Script (OBS) preserves\nthe cultural and intellectual heritage of ancient civilizations. However,\ncurrent OBS research faces two major challenges: (1) the interpretation of OBS\ninvolves a complex workflow comprising multiple serial and parallel sub-tasks,\nand (2) the efficiency of OBS information organization and retrieval remains a\ncritical bottleneck, as scholars often spend substantial effort searching for,\ncompiling, and managing relevant resources. To address these challenges, we\npresent OracleAgent, the first agent system designed for the structured\nmanagement and retrieval of OBS-related information. OracleAgent seamlessly\nintegrates multiple OBS analysis tools, empowered by large language models\n(LLMs), and can flexibly orchestrate these components. Additionally, we\nconstruct a comprehensive domain-specific multimodal knowledge base for OBS,\nwhich is built through a rigorous multi-year process of data collection,\ncleaning, and expert annotation. The knowledge base comprises over 1.4M\nsingle-character rubbing images and 80K interpretation texts. OracleAgent\nleverages this resource through its multimodal tools to assist experts in\nretrieval tasks of character, document, interpretation text, and rubbing image.\nExtensive experiments demonstrate that OracleAgent achieves superior\nperformance across a range of multimodal reasoning and generation tasks,\nsurpassing leading mainstream multimodal large language models (MLLMs) (e.g.,\nGPT-4o). Furthermore, our case study illustrates that OracleAgent can\neffectively assist domain experts, significantly reducing the time cost of OBS\nresearch. These results highlight OracleAgent as a significant step toward the\npractical deployment of OBS-assisted research and automated interpretation\nsystems.", "AI": {"tldr": "OracleAgent is the first agent system for Oracle Bone Script (OBS) research that integrates multiple analysis tools with LLMs and uses a comprehensive multimodal knowledge base to address workflow complexity and retrieval inefficiency challenges.", "motivation": "Current OBS research faces challenges: (1) complex interpretation workflow with multiple serial/parallel sub-tasks, and (2) inefficient information organization and retrieval that requires substantial effort from scholars.", "method": "Developed OracleAgent system that integrates multiple OBS analysis tools empowered by LLMs and flexibly orchestrates them. Built a comprehensive domain-specific multimodal knowledge base with over 1.4M single-character rubbing images and 80K interpretation texts through multi-year data collection, cleaning, and expert annotation.", "result": "OracleAgent achieves superior performance across multimodal reasoning and generation tasks, surpassing leading MLLMs like GPT-4o. Case studies show it significantly reduces time cost for OBS research and effectively assists domain experts in retrieval tasks.", "conclusion": "OracleAgent represents a significant step toward practical deployment of OBS-assisted research and automated interpretation systems, demonstrating effective integration of multimodal tools and knowledge resources."}}
{"id": "2510.26117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26117", "abs": "https://arxiv.org/abs/2510.26117", "authors": ["Yuxuan Li", "Tao Wang", "Xianben Yang"], "title": "JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting", "comment": null, "summary": "Traditional novel view synthesis methods heavily rely on external camera pose\nestimation tools such as COLMAP, which often introduce computational\nbottlenecks and propagate errors. To address these challenges, we propose a\nunified framework that jointly optimizes 3D Gaussian points and camera poses\nwithout requiring pre-calibrated inputs. Our approach iteratively refines 3D\nGaussian parameters and updates camera poses through a novel co-optimization\nstrategy, ensuring simultaneous improvements in scene reconstruction fidelity\nand pose accuracy. The key innovation lies in decoupling the joint optimization\ninto two interleaved phases: first, updating 3D Gaussian parameters via\ndifferentiable rendering with fixed poses, and second, refining camera poses\nusing a customized 3D optical flow algorithm that incorporates geometric and\nphotometric constraints. This formulation progressively reduces projection\nerrors, particularly in challenging scenarios with large viewpoint variations\nand sparse feature distributions, where traditional methods struggle. Extensive\nevaluations on multiple datasets demonstrate that our approach significantly\noutperforms existing COLMAP-free techniques in reconstruction quality, and also\nsurpasses the standard COLMAP-based baseline in general.", "AI": {"tldr": "A unified framework that jointly optimizes 3D Gaussian points and camera poses without requiring pre-calibrated camera inputs, eliminating the need for external pose estimation tools like COLMAP.", "motivation": "Traditional novel view synthesis methods rely on external camera pose estimation tools (e.g., COLMAP) which introduce computational bottlenecks and propagate errors, limiting performance in challenging scenarios.", "method": "Joint optimization framework with interleaved phases: 1) update 3D Gaussian parameters via differentiable rendering with fixed poses, 2) refine camera poses using customized 3D optical flow algorithm with geometric and photometric constraints.", "result": "Significantly outperforms existing COLMAP-free techniques in reconstruction quality and surpasses standard COLMAP-based baseline in general performance, particularly in challenging scenarios with large viewpoint variations and sparse features.", "conclusion": "The proposed co-optimization strategy effectively eliminates dependency on external pose estimation tools while achieving superior scene reconstruction fidelity and pose accuracy through progressive error reduction."}}
{"id": "2510.26125", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26125", "abs": "https://arxiv.org/abs/2510.26125", "authors": ["Runsheng Xu", "Hubert Lin", "Wonseok Jeon", "Hao Feng", "Yuliang Zou", "Liting Sun", "John Gorman", "Kate Tolstaya", "Sarah Tang", "Brandyn White", "Ben Sapp", "Mingxing Tan", "Jyh-Jing Hwang", "Drago Anguelov"], "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios", "comment": null, "summary": "Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.", "AI": {"tldr": "WOD-E2E is a new dataset for end-to-end driving focusing on challenging long-tail scenarios, with a novel Rater Feedback Score metric for better evaluation.", "motivation": "Current E2E driving benchmarks lack challenging long-tail scenarios and existing metrics don't effectively evaluate multi-modal driving performance in rare situations.", "method": "Created WOD-E2E dataset with 4,021 driving segments (12 hours) of rare scenarios (<0.03% frequency), including routing info, ego states, and 360-degree camera views. Introduced Rater Feedback Score metric that measures trajectory preference alignment.", "result": "Developed comprehensive dataset and evaluation framework specifically designed for testing E2E driving systems on challenging long-tail scenarios that are rare in daily driving.", "conclusion": "WOD-E2E aims to advance research in generalizable, robust, and safe end-to-end autonomous driving by providing better benchmarks and evaluation methods for complex real-world situations."}}
{"id": "2510.26131", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26131", "abs": "https://arxiv.org/abs/2510.26131", "authors": ["Ali Caglayan", "Nevrez Imamoglu", "Oguzhan Guclu", "Ali Osman Serhatoglu", "Ahmet Burak Can", "Ryosuke Nakamura"], "title": "Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM", "comment": "double-column 5 pages, 3 figures", "summary": "Attention models have recently emerged as a powerful approach, demonstrating\nsignificant progress in various fields. Visualization techniques, such as class\nactivation mapping, provide visual insights into the reasoning of convolutional\nneural networks (CNNs). Using network gradients, it is possible to identify\nregions where the network pays attention during image recognition tasks.\nFurthermore, these gradients can be combined with CNN features to localize more\ngeneralizable, task-specific attentive (salient) regions within scenes.\nHowever, explicit use of this gradient-based attention information integrated\ndirectly into CNN representations for semantic object understanding remains\nlimited. Such integration is particularly beneficial for visual tasks like\nsimultaneous localization and mapping (SLAM), where CNN representations\nenriched with spatially attentive object locations can enhance performance. In\nthis work, we propose utilizing task-specific network attention for RGB-D\nindoor SLAM. Specifically, we integrate layer-wise attention information\nderived from network gradients with CNN feature representations to improve\nframe association performance. Experimental results indicate improved\nperformance compared to baseline methods, particularly for large environments.", "AI": {"tldr": "The paper proposes integrating gradient-based attention information with CNN features to improve RGB-D indoor SLAM performance, particularly for large environments.", "motivation": "Existing gradient-based attention methods provide visual insights but are not effectively integrated into CNN representations for semantic object understanding, which could benefit visual tasks like SLAM.", "method": "Integrate layer-wise attention information derived from network gradients with CNN feature representations to enhance frame association in RGB-D indoor SLAM.", "result": "Experimental results show improved performance compared to baseline methods, especially for large environments.", "conclusion": "Integrating task-specific network attention with CNN features effectively enhances SLAM performance, demonstrating the value of attention mechanisms in visual tasks."}}
{"id": "2510.26140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26140", "abs": "https://arxiv.org/abs/2510.26140", "authors": ["Lihe Ding", "Shaocong Dong", "Yaokun Li", "Chenjian Gao", "Xiao Chen", "Rui Han", "Yihao Kuang", "Hong Zhang", "Bo Huang", "Zhanpeng Huang", "Zibin Wang", "Dan Xu", "Tianfan Xue"], "title": "FullPart: Generating each 3D Part at Full Resolution", "comment": "Project page: https://fullpart3d.github.io", "summary": "Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.", "AI": {"tldr": "FullPart combines implicit and explicit paradigms for 3D part generation, using implicit diffusion for bounding box layout and full-resolution voxel grids for detailed part generation, achieving state-of-the-art results.", "motivation": "Previous methods either use implicit representations with insufficient geometric details or explicit voxel representations where small parts occupy too few voxels, leading to degraded quality.", "method": "Uses implicit diffusion for bounding box layout and generates each part in its own fixed full-resolution voxel grid. Introduces center-point encoding to address misalignment between parts of different sizes.", "result": "Achieves state-of-the-art results in 3D part generation, enabling synthesis of intricate details even for small parts.", "conclusion": "FullPart effectively combines implicit and explicit paradigms for high-quality 3D part generation and introduces PartVerse-XL, the largest human-annotated 3D part dataset."}}
{"id": "2510.26149", "categories": ["cs.CV", "I.4.3"], "pdf": "https://arxiv.org/pdf/2510.26149", "abs": "https://arxiv.org/abs/2510.26149", "authors": ["Wei Shang", "Wanying Zhang", "Shuhang Gu", "Pengfei Zhu", "Qinghua Hu", "Dongwei Ren"], "title": "BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation", "comment": "13 pages, 10 figures, 5 tables", "summary": "Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution\nof video frames, potentially at various scaling factors, which presents several\nchallenges regarding spatial detail reproduction, temporal consistency, and\ncomputational complexity. In this paper, we propose a strong baseline BasicAVSR\nfor AVSR by integrating four key components: 1) adaptive multi-scale frequency\npriors generated from image Laplacian pyramids, 2) a flow-guided propagation\nunit to aggregate spatiotemporal information from adjacent frames, 3) a\nsecond-order motion compensation unit for more accurate spatial alignment of\nadjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and\ncontent-independent upsampling kernels. To meet diverse application demands, we\ninstantiate three propagation variants: (i) a unidirectional RNN unit for\nstrictly online inference, (ii) a unidirectional RNN unit empowered with a\nlimited lookahead that tolerates a small output delay, and (iii) a\nbidirectional RNN unit designed for offline tasks where computational resources\nare less constrained. Experimental results demonstrate the effectiveness and\nadaptability of our model across these different scenarios. Through extensive\nexperiments, we show that BasicAVSR significantly outperforms existing methods\nin terms of super-resolution quality, generalization ability, and inference\nspeed. Our work not only advances the state-of-the-art in AVSR but also extends\nits core components to multiple frameworks for diverse scenarios. The code is\navailable at https://github.com/shangwei5/BasicAVSR.", "AI": {"tldr": "BasicAVSR is a strong baseline for arbitrary-scale video super-resolution that integrates adaptive multi-scale frequency priors, flow-guided propagation, second-order motion compensation, and hyper-upsampling to achieve superior performance across different scenarios.", "motivation": "Arbitrary-scale video super-resolution faces challenges in spatial detail reproduction, temporal consistency, and computational complexity. The paper aims to create a versatile baseline that can handle various scaling factors while maintaining high quality and efficiency.", "method": "Proposes BasicAVSR with four key components: 1) adaptive multi-scale frequency priors from image Laplacian pyramids, 2) flow-guided propagation unit for spatiotemporal aggregation, 3) second-order motion compensation for accurate frame alignment, and 4) hyper-upsampling unit for scale-aware upsampling. Three propagation variants are instantiated for different scenarios.", "result": "BasicAVSR significantly outperforms existing methods in super-resolution quality, generalization ability, and inference speed. The model demonstrates effectiveness and adaptability across different scenarios including online inference, limited lookahead, and offline tasks.", "conclusion": "The work advances state-of-the-art in arbitrary-scale video super-resolution and extends core components to multiple frameworks for diverse scenarios, providing a strong baseline for future research."}}
{"id": "2510.26151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26151", "abs": "https://arxiv.org/abs/2510.26151", "authors": ["Shunjie-Fabian Zheng", "Hyeonjun Lee", "Thijs Kooi", "Ali Diba"], "title": "MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction", "comment": "Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025", "summary": "Large annotated datasets are essential for training robust Computer-Aided\nDiagnosis (CAD) models for breast cancer detection or risk prediction. However,\nacquiring such datasets with fine-detailed annotation is both costly and\ntime-consuming. Vision-Language Models (VLMs), such as CLIP, which are\npre-trained on large image-text pairs, offer a promising solution by enhancing\nrobustness and data efficiency in medical imaging tasks. This paper introduces\na novel Multi-View Mammography and Language Model for breast cancer\nclassification and risk prediction, trained on a dataset of paired mammogram\nimages and synthetic radiology reports. Our MV-MLM leverages multi-view\nsupervision to learn rich representations from extensive radiology data by\nemploying cross-modal self-supervision across image-text pairs. This includes\nmultiple views and the corresponding pseudo-radiology reports. We propose a\nnovel joint visual-textual learning strategy to enhance generalization and\naccuracy performance over different data types and tasks to distinguish breast\ntissues or cancer characteristics(calcification, mass) and utilize these\npatterns to understand mammography images and predict cancer risk. We evaluated\nour method on both private and publicly available datasets, demonstrating that\nthe proposed model achieves state-of-the-art performance in three\nclassification tasks: (1) malignancy classification, (2) subtype\nclassification, and (3) image-based cancer risk prediction. Furthermore, the\nmodel exhibits strong data efficiency, outperforming existing fully supervised\nor VLM baselines while trained on synthetic text reports and without the need\nfor actual radiology reports.", "AI": {"tldr": "A novel Multi-View Mammography and Language Model (MV-MLM) that uses synthetic radiology reports and multi-view supervision for breast cancer classification and risk prediction, achieving state-of-the-art performance without needing real radiology reports.", "motivation": "Large annotated datasets for breast cancer CAD models are costly and time-consuming to acquire. Vision-Language Models offer a promising solution for enhancing robustness and data efficiency in medical imaging.", "method": "MV-MLM uses multi-view supervision and cross-modal self-supervision across image-text pairs, including multiple mammogram views and corresponding synthetic radiology reports. A joint visual-textual learning strategy enhances generalization across different data types and tasks.", "result": "The model achieves state-of-the-art performance in three classification tasks: malignancy classification, subtype classification, and image-based cancer risk prediction. It demonstrates strong data efficiency, outperforming fully supervised and VLM baselines.", "conclusion": "MV-MLM effectively leverages synthetic text reports and multi-view learning for robust breast cancer analysis without requiring actual radiology reports, showing promising data efficiency and performance."}}
{"id": "2510.26154", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26154", "abs": "https://arxiv.org/abs/2510.26154", "authors": ["Sudipto Das Sukanto", "Diponker Roy", "Fahim Shakil", "Nirjhar Singha", "Abdullah Asik", "Aniket Joarder", "Mridha Md Nafis Fuad", "Muhammad Ibrahim"], "title": "Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh", "comment": "16 pages", "summary": "Modes of transportation vary across countries depending on geographical\nlocation and cultural context. In South Asian countries rickshaws are among the\nmost common means of local transport. Based on their mode of operation,\nrickshaws in cities across Bangladesh can be broadly classified into non-auto\n(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of\nauto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from\naccessing certain routes. However, existing surveillance systems make it quite\ndifficult to monitor them due to their similarity to other vehicles, especially\nnon-auto rickshaws whereas manual video analysis is too time-consuming. This\npaper presents a machine learning-based approach to automatically detect\nauto-rickshaws in traffic images. In this system, we used real-time object\ndetection using the YOLOv8 model. For training purposes, we prepared a set of\n1,730 annotated images that were captured under various traffic conditions. The\nresults show that our proposed model performs well in real-time auto-rickshaw\ndetection and offers an mAP50 of 83.447% and binary precision and recall values\nabove 78%, demonstrating its effectiveness in handling both dense and sparse\ntraffic scenarios. The dataset has been publicly released for further research.", "AI": {"tldr": "This paper presents a YOLOv8-based real-time object detection system for automatically identifying auto-rickshaws in traffic images, achieving 83.447% mAP50 and over 78% precision/recall.", "motivation": "Auto-rickshaws in South Asian countries like Bangladesh need monitoring due to traffic restrictions, but existing surveillance systems struggle to distinguish them from similar vehicles like non-auto rickshaws, and manual video analysis is too slow.", "method": "Used YOLOv8 model for real-time object detection, trained on a custom dataset of 1,730 annotated images captured under various traffic conditions.", "result": "The model achieved 83.447% mAP50 and binary precision and recall values above 78%, demonstrating effectiveness in both dense and sparse traffic scenarios.", "conclusion": "The proposed machine learning approach successfully enables automated auto-rickshaw detection in real-time traffic monitoring, with the dataset publicly released for further research."}}
{"id": "2510.26160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26160", "abs": "https://arxiv.org/abs/2510.26160", "authors": ["Jiaqi Wang", "Xiao Yang", "Kai Sun", "Parth Suresh", "Sanat Sharma", "Adam Czyzewski", "Derek Andersen", "Surya Appini", "Arkav Banerjee", "Sajal Choudhary", "Shervin Ghasemlou", "Ziqiang Guan", "Akil Iyer", "Haidar Khan", "Lingkun Kong", "Roy Luo", "Tiffany Ma", "Zhen Qiao", "David Tran", "Wenfang Xu", "Skyler Yeatman", "Chen Zhou", "Gunveer Gujral", "Yinglong Xia", "Shane Moon", "Nicolas Scheffer", "Nirav Shah", "Eun Chang", "Yue Liu", "Florian Metze", "Tammy Stark", "Zhaleh Feizollahi", "Andrea Jessee", "Mangesh Pujari", "Ahmed Aly", "Babak Damavandi", "Rakesh Wanga", "Anuj Kumar", "Rohit Patel", "Wen-tau Yih", "Xin Luna Dong"], "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark", "comment": null, "summary": "Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.", "AI": {"tldr": "CRAG-MM is a comprehensive benchmark for multi-modal RAG with 6.5K image-question-answer triplets and 2K multi-turn conversations across 13 domains, designed for wearable device scenarios with egocentric images.", "motivation": "There is no comprehensive benchmark for multi-modal RAG in wearable scenarios, despite the growing importance of smart glasses and similar devices that enable visual-based information seeking.", "method": "Created a diverse dataset with 6.5K triplets and 2K multi-turn conversations across 13 domains, featuring 6.2K egocentric images, various question types, image quality issues, entity popularity levels, and conversation turns. Designed three tasks with retrieval corpus and APIs.", "result": "Current RAG approaches achieve only 32-43% truthfulness, while industry solutions show similar performance (32-45%), indicating significant room for improvement. The benchmark attracted 1K participants and 5K submissions in KDD Cup 2025.", "conclusion": "CRAG-MM fills a critical gap in multi-modal RAG benchmarking and demonstrates substantial performance improvement potential, with winning solutions already showing 28% improvement over baselines."}}
{"id": "2510.26173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26173", "abs": "https://arxiv.org/abs/2510.26173", "authors": ["Wontae Choi", "Jaelin Lee", "Hyung Sup Yun", "Byeungwoo Jeon", "Il Yong Chun"], "title": "MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models", "comment": "10 pages, 6 figures", "summary": "Accurate estimation of motion information is crucial in diverse computational\nimaging and computer vision applications. Researchers have investigated various\nmethods to extract motion information from a single blurred image, including\nblur kernels and optical flow. However, existing motion representations are\noften of low quality, i.e., coarse-grained and inaccurate. In this paper, we\npropose the first high-resolution (HR) Motion Trajectory estimation framework\nusing Diffusion models (MoTDiff). Different from existing motion\nrepresentations, we aim to estimate an HR motion trajectory with high-quality\nfrom a single motion-blurred image. The proposed MoTDiff consists of two key\ncomponents: 1) a new conditional diffusion framework that uses multi-scale\nfeature maps extracted from a single blurred image as a condition, and 2) a new\ntraining method that can promote precise identification of a fine-grained\nmotion trajectory, consistent estimation of overall shape and position of a\nmotion path, and pixel connectivity along a motion trajectory. Our experiments\ndemonstrate that the proposed MoTDiff can outperform state-of-the-art methods\nin both blind image deblurring and coded exposure photography applications.", "AI": {"tldr": "MoTDiff is a high-resolution motion trajectory estimation framework using diffusion models that extracts fine-grained motion information from single motion-blurred images, outperforming state-of-the-art methods in blind image deblurring and coded exposure photography.", "motivation": "Existing motion representations from single blurred images are often low quality (coarse-grained and inaccurate), limiting performance in computational imaging and computer vision applications that require precise motion information.", "method": "Proposes MoTDiff framework with: 1) conditional diffusion using multi-scale feature maps from blurred images as condition, and 2) training method promoting fine-grained trajectory identification, consistent shape/position estimation, and pixel connectivity.", "result": "Outperforms state-of-the-art methods in both blind image deblurring and coded exposure photography applications.", "conclusion": "MoTDiff successfully enables high-quality, high-resolution motion trajectory estimation from single motion-blurred images using diffusion models, demonstrating superior performance over existing approaches."}}
{"id": "2510.26186", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26186", "abs": "https://arxiv.org/abs/2510.26186", "authors": ["Jinho Choi", "Hyesu Lim", "Steffen Schneider", "Jaegul Choo"], "title": "ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts", "comment": "Published in the Thirty-Ninth Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Dataset bias, where data points are skewed to certain concepts, is ubiquitous\nin machine learning datasets. Yet, systematically identifying these biases is\nchallenging without costly, fine-grained attribute annotations. We present\nConceptScope, a scalable and automated framework for analyzing visual datasets\nby discovering and quantifying human-interpretable concepts using Sparse\nAutoencoders trained on representations from vision foundation models.\nConceptScope categorizes concepts into target, context, and bias types based on\ntheir semantic relevance and statistical correlation to class labels, enabling\nclass-level dataset characterization, bias identification, and robustness\nevaluation through concept-based subgrouping. We validate that ConceptScope\ncaptures a wide range of visual concepts, including objects, textures,\nbackgrounds, facial attributes, emotions, and actions, through comparisons with\nannotated datasets. Furthermore, we show that concept activations produce\nspatial attributions that align with semantically meaningful image regions.\nConceptScope reliably detects known biases (e.g., background bias in\nWaterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects\nin ImageNet), offering a practical tool for dataset auditing and model\ndiagnostics.", "AI": {"tldr": "ConceptScope is an automated framework that uses Sparse Autoencoders on vision foundation models to discover and quantify human-interpretable concepts in datasets, enabling systematic bias identification and dataset analysis without requiring fine-grained annotations.", "motivation": "Dataset bias is widespread in machine learning but challenging to identify without costly manual annotations. There's a need for scalable, automated methods to systematically discover and analyze these biases.", "method": "Uses Sparse Autoencoders trained on representations from vision foundation models to discover interpretable concepts. Categorizes concepts into target, context, and bias types based on semantic relevance and statistical correlation to class labels. Enables class-level dataset characterization and concept-based subgrouping for robustness evaluation.", "result": "Validated to capture diverse visual concepts (objects, textures, backgrounds, facial attributes, emotions, actions). Concept activations produce spatial attributions aligned with meaningful image regions. Successfully detected known biases (e.g., background bias in Waterbirds) and uncovered previously unannotated biases (e.g., co-occurring objects in ImageNet).", "conclusion": "ConceptScope provides a practical tool for automated dataset auditing and model diagnostics, offering scalable bias detection without requiring expensive manual annotations."}}
{"id": "2510.26196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26196", "abs": "https://arxiv.org/abs/2510.26196", "authors": ["Li Wang", "Yiyu Zhuang", "Yanwen Wang", "Xun Cao", "Chuan Guo", "Xinxin Zuo", "Hao Zhu"], "title": "Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction", "comment": "SIGGRAPH Asia 2025", "summary": "3D human pose estimation from sketches has broad applications in computer\nanimation and film production. Unlike traditional human pose estimation, this\ntask presents unique challenges due to the abstract and disproportionate nature\nof sketches. Previous sketch-to-pose methods, constrained by the lack of\nlarge-scale sketch-3D pose annotations, primarily relied on optimization with\nheuristic rules-an approach that is both time-consuming and limited in\ngeneralizability. To address these challenges, we propose a novel approach\nleveraging a \"learn from synthesis\" strategy. First, a diffusion model is\ntrained to synthesize sketch images from 2D poses projected from 3D human\nposes, mimicking disproportionate human structures in sketches. This process\nenables the creation of a synthetic dataset, SKEP-120K, consisting of 120k\naccurate sketch-3D pose annotation pairs across various sketch styles. Building\non this synthetic dataset, we introduce an end-to-end data-driven framework for\nestimating human poses and shapes from diverse sketch styles. Our framework\ncombines existing 2D pose detectors and generative diffusion priors for sketch\nfeature extraction with a feed-forward neural network for efficient 2D pose\nestimation. Multiple heuristic loss functions are incorporated to guarantee\ngeometric coherence between the derived 3D poses and the detected 2D poses\nwhile preserving accurate self-contacts. Qualitative, quantitative, and\nsubjective evaluations collectively show that our model substantially surpasses\nprevious ones in both estimation accuracy and speed for sketch-to-pose tasks.", "AI": {"tldr": "Proposes a novel approach for 3D human pose estimation from sketches using a \"learn from synthesis\" strategy with diffusion models to create synthetic sketch-pose data, achieving superior accuracy and speed.", "motivation": "Addresses challenges in sketch-to-3D pose estimation due to abstract and disproportionate sketch nature, overcoming limitations of previous optimization-based methods that lack large-scale annotated data.", "method": "Uses diffusion model to synthesize sketches from 2D poses, creates SKEP-120K synthetic dataset, combines 2D pose detectors with diffusion priors and feed-forward network, incorporates heuristic loss functions for geometric coherence.", "result": "Qualitative, quantitative, and subjective evaluations show substantial improvements in both estimation accuracy and speed compared to previous methods.", "conclusion": "The proposed data-driven framework effectively handles diverse sketch styles and outperforms existing sketch-to-pose estimation approaches."}}
{"id": "2510.26203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26203", "abs": "https://arxiv.org/abs/2510.26203", "authors": ["Mehdi Khaleghi", "Nastaran Khaleghi", "Sobhan Sheykhivand", "Sebelan Danishvar"], "title": "Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management", "comment": null, "summary": "The sustainability of supply chain plays a key role in achieving optimal\nperformance in controlling the supply chain. The management of risks that occur\nin a supply chain is a fundamental problem for the purpose of developing the\nsustainability of the network and elevating the performance efficiency of the\nsupply chain. The correct classification of products is another essential\nelement in a sustainable supply chain. Acknowledging recent breakthroughs in\nthe context of deep networks, several architectural options have been deployed\nto analyze supply chain datasets. A novel geometric deep network is used to\npropose an ensemble deep network. The proposed Chebyshev ensemble geometric\nnetwork (Ch-EGN) is a hybrid convolutional and geometric deep learning. This\nnetwork is proposed to leverage the information dependencies in supply chain to\nderive invisible states of samples in the database. The functionality of the\nproposed deep network is assessed on the two different databases. The\nSupplyGraph Dataset and DataCo are considered in this research. The prediction\nof delivery status of DataCo supply chain is done for risk administration. The\nproduct classification and edge classification are performed using the\nSupplyGraph database to enhance the sustainability of the supply network. An\naverage accuracy of 98.95% is obtained for the ensemble network for risk\nmanagement. The average accuracy of 100% and 98.07% are obtained for\nsustainable supply chain in terms of 5 product group classification and 4\nproduct relation classification, respectively. The average accuracy of 92.37%\nis attained for 25 company relation classification. The results confirm an\naverage improvement and efficiency of the proposed method compared to the\nstate-of-the-art approaches.", "AI": {"tldr": "A novel Chebyshev ensemble geometric network (Ch-EGN) is proposed for supply chain risk management and sustainability, achieving high accuracy in delivery status prediction (98.95%), product classification (100%), and relationship classification (98.07%-92.37%).", "motivation": "Supply chain sustainability requires effective risk management and product classification to optimize performance. Deep learning approaches can leverage information dependencies in supply chain data to identify invisible states and patterns.", "method": "Proposed Chebyshev ensemble geometric network (Ch-EGN) - a hybrid convolutional and geometric deep learning approach that captures information dependencies in supply chain networks. Evaluated on SupplyGraph Dataset and DataCo databases.", "result": "Achieved 98.95% accuracy for delivery status prediction (risk management), 100% accuracy for 5 product group classification, 98.07% for 4 product relation classification, and 92.37% for 25 company relation classification. Outperformed state-of-the-art methods.", "conclusion": "The proposed Ch-EGN ensemble network effectively enhances supply chain sustainability through accurate risk management and classification tasks, demonstrating superior performance compared to existing approaches."}}
{"id": "2510.26213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26213", "abs": "https://arxiv.org/abs/2510.26213", "authors": ["Hengrui Kang", "Zhuangcheng Gu", "Zhiyuan Zhao", "Zichen Wen", "Bin Wang", "Weijia Li", "Conghui He"], "title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation", "comment": "TL;DR: With OmniLayout-1M dataset and LLM-based coarse-to-fine\n  learning, we enable universal and diverse document layout generation", "summary": "Document AI has advanced rapidly and is attracting increasing attention. Yet,\nwhile most efforts have focused on document layout analysis (DLA), its\ngenerative counterpart, document layout generation, remains underexplored. A\nmajor obstacle lies in the scarcity of diverse layouts: academic papers with\nManhattan-style structures dominate existing studies, while open-world genres\nsuch as newspapers and magazines remain severely underrepresented. To address\nthis gap, we curate OmniLayout-1M, the first million-scale dataset of diverse\ndocument layouts, covering six common document types and comprising\ncontemporary layouts collected from multiple sources. Moreover, since existing\nmethods struggle in complex domains and often fail to arrange long sequences\ncoherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage\nCoarse-to-Fine learning paradigm: 1) learning universal layout principles from\nOmniLayout-1M with coarse category definitions, and 2) transferring the\nknowledge to a specific domain with fine-grained annotations. Extensive\nexperiments demonstrate that our approach achieves strong performance on\nmultiple domains in M$^{6}$Doc dataset, substantially surpassing both existing\nlayout generation experts and several latest general-purpose LLMs. Our code,\nmodels, and dataset will be publicly released.", "AI": {"tldr": "The paper introduces OmniLayout-1M, a million-scale diverse document layout dataset, and OmniLayout-LLM, a 0.5B model with coarse-to-fine learning for document layout generation.", "motivation": "Document layout generation is underexplored compared to layout analysis, with existing datasets dominated by academic papers and lacking diversity in open-world genres like newspapers and magazines.", "method": "Two-stage coarse-to-fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse categories, 2) transferring knowledge to specific domains with fine-grained annotations.", "result": "Achieves strong performance on multiple domains in M$^{6}$Doc dataset, substantially surpassing existing layout generation experts and latest general-purpose LLMs.", "conclusion": "The proposed approach effectively addresses the scarcity of diverse layouts and improves document layout generation across multiple domains."}}
{"id": "2510.26241", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26241", "abs": "https://arxiv.org/abs/2510.26241", "authors": ["Shiho Matta", "Lis Kanashiro Pereira", "Peitao Han", "Fei Cheng", "Shigeru Kitazawa"], "title": "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models", "comment": "10 pages", "summary": "Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.", "AI": {"tldr": "VLMs struggle with temporal reasoning, performing poorly on judging video playback direction (forward/backward) compared to humans, especially for irreversible physical processes and causal actions.", "motivation": "To evaluate and expose the weak temporal understanding in modern vision-language models (VLMs) by testing their ability to judge the arrow of time in videos, using human behavioral baselines.", "method": "Created AoT-PsyPhyBENCH benchmark with psychophysically validated stimuli to test VLMs' temporal direction inference in natural videos, evaluating both open-weight and proprietary models.", "result": "Most VLMs performed near chance level, with even the best models significantly lagging behind human accuracy on physically irreversible processes and causal manual actions.", "conclusion": "Current VLMs lack essential inductive biases for temporal continuity and causal understanding despite capturing visual-semantic correlations, revealing a fundamental gap in multimodal systems."}}
{"id": "2510.26268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26268", "abs": "https://arxiv.org/abs/2510.26268", "authors": ["Lin Guo", "Xiaoqing Luo", "Wei Xie", "Zhancheng Zhang", "Hui Li", "Rui Wang", "Zhenhua Feng", "Xiaoning Song"], "title": "Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws", "comment": "NeurIPS 2025 spotlight", "summary": "Existing infrared and visible image fusion methods often face the dilemma of\nbalancing modal information. Generative fusion methods reconstruct fused images\nby learning from data distributions, but their generative capabilities remain\nlimited. Moreover, the lack of interpretability in modal information selection\nfurther affects the reliability and consistency of fusion results in complex\nscenarios. This manuscript revisits the essence of generative image fusion\nunder the inspiration of human cognitive laws and proposes a novel infrared and\nvisible image fusion method, termed HCLFuse. First, HCLFuse investigates the\nquantification theory of information mapping in unsupervised fusion networks,\nwhich leads to the design of a multi-scale mask-regulated variational\nbottleneck encoder. This encoder applies posterior probability modeling and\ninformation decomposition to extract accurate and concise low-level modal\ninformation, thereby supporting the generation of high-fidelity structural\ndetails. Furthermore, the probabilistic generative capability of the diffusion\nmodel is integrated with physical laws, forming a time-varying physical\nguidance mechanism that adaptively regulates the generation process at\ndifferent stages, thereby enhancing the ability of the model to perceive the\nintrinsic structure of data and reducing dependence on data quality.\nExperimental results show that the proposed method achieves state-of-the-art\nfusion performance in qualitative and quantitative evaluations across multiple\ndatasets and significantly improves semantic segmentation metrics. This fully\ndemonstrates the advantages of this generative image fusion method, drawing\ninspiration from human cognition, in enhancing structural consistency and\ndetail quality.", "AI": {"tldr": "HCLFuse is a novel infrared and visible image fusion method inspired by human cognitive laws, using multi-scale mask-regulated variational bottleneck encoder and diffusion model with physical guidance to achieve state-of-the-art fusion performance.", "motivation": "Existing fusion methods struggle with balancing modal information and lack interpretability in modal selection, affecting reliability in complex scenarios. Generative methods have limited capabilities.", "method": "Uses multi-scale mask-regulated variational bottleneck encoder for information decomposition and diffusion model with time-varying physical guidance mechanism to regulate generation process.", "result": "Achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets, significantly improving semantic segmentation metrics.", "conclusion": "The human cognition-inspired generative fusion method enhances structural consistency and detail quality, demonstrating advantages in reliable image fusion."}}
{"id": "2510.26282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26282", "abs": "https://arxiv.org/abs/2510.26282", "authors": ["Fernando Alonso-Fernandez", "Kevin Hernandez Diaz", "Jose M. Buades", "Kiran Raja", "Josef Bigun"], "title": "Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances", "comment": "Accepted at BIOSIG 2025 conference", "summary": "We study the complementarity of different CNNs for periocular verification at\ndifferent distances on the UBIPr database. We train three architectures of\nincreasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of\neye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,\ncompare different network initialisations, and apply score-level fusion via\nlogistic regression. In addition, we use LIME heatmaps and Jensen-Shannon\ndivergence to compare attention patterns of the CNNs. While ResNet50\nconsistently performs best individually, the fusion provides substantial gains,\nespecially when combining all three networks. Heatmaps show that networks\nusually focus on distinct regions of a given image, which explains their\ncomplementarity. Our method significantly outperforms previous works on UBIPr,\nachieving a new state-of-the-art.", "AI": {"tldr": "Analysis of CNN complementarity for periocular verification at different distances, showing that fusion of multiple architectures (SqueezeNet, MobileNetv2, ResNet50) achieves state-of-the-art performance on UBIPr database.", "motivation": "To study how different CNN architectures complement each other for periocular verification, particularly at varying distances, and leverage their complementary strengths through fusion techniques.", "method": "Trained three CNN architectures (SqueezeNet, MobileNetv2, ResNet50) on eye crops from VGGFace2, analyzed performance with cosine and chi2 metrics, compared network initializations, applied score-level fusion via logistic regression, and used LIME heatmaps and Jensen-Shannon divergence to compare attention patterns.", "result": "ResNet50 performed best individually, but fusion of all three networks provided substantial performance gains. Heatmaps revealed that networks focus on distinct regions of images, explaining their complementarity. The method significantly outperformed previous works on UBIPr database.", "conclusion": "Combining multiple CNN architectures through fusion exploits their complementary attention patterns and significantly improves periocular verification performance, achieving new state-of-the-art results on UBIPr database."}}
{"id": "2510.26292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26292", "abs": "https://arxiv.org/abs/2510.26292", "authors": ["Lin Liu", "Guanyi Yu", "Ziying Song", "Junqiao Li", "Caiyan Jia", "Feiyang Jia", "Peiliang Wu", "Yandan Luo"], "title": "Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving", "comment": null, "summary": "Planning is a critical component of end-to-end autonomous driving. However,\nprevailing imitation learning methods often suffer from mode collapse, failing\nto produce diverse trajectory hypotheses. Meanwhile, existing generative\napproaches struggle to incorporate crucial safety and physical constraints\ndirectly into the generative process, necessitating an additional optimization\nstage to refine their outputs. To address these limitations, we propose CATG, a\nnovel planning framework that leverages Constrained Flow Matching. Concretely,\nCATG explicitly models the flow matching process, which inherently mitigates\nmode collapse and allows for flexible guidance from various conditioning\nsignals. Our primary contribution is the novel imposition of explicit\nconstraints directly within the flow matching process, ensuring that the\ngenerated trajectories adhere to vital safety and kinematic rules. Secondly,\nCATG parameterizes driving aggressiveness as a control signal during\ngeneration, enabling precise manipulation of trajectory style. Notably, on the\nNavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and\nwas honored with the Innovation Award.", "AI": {"tldr": "CATG is a novel autonomous driving planning framework that uses Constrained Flow Matching to generate diverse trajectories while incorporating safety and kinematic constraints directly into the generative process, avoiding mode collapse and eliminating the need for post-processing optimization.", "motivation": "Existing imitation learning methods suffer from mode collapse and fail to produce diverse trajectory hypotheses, while generative approaches cannot directly incorporate safety and physical constraints during generation, requiring additional optimization stages.", "method": "The framework leverages Constrained Flow Matching to explicitly model the flow matching process, allowing flexible guidance from conditioning signals. It imposes explicit constraints directly within flow matching to ensure trajectory adherence to safety and kinematic rules, and parameterizes driving aggressiveness as a control signal.", "result": "On the NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and received the Innovation Award.", "conclusion": "CATG successfully addresses the limitations of existing planning methods by enabling diverse trajectory generation while directly incorporating safety constraints during the generative process, without requiring post-processing optimization."}}
{"id": "2510.26294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26294", "abs": "https://arxiv.org/abs/2510.26294", "authors": ["Fernando Alonso-Fernandez", "Kevin Hernandez-Diaz", "Jose Maria Buades Rubio", "Josef Bigun"], "title": "Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping", "comment": "Published at IWAIPR 2025 conference", "summary": "We focus on ocular biometrics, specifically the periocular region (the area\naround the eye), which offers high discrimination and minimal acquisition\nconstraints. We evaluate three Convolutional Neural Network architectures of\nvarying depth and complexity to assess their effectiveness for periocular\nrecognition. The networks are trained on 1,907,572 ocular crops extracted from\nthe large-scale VGGFace2 database. This significantly contrasts with existing\nworks, which typically rely on small-scale periocular datasets for training\nhaving only a few thousand images. Experiments are conducted with ocular images\nfrom VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,\nand the UFPR-Periocular database, which consists of selfies captured via mobile\ndevices with user guidance on the screen. Due to the uncontrolled conditions of\nVGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from\n9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In\ncontrast, UFPR-Periocular yields significantly better performance (EERs of\n1-2%), thanks to higher image quality and more consistent acquisition\nprotocols. To the best of our knowledge, these are the lowest reported EERs on\nthe UFPR dataset to date.", "AI": {"tldr": "This paper evaluates three CNN architectures for periocular recognition using large-scale training data from VGGFace2, achieving state-of-the-art performance on the UFPR-Periocular dataset with 1-2% EER.", "motivation": "To leverage the periocular region's high discrimination capability and minimal acquisition constraints for biometric recognition, addressing limitations of small-scale datasets in existing works.", "method": "Three CNN architectures of varying depth/complexity trained on 1,907,572 ocular crops from VGGFace2, evaluated on VGGFace2-Pose and UFPR-Periocular datasets under different acquisition conditions.", "result": "VGGFace2-Pose yielded 9-15% EER due to uncontrolled conditions, while UFPR-Periocular achieved 1-2% EER - the lowest reported on this dataset, demonstrating the impact of image quality and consistent acquisition protocols.", "conclusion": "Large-scale training data significantly improves periocular recognition performance, with controlled acquisition conditions enabling state-of-the-art results, highlighting the periocular region's potential as a robust biometric modality."}}
{"id": "2510.26297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26297", "abs": "https://arxiv.org/abs/2510.26297", "authors": ["Luting Wang", "Yinghao Xiang", "Hongliang Huang", "Dongjun Li", "Chen Gao", "Si Liu"], "title": "Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology", "comment": null, "summary": "Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented\nflexibility for monitoring the Earth's surface, but their scheduling remains\nchallenging under large-scale scenarios, dynamic environments, and stringent\nconstraints. Existing methods often simplify these complexities, limiting their\nreal-world performance. We address this gap with a unified framework\nintegrating a standardized benchmark suite and a novel scheduling model. Our\nbenchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and\n$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to\n$300$ imaging tasks. These scenarios are generated via a high-fidelity\nsimulation platform, ensuring realistic satellite behavior such as orbital\ndynamics and resource constraints. Ground truth scheduling annotations are\nprovided for each scenario. To our knowledge, AEOS-Bench is the first\nlarge-scale benchmark suite tailored for realistic constellation scheduling.\nBuilding upon this benchmark, we introduce AEOS-Former, a Transformer-based\nscheduling model that incorporates a constraint-aware attention mechanism. A\ndedicated internal constraint module explicitly models the physical and\noperational limits of each satellite. Through simulation-based iterative\nlearning, AEOS-Former adapts to diverse scenarios, offering a robust solution\nfor AEOS constellation scheduling. Experimental results demonstrate that\nAEOS-Former outperforms baseline models in task completion and energy\nefficiency, with ablation studies highlighting the contribution of each\ncomponent. Code and data are provided in\nhttps://github.com/buaa-colalab/AEOSBench.", "AI": {"tldr": "A unified framework for Agile Earth Observation Satellites scheduling featuring AEOS-Bench benchmark suite with 16,410 scenarios and AEOS-Former Transformer model with constraint-aware attention.", "motivation": "Existing AEOS scheduling methods simplify complexities, limiting real-world performance under large-scale scenarios, dynamic environments, and stringent constraints.", "method": "Developed AEOS-Bench benchmark suite with 3,907 satellite assets and 16,410 scenarios, plus AEOS-Former Transformer model with constraint-aware attention mechanism and internal constraint module for physical/operational limits.", "result": "AEOS-Former outperforms baseline models in task completion and energy efficiency, with ablation studies confirming component contributions.", "conclusion": "The framework provides robust AEOS constellation scheduling solution, with code and data available for community use."}}
{"id": "2510.26304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26304", "abs": "https://arxiv.org/abs/2510.26304", "authors": ["Jelizaveta Jankowska", "Bo\u017cena Kostek", "Fernando Alonso-Fernandez", "Prayag Tiwari"], "title": "Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG", "comment": "Published at IWAIPR 2025 conference", "summary": "The subject of this work is to check how different types of music affect\nhuman emotions. While listening to music, a subjective survey and brain\nactivity measurements were carried out using an EEG helmet. The aim is to\ndemonstrate the impact of different music genres on emotions. The research\ninvolved a diverse group of participants of different gender and musical\npreferences. This had the effect of capturing a wide range of emotional\nresponses to music. After the experiment, a relationship analysis of the\nrespondents' questionnaires with EEG signals was performed. The analysis\nrevealed connections between emotions and observed brain activity.", "AI": {"tldr": "Study examines how different music genres affect human emotions using EEG measurements and surveys, finding connections between brain activity and emotional responses.", "motivation": "To understand the impact of different music genres on human emotions and demonstrate measurable brain activity patterns associated with emotional responses to music.", "method": "Used EEG helmet to measure brain activity while participants listened to different music genres, combined with subjective surveys from a diverse group of participants with varying gender and musical preferences.", "result": "Analysis revealed connections between emotions and observed brain activity, showing that different music genres elicit distinct emotional responses measurable through EEG signals.", "conclusion": "Different types of music significantly affect human emotions, and these emotional responses can be detected and correlated with specific brain activity patterns measured by EEG."}}
{"id": "2510.26315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26315", "abs": "https://arxiv.org/abs/2510.26315", "authors": ["Junlai Qiu", "Yunzhu Chen", "Hao Zheng", "Yawen Huang", "Yuexiang Li"], "title": "A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged\nand elderly people, which significantly impacts their daily lives and mental\nhealth. To improve the efficiency of clinical screening and enable the early\ndetection of DR, a variety of automated DR diagnosis systems have been recently\nestablished based on convolutional neural network (CNN) or vision Transformer\n(ViT). However, due to the own shortages of CNN / ViT, the performance of\nexisting methods using single-type backbone has reached a bottleneck. One\npotential way for the further improvements is integrating different kinds of\nbackbones, which can fully leverage the respective strengths of them\n(\\emph{i.e.,} the local feature extraction capability of CNN and the global\nfeature capturing ability of ViT). To this end, we propose a novel paradigm to\neffectively fuse the features extracted by different backbones based on the\ntheory of evidence. Specifically, the proposed evidential fusion paradigm\ntransforms the features from different backbones into supporting evidences via\na set of deep evidential networks. With the supporting evidences, the\naggregated opinion can be accordingly formed, which can be used to adaptively\ntune the fusion pattern between different backbones and accordingly boost the\nperformance of our hybrid model. We evaluated our method on two publicly\navailable DR grading datasets. The experimental results demonstrate that our\nhybrid model not only improves the accuracy of DR grading, compared to the\nstate-of-the-art frameworks, but also provides the excellent interpretability\nfor feature fusion and decision-making.", "AI": {"tldr": "Proposes an evidential fusion paradigm to combine CNN and ViT backbones for diabetic retinopathy grading, leveraging local and global feature strengths respectively.", "motivation": "Existing DR diagnosis systems using single-type backbones (CNN or ViT) have performance bottlenecks due to their respective limitations. Integrating both can leverage CNN's local feature extraction and ViT's global feature capturing capabilities.", "method": "Uses deep evidential networks to transform features from different backbones into supporting evidences, forming aggregated opinions to adaptively tune fusion patterns between CNN and ViT backbones.", "result": "Experimental results on two DR grading datasets show improved accuracy compared to state-of-the-art frameworks and provide excellent interpretability for feature fusion and decision-making.", "conclusion": "The proposed evidential fusion paradigm effectively combines CNN and ViT strengths, boosting DR grading performance while offering interpretable feature fusion and decision processes."}}
{"id": "2510.26339", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26339", "abs": "https://arxiv.org/abs/2510.26339", "authors": ["Mingyu Sung", "Seungjae Ham", "Kangwoo Kim", "Yeokyoung Yoon", "Sangseok Yun", "Il-Min Kim", "Jae-Mo Kang"], "title": "GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?", "comment": "11 pages, 6 figures. Includes supplementary material. Under review as\n  a conference paper at ICLR 2026", "summary": "Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.", "AI": {"tldr": "GLYPH-SR is a vision-language-guided diffusion framework for image super-resolution that specifically optimizes for both text legibility and perceptual quality in natural scenes, addressing the limitations of previous SR methods that treat scene-text as generic texture.", "motivation": "Scene-text in natural images carries actionable information but current SR methods optimized for distortion or perceptual metrics are insensitive to character-level errors, causing OCR failures even when the rest of the image appears sharp.", "method": "GLYPH-SR uses a Text-SR Fusion ControlNet guided by OCR data and a ping-pong scheduler that alternates between text- and scene-centric guidance, trained on synthetic corpus while keeping the main SR branch frozen.", "result": "Across SVT, SCUT-CTW1500, and CUTE80 datasets at x4 and x8 scales, GLYPH-SR improves OCR F1 by up to +15.18 percentage points over diffusion/GAN baselines while maintaining competitive perceptual quality metrics (MANIQA, CLIP-IQA, MUSIQ).", "conclusion": "GLYPH-SR simultaneously achieves high text readability and visual realism, delivering super-resolution that both looks right and reads right for practical vision system deployments."}}
{"id": "2510.26391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26391", "abs": "https://arxiv.org/abs/2510.26391", "authors": ["Igor Abramov", "Ilya Makarov"], "title": "EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models", "comment": "Demo paper", "summary": "Existing EEG-driven image reconstruction methods often overlook spatial\nattention mechanisms, limiting fidelity and semantic coherence. To address\nthis, we propose a dual-conditioning framework that combines EEG embeddings\nwith spatial saliency maps to enhance image generation. Our approach leverages\nthe Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes\nStable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals\nwith visual semantics, while a ControlNet branch conditions generation on\nsaliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves\na significant improvement in the quality of low- and high-level image features\nover existing approaches. Simultaneously, strongly aligning with human visual\nattention. The results demonstrate that attentional priors resolve EEG\nambiguities, enabling high-fidelity reconstructions with applications in\nmedical diagnostics and neuroadaptive interfaces, advancing neural decoding\nthrough efficient adaptation of pre-trained diffusion models.", "AI": {"tldr": "A dual-conditioning framework combining EEG embeddings with spatial saliency maps improves EEG-driven image reconstruction by enhancing fidelity and semantic coherence through attentional priors.", "motivation": "Existing EEG-driven image reconstruction methods lack spatial attention mechanisms, limiting fidelity and semantic coherence in generated images.", "method": "Proposes a dual-conditioning framework using Adaptive Thinking Mapper (ATM) for EEG feature extraction, fine-tunes Stable Diffusion 2.1 via LoRA for neural-visual alignment, and employs ControlNet for saliency map conditioning.", "result": "Achieves significant improvement in quality of low- and high-level image features on THINGS-EEG dataset, with strong alignment to human visual attention.", "conclusion": "Attentional priors resolve EEG ambiguities, enabling high-fidelity reconstructions for medical diagnostics and neuroadaptive interfaces through efficient adaptation of pre-trained diffusion models."}}
{"id": "2510.26412", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26412", "abs": "https://arxiv.org/abs/2510.26412", "authors": ["Xiangqing Zheng", "Chengyue Wu", "Kehai Chen", "Min Zhang"], "title": "LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation", "comment": null, "summary": "Recently text-to-video generation has made impressive progress in producing\nshort, high-quality clips, but evaluating long-form outputs remains a major\nchallenge especially when processing complex prompts. Existing benchmarks\nmostly rely on simplified prompts and focus on low-level metrics, overlooking\nfine-grained alignment with prompts and abstract dimensions such as narrative\ncoherence and thematic expression. To address these gaps, we propose\nLoCoT2V-Bench, a benchmark specifically designed for long video generation\n(LVG) under complex input conditions. Based on various real-world videos,\nLoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating\nelements like scene transitions and event dynamics. Moreover, it constructs a\nmulti-dimensional evaluation framework that includes our newly proposed metrics\nsuch as event-level alignment, fine-grained temporal consistency, content\nclarity, and the Human Expectation Realization Degree (HERD) that focuses on\nmore abstract attributes like narrative flow, emotional response, and character\ndevelopment. Using this framework, we conduct a comprehensive evaluation of\nnine representative LVG models, finding that while current methods perform well\non basic visual and temporal aspects, they struggle with inter-event\nconsistency, fine-grained alignment, and high-level thematic adherence, etc.\nOverall, LoCoT2V-Bench provides a comprehensive and reliable platform for\nevaluating long-form complex text-to-video generation and highlights critical\ndirections for future method improvement.", "AI": {"tldr": "LoCoT2V-Bench is a new benchmark for evaluating long video generation (LVG) that addresses limitations in existing benchmarks by using complex prompts and multi-dimensional evaluation metrics including event-level alignment, temporal consistency, and abstract narrative attributes.", "motivation": "Existing text-to-video benchmarks rely on simplified prompts and focus on low-level metrics, overlooking fine-grained alignment with prompts and abstract dimensions like narrative coherence and thematic expression in long-form video generation.", "method": "The authors propose LoCoT2V-Bench based on real-world videos with realistic complex prompts containing scene transitions and event dynamics. They construct a multi-dimensional evaluation framework with new metrics including event-level alignment, fine-grained temporal consistency, content clarity, and Human Expectation Realization Degree (HERD) for narrative flow and emotional response.", "result": "Evaluation of nine representative LVG models shows that while current methods perform well on basic visual and temporal aspects, they struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence.", "conclusion": "LoCoT2V-Bench provides a comprehensive platform for evaluating long-form complex text-to-video generation and highlights critical directions for future method improvement, particularly in handling complex narrative structures and thematic consistency."}}
{"id": "2510.26441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26441", "abs": "https://arxiv.org/abs/2510.26441", "authors": ["Shihab Aaqil Ahamed", "Udaya S. K. P. Miriya Thanthrige", "Ranga Rodrigo", "Muhammad Haris Khan"], "title": "A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models", "comment": "23 pages, 14 figures", "summary": "Test-time prompt tuning (TPT) has emerged as a promising technique for\nadapting large vision-language models (VLMs) to unseen tasks without relying on\nlabeled data. However, the lack of dispersion between textual features can hurt\ncalibration performance, which raises concerns about VLMs' reliability,\ntrustworthiness, and safety. Current TPT approaches primarily focus on\nimproving prompt calibration by either maximizing average textual feature\ndispersion or enforcing orthogonality constraints to encourage angular\nseparation. However, these methods may not always have optimal angular\nseparation between class-wise textual features, which implies overlooking the\ncritical role of angular diversity. To address this, we propose A-TPT, a novel\nTPT framework that introduces angular diversity to encourage uniformity in the\ndistribution of normalized textual features induced by corresponding learnable\nprompts. This uniformity is achieved by maximizing the minimum pairwise angular\ndistance between features on the unit hypersphere. We show that our approach\nconsistently surpasses state-of-the-art TPT methods in reducing the aggregate\naverage calibration error while maintaining comparable accuracy through\nextensive experiments with various backbones on different datasets. Notably,\nour approach exhibits superior zero-shot calibration performance on natural\ndistribution shifts and generalizes well to medical datasets. We provide\nextensive analyses, including theoretical aspects, to establish the grounding\nof A-TPT. These results highlight the potency of promoting angular diversity to\nachieve well-dispersed textual features, significantly improving VLM\ncalibration during test-time adaptation. Our code will be made publicly\navailable.", "AI": {"tldr": "A-TPT is a test-time prompt tuning framework that improves vision-language model calibration by maximizing angular diversity between class-wise textual features on the unit hypersphere.", "motivation": "Current TPT methods lack optimal angular separation between class-wise textual features, which hurts calibration performance and raises concerns about VLMs' reliability and safety.", "method": "Introduces angular diversity by maximizing the minimum pairwise angular distance between normalized textual features on the unit hypersphere to achieve uniform distribution of features.", "result": "Consistently surpasses state-of-the-art TPT methods in reducing aggregate average calibration error while maintaining comparable accuracy, with superior zero-shot calibration on distribution shifts and medical datasets.", "conclusion": "Promoting angular diversity achieves well-dispersed textual features, significantly improving VLM calibration during test-time adaptation."}}
{"id": "2510.26443", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26443", "abs": "https://arxiv.org/abs/2510.26443", "authors": ["Rhodri Guerrier", "Adam W. Harley", "Dima Damen"], "title": "PointSt3R: Point Tracking through 3D Grounded Correspondence", "comment": "http://rhodriguerrier.github.io/PointSt3R", "summary": "Recent advances in foundational 3D reconstruction models, such as DUSt3R and\nMASt3R, have shown great potential in 2D and 3D correspondence in static\nscenes. In this paper, we propose to adapt them for the task of point tracking\nthrough 3D grounded correspondence. We first demonstrate that these models are\ncompetitive point trackers when focusing on static points, present in current\npoint tracking benchmarks ($+33.5\\%$ on EgoPoints vs. CoTracker2). We propose\nto combine the reconstruction loss with training for dynamic correspondence\nalong with a visibility head, and fine-tuning MASt3R for point tracking using a\nrelatively small amount of synthetic data. Importantly, we only train and\nevaluate on pairs of frames where one contains the query point, effectively\nremoving any temporal context. Using a mix of dynamic and static point\ncorrespondences, we achieve competitive or superior point tracking results on\nfour datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\%\nocclusion acc. for PointSt3R compared to 75.7 / 88.3\\% for CoTracker2; and\nsignificantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs\n82.8). We also present results on 3D point tracking along with several\nablations on training datasets and percentage of dynamic correspondences.", "AI": {"tldr": "The paper adapts foundational 3D reconstruction models (DUSt3R/MASt3R) for point tracking using 3D grounded correspondence, achieving competitive results on multiple benchmarks while requiring only frame pairs without temporal context.", "motivation": "To leverage recent advances in 3D reconstruction models for point tracking tasks, addressing limitations of existing methods and improving performance on both static and dynamic point tracking.", "method": "Fine-tune MASt3R for point tracking using synthetic data, combine reconstruction loss with dynamic correspondence training, add visibility head, and train/evaluate on frame pairs without temporal context.", "result": "Achieves competitive or superior performance on four datasets: TAP-Vid-DAVIS (73.8 \u03b4_avg / 85.8% occlusion acc), significantly outperforms CoTracker3 on EgoPoints (61.3 vs 54.2) and RGB-S (87.0 vs 82.8).", "conclusion": "3D reconstruction models can be effectively adapted for point tracking, achieving strong performance with minimal temporal context and showing particular strength on static points (+33.5% on EgoPoints)."}}
{"id": "2510.26464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26464", "abs": "https://arxiv.org/abs/2510.26464", "authors": ["Yuanting Fan", "Jun Liu", "Xiaochen Chen", "Bin-Bin Gao", "Jian Li", "Yong Liu", "Jinlong Peng", "Chengjie Wang"], "title": "Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection", "comment": "12 pages, 7 figures", "summary": "Few-shot anomaly detection (FSAD) methods identify anomalous regions with few\nknown normal samples. Most existing methods rely on the generalization ability\nof pre-trained vision-language models (VLMs) to recognize potentially anomalous\nregions through feature similarity between text descriptions and images.\nHowever, due to the lack of detailed textual descriptions, these methods can\nonly pre-define image-level descriptions to match each visual patch token to\nidentify potential anomalous regions, which leads to the semantic misalignment\nbetween image descriptions and patch-level visual anomalies, achieving\nsub-optimal localization performance. To address the above issues, we propose\nthe Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and\nfine-grained textual descriptions for existing anomaly detection datasets with\nautomatic construction pipeline. Based on the MFSC, we propose a novel\nframework named FineGrainedAD to improve anomaly localization performance,\nwhich consists of two components: Multi-Level Learnable Prompt (MLLP) and\nMulti-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics\ninto multi-level learnable prompts through automatic replacement and\nconcatenation mechanism, while MLSA designs region aggregation strategy and\nmulti-level alignment training to facilitate learnable prompts better align\nwith corresponding visual regions. Experiments demonstrate that the proposed\nFineGrainedAD achieves superior overall performance in few-shot settings on\nMVTec-AD and VisA datasets.", "AI": {"tldr": "FineGrainedAD improves few-shot anomaly detection by providing multi-level fine-grained textual descriptions to address semantic misalignment between image descriptions and patch-level visual anomalies.", "motivation": "Existing FSAD methods suffer from semantic misalignment due to lack of detailed textual descriptions, leading to sub-optimal localization performance when using pre-defined image-level descriptions.", "method": "Proposes Multi-Level Fine-Grained Semantic Caption (MFSC) with automatic construction pipeline, and a framework with Multi-Level Learnable Prompt (MLLP) for fine-grained semantics and Multi-Level Semantic Alignment (MLSA) for better visual-text alignment.", "result": "Achieves superior overall performance in few-shot settings on MVTec-AD and VisA datasets compared to existing methods.", "conclusion": "Fine-grained multi-level textual descriptions significantly improve anomaly localization performance in few-shot anomaly detection by addressing semantic misalignment issues."}}
{"id": "2510.26466", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26466", "abs": "https://arxiv.org/abs/2510.26466", "authors": ["Pei Peng", "MingKun Xie", "Hang Hao", "Tong Jin", "ShengJun Huang"], "title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition", "comment": null, "summary": "Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.", "AI": {"tldr": "A causal inference approach to address object-context shortcuts in vision-language models by synthesizing counterfactual embeddings and estimating Total Direct Effect to improve zero-shot reliability.", "motivation": "Object-context shortcuts undermine zero-shot reliability in vision-language models when test scenes differ from training co-occurrences, creating biased predictions.", "method": "Estimate object and background expectations in CLIP's representation space, synthesize counterfactual embeddings by recombining object features with diverse alternative contexts, and use Total Direct Effect to subtract background-only activation.", "result": "Substantially improves both worst-group and average accuracy on context-sensitive benchmarks, establishing new zero-shot state of the art without retraining or prompt design.", "conclusion": "Provides a lightweight representation-level counterfactual approach for debiased and reliable multimodal reasoning through causal inference."}}
{"id": "2510.26474", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26474", "abs": "https://arxiv.org/abs/2510.26474", "authors": ["Xin Guo", "Zhiheng Xi", "Yiwen Ding", "Yitao Zhai", "Xiaowei Shi", "Xunliang Cai", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing", "comment": "Preprint", "summary": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.", "AI": {"tldr": "The paper identifies a \"Matthew effect\" in self-improvement of large vision-language models where models excel at simple queries but struggle with complex ones, leading to imbalanced optimization. It proposes distribution-reshaping and trajectory-resampling strategies to re-balance head-tail data and improve visual reasoning.", "motivation": "Current self-improvement paradigms for LVLMs create an imbalance where models prioritize simple reasoning tasks over complex ones, leading to performance bottlenecks and hindered improvement over iterations.", "method": "Four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, designed to achieve head-tail re-balancing during the exploration-and-learning self-improvement process.", "result": "Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models show consistent improvement in visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.", "conclusion": "The proposed head-tail re-balancing strategies effectively counteract the Matthew effect in self-improvement, enabling better optimization across both simple and complex reasoning tasks."}}
{"id": "2510.26509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26509", "abs": "https://arxiv.org/abs/2510.26509", "authors": ["Vin\u00edcius Ferraria", "Eurico Ruivo"], "title": "Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm", "comment": null, "summary": "The edge detection task is essential in image processing aiming to extract\nrelevant information from an image. One recurring problem in this task is the\nweaknesses found in some detectors, such as the difficulty in detecting loose\nedges and the lack of context to extract relevant information from specific\nproblems. To address these weaknesses and adapt the detector to the properties\nof an image, an adaptable detector described by two-dimensional cellular\nautomaton and optimized by meta-heuristic combined with transfer learning\ntechniques was developed. This study aims to analyze the impact of expanding\nthe search space of the optimization phase and the robustness of the\nadaptability of the detector in identifying edges of a set of natural images\nand specialized subsets extracted from the same image set. The results obtained\nprove that expanding the search space of the optimization phase was not\neffective for the chosen image set. The study also analyzed the adaptability of\nthe model through a series of experiments and validation techniques and found\nthat, regardless of the validation, the model was able to adapt to the input\nand the transfer learning techniques applied to the model showed no significant\nimprovements.", "AI": {"tldr": "An adaptable edge detector using cellular automata optimized by meta-heuristics with transfer learning was developed, but expanding the search space didn't improve results and transfer learning showed no significant benefits.", "motivation": "To address weaknesses in edge detection like difficulty detecting loose edges and lack of context, and to create a detector adaptable to specific image properties.", "method": "Developed an adaptable detector using two-dimensional cellular automaton optimized by meta-heuristic combined with transfer learning techniques.", "result": "Expanding the search space in optimization phase was ineffective for the chosen image set, and transfer learning techniques showed no significant improvements despite the model's adaptability.", "conclusion": "The model demonstrated adaptability to input images, but neither search space expansion nor transfer learning provided meaningful improvements for edge detection performance."}}
{"id": "2510.26568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26568", "abs": "https://arxiv.org/abs/2510.26568", "authors": ["Hao Xie", "Zixun Huang", "Yushen Zuo", "Yakun Ju", "Frank H. F. Leung", "N. F. Law", "Kin-Man Lam", "Yong-Ping Zheng", "Sai Ho Ling"], "title": "SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging", "comment": "Accepted by Computerized Medical Imaging and Graphics (CMIG)", "summary": "Spine segmentation, based on ultrasound volume projection imaging (VPI),\nplays a vital role for intelligent scoliosis diagnosis in clinical\napplications. However, this task faces several significant challenges. Firstly,\nthe global contextual knowledge of spines may not be well-learned if we neglect\nthe high spatial correlation of different bone features. Secondly, the spine\nbones contain rich structural knowledge regarding their shapes and positions,\nwhich deserves to be encoded into the segmentation process. To address these\nchallenges, we propose a novel scale-adaptive structure-aware network\n(SA$^{2}$Net) for effective spine segmentation. First, we propose a\nscale-adaptive complementary strategy to learn the cross-dimensional\nlong-distance correlation features for spinal images. Second, motivated by the\nconsistency between multi-head self-attention in Transformers and semantic\nlevel affinity, we propose structure-affinity transformation to transform\nsemantic features with class-specific affinity and combine it with a\nTransformer decoder for structure-aware reasoning. In addition, we adopt a\nfeature mixing loss aggregation method to enhance model training. This method\nimproves the robustness and accuracy of the segmentation process. The\nexperimental results demonstrate that our SA$^{2}$Net achieves superior\nsegmentation performance compared to other state-of-the-art methods. Moreover,\nthe adaptability of SA$^{2}$Net to various backbones enhances its potential as\na promising tool for advanced scoliosis diagnosis using intelligent spinal\nimage analysis. The code and experimental demo are available at\nhttps://github.com/taetiseo09/SA2Net.", "AI": {"tldr": "SA\u00b2Net is a scale-adaptive structure-aware network for spine segmentation in ultrasound images that addresses challenges in learning global contextual knowledge and encoding structural bone features through cross-dimensional correlation learning and structure-affinity transformation.", "motivation": "Spine segmentation from ultrasound VPI is crucial for scoliosis diagnosis but faces challenges: poor learning of global contextual knowledge due to neglected spatial correlation of bone features, and insufficient encoding of rich structural knowledge about bone shapes and positions.", "method": "Proposes SA\u00b2Net with: 1) scale-adaptive complementary strategy for cross-dimensional long-distance correlation features, 2) structure-affinity transformation that combines semantic features with class-specific affinity using Transformer decoder for structure-aware reasoning, and 3) feature mixing loss aggregation for enhanced training.", "result": "SA\u00b2Net achieves superior segmentation performance compared to state-of-the-art methods, demonstrating improved robustness and accuracy in spine segmentation.", "conclusion": "SA\u00b2Net shows strong potential as a promising tool for advanced scoliosis diagnosis through intelligent spinal image analysis, with adaptability to various backbones enhancing its practical utility."}}
{"id": "2510.26569", "categories": ["cs.CV", "cs.IR", "cs.MM", "68T05", "I.4.0; H.3.1; I.2.10; K.4.4"], "pdf": "https://arxiv.org/pdf/2510.26569", "abs": "https://arxiv.org/abs/2510.26569", "authors": ["Wen Xie", "Yanjun Zhu", "Gijs Overgoor", "Yakov Bart", "Agata Lapedriza Garcia", "Sarah Ostadabbas"], "title": "AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping", "comment": "Accepted at 32nd International Conference on MultiMedia Modeling", "summary": "Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.", "AI": {"tldr": "Automated video ad clipping framework using audio-visual fusion for shot selection, outperforming existing methods on multiple metrics.", "motivation": "Manual creation of multiple ad versions at different durations is labor-intensive; need automated solution specifically for advertising context.", "method": "Two-stream audio-visual fusion model that predicts frame importance based on likelihood of selection in firm-produced ads; frames video clipping as shot selection problem.", "result": "Outperforms state-of-the-art methods across Average Precision, Area Under Curve, Spearman, and Kendall metrics.", "conclusion": "Audio plays critical role in advertising; proposed framework effectively automates video ad clipping with superior performance."}}
{"id": "2510.26580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26580", "abs": "https://arxiv.org/abs/2510.26580", "authors": ["Manjunath Prasad Holenarasipura Rajiv", "B. M. Vidyavathi"], "title": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios", "comment": "Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025", "summary": "In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.", "AI": {"tldr": "A Dynamic Context-Aware Scene Reasoning framework that uses Vision-Language Alignment for zero-shot scene understanding in unfamiliar real-world environments without labeled data.", "motivation": "AI systems struggle with unfamiliar scenarios lacking labeled data, limiting deployment in dynamic, unstructured settings. Conventional models cannot generalize across unseen contexts.", "method": "Integrates pre-trained vision transformers and large language models to align visual semantics with natural language descriptions. Uses a dynamic reasoning module that combines global scene cues and object-level interactions guided by linguistic priors.", "result": "Achieves up to 18% improvement in scene understanding accuracy over baseline models on zero-shot benchmarks (COCO, Visual Genome, Open Images). Shows robust performance in ambiguous or cluttered scenes through synergistic vision-language fusion.", "conclusion": "Provides a scalable and interpretable approach for context-aware reasoning, advancing zero-shot generalization in dynamic real-world settings."}}
{"id": "2510.26582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26582", "abs": "https://arxiv.org/abs/2510.26582", "authors": ["Xinjin Li", "Yulie Lu", "Jinghan Cao", "Yu Ma", "Zhenglin Li", "Yeyang Zhou"], "title": "CATCH: A Modular Cross-domain Adaptive Template with Hook", "comment": null, "summary": "Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains.", "AI": {"tldr": "CATCH is a plug-and-play framework for cross-domain VQA adaptation that uses lightweight domain classification and dual adapters to improve generalization without retraining backbone models.", "motivation": "Current VQA models like LLaVA perform well on natural images but degrade significantly in out-of-domain scenarios (remote sensing, medical imaging, math diagrams) due to distribution shifts and lack of effective domain adaptation mechanisms.", "method": "Decouples visual and linguistic adaptation using two lightweight modules: a domain classifier to identify input image type, and dual adapters (Prompt Adapter for language modulation and Visual Adapter for vision feature adjustment) dynamically injected via unified hook interface.", "result": "Achieves consistent performance gains across four domain-specific VQA benchmarks: +2.3 BLEU on MathVQA, +2.6 VQA on MedVQA-RAD, and +3.1 ROUGE on ChartQA without retraining the backbone model.", "conclusion": "CATCH provides a scalable and extensible approach to multi-domain VQA, enabling practical deployment across diverse application domains with minimal architectural changes."}}
{"id": "2510.26583", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26583", "abs": "https://arxiv.org/abs/2510.26583", "authors": ["Yufeng Cui", "Honghao Chen", "Haoge Deng", "Xu Huang", "Xinghang Li", "Jirong Liu", "Yang Liu", "Zhuoyan Luo", "Jinsheng Wang", "Wenxuan Wang", "Yueze Wang", "Chengyuan Wang", "Fan Zhang", "Yingli Zhao", "Ting Pan", "Xianduo Li", "Zecheng Hao", "Wenxuan Ma", "Zhuo Chen", "Yulong Ao", "Tiejun Huang", "Zhongyuan Wang", "Xinlong Wang"], "title": "Emu3.5: Native Multimodal Models are World Learners", "comment": "project page: https://emu.world", "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.", "AI": {"tldr": "Emu3.5 is a large-scale multimodal world model that predicts next states across vision and language using unified next-token prediction on 10+ trillion tokens, enhanced with reinforcement learning and accelerated inference via Discrete Diffusion Adaptation.", "motivation": "To create a unified multimodal world model that can naturally handle interleaved vision-language inputs and outputs, enabling long-horizon generation and world-modeling capabilities across diverse scenarios.", "method": "End-to-end pre-training with unified next-token prediction on vision-language interleaved data, followed by large-scale reinforcement learning post-training, and Discrete Diffusion Adaptation for efficient inference.", "result": "Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image on generation tasks, shows superior results on interleaved generation tasks, and enables 20x faster inference without performance loss.", "conclusion": "Emu3.5 demonstrates strong multimodal capabilities including world exploration and embodied manipulation, and is open-sourced to support community research."}}
{"id": "2510.26601", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26601", "abs": "https://arxiv.org/abs/2510.26601", "authors": ["Anirban Ray", "Vera Galinova", "Florian Jug"], "title": "ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching", "comment": "5 pages, 4 figures", "summary": "Computational Super-Resolution (CSR) in fluorescence microscopy has, despite\nbeing an ill-posed problem, a long history. At its very core, CSR is about\nfinding a prior that can be used to extrapolate frequencies in a micrograph\nthat have never been imaged by the image-generating microscope. It stands to\nreason that, with the advent of better data-driven machine learning techniques,\nstronger prior can be learned and hence CSR can lead to better results. Here,\nwe present ResMatching, a novel CSR method that uses guided conditional flow\nmatching to learn such improved data-priors. We evaluate ResMatching on 4\ndiverse biological structures from the BioSR dataset and compare its results\nagainst 7 baselines. ResMatching consistently achieves competitive results,\ndemonstrating in all cases the best trade-off between data fidelity and\nperceptual realism. We observe that CSR using ResMatching is particularly\neffective in cases where a strong prior is hard to learn, e.g. when the given\nlow-resolution images contain a lot of noise. Additionally, we show that\nResMatching can be used to sample from an implicitly learned posterior\ndistribution and that this distribution is calibrated for all tested use-cases,\nenabling our method to deliver a pixel-wise data-uncertainty term that can\nguide future users to reject uncertain predictions.", "AI": {"tldr": "ResMatching is a novel computational super-resolution method using guided conditional flow matching that achieves the best trade-off between data fidelity and perceptual realism, particularly effective in noisy conditions and providing uncertainty estimation.", "motivation": "Computational super-resolution in fluorescence microscopy is an ill-posed problem that requires strong priors to extrapolate missing frequencies. With advances in machine learning, better data-driven priors can be learned to improve CSR results.", "method": "ResMatching uses guided conditional flow matching to learn improved data priors for computational super-resolution. The method can sample from an implicitly learned posterior distribution and provides pixel-wise uncertainty estimation.", "result": "ResMatching consistently achieved competitive results on 4 diverse biological structures from BioSR dataset against 7 baselines, showing the best trade-off between data fidelity and perceptual realism. It performs particularly well with noisy low-resolution images and provides calibrated uncertainty estimates.", "conclusion": "ResMatching is an effective CSR method that learns strong data priors through flow matching, delivers high-quality super-resolution with uncertainty quantification, and excels in challenging scenarios with noisy inputs."}}
{"id": "2510.26609", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.26609", "abs": "https://arxiv.org/abs/2510.26609", "authors": ["Shayan Nejadshamsi", "Yuanyuan Zhang", "Shadi Zaki", "Brock Porth", "Lysa Porth", "Vahab Khoshdel"], "title": "CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing", "comment": null, "summary": "Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.", "AI": {"tldr": "CYPRESS is a deep learning model for high-resolution canola yield prediction using satellite imagery and foundation models, outperforming existing methods.", "motivation": "Traditional crop yield prediction methods lack scalability and granularity needed for precision farming, creating a need for more detailed agricultural monitoring tools.", "method": "Fine-tunes Prithvi-EO-2.0-600M geospatial foundation model for continuous regression, transforming multi-temporal satellite imagery into pixel-level yield maps.", "result": "Demonstrates superior performance over existing deep learning models on Canadian Prairies dataset, providing continuous high-resolution yield predictions.", "conclusion": "Validates approach of bridging large-scale Earth observation with on-farm decision-making, offering scalable solution for precision agriculture."}}
{"id": "2510.26614", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26614", "abs": "https://arxiv.org/abs/2510.26614", "authors": ["Christoffer Koo \u00d8hrstr\u00f8m", "Ronja G\u00fcldenring", "Lazaros Nalpantidis"], "title": "Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras", "comment": null, "summary": "We propose tokenization of events and present a tokenizer, Spiking Patches,\nspecifically designed for event cameras. Given a stream of asynchronous and\nspatially sparse events, our goal is to discover an event representation that\npreserves these properties. Prior works have represented events as frames or as\nvoxels. However, while these representations yield high accuracy, both frames\nand voxels are synchronous and decrease the spatial sparsity. Spiking Patches\ngives the means to preserve the unique properties of event cameras and we show\nin our experiments that this comes without sacrificing accuracy. We evaluate\nour tokenizer using a GNN, PCN, and a Transformer on gesture recognition and\nobject detection. Tokens from Spiking Patches yield inference times that are up\nto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We\nachieve this while matching their accuracy and even surpassing in some cases\nwith absolute improvements up to 3.8 for gesture recognition and up to 1.4 for\nobject detection. Thus, tokenization constitutes a novel direction in\nevent-based vision and marks a step towards methods that preserve the\nproperties of event cameras.", "AI": {"tldr": "Spiking Patches tokenizer preserves event camera properties (asynchronous, sparse) while matching or exceeding frame/voxel accuracy with 3.4-10.4x faster inference.", "motivation": "Existing event representations (frames, voxels) lose the asynchronous and spatially sparse properties of event cameras, which are their key advantages.", "method": "Propose Spiking Patches tokenizer that converts event streams into tokens preserving asynchronous and sparse properties, evaluated with GNN, PCN, and Transformer on gesture recognition and object detection.", "result": "3.4x faster than voxels, 10.4x faster than frames while matching accuracy; absolute improvements up to 3.8% for gesture recognition and 1.4% for object detection.", "conclusion": "Tokenization is a novel direction for event-based vision that preserves event camera properties without sacrificing accuracy."}}
{"id": "2510.26630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26630", "abs": "https://arxiv.org/abs/2510.26630", "authors": ["Bingcong Huo", "Zhiming Wang"], "title": "PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus", "comment": null, "summary": "To address the challenges in UAV object detection, such as complex\nbackgrounds, severe occlusion, dense small objects, and varying lighting\nconditions,this paper proposes PT-DETR based on RT-DETR, a novel detection\nalgorithm specifically designed for small objects in UAV imagery. In the\nbackbone network, we introduce the Partially-Aware Detail Focus (PADF) Module\nto enhance feature extraction for small objects. Additionally,we design the\nMedian-Frequency Feature Fusion (MFFF) module,which effectively improves the\nmodel's ability to capture small-object details and contextual information.\nFurthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box\nmatching capability and increase its sensitivity to small-object features,\nthereby further enhancing detection accuracy and robustness. Compared with\nRT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the\nVisDrone2019 dataset with lower computational complexity and fewer parameters,\ndemonstrating its robustness and feasibility for small-object detection tasks.", "AI": {"tldr": "PT-DETR improves UAV small object detection by enhancing feature extraction with PADF and MFFF modules, and using Focaler-SIoU for better bounding box matching, achieving higher mAP with lower complexity.", "motivation": "Address challenges in UAV object detection including complex backgrounds, severe occlusion, dense small objects, and varying lighting conditions.", "method": "Based on RT-DETR, introduces Partially-Aware Detail Focus (PADF) Module for enhanced small object feature extraction, Median-Frequency Feature Fusion (MFFF) module for better detail and context capture, and Focaler-SIoU for improved bounding box matching.", "result": "Achieves mAP improvements of 1.6% and 1.7% on VisDrone2019 dataset compared to RT-DETR, with lower computational complexity and fewer parameters.", "conclusion": "PT-DETR demonstrates robustness and feasibility for small-object detection tasks in UAV imagery."}}
{"id": "2510.26641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26641", "abs": "https://arxiv.org/abs/2510.26641", "authors": ["Sayed Pedram Haeri Boroujeni", "Niloufar Mehrabi", "Hazim Alzorgan", "Ahmad Sarlak", "Mahlagha Fazeli", "Abolfazl Razi"], "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles", "comment": null, "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.", "AI": {"tldr": "This survey provides a forward-looking analysis of object detection in Autonomous Vehicles, focusing on emerging AI paradigms like Vision-Language Models, Large Language Models, and Generative AI rather than outdated techniques.", "motivation": "Knowledge in autonomous vehicle object detection remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence, creating a need to bridge this gap with a comprehensive survey.", "method": "Systematic review of AV sensors and fusion strategies, structured categorization of AV datasets (ego-vehicle, infrastructure-based, cooperative), and analysis of cutting-edge detection methodologies including 2D/3D pipelines and transformer-driven approaches.", "result": "The survey synthesizes perspectives on current capabilities, open challenges, and future opportunities in AV object detection, with particular attention to emerging transformer-based methods and multimodal integration.", "conclusion": "The paper delivers a clear roadmap for advancing object detection in autonomous vehicles by integrating recent AI advances with traditional sensor fusion approaches."}}
{"id": "2510.26653", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.26653", "abs": "https://arxiv.org/abs/2510.26653", "authors": ["Daniela Martin", "Joseph Gallego"], "title": "Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2", "comment": null, "summary": "Accurate estimation of sea ice drift is critical for Arctic navigation,\nclimate research, and operational forecasting. While optical flow, a computer\nvision technique for estimating pixel wise motion between consecutive images,\nhas advanced rapidly in computer vision, its applicability to geophysical\nproblems and to satellite SAR imagery remains underexplored. Classical optical\nflow methods rely on mathematical models and strong assumptions about motion,\nwhich limit their accuracy in complex scenarios. Recent deep learning based\napproaches have substantially improved performance and are now the standard in\ncomputer vision, motivating their application to sea ice drift estimation. We\npresent the first large scale benchmark of 48 deep learning optical flow models\non RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and\nFl all metrics against GNSS tracked buoys. Several models achieve sub kilometer\naccuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the\nspatial scales of sea ice motion and typical navigation requirements in the\nArctic. Our results demonstrate that the models are capable of capturing\nconsistent regional drift patterns and that recent deep learning based optical\nflow methods, which have substantially improved motion estimation accuracy\ncompared to classical methods, can be effectively transferred to polar remote\nsensing. Optical flow produces spatially continuous drift fields, providing\nmotion estimates for every image pixel rather than at sparse buoy locations,\noffering new opportunities for navigation and climate modeling.", "AI": {"tldr": "Deep learning optical flow models achieve sub-kilometer accuracy for sea ice drift estimation from SAR imagery, outperforming classical methods and providing spatially continuous motion fields.", "motivation": "Accurate sea ice drift estimation is critical for Arctic navigation and climate research, but traditional optical flow methods have limitations in complex scenarios. Deep learning approaches have shown superior performance in computer vision, motivating their application to satellite SAR imagery.", "method": "Conducted the first large-scale benchmark of 48 deep learning optical flow models on RADARSAT-2 ScanSAR sea ice imagery, evaluated using endpoint error (EPE) and F1-all metrics against GNSS-tracked buoys.", "result": "Several models achieved sub-kilometer accuracy (EPE 6-8 pixels, 300-400m), which is small relative to spatial scales of sea ice motion. Models captured consistent regional drift patterns and demonstrated substantial improvement over classical methods.", "conclusion": "Deep learning optical flow methods can be effectively transferred to polar remote sensing, providing spatially continuous drift fields for every image pixel rather than sparse buoy locations, offering new opportunities for navigation and climate modeling."}}
{"id": "2510.26681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26681", "abs": "https://arxiv.org/abs/2510.26681", "authors": ["Courtney M. King", "Daniel D. Leeds", "Damian Lyons", "George Kalaitzis"], "title": "Improving Classification of Occluded Objects through Scene Context", "comment": null, "summary": "The presence of occlusions has provided substantial challenges to\ntypically-powerful object recognition algorithms. Additional sources of\ninformation can be extremely valuable to reduce errors caused by occlusions.\nScene context is known to aid in object recognition in biological vision. In\nthis work, we attempt to add robustness into existing Region Proposal\nNetwork-Deep Convolutional Neural Network (RPN-DCNN) object detection networks\nthrough two distinct scene-based information fusion techniques. We present one\nalgorithm under each methodology: the first operates prior to prediction,\nselecting a custom object network to use based on the identified background\nscene, and the second operates after detection, fusing scene knowledge into\ninitial object scores output by the RPN. We demonstrate our algorithms on\nchallenging datasets featuring partial occlusions, which show overall\nimprovement in both recall and precision against baseline methods. In addition,\nour experiments contrast multiple training methodologies for occlusion\nhandling, finding that training on a combination of both occluded and\nunoccluded images demonstrates an improvement over the others. Our method is\ninterpretable and can easily be adapted to other datasets, offering many future\ndirections for research and practical applications.", "AI": {"tldr": "This paper presents two scene-based information fusion techniques to improve object detection robustness against occlusions in RPN-DCNN networks, showing improved recall and precision on challenging datasets.", "motivation": "Occlusions pose significant challenges to object recognition algorithms, and scene context can provide valuable additional information to reduce errors caused by occlusions.", "method": "Two distinct scene-based fusion techniques: 1) selecting custom object networks based on identified background scene (pre-prediction), and 2) fusing scene knowledge into initial object scores from RPN (post-detection). Also explores training methodologies using both occluded and unoccluded images.", "result": "Demonstrated overall improvement in both recall and precision against baseline methods on challenging datasets with partial occlusions. Training on combination of occluded and unoccluded images showed improvement over other training approaches.", "conclusion": "The method is interpretable, easily adaptable to other datasets, and offers promising directions for future research and practical applications in occlusion-robust object detection."}}
{"id": "2510.26684", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26684", "abs": "https://arxiv.org/abs/2510.26684", "authors": ["Vaibhav Kurrey", "Sivakalyan Pujari", "Gagan Raj Gupta"], "title": "Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill", "comment": null, "summary": "We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.", "AI": {"tldr": "A machine vision system using deep learning for real-time anomaly detection in steel rolling mills, integrating camera data with sensor inputs to predict equipment failures and enable proactive maintenance.", "motivation": "To reduce unplanned breakdown costs and improve operational reliability in industrial manufacturing by enabling early prediction of equipment failures and process interruptions.", "method": "Integration of industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time. Live video streams are processed on a centralized video server using deep learning models, with joint analysis of sensor data from data acquisition systems and visual inputs.", "result": "The system enables early prediction of equipment failures, identifies location and root causes of failures, reduces computational load on industrial PLCs, and supports scalable deployment across production lines with minimal additional resources.", "conclusion": "This integrated machine vision approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments by providing actionable insights for proactive maintenance."}}
{"id": "2510.26694", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.26694", "abs": "https://arxiv.org/abs/2510.26694", "authors": ["Bernhard Kerbl"], "title": "The Impact and Outlook of 3D Gaussian Splatting", "comment": "Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2025", "summary": "Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed\nthe landscape of 3D scene representations, inspiring an extensive body of\nassociated research. Follow-up work includes analyses and contributions that\nenhance the efficiency, scalability, and real-world applicability of 3DGS. In\nthis summary, we present an overview of several key directions that have\nemerged in the wake of 3DGS. We highlight advances enabling resource-efficient\ntraining and rendering, the evolution toward dynamic (or four-dimensional,\n4DGS) representations, and deeper exploration of the mathematical foundations\nunderlying its appearance modeling and rendering process. Furthermore, we\nexamine efforts to bring 3DGS to mobile and virtual reality platforms, its\nextension to massive-scale environments, and recent progress toward\nnear-instant radiance field reconstruction via feed-forward or distributed\ncomputation. Collectively, these developments illustrate how 3DGS has evolved\nfrom a breakthrough representation into a versatile and foundational tool for\n3D vision and graphics.", "AI": {"tldr": "A comprehensive overview of 3D Gaussian Splatting's evolution, covering efficiency improvements, dynamic representations, mathematical foundations, mobile/VR applications, large-scale environments, and fast reconstruction methods.", "motivation": "To summarize the rapid development and diverse research directions that have emerged following the introduction of 3D Gaussian Splatting as a transformative 3D scene representation method.", "method": "Survey and analysis of key research directions including: resource-efficient training/rendering, dynamic 4D representations, mathematical foundations exploration, mobile/VR platform adaptation, large-scale environment extension, and fast reconstruction techniques.", "result": "Identified major advancements that have transformed 3DGS from a breakthrough representation into a versatile foundational tool for 3D vision and graphics applications.", "conclusion": "3D Gaussian Splatting has evolved significantly across multiple dimensions, establishing itself as a foundational and versatile tool in 3D vision and graphics with broad applicability and ongoing innovation."}}
{"id": "2510.26769", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26769", "abs": "https://arxiv.org/abs/2510.26769", "authors": ["Anushka Sivakumar", "Andrew Zhang", "Zaber Hakim", "Chris Thomas"], "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models", "comment": null, "summary": "This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.", "AI": {"tldr": "SteerVLM introduces a lightweight steering module that guides Vision-Language Models to better follow instructions by dynamically adjusting activations between language and image modalities, requiring only 0.14% of the original model's parameters.", "motivation": "To enable fine-grained control over VLM outputs during inference without modifying model weights, while preserving performance on other tasks and avoiding manual intervention tuning.", "method": "Learns from latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting language modality with image context, using dimension-wise activation modulation and adaptive steering across layers.", "result": "Outperforms existing intervention techniques on steering and hallucination mitigation benchmarks, and introduces VNIA dataset for VLM steering evaluation.", "conclusion": "Proposes a robust solution for multimodal model control through activation engineering that enables inference-time control over complex output semantics while maintaining off-target task performance."}}
{"id": "2510.26778", "categories": ["cs.CV", "cs.LG", "eess.IV", "68T07, 68T05, 68T45, 92C55", "I.2.6; J.3"], "pdf": "https://arxiv.org/pdf/2510.26778", "abs": "https://arxiv.org/abs/2510.26778", "authors": ["Valentyna Starodub", "Mantas Luko\u0161evi\u010dius"], "title": "Surpassing state of the art on AMD area estimation from RGB fundus images through careful selection of U-Net architectures and loss functions for class imbalance", "comment": null, "summary": "Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.", "AI": {"tldr": "This paper presents an AMD lesion detection framework using semantic segmentation on RGB fundus images, achieving state-of-the-art performance on the ADAM challenge benchmark.", "motivation": "AMD is a leading cause of irreversible vision impairment in elderly people, and there's a need for effective detection methods using non-invasive, cost-effective RGB fundus imaging.", "method": "Used U-Net as base framework, evaluated various improvements including pre-processing techniques, different encoder backbone networks of varying complexity, and specialized loss functions to handle class imbalances at image and pixel levels.", "result": "The final framework configuration outperformed all prior ADAM challenge submissions for multi-class segmentation of different AMD lesion types in RGB fundus images.", "conclusion": "The proposed AMD detection framework achieves superior performance for lesion segmentation in non-invasive fundus images, with source code made publicly available."}}
{"id": "2510.26781", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26781", "abs": "https://arxiv.org/abs/2510.26781", "authors": ["Aniruddh Bansal", "Davit Soselia", "Dang Nguyen", "Tianyi Zhou"], "title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment", "comment": null, "summary": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.", "AI": {"tldr": "The paper introduces ChartAlign Benchmark (ChartAB) to evaluate vision-language models' chart grounding capabilities, including data extraction, element localization, and attribute recognition from diverse charts.", "motivation": "Existing VLMs lack accurate perception of details and struggle with fine-grained structure extraction from charts, which limits their ability to compare multiple charts and reason over them.", "method": "Developed a comprehensive benchmark with JSON template for evaluation metrics, and incorporated a two-stage inference workflow to evaluate VLMs' capability to align and compare elements across charts.", "result": "Evaluation of recent VLMs revealed perception biases, weaknesses, robustness issues, and hallucinations in chart understanding, highlighting fine-grained discrepancies among models.", "conclusion": "The findings identify specific skills that need strengthening in current VLMs for chart understanding tasks and provide insights into their limitations."}}
{"id": "2510.26786", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26786", "abs": "https://arxiv.org/abs/2510.26786", "authors": ["Cheng Zheng", "William Koch", "Baiang Li", "Felix Heide"], "title": "HEIR: Learning Graph-Based Motion Hierarchies", "comment": "Code link: https://github.com/princeton-computational-imaging/HEIR", "summary": "Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/", "AI": {"tldr": "A general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data using graph-based hierarchies and differentiable graph learning.", "motivation": "Existing methods rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting generalizability across different tasks. There's a need for adaptable, data-driven hierarchical modeling.", "method": "Represents motions using graph-based hierarchies that decompose global absolute motions into parent-inherited patterns and local motion residuals. Formulates hierarchy inference as a differentiable graph learning problem using graph neural networks.", "result": "Successfully reconstructs intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to baseline on dynamic 3D Gaussian splatting scenes.", "conclusion": "Provides an adaptable, data-driven hierarchical modeling paradigm applicable to a broad range of motion-centric tasks across computer vision, graphics, and robotics."}}
{"id": "2510.26794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26794", "abs": "https://arxiv.org/abs/2510.26794", "authors": ["Jing Lin", "Ruisi Wang", "Junzhe Lu", "Ziqi Huang", "Guorui Song", "Ailing Zeng", "Xian Liu", "Chen Wei", "Wanqi Yin", "Qingping Sun", "Zhongang Cai", "Lei Yang", "Ziwei Liu"], "title": "The Quest for Generalizable Motion Generation: Data, Model, and Evaluation", "comment": null, "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.", "AI": {"tldr": "A framework that transfers knowledge from video generation to 3D human motion generation through large-scale dataset creation, unified modeling, and comprehensive evaluation.", "motivation": "Existing 3D motion generation models face generalization bottlenecks, while video generation has shown remarkable generalization in modeling human behaviors, suggesting transferable insights.", "method": "Created ViMoGen-228K dataset with 228K motion samples, proposed ViMoGen (flow-matching diffusion transformer) with gated multimodal conditioning, and developed ViMoGen-light distilled variant and MBench evaluation benchmark.", "result": "Significantly outperforms existing approaches in both automatic and human evaluations, demonstrating improved generalization capability.", "conclusion": "The framework successfully bridges video generation and motion generation domains, providing comprehensive solutions for data, modeling, and evaluation to address generalization challenges."}}
{"id": "2510.26795", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26795", "abs": "https://arxiv.org/abs/2510.26795", "authors": ["Philipp Lindenberger", "Paul-Edouard Sarlin", "Jan Hosang", "Matteo Balice", "Marc Pollefeys", "Simon Lynen", "Eduard Trulls"], "title": "Scaling Image Geo-Localization to Continent Level", "comment": "NeurIPS 2025", "summary": "Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.", "AI": {"tldr": "A hybrid approach for fine-grained image geo-localization at continental scale using proxy classification and aerial imagery embeddings to achieve precise location within 200m for 68% of queries in Europe.", "motivation": "Standard image retrieval fails at global scale due to large data volumes and insufficient coverage, while existing scalable solutions trade off between coarse classification results and domain gap issues in cross-view retrieval.", "method": "Uses proxy classification during training to learn rich feature representations encoding location information, then combines these learned prototypes with aerial imagery embeddings to handle ground-level data sparsity.", "result": "Achieves fine-grained geo-localization across continental scale, localizing within 200m for more than 68% of queries on a dataset covering large parts of Europe.", "conclusion": "The hybrid approach enables direct, fine-grained retrieval over multi-country areas, demonstrating effective scaling of geo-localization while maintaining precision."}}
{"id": "2510.26796", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.26796", "abs": "https://arxiv.org/abs/2510.26796", "authors": ["Dongyue Lu", "Ao Liang", "Tianxin Huang", "Xiao Fu", "Yuyang Zhao", "Baorui Ma", "Liang Pan", "Wei Yin", "Lingdong Kong", "Wei Tsang Ooi", "Ziwei Liu"], "title": "SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting", "comment": "26 pages; 21 figures; 3 tables; project page:\n  https://see-4d.github.io/", "summary": "Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos.", "AI": {"tldr": "SEE4D is a pose-free video-to-4D method that uses virtual cameras and view-conditional inpainting to synthesize spatiotemporal content from casual videos without 3D supervision, outperforming trajectory-based approaches.", "motivation": "Existing video-to-4D methods require manually annotated camera poses, which are labor-intensive and brittle for in-the-wild footage. Warp-then-inpaint approaches still entangle camera motion with scene dynamics, complicating modeling and inference.", "method": "Proposes trajectory-to-camera framework with fixed virtual cameras, separating camera control from scene modeling. Uses view-conditional video inpainting trained to denoise warped images and inpaint missing regions across viewpoints. Implements spatiotemporal autoregressive inference with virtual-camera splines and overlapping windows.", "result": "Achieves superior generalization and improved performance on cross-view video generation and sparse reconstruction benchmarks compared to pose- or trajectory-conditioned baselines, as validated by quantitative metrics and qualitative assessments.", "conclusion": "SEE4D advances practical 4D world modeling from casual videos by eliminating the need for explicit 3D annotations and providing a more robust framework for synthesizing spatiotemporal content."}}
{"id": "2510.26799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26799", "abs": "https://arxiv.org/abs/2510.26799", "authors": ["Chao Feng", "Zihao Wei", "Andrew Owens"], "title": "Masked Diffusion Captioning for Visual Feature Learning", "comment": "EMNLP 2025 (Findings). Project page:\n  https://cfeng16.github.io/mdlm4vfl/", "summary": "We learn visual features by captioning images with an image-conditioned\nmasked diffusion language model, a formulation we call masked diffusion\ncaptioning (MDC). During training, text tokens in each image-caption pair are\nmasked at a randomly chosen ratio, and a decoder conditioned on visual features\nis trained to reconstruct the original text. After training, the learned visual\nfeatures can be applied to downstream vision tasks. Unlike autoregressive\ncaptioning, the strength of the visual learning signal in MDC does not depend\non each token's position in the sequence, reducing the need for auxiliary\nobjectives. Linear probing experiments across a variety of academic-scale\nmodels and datasets show that the learned visual features are competitive with\nthose produced by autoregressive and contrastive approaches.", "AI": {"tldr": "Masked diffusion captioning (MDC) learns visual features by reconstructing masked text tokens from images using diffusion models, achieving competitive performance with autoregressive and contrastive methods.", "motivation": "To develop a visual feature learning approach that doesn't depend on token position like autoregressive methods and reduces the need for auxiliary objectives.", "method": "Train an image-conditioned masked diffusion language model where text tokens in image-caption pairs are randomly masked, and a visual-conditioned decoder reconstructs the original text.", "result": "Linear probing experiments show MDC produces visual features competitive with autoregressive and contrastive approaches across various models and datasets.", "conclusion": "MDC provides an effective alternative for visual feature learning that decouples visual signal strength from token position and reduces auxiliary objective requirements."}}
{"id": "2510.26800", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26800", "abs": "https://arxiv.org/abs/2510.26800", "authors": ["Yukun Huang", "Jiwen Yu", "Yanning Zhou", "Jianan Wang", "Xintao Wang", "Pengfei Wan", "Xihui Liu"], "title": "OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes", "comment": "Project page: https://yukun-huang.github.io/OmniX/", "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.", "AI": {"tldr": "OmniX is a unified framework that repurposes 2D generative models for panoramic perception of geometry, textures, and PBR materials to create graphics-ready 3D scenes suitable for physically based rendering, relighting, and simulation.", "motivation": "To advance panorama-based 2D lifting techniques beyond appearance generation and enable the creation of physically realistic 3D environments with intrinsic properties like geometry and materials.", "method": "Uses a lightweight cross-modal adapter structure to reuse 2D generative priors for panoramic vision tasks including perception, generation, and completion. Built a large-scale synthetic panorama dataset with multimodal panoramas from diverse scenes.", "result": "Extensive experiments demonstrate effectiveness in panoramic visual perception and graphics-ready 3D scene generation, enabling physically realistic virtual world creation.", "conclusion": "OmniX opens new possibilities for immersive and physically realistic virtual world generation by unifying panoramic perception and generation capabilities."}}
{"id": "2510.26802", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26802", "abs": "https://arxiv.org/abs/2510.26802", "authors": ["Ziyu Guo", "Xinyan Chen", "Renrui Zhang", "Ruichuan An", "Yu Qi", "Dongzhi Jiang", "Xiangtai Li", "Manyuan Zhang", "Hongsheng Li", "Pheng-Ann Heng"], "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark", "comment": "Project Page: https://video-cof.github.io", "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io", "AI": {"tldr": "Video generation models like Veo-3 show emerging visual reasoning capabilities but are not yet reliable as standalone zero-shot reasoners, exhibiting strengths in short-horizon spatial coherence but limitations in long-horizon causal reasoning.", "motivation": "To investigate whether current video generation models can serve as zero-shot reasoners in challenging visual reasoning scenarios, given their demonstrated capabilities in producing high-fidelity, temporally coherent videos.", "method": "Conducted empirical study evaluating Veo-3 across 12 reasoning dimensions (spatial, geometric, physical, temporal, embodied logic) using MME-CoF benchmark for Chain-of-Frame reasoning assessment.", "result": "Models show promising reasoning in short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, but struggle with long-horizon causal reasoning, strict geometric constraints, and abstract logic.", "conclusion": "Current video models are not reliable as standalone zero-shot reasoners but show potential as complementary visual engines alongside dedicated reasoning models."}}
