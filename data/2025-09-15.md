<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: ASOS is a new 3D dataset of 50 common Australian supermarket items with high-quality textured meshes for robotics and computer vision benchmarking.


<details>
  <summary>Details</summary>
Motivation: Existing datasets use synthetic models or specialized objects with limited accessibility, so there's a need for cost-effective, real-world objects that are readily available.

Method: 50 supermarket items from 10 categories were scanned using structure-from-motion techniques with high-resolution imaging to create watertight 3D meshes.

Result: Created a comprehensive dataset of common household items with diverse shapes, sizes, and weights that can be easily sourced from Australian supermarkets.

Conclusion: ASOS provides valuable benchmarking capabilities for object detection, pose estimation, and robotics applications due to its emphasis on accessibility and real-world applicability.

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [2] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: Novel multimodal RAG framework for post-disaster housing damage assessment combining image analysis and text retrieval to improve insurance claim processing.


<details>
  <summary>Details</summary>
Motivation: Accurate housing damage evaluation after natural disasters is crucial for insurance claims and resource planning, requiring better multimodal understanding of visual damage and policy documents.

Method: Two-branch multimodal encoder with ResNet+Transformer for images and BERT for text, cross-modal attention for alignment, modal attention gating for generation control, and multi-task optimization with comparison/retrieval/generation losses.

Result: Superior performance with 9.6% improvement in Top-1 retrieval accuracy and better classification of damage severity.

Conclusion: The MM-RAG framework effectively integrates visual and textual information for improved disaster damage assessment and insurance policy matching through collaborative learning.

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [3] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: Novel ensemble framework using Gemini 2.0 Flash and custom alignment to improve LLM-based text extraction from noisy historical documents by 4% accuracy over baseline.


<details>
  <summary>Details</summary>
Motivation: To stabilize text extraction from noisy historical documents using LLMs, addressing the challenge of transcription accuracy in degraded or imperfect document images.

Method: Transcribe multiple augmented variants of each image with Gemini 2.0 Flash, then fuse outputs using a custom Needleman-Wunsch style aligner to produce consensus transcription and confidence scores.

Result: 4 percentage point improvement in transcription accuracy compared to single-shot baseline; padding and blurring most effective for accuracy, grid warp best for confidence separation.

Conclusion: The approach is simple, scalable, and immediately deployable to other document collections and transcription models, demonstrating practical utility for historical document processing.

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [4] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: MITS is the first large-scale multimodal benchmark dataset for Intelligent Traffic Surveillance, containing 170,400 real-world images with comprehensive annotations and 5 million instruction-following QA pairs. It significantly improves LMM performance in ITS applications by 27-83% across various models.


<details>
  <summary>Details</summary>
Motivation: General-domain large multimodal models (LMMs) underperform in Intelligent Traffic Surveillance due to lack of dedicated multimodal datasets, creating a need for domain-specific training data.

Method: Created MITS dataset with 170,400 real-world ITS images annotated with 8 main categories and 24 subcategories. Generated high-quality captions and 5 million instruction-following QA pairs covering five critical ITS tasks through systematic data generation pipeline.

Result: Fine-tuning mainstream LMMs on MITS dramatically improved performance: LLaVA-1.5 from 0.494 to 0.905 (+83.2%), LLaVA-1.6 from 0.678 to 0.921 (+35.8%), Qwen2-VL from 0.584 to 0.926 (+58.6%), Qwen2.5-VL from 0.732 to 0.930 (+27.0%).

Conclusion: MITS effectively bridges the gap in ITS multimodal data and significantly enhances LMM performance for traffic surveillance applications. The dataset, code, and models are released as open-source to advance both ITS and LMM research.

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [5] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: Tree-based reasoning framework for VLMs underperforms standard zero-shot prompting despite achieving high accuracy in understanding tree knowledge, though image descriptions can enhance both methods.


<details>
  <summary>Details</summary>
Motivation: To investigate whether structured, tree-based reasoning can enhance vision language model performance on fine-grained tasks and large hierarchical label spaces, which are currently understudied.

Method: Introduces a framework that decomposes classification into interpretable decisions using decision trees, evaluated on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Also explores enhancing tree prompts with LLM-generated classes and image descriptions.

Result: Tree-based reasoning consistently underperforms standard zero-shot prompting despite 98.2% accuracy in understanding tree knowledge. Adding image descriptions enhances performance for both tree-based and zero-shot methods.

Conclusion: Structured reasoning has limitations in visual classification, but findings offer insights for designing more interpretable VLM systems.

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [6] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI is a system that learns controllable world models through a 3-step cycle: probabilistic prediction, structure extraction, and integration, achieving state-of-the-art results on video tasks.


<details>
  <summary>Details</summary>
Motivation: To create richly controllable and flexibly promptable world models that can extract meaningful intermediate structures from data in a zero-shot fashion.

Method: Three-step cycle: 1) Build probabilistic graphical model (Psi) as random-access autoregressive sequence model, 2) Extract low-dimensional structures via causal inference, 3) Integrate structures as new token types for continual training.

Result: Trained on 1.4T video tokens; achieved SOTA optical flow, self-supervised depth, object segmentation; enabled useful video prediction and understanding inferences.

Conclusion: PSI creates universal prompting language capabilities through iterative structure integration, improving both data modeling and control handles.

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [7] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: First analysis of video data leakage in federated learning via gradient inversion attacks, showing feature extractors provide some protection but leakage is still possible, with super-resolution techniques enhancing attack quality.


<details>
  <summary>Details</summary>
Motivation: Federated learning's privacy protection is threatened by gradient inversion attacks that can reconstruct private training data from shared gradients. While known for images/text/tabular data, video data vulnerability remains unexamined.

Method: Evaluated two video classification approaches: pre-trained feature extractors vs raw video frame processing. Tested gradient inversion attacks across scenarios with zero, one, or multiple reference frames, using super-resolution to enhance reconstructed videos.

Result: Feature extractors offer greater resilience against attacks but leakage still occurs if classifier lacks complexity. Super-resolution techniques successfully enhance frame quality from gradient inversion. Attacks viable across all tested scenarios.

Conclusion: Video data leakage in federated learning is a viable threat that warrants further investigation, as current protection methods are insufficient against determined gradient inversion attacks.

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [8] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: Semi-supervised co-training framework combining Faster R-CNN and YOLO for object detection in retail environments, with ensemble classification and metaheuristic optimization, reducing labeling costs while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Address challenges in densely packed retail environments with limited labeled data, complex conditions, occlusion, and overlapping objects, while reducing annotation costs and adapting to frequent product/layout changes.

Method: Co-training framework with Faster R-CNN (ResNet backbone) for precise localization and YOLO (Darknet backbone) for global context, mutual pseudo-label exchange, ensemble classification (XGBoost, Random Forest, SVM), and metaheuristic hyperparameter optimization.

Result: Strong performance demonstrated on SKU-110k dataset, showing improved accuracy in scenes with occlusion and overlapping objects, highlighting scalability and practicality for real-world retail applications.

Conclusion: The proposed framework effectively reduces reliance on manual labeling, adapts to retail environment changes, and provides practical solutions for automated inventory tracking, product monitoring, and checkout systems.

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [9] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: Token Purging (PG) is a backpropagation-free test-time adaptation method for 3D point cloud classification that removes domain-shifted tokens before attention layers, achieving superior accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address performance degradation caused by distribution shifts in 3D point cloud classification without requiring iterative updates or backpropagation during test-time adaptation.

Method: Proposes Token Purging (PG) with two variants: PG-SP (uses source statistics) and PG-SF (fully source-free using CLS-token-driven adaptation). Removes tokens highly affected by domain shifts before they reach attention layers.

Result: PG-SP achieves +10.3% higher accuracy than state-of-the-art backpropagation-free methods. PG-SF sets new benchmarks for source-free adaptation. 12.4x faster and 5.5x more memory efficient than baseline.

Conclusion: Token Purging provides an effective and efficient solution for test-time adaptation in 3D point cloud classification, making it suitable for real-world deployment with superior performance and resource efficiency.

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [10] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: A novel cross-view localization method that directly matches ground-level image features with aerial images using monocular depth prior, supporting both metric and relative depth with scale-aware alignment.


<details>
  <summary>Details</summary>
Motivation: Previous methods transform ground images to bird's-eye view, causing information loss from perspective distortion and height compression, which degrades alignment quality with aerial imagery.

Method: Directly establishes correspondences between ground and aerial images, lifts matched keypoints to BEV space using monocular depth prior, and employs scale-aware Procrustes alignment for pose estimation with optional scale recovery for relative depth.

Result: Achieves superior localization performance under challenging conditions (cross-area generalization, unknown orientation) with only weak pose supervision, learns accurate feature correspondences, and works with various depth models without per-model finetuning.

Conclusion: The method provides accurate, interpretable localization with flexibility for real-world deployment, overcoming limitations of traditional BEV transformation approaches.

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [11] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: Mobile app uses smartphone cameras and AI to perform pediatric vision screening via red-eye reflex analysis, achieving 90% accuracy without specialist equipment.


<details>
  <summary>Details</summary>
Motivation: Make pediatric vision screening accessible worldwide by recreating the clinical Bruckner test using smartphone technology and AI, enabling early detection of visual impairments.

Method: Developed KidsVisionCheck app using deep neural networks trained on ophthalmologist-labeled red-eye reflex images from children's pupils.

Result: Achieved 90% accuracy on unseen test data, identified optimal data collection conditions, and provided immediate user feedback capabilities.

Conclusion: This represents a significant step toward accessible pediatric vision screening and early intervention for vision abnormalities globally using mobile technology.

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [12] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: DGFusion is a depth-guided multimodal fusion method that improves semantic perception for autonomous vehicles by dynamically adapting sensor fusion based on spatially varying depth information and sensor reliability.


<details>
  <summary>Details</summary>
Motivation: Current sensor fusion approaches treat sensor data uniformly across spatial inputs, which hinders performance in challenging conditions. Autonomous vehicles need robust perception that effectively combines complementary sensors like cameras and lidar.

Method: Proposes DGFusion network that treats multimodal segmentation as multi-task problem. Uses lidar measurements as input and depth ground truth. Includes auxiliary depth head to learn depth-aware features, encodes them into local depth tokens for attentive cross-modal fusion, and uses global condition token to dynamically adapt fusion based on sensor reliability across different depths.

Result: Achieves state-of-the-art panoptic and semantic segmentation performance on challenging MUSES and DELIVER datasets. The method effectively handles sparse and noisy lidar inputs in adverse conditions.

Conclusion: Depth-guided fusion with spatially adaptive conditioning significantly improves multimodal perception performance by accounting for varying sensor reliability across different depth ranges in complex outdoor environments.

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [13] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: Patch-based automatic rosacea detection using ResNet-18 achieves competitive accuracy while preserving patient privacy by focusing on localized facial patches instead of full images.


<details>
  <summary>Details</summary>
Motivation: Rosacea requires precise early detection for effective treatment, and there's a need for automated detection methods that can maintain patient privacy while providing accurate diagnostics.

Method: Used ResNet-18 deep learning framework with various image patches extracted from facial images in different sizes, shapes, and locations. Conducted investigation studies to evaluate how localized visual information affects model performance.

Result: Patch-based strategies achieved competitive or superior accuracy and sensitivity compared to full-image methods. The approach guides the model to focus on clinically relevant regions, enhances robustness and interpretability, and protects patient privacy.

Conclusion: The proposed patch-based strategies offer practical insights for improving automated dermatological diagnostics by balancing accuracy with privacy preservation.

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [14] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: Privacy-preserving rosacea detection using synthetic data and clinical priors with a redness-informed mask to focus on diagnostically relevant facial areas while excluding identity-revealing features.


<details>
  <summary>Details</summary>
Motivation: Rosacea is underdiagnosed and automated detection faces challenges due to diffuse symptoms, scarce labeled datasets, and privacy concerns with identifiable facial images.

Method: Uses a fixed redness-informed mask to select high red-intensity regions (cheeks, nose, forehead) and trains ResNet-18 on masked synthetic images, excluding identity-revealing features.

Result: Achieves superior performance over full-face baselines with notable gains in accuracy, recall, and F1 score on real-world test data.

Conclusion: Synthetic data combined with clinical priors enables accurate and ethical dermatological AI systems suitable for privacy-sensitive telemedicine and large-scale screening applications.

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [15] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: Ablation study of ULW framework for laparoscopic image desmoking, evaluating contributions of learnable Wiener filter and individual loss components.


<details>
  <summary>Details</summary>
Motivation: To rigorously assess the effectiveness and necessity of individual components within the ULW framework for laparoscopic image desmoking.

Method: Systematic ablation study removing the learnable Wiener filter and selectively using individual loss terms (MSE, SSIM loss, perceptual loss) from the compound loss function.

Result: All variants benchmarked on publicly available paired laparoscopic images dataset using quantitative metrics (SSIM, PSNR, MSE, CIEDE-2000) and qualitative visual comparisons.

Conclusion: The study provides comprehensive evaluation of each component's specific contribution to the overall performance of the ULW desmoking framework.

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [16] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: WAVE-DETR is a multi-modal drone detector that fuses RGB visual and acoustic signals using Deformable DETR and Wav2Vec2 architectures, achieving significant performance improvements across all drone sizes in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: To create a robust UAV detection system that works effectively under challenging environmental conditions by leveraging both visual and acoustic information, addressing limitations of single-modal approaches.

Method: Combines Deformable DETR for visual processing with Wav2Vec2 for acoustic feature extraction. Tests four fusion configurations: gated mechanism, linear layer, MLP, and cross attention. Uses Drone-vs-Bird dataset and new ARDrone dataset with 7,500+ synchronized image-audio segments.

Result: Gated fusion approach performed best, improving mAP by 11.1-15.3% for small drones across IoU thresholds 0.5-0.9. Medium and large drones also showed improvements of 3.27-5.84% mAP gain across all sizes.

Conclusion: Acoustic information significantly enhances drone detection performance, with multi-modal fusion proving particularly effective for small drone detection in real-world scenarios.

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [17] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: Surrogate supervision decouples input domain from supervision domain to improve robustness of deep learning-based deformable image registration against input variations like artifacts, field-of-view mismatch, and modality differences.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based deformable image registration achieves strong accuracy but remains sensitive to variations in input image characteristics such as artifacts, field-of-view mismatch, or modality differences.

Method: Introduces surrogate supervision which applies estimated spatial transformations to surrogate images, allowing training on heterogeneous inputs while ensuring supervision is computed in domains where similarity is well defined.

Result: Demonstrated strong resilience to input variations including inhomogeneity field, inconsistent field-of-view, and modality differences across three applications (artifact-robust brain MR, mask-agnostic lung CT, and multi-modal MR registration), while maintaining high performance on well-curated data.

Conclusion: Surrogate supervision provides a principled framework for training robust and generalizable deep learning-based registration models without increasing complexity, enabling broader applicability in diverse biomedical imaging scenarios.

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [18] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: A framework combining convolutional autoencoder and Vision Transformer improves dental age estimation accuracy and provides diagnostic insights, revealing data-centric limitations in tooth morphology variability.


<details>
  <summary>Details</summary>
Motivation: To address the 'black box' nature of deep learning models in high-stakes forensic applications like dental age estimation, and to enhance both performance and transparency.

Method: Proposed framework combining convolutional autoencoder (AE) with Vision Transformer (ViT) for automated staging of mandibular second and third molars, using latent space analysis and image reconstructions for diagnostic insights.

Result: Improved classification accuracy from 0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38 over baseline ViT. Analysis revealed performance gap is data-centric due to high intra-class morphological variability in tooth 38 dataset.

Conclusion: The framework provides both enhanced accuracy and evidence for model uncertainty, serving as a robust tool for forensic age estimation. Highlights insufficiency of single interpretability modes like attention maps and emphasizes the importance of multi-faceted diagnostic approaches.

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [19] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: SCoDA introduces self-supervised learning and geometric manifold alignment for source-free domain adaptation, outperforming existing methods without relying on supervised pre-training.


<details>
  <summary>Details</summary>
Motivation: Existing SFDA methods discard crucial geometric information about the latent manifold by relying on cosine similarity over L2-normalized feature vectors, and they depend on supervised pre-training.

Method: Initializes with self-supervised pre-trained teacher model, uses geometric manifold alignment with Space Similarity Loss, and updates teacher via EMA to prevent catastrophic forgetting.

Result: Extensive experiments show SCoDA significantly outperforms state-of-the-art SFDA methods on benchmark datasets.

Conclusion: SCoDA successfully addresses limitations of previous SFDA approaches by leveraging self-supervised learning and preserving geometric manifold information through novel alignment techniques.

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [20] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: Zero-shot cell tracking framework using SAM2 foundation model for unsupervised microscopy image analysis without manual labeling or dataset-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods require costly manual labeling and have limited generalizability across diverse microscopy datasets due to data diversity and challenges like dividing objects, low signal-to-noise ratios, and indistinct cell boundaries.

Method: Integrate Segment Anything 2 (SAM2), a large foundation model for general image/video segmentation, into the tracking pipeline as a fully-unsupervised approach that doesn't require training data or inherit dataset biases.

Result: Achieves competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos without needing dataset-specific adaptation or fine-tuning.

Conclusion: The proposed zero-shot framework successfully overcomes limitations of traditional methods by leveraging foundation models, enabling generalization across diverse microscopy datasets while eliminating manual labeling requirements.

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [21] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: A method to extend 2D multi-camera tracking systems into 3D space using depth information and point-cloud reconstruction, achieving 3rd place in AI City Challenge 2025.


<details>
  <summary>Details</summary>
Motivation: Existing MTMC systems are built for 2D tracking and replacing all components for 3D tracking from scratch is infeasible, so a solution is needed to extend current 2D systems into 3D space.

Method: Utilizes depth information to reconstruct targets in point-cloud space, performs clustering and yaw refinement for 3D box recovery, and introduces enhanced online data association using target's local ID consistency for global ID assignment.

Result: Achieved 3rd place on the leaderboard of the 2025 AI City Challenge's 3D MTMC dataset.

Conclusion: The proposed framework successfully extends any online 2D multi-camera tracking system into 3D space without requiring complete system overhaul, demonstrating practical viability for large-scale surveillance automation.

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [22] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: A zero-shot workflow using visual-language verification with generic object proposals achieves competitive REC performance without task-specific training, outperforming trained baselines.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that strong Referring Expression Comprehension performance can be achieved through workflow design rather than task-specific pretraining, reducing the need for REC-trained models.

Method: Reformulates REC as box-wise visual-language verification using COCO-clean generic detector proposals (YOLO-World) and a general-purpose VLM that answers True/False queries for each region independently.

Result: Surpasses zero-shot GroundingDINO baseline and exceeds reported results for trained GroundingDINO and GroundingDINO+CRG on RefCOCO, RefCOCO+, and RefCOCOg datasets. Verification significantly outperforms selection-based prompting.

Conclusion: Workflow design, rather than task-specific pretraining, drives strong zero-shot REC performance, with verification-based approach reducing cross-box interference and supporting abstention and multiple matches.

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [23] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: Proposes Random Projected Copy-and-Paste (RPCP) augmentation to address extreme pixel imbalance in wheat disease/insect damage segmentation, improving rare class performance while maintaining overall accuracy.


<details>
  <summary>Details</summary>
Motivation: Extreme pixel-level imbalance in wheat disease segmentation (insect damage occupies tiny fraction of pixels) causes overfitting to common classes and insufficient learning of rare classes, impairing overall segmentation performance.

Method: Extract rare insect-damage patches from training images, apply random geometric transformations, paste them in appropriate regions avoiding overlaps, and apply random projection filter to refine local features and ensure natural blending with background.

Result: Method substantially improves segmentation performance on insect damage class while maintaining or slightly enhancing accuracy on other categories.

Conclusion: Targeted augmentation effectively mitigates extreme pixel imbalance, offering a straightforward yet effective solution for agricultural segmentation problems.

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [24] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: A new HMM framework that combines uncertain identities from sporadic sources (like feeders) with tracking to improve long-term multi-object tracking performance, validated on pig tracking and MOT benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MOT approaches suffer from identity switches over time, making them unsuitable for long-term tracking needed in applications like livestock monitoring where sporadic identifications are available.

Method: Proposes a Hidden Markov Model framework that integrates uncertain identities from external sources (e.g., feeders) with tracking data to maintain consistent long-term object identities.

Result: Improves F1 score of ByteTrack on a 10-minute pig tracking dataset with 21 identifications, shows robustness to identification uncertainty, and validates performance on MOT17 and MOT20 benchmarks with both ByteTrack and FairMOT.

Conclusion: The HMM framework effectively leverages sporadic identifications to enhance long-term tracking performance and is robust to identification uncertainty, making it suitable for real-world applications like livestock monitoring.

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [25] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: Survey paper on event camera fusion with traditional frame-based capture for video restoration and 3D reconstruction tasks, covering deep learning approaches for temporal and spatial enhancement.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer low latency, low power consumption, and high capture rates, but their fusion with traditional frame-based systems can significantly benefit video restoration and 3D reconstruction tasks.

Method: Systematic review of deep learning contributions to image/video enhancement and restoration, focusing on temporal enhancement (frame interpolation, motion deblurring) and spatial enhancement (super-resolution, low-light/HDR enhancement, artifact reduction), plus 3D reconstruction with event-driven fusion.

Result: Comprehensive survey covering diverse topics with in-depth discussions on improving visual quality under challenging conditions, including compilation of openly available datasets for reproducible research.

Conclusion: The survey consolidates recent progress to inspire further research into leveraging event camera systems combined with deep learning for advanced visual media restoration and enhancement.

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [26] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: ISTASTrack is a transformer-based ANN-SNN hybrid tracker that uses ISTA adapters to effectively fuse RGB and event data for superior visual object tracking performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing ANN-based RGB-Event trackers struggle to fully exploit the sparse and asynchronous nature of event streams, and hybrid ANN-SNN architectures face challenges in effectively fusing features across heterogeneous paradigms.

Method: Two-branch model with vision transformer for RGB inputs and spiking transformer for event streams, featuring model-based ISTA adapters derived from sparse representation theory for bidirectional feature interaction, plus temporal downsampling attention for feature alignment.

Result: Achieves state-of-the-art performance on RGB-Event tracking benchmarks (FE240hz, VisEvent, COESOT, FELT) while maintaining high energy efficiency.

Conclusion: ISTASTrack demonstrates the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking, successfully bridging modality and paradigm gaps between RGB and event data processing.

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [27] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: A solar flare prediction model using multiple deep state space models with FLARE loss to handle class imbalance, achieving better performance than baselines on standard metrics.


<details>
  <summary>Details</summary>
Motivation: Accurate solar flare prediction is crucial for infrastructure protection, but current methods perform poorly due to severe class imbalance across flare classes.

Method: Proposed a solar flare prediction model based on multiple deep state space models and introduced FLARE loss (frequency & local-boundary-aware reliability loss) to improve performance under class imbalance.

Result: Outperformed baseline approaches in both Gandin-Murphy-Gerrity score and true skill statistic on a multi-wavelength solar image dataset covering a full 11-year solar activity cycle.

Conclusion: The proposed method with multiple deep state space models and FLARE loss effectively addresses class imbalance and improves solar flare prediction performance and reliability.

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [28] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: TUNI is a novel RGB-thermal semantic segmentation model that unifies feature extraction and cross-modal fusion in a single encoder, achieving competitive performance with fewer parameters and real-time inference speed.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-T models use separate encoders pre-trained on RGB images, leading to limited thermal feature extraction, suboptimal cross-modal fusion, and redundant architecture that compromises real-time efficiency.

Method: Proposes TUNI with a unified RGB-T encoder that simultaneously performs multi-modal feature extraction and fusion. Uses large-scale pre-training with RGB and pseudo-thermal data, slims down thermal branch, and introduces RGB-T local module with adaptive cosine similarity for selective feature emphasis.

Result: Achieves competitive performance with state-of-the-art models on FMB, PST900 and CART datasets, with fewer parameters and lower computational cost. Achieves 27 FPS inference speed on Jetson Orin NX.

Conclusion: TUNI successfully addresses limitations of previous RGB-T models by unifying feature extraction and fusion, demonstrating efficient real-time capability while maintaining competitive segmentation performance.

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [29] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: A novel few-part-shot font generation model that creates entire fonts using partial design elements instead of complete characters, improving efficiency and providing insights into how partial details influence character structure.


<details>
  <summary>Details</summary>
Motivation: To improve font creation efficiency by reducing the input requirement from complete character shapes to partial design elements, while also gaining understanding of how partial details affect overall character structure.

Method: Proposes a model that designs entire fonts based on partial shapes (design elements) rather than requiring complete character shapes for few-shot generation.

Result: The approach enables font generation with only partial shapes as input, making the process more efficient while maintaining the ability to create complete character sets.

Conclusion: This few-part-shot font generation method offers a more efficient alternative to traditional few-shot approaches and provides valuable insights into the relationship between partial design elements and overall character structure.

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [30] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: An efficient VIO pipeline optimized for micro/nano-UAVs using quantized feature tracking methods (SuperPoint, PX4FLOW, ORB) on RISC-V SoCs, achieving 3.65x RMSE reduction with rigid body motion model.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between high-accuracy VIO systems (traditionally requiring powerful computers) and lightweight implementations suitable for microcontrollers on small UAVs.

Method: Developed optimized VIO pipeline with quantized feature detection/tracking (SuperPoint, PX4FLOW, ORB) on RISC-V SoCs, incorporating rigid body motion model to reduce estimation errors in planar motion.

Result: Achieved average 3.65x RMSE reduction over baseline using ORB tracker on GAP9 SoC. PX4FLOW showed comparable accuracy to ORB with lower runtime for speeds below 24 pixels/frame.

Conclusion: The pipeline successfully enables high-accuracy VIO on ultra-low-power systems, making it suitable for real-time applications on micro- and nano-UAVs with significant performance improvements.

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [31] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: MLANet: Hierarchical Multi-Level Attention Network for 3D face reconstruction from single in-the-wild images using CNN with attention mechanisms and semi-supervised training.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in 3D face reconstruction from 2D images due to lack of ground-truth labeled datasets and complexity of real-world environments.

Method: Uses pre-trained hierarchical backbone network with multi-level attention mechanisms at different feature extraction stages. Employs semi-supervised training with 3DMM parameters and differentiable renderer for end-to-end training.

Result: Extensive experiments on AFLW2000-3D and MICC Florence datasets show effectiveness in 3D face reconstruction and alignment tasks, evaluated both quantitatively and qualitatively.

Conclusion: Proposed MLANet effectively reconstructs detailed facial geometry, texture, pose, and illumination parameters from single in-the-wild images using attention mechanisms and semi-supervised approach.

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [32] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: LaV-CoT is a novel multilingual visual question answering framework that combines visual chain-of-thought reasoning with multi-aspect reward optimization, achieving state-of-the-art performance on multiple benchmarks and outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual VQA approaches rely primarily on textual CoT and lack support for multilingual multimodal reasoning, limiting their real-world deployment capabilities.

Method: Uses a multi-stage reasoning pipeline with text summary, language identification, spatial captioning, and logical reasoning. Employs automated data curation and two-stage training with SFT and Language-aware GRPO guided by multi-aspect rewards.

Result: Achieves up to 9.5% accuracy improvements over similar-sized open-source baselines, surpasses 2x larger models by 2.6%, and outperforms proprietary models like GPT-4o and Gemini-2.5-flash.

Conclusion: LaV-CoT demonstrates superior multilingual visual reasoning capabilities and is validated for industrial deployment through online A/B testing, providing an effective framework for real-world applications.

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [33] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: Training-free framework using LLM to disambiguate color terms and refine text embeddings in CIELAB space for accurate color alignment in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models struggle with nuanced and compound color terms, producing images misaligned with human intent, especially in applications like fashion and design.

Method: Uses LLM to resolve ambiguous color terms, then refines text embeddings based on spatial relationships in CIELAB color space for precise color blending.

Result: Improves color alignment without compromising image quality, working without additional training or reference images.

Conclusion: Bridges the gap between text semantics and visual generation by systematically handling color ambiguity through LLM-guided embedding refinement.

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [34] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: AVI-Math is the first benchmark for evaluating multimodal mathematical reasoning in aerial vehicle imagery, covering geometry, logic, and algebra. Current VLMs struggle significantly with these tasks despite success on other benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models have not been adequately tested for mathematical reasoning in UAV-based remote sensing applications, which require precise computations for distance, area, trajectory estimations, and spatial analysis.

Method: Created AVI-Math benchmark with 3,773 high-quality vehicle-related questions from UAV views, covering 6 mathematical subjects and 20 topics. Data collected at varying altitudes and multiple angles. Evaluated 14 prominent VLMs and explored Chain-of-Thought prompting and fine-tuning techniques.

Result: Current VLMs struggle significantly with mathematical reasoning tasks in AVI-Math despite their success on previous multimodal benchmarks. Chain-of-Thought prompting and fine-tuning show promise but don't fully solve the reasoning challenges.

Conclusion: The study exposes significant limitations in VLMs' mathematical reasoning capabilities for UAV applications and provides valuable insights for advancing trustworthy VLMs in real-world UAV scenarios. The benchmark will be publicly released to support future research.

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [35] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: BEVTraj is a novel trajectory prediction framework that uses real-time sensor data in bird's-eye view space without relying on pre-built HD maps, achieving comparable performance to map-based models with greater flexibility.


<details>
  <summary>Details</summary>
Motivation: Traditional trajectory prediction methods depend on pre-built HD maps (limited to specific regions) or local map construction modules (may miss critical details or introduce errors), creating limitations in adaptability and accuracy.

Method: Operates directly in BEV space using real-time sensor data, employs deformable attention to extract context from dense BEV features, and introduces Sparse Goal Candidate Proposal module for end-to-end prediction without post-processing.

Result: Extensive experiments show BEVTraj achieves performance comparable to state-of-the-art HD map-based models while offering greater flexibility by eliminating dependency on pre-built maps.

Conclusion: BEVTraj provides an effective alternative to map-dependent approaches, demonstrating that real-time sensor data in BEV space can achieve similar prediction accuracy without the limitations of pre-built maps.

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [36] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: A novel multi-view training framework improves multi-human parsing performance in occlusion scenarios by leveraging weak supervision and multi-view consistency loss, achieving 4.20% relative improvement over baseline.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art multi-human parsing methods struggle significantly with overlapping/occluded human bodies, as overlapping people appear separated from different viewpoints.

Method: Proposes a training framework that exploits multi-view information using weak supervision on human instances and a multi-view consistency loss. Uses semi-automatic annotation strategy to generate human instance segmentation masks from multi-view RGB+D data and 3D human skeletons.

Result: The approach achieves up to 4.20% relative improvement on human parsing over the baseline model in occlusion scenarios.

Conclusion: Multi-view information integration through weak supervision and consistency loss effectively enhances multi-human parsing performance under challenging occlusion conditions.

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [37] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0 is an open-weight bilingual vision-language model for Korean and English that improves upon previous models with multi-image understanding, layout-aware OCR, and enhanced multimodal capabilities through a four-stage curriculum training approach.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced bilingual vision-language model that can handle complex multimodal inputs like documents, charts, and tables while supporting both Korean and English languages with improved spatial understanding and safety features.

Method: Four-stage curriculum training with memory-efficient techniques, supporting multi-image understanding and layout-aware OCR that predicts both text content and spatial location. Includes preference optimization for safety.

Result: Achieves strong spatial grounding and competitive results in both languages, with the 14B model ranking 8th on OpenCompass VLM leaderboard among comparable models. Also releases a 1.7B version for on-device deployment.

Conclusion: VARCO-VISION-2.0 advances bilingual VLM development with practical applications, offering both full-scale (14B) and lightweight (1.7B) variants available on Hugging Face.

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [38] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: Lightweight face image quality assessment method using ensemble of MobileNetV3-Small and ShuffleNetV2 with correlation-aware loss, achieving high accuracy with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing FIQA methods are either too generic (failing to capture face-specific degradations) or computationally intensive, limiting practical deployment in real-world face recognition systems.

Method: Integrates ensemble of two compact CNNs (MobileNetV3-Small and ShuffleNetV2) with prediction-level fusion via averaging. Uses MSECorrLoss combining MSE with Pearson correlation regularizer to align with human perceptual judgments.

Result: Achieves SRCC of 0.9829 and PLCC of 0.9894 on VQualA FIQA benchmark while remaining within competition efficiency constraints.

Conclusion: Proposed method provides strong balance between accuracy and computational efficiency, making it suitable for real-world deployment in face recognition systems.

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [39] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: RCOD is a one-step diffusion framework for real-world image super-resolution that enables explicit control over fidelity-realism trade-offs through latent domain grouping and degradation-aware sampling.


<details>
  <summary>Details</summary>
Motivation: One-step diffusion methods for super-resolution lack flexible control mechanisms to balance fidelity and realism across diverse scenarios, unlike multi-step methods that can adjust sampling steps.

Method: Proposes a Realism Controlled One-step Diffusion (RCOD) framework with latent domain grouping strategy, degradation-aware sampling, and visual prompt injection module to replace text prompts with degradation-aware visual tokens.

Result: Achieves superior fidelity and perceptual quality while maintaining computational efficiency, outperforming state-of-the-art OSD methods in both quantitative metrics and visual qualities.

Conclusion: RCOD provides flexible realism control capabilities during inference while maintaining the efficiency of one-step diffusion methods for real-world image super-resolution tasks.

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [40] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: Grad-CL is a source-free domain adaptation framework for optic disc and cup segmentation that uses gradient-guided pseudolabel refinement and contrastive learning to improve cross-domain performance without accessing source data.


<details>
  <summary>Details</summary>
Motivation: Segmentation models trained on one dataset perform poorly on target data from different imaging protocols/conditions, creating a need for domain adaptation methods that don't require original source data access.

Method: Two-stage approach: 1) Gradient-guided pseudolabel refinement using class-specific feature extraction and uncertainty quantification, 2) Cosine similarity-based contrastive learning to enforce inter-class separability between optic cup and disc features.

Result: Outperforms state-of-the-art unsupervised and source-free domain adaptation methods on cross-domain fundus imaging datasets, achieving superior segmentation accuracy and improved boundary delineation.

Conclusion: Grad-CL provides an effective source-free domain adaptation solution for medical image segmentation that maintains performance across different imaging conditions without requiring access to original training data.

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [41] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: VQBridge is a novel method that achieves 100% codebook usage in vector quantization networks through a compress-process-recover pipeline, enabling stable training and superior reconstruction performance for image generation tokenizers.


<details>
  <summary>Details</summary>
Motivation: Traditional VQ training suffers from instability due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, leading to suboptimal reconstruction and low codebook usage.

Method: Proposes VQBridge - a robust projector based on map function method that optimizes code vectors through compress-process-recover pipeline, combined with learning annealing for stable codebook training.

Result: Achieves 100% codebook usage even with 262k-codebook, state-of-the-art reconstruction performance, and consistent improvement with larger codebooks. When integrated with LlamaGen, surpasses VAR by 0.5 and DiT by 0.2 rFID.

Conclusion: FVQ demonstrates that high-quality tokenizers with full codebook usage are crucial for strong autoregressive image generation, providing an effective, scalable, and generalizable solution to VQ training challenges.

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [42] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: LayerLock is a self-supervised learning method that progressively freezes ViT layers during training to accelerate masked autoencoding and enable effective latent prediction without representation collapse.


<details>
  <summary>Details</summary>
Motivation: The authors observed that ViT layers converge sequentially during video masked-autoencoding training (shallower layers first, deeper layers later), and sought to exploit this pattern to improve training efficiency and enable latent prediction.

Method: Progressive layer freezing according to an explicit schedule based on layer convergence order. The approach starts with pixel prediction and gradually transitions to latent prediction as layers are frozen.

Result: Applied to models up to 4B parameters, LayerLock surpassed non-latent masked prediction methods on the 4DS perception suite benchmark.

Conclusion: Progressive layer freezing provides an effective and scalable approach for self-supervised visual representation learning that accelerates training and enables stable latent prediction without representation collapse.

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [43] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: Appearance embeddings improve photometric quality but not geometric accuracy in 3D reconstruction for space robotics. Convex splatting provides more compact representations than Gaussian splatting for safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: To understand the role of appearance embeddings in space-based 3D object reconstruction and evaluate their impact on geometric accuracy for space robotics applications where precise geometry is critical.

Method: Systematic comparison of implicit (K-Planes) and explicit (Gaussian Splatting, Convex Splatting) Novel View Synthesis methods using the SPEED+ dataset, analyzing the effects of appearance embeddings on photometric fidelity and geometric accuracy.

Result: Appearance embeddings improve photometric fidelity by modeling lighting variation but do not enhance geometric accuracy. Convex splatting achieves more compact and clutter-free representations than Gaussian splatting.

Conclusion: Appearance embeddings have limited benefits for geometry-centric tasks in space scenarios. Convex splatting offers advantages for safety-critical applications like interaction and collision avoidance due to its efficient representation.

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [44] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: GAMMA is a novel training framework that improves AI-generated image detection by reducing domain bias and enhancing semantic alignment through diverse manipulation strategies and multi-task supervision.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detectors perform well on in-distribution images but generalize poorly to unseen generative models due to reliance on generation-specific artifacts like stylistic priors and compression patterns.

Method: Proposes GAMMA framework with diverse manipulation strategies (inpainting-based manipulation, semantics-preserving perturbations), multi-task supervision with dual segmentation heads and classification head, and a reverse cross-attention mechanism to correct biased representations.

Result: Achieves state-of-the-art generalization performance on GenImage benchmark with 5.8% accuracy improvement and maintains strong robustness on newly released generative models like GPT-4o.

Conclusion: GAMMA effectively addresses generalization limitations in AI-generated image detection by reducing domain bias and improving semantic alignment through innovative manipulation strategies and architectural design.

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [45] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: Comparative analysis of three super-resolution reconstruction methods (NiftyMIC, SVRTK, NeSVoR) for fetal brain MRI shows NeSVoR achieves highest reconstruction success rates (>90%) across healthy and pathological cases, with diagnostic classification performance remaining unaffected despite volumetric measurement differences between methods.


<details>
  <summary>Details</summary>
Motivation: Fetal brain MRI suffers from low resolution, motion artifacts, and inadequate 3D anatomy capture. Existing super-resolution reconstruction methods lack comprehensive comparative evaluation, especially for pathological cases and their impact on downstream diagnostic tasks.

Method: Applied three state-of-the-art SRR methods (NiftyMIC, SVRTK, NeSVoR) to 140 fetal brain MRI scans including healthy controls and pathological cases with ventriculomegaly. Used BoUNTi algorithm for segmentation of nine brain structures and evaluated visual quality, success rates, volumetric agreement, and diagnostic classification.

Result: NeSVoR demonstrated highest reconstruction success rate (>90%) across both healthy and pathological groups. Significant differences in volumetric estimates were observed between SRR methods, but classification performance for ventriculomegaly was not affected by the choice of SRR method.

Conclusion: NeSVoR shows superior robustness in fetal brain MRI reconstruction, and diagnostic performance remains resilient despite volumetric variability introduced by different super-resolution reconstruction methods.

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [46] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: Proposes Mask Consistency Regularization (MCR) to address mask hallucination and mask-shape bias in object removal image inpainting using diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based inpainting methods suffer from mask hallucination (generating irrelevant content) and mask-shape bias (filling mask with shape-matching objects rather than contextual content).

Method: Introduces MCR training strategy with two mask perturbations: dilation to align output with surrounding content, and reshape to break mask-shape bias. Enforces consistency between outputs of perturbed branches and original mask.

Result: MCR significantly reduces hallucinations and mask-shape bias, leading to improved object removal performance with more robust and contextually coherent results.

Conclusion: The proposed Mask Consistency Regularization effectively addresses key challenges in object removal tasks, producing higher quality inpainting results by enforcing consistency across mask perturbations.

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [47] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: MagicMirror is a comprehensive framework for assessing physical artifacts in text-to-image generation, featuring a detailed artifact taxonomy, large-scale human-annotated dataset, and a trained VLM for automated artifact evaluation.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models suffer from persistent physical artifacts (anatomical and structural flaws) that degrade perceptual quality, but lack systematic evaluation frameworks to address these issues.

Method: Developed a detailed artifact taxonomy, created MagicData340K (340K human-annotated images), trained MagicAssessor VLM with novel data sampling and multi-level reward system using GRPO, and built MagicBench automated benchmark.

Result: Evaluation revealed that even top-tier T2I models like GPT-image-1 consistently suffer from significant artifacts, demonstrating the framework's effectiveness in identifying model weaknesses.

Conclusion: Artifact reduction remains a critical challenge for T2I development, and MagicMirror provides an essential systematic evaluation framework to drive future improvements in generated image quality.

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [48] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: SignClip improves sign language translation by incorporating both manual (hand gestures) and non-manual (lip movements) cues using hierarchical contrastive learning, achieving state-of-the-art results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Most current sign language translation approaches focus only on manual signals (hand gestures) and overlook non-manual cues like mouthing, which are essential for disambiguating visually similar signs and conveying linguistic information.

Method: Proposes SignClip framework that fuses spatial gesture and lip movement features, and introduces hierarchical contrastive learning with multi-level alignment objectives to ensure semantic consistency across sign-lip and visual-text modalities.

Result: Extensive experiments on PHOENIX14T and How2Sign datasets show superiority. On PHOENIX14T (gloss-free setting), SignClip improves BLEU-4 from 24.32 to 24.71 and ROUGE from 46.57 to 48.38 compared to previous state-of-the-art SpaMo.

Conclusion: SignClip demonstrates that incorporating both manual and non-manual cues significantly improves sign language translation accuracy, highlighting the importance of mouthing information in disambiguating signs and achieving better translation performance.

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [49] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: Analysis of VLMs for text manipulation detection shows open-source models are improving but still lag behind closed-source models like GPT-4o, with generalization issues in specialized models.


<details>
  <summary>Details</summary>
Motivation: To bridge the knowledge gap in text manipulation detection using VLMs, as previous studies focused mainly on image manipulation detection.

Method: Benchmarking closed- and open-source VLMs on different text manipulation datasets, including in-the-wild scene texts and fantasy ID cards that mimic real-world misuse scenarios.

Result: Open-source models are getting closer but still behind closed-source models like GPT-4o. Image manipulation detection-specific VLMs suffer from generalization problems when applied to text manipulation tasks.

Conclusion: While progress is being made, there remains a performance gap between open-source and closed-source VLMs for text manipulation detection, and specialized models need better generalization capabilities.

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [50] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: MCL-AD is a novel zero-shot 3D anomaly detection framework that leverages multimodal collaboration across point clouds, RGB images, and text semantics through prompt learning and contrastive mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus only on point clouds and neglect rich semantic cues from complementary modalities like RGB images and text priors, which limits performance in zero-shot 3D anomaly detection scenarios.

Method: Proposes Multimodal Prompt Learning Mechanism (MPLM) with object-agnostic decoupled text prompts and multimodal contrastive loss, plus Collaborative Modulation Mechanism (CMM) to jointly modulate RGB image-guided and point cloud-guided branches.

Result: Extensive experiments demonstrate state-of-the-art performance in zero-shot 3D anomaly detection.

Conclusion: MCL-AD framework effectively leverages multimodal collaboration to achieve superior zero-shot 3D anomaly detection by integrating point clouds, RGB images, and text semantics.

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [51] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: Lipschitz-guided stochastic depth method with depth-dependent drop probabilities improves robustness while maintaining accuracy and reducing computation in Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks and Vision Transformers achieve state-of-the-art performance but are highly vulnerable to adversarial perturbations, while standard defenses often incur high computational cost or lack formal guarantees.

Method: Propose a Lipschitz-guided stochastic depth (DropPath) method where drop probabilities increase with depth to control the effective Lipschitz constant of the network, regularizing deeper layers.

Result: Experiments on CIFAR-10 with ViT-Tiny show maintained near-baseline clean accuracy, enhanced robustness under FGSM, PGD-20, and AutoAttack attacks, and significant FLOPs reduction compared to baseline and linear DropPath schedules.

Conclusion: The depth-dependent drop probability schedule effectively improves adversarial robustness while preserving clean performance and computational efficiency in Vision Transformers.

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [52] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: Probabilistic framework using energy maps for precise geolocation of street furniture in urban environments, integrating GIS data with stochastic optimization.


<details>
  <summary>Details</summary>
Motivation: Need for accurate monitoring and maintenance of public infrastructure by local authorities and private stakeholders in complex urban settings.

Method: Energy maps encoding spatial likelihood of object locations, integrated with external geospatial information (GIS layers, road maps, constraints) using stochastic birth-and-death optimization algorithm.

Result: Evaluated with realistic simulation based on Dublin street lighting dataset, showing potential for scalable and accurate urban asset mapping.

Conclusion: Proposed framework enables precise geolocation of street furniture by combining probabilistic energy maps with contextual geospatial constraints, with implementation made publicly available.

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [53] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: ClusCa accelerates diffusion transformers by performing spatial clustering to reduce token computation by over 90%, achieving 4.96x speedup on FLUX with improved quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion transformers suffer from high computational costs due to iterative denoising. Existing feature caching methods only leverage temporal similarity but ignore spatial similarity.

Method: Cluster-Driven Feature Caching (ClusCa) performs spatial clustering on tokens, computes only one token per cluster, and propagates information to all other tokens in the cluster.

Result: Achieves 4.96x acceleration on FLUX with 99.49% ImageReward (0.51% improvement over original), reduces tokens by over 90%, works on DiT, FLUX and HunyuanVideo without training.

Conclusion: ClusCa provides an effective orthogonal approach to existing feature caching methods by leveraging spatial similarity, enabling significant acceleration while maintaining or improving quality in text-to-image and text-to-video generation.

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [54] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: I-Segmenter is the first fully integer-only Vision Transformer for semantic segmentation that achieves efficient deployment while maintaining competitive accuracy through quantization optimization techniques.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers show strong semantic segmentation performance but are computationally expensive and memory-intensive for resource-constrained devices. Quantization helps but ViT-based segmentation models are fragile under low precision due to error accumulation.

Method: Built on Segmenter architecture, replaces floating-point operations with integer-only counterparts. Introduces λ-ShiftGELU activation to handle long-tailed distributions, removes L2 normalization, and replaces bilinear interpolation with nearest neighbor upsampling for full integer execution.

Result: Achieves accuracy within 5.1% of FP32 baseline on average, reduces model size by up to 3.8x, enables 1.2x faster inference. Works well even with one-shot PTQ using a single calibration image.

Conclusion: I-Segmenter provides a practical integer-only ViT segmentation framework that maintains competitive accuracy while significantly improving efficiency, making it suitable for real-world deployment on resource-constrained devices.

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [55] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: GARD is a novel deep learning approach using Gamma-based diffusion models for OCT image denoising, outperforming traditional methods while preserving anatomical details.


<details>
  <summary>Details</summary>
Motivation: OCT images suffer from speckle noise that obscures fine details and hinders accurate diagnosis. Existing denoising methods struggle to balance noise reduction with preservation of crucial anatomical structures.

Method: GARD uses a Denoising Diffusion Gamma Model (instead of Gaussian noise assumption) and introduces a Noise-Reduced Fidelity Term with pre-processed guidance. It adapts the Denoising Diffusion Implicit Model framework for accelerated inference.

Result: GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in PSNR, SSIM, and MSE metrics. Qualitative results show sharper edges and better preservation of fine anatomical details.

Conclusion: The Gamma-based diffusion approach with noise-reduced guidance effectively addresses OCT speckle noise while maintaining crucial anatomical information, representing a significant advancement in OCT image denoising.

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [56] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: GLAM is a novel visual language model for mammography that uses geometry-guided global and local alignment to better capture multi-view relationships between mammogram images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current mammography VLMs adapted from natural images ignore domain-specific multi-view relationships that radiologists use, losing critical geometric context and resulting in suboptimal performance.

Method: Proposes GLAM model with geometry-guided pretraining using joint global and local, visual-visual, and visual-language contrastive learning to capture cross-view alignments and fine-grained local features.

Result: Pretrained on the large EMBED mammography dataset, GLAM outperforms baseline models across multiple datasets under different evaluation settings.

Conclusion: The geometry-guided approach for multi-view mammography VLM pretraining successfully captures critical domain-specific characteristics and improves performance over existing methods.

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [57] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: Survey paper reviewing visual grounding in vision language models, covering core components, applications, benchmarks, and challenges.


<details>
  <summary>Details</summary>
Motivation: Visual grounding enables models to identify image regions matching textual descriptions, supporting applications like referring expression comprehension, visual question answering, and environment control.

Method: Survey methodology reviewing representative works across key research areas, outlining core components of grounded model development paradigm, examining practical applications and benchmarks.

Result: Comprehensive analysis of visual grounding in VLMs, delineating relationships between grounding, multimodal chain-of-thought, and reasoning capabilities.

Conclusion: Identifies challenges in visual grounding and suggests promising future research directions for improving grounded multimodal generation and reasoning.

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [58] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: Attention Attack disrupts text-based image editing by using auto-generated captions to break cross-attention alignment between images and text prompts, degrading editing performance while remaining imperceptible.


<details>
  <summary>Details</summary>
Motivation: Text-based image editing methods are vulnerable to adversarial attacks, particularly targeting the visual-textual alignment mechanism that enables fine-grained manipulation guided by natural language.

Method: Proposes Attention Attack that uses automatically generated captions of source images as proxy edit prompts to disrupt cross-attention between textual prompts and visual representations, without requiring knowledge of the editing method or prompt.

Result: Experiments on TEDBench++ benchmark show the attack significantly degrades editing performance while remaining imperceptible. Two novel evaluation metrics (Caption Similarity and semantic IoU) demonstrate effectiveness.

Conclusion: The proposed attack successfully targets the visual component of text-based image editing methods by breaking content-text alignment, revealing vulnerabilities in current editing systems and providing new evaluation strategies for immunization success.

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [59] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: Knowledge distillation reduces neural network image compression resource requirements while maintaining performance across different architectures and quality/bitrate tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Neural network-based image compression outperforms conventional codecs but requires significant processing power, making it unsuitable for real-time use on resource-constrained platforms.

Method: Leveraging knowledge distillation where smaller neural networks are trained on outputs of larger, more complex models to achieve better performance than independent training.

Result: Knowledge distillation effectively reduces processing and energy requirements for image compression across various architecture sizes while maintaining different image quality/bit rate tradeoffs.

Conclusion: Knowledge distillation is a viable approach for making neural image compression more practical for resource-constrained applications, with potential for extension to transformer-based models and exploration of different teacher models and loss functions.

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [60] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: Training-free intrinsic image decomposition using visible-thermal image pairs, leveraging thermal absorption to derive shading/reflectance ordinalities for self-supervised neural network optimization.


<details>
  <summary>Details</summary>
Motivation: Lack of extensive ground-truth data for real-world intrinsic image decomposition, with existing methods relying on synthetic data or sparse annotations for limited scenes.

Method: Uses visible-thermal image pairs, leveraging that absorbed light appears as heat in thermal images. Relates intensity ordinalities between visible and thermal images to shading/reflectance ordinalities for dense self-supervision of neural network optimization.

Result: Superior performance over recent learning-based models in quantitative evaluations with known reflectance/shading under natural/artificial lighting, and qualitative experiments across diverse outdoor scenes.

Conclusion: Provides scalable path for curating real-world ordinal supervision previously infeasible through manual labeling, demonstrating effective training-free approach for intrinsic image decomposition.

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [61] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: This paper presents a comprehensive survey on compressed video quality enhancement (CVQE) methods, addressing limitations in existing surveys by introducing a novel taxonomy, unified benchmarking framework, and systematic analysis of performance-complexity trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing CVQE surveys lack systematic classification linking methods to specific coding standards and artifacts, insufficient comparative analysis across architectural paradigms, and underdeveloped benchmarking practices, creating gaps in the field.

Method: The paper introduces three key contributions: 1) a novel taxonomy classifying CVQE methods across architectural paradigms, coding standards, and compressed-domain feature utilization; 2) a unified benchmarking framework with modern compression protocols and standard test sequences; 3) systematic analysis of reconstruction performance vs. computational complexity trade-offs.

Result: The review establishes a foundation for consistent assessment and informed model selection in CVQE research and deployment, providing comprehensive analysis of state-of-the-art methods and highlighting promising future research directions.

Conclusion: This comprehensive survey addresses critical gaps in CVQE literature by providing systematic classification, standardized benchmarking, and thorough analysis of performance-complexity trade-offs, serving as a valuable resource for researchers and practitioners in video quality enhancement.

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [62] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: MM SAM-adapter extends Segment Anything Model for multimodal semantic segmentation using adapter network to fuse auxiliary sensor data with RGB features, achieving state-of-the-art performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current semantic segmentation methods are vulnerable to challenging conditions like poor lighting, occlusions, and adverse weather. Multimodal approaches integrating auxiliary sensor data can provide complementary information to enhance robustness.

Method: Proposes an adapter network that injects fused multimodal features into SAM's RGB features, enabling retention of RGB generalization while selectively incorporating auxiliary modalities only when they provide additional cues.

Result: Achieves state-of-the-art performance on three benchmarks (DeLiVER, FMB, MUSES). Outperforms competing methods in both RGB-easy and RGB-hard subsets, demonstrating effectiveness in both favorable and adverse conditions.

Conclusion: MM SAM-adapter provides a balanced and efficient framework for multimodal semantic segmentation, effectively leveraging auxiliary modalities to enhance robustness while maintaining the strong generalization of RGB features.

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [63] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: InfGen enables arbitrary resolution image generation from fixed-size latents using a one-step generator, reducing 4K generation time from 100+ seconds to under 10 seconds without retraining diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models have quadratic computational complexity with resolution, making 4K image generation slow (over 100 seconds). There's a need for efficient arbitrary resolution generation across devices.

Method: Replaces VAE decoder with a one-step generator that decodes fixed latent representations from diffusion models into arbitrary resolution images. Uses second generation latent diffusion approach.

Result: Achieves 4K image generation in under 10 seconds (10x speedup), enables arbitrary resolution generation without retraining, and maintains compatibility with existing latent space models.

Conclusion: InfGen provides an efficient solution for arbitrary resolution image generation, significantly reducing computational complexity while maintaining quality and compatibility with existing diffusion models.

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [64] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: Self-supervised learning model for Alzheimer's prediction using 3D brain MRI with temporal order prediction and contrastive learning, outperforming supervised methods on most tasks while handling variable-length inputs and time intervals.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current deep learning models for Alzheimer's prediction including lack of labeled data, poor generalization across datasets, and inflexibility to varying numbers of input scans and time intervals.

Method: Adapted three state-of-the-art temporal SSL approaches for 3D brain MRI analysis with novel extensions for variable-length inputs and robust spatial features. Used four datasets (3,161 patients) for pre-training with temporal order prediction and contrastive learning.

Result: SSL model outperformed supervised learning on 6 out of 7 downstream tasks including diagnosis classification, conversion detection, and future conversion prediction. Demonstrated adaptability across tasks and varying input configurations.

Conclusion: The SSL approach shows superior performance, adaptability, and generalizability for Alzheimer's prediction tasks, highlighting its potential for robust clinical applications across different data configurations.

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>
