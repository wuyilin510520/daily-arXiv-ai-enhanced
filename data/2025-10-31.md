<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

TL;DR: This study evaluates spatio-temporal modeling and spatial attention mechanisms in deep learning for underwater object detection, comparing YOLOv5, T-YOLOv5, and T-YOLOv5 with CBAM, showing significant improvements in detection accuracy for complex marine environments.


<details>
  <summary>Details</summary>
Motivation: To improve underwater object detection accuracy in dynamic marine environments with challenges like sudden movements, partial occlusions, and gradual motion where standard models may underperform.

Method: Two-phase approach: first evaluating temporal-enhanced T-YOLOv5 vs standard YOLOv5, then developing an augmented T-YOLOv5 with Convolutional Block Attention Module (CBAM) to enhance spatial attention mechanisms.

Result: T-YOLOv5 and T-YOLOv5 with CBAM significantly outperformed standard YOLOv5 with mAP@50-95 scores of 0.813 and 0.811 respectively, compared to 0.563 for YOLOv5. T-YOLOv5 with CBAM showed better performance in challenging scenarios but slightly reduced accuracy in simpler scenarios.

Conclusion: Temporal modeling significantly enhances detection reliability in underwater environments, and spatial attention mechanisms further improve performance in complex scenarios, though with trade-offs in simpler detection tasks.

Abstract: This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [2] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

TL;DR: MIRO conditions text-to-image models on multiple reward models during training to directly learn user preferences, improving visual quality, training speed, and achieving state-of-the-art performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models trained on uncurated datasets don't align well with user preferences. Post-hoc reward-based selection methods discard data and harm diversity, semantic fidelity, and efficiency.

Method: Instead of post-processing, MIRO conditions the model on multiple reward models during training to directly learn user preferences.

Result: MIRO dramatically improves visual quality of generated images and significantly speeds up training. It achieves state-of-the-art performance on GenEval compositional benchmark and user-preference scores (PickAScore, ImageReward, HPSv2).

Conclusion: Directly conditioning models on multiple rewards during training is more effective than post-hoc selection for aligning text-to-image generation with user preferences.

Abstract: Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [3] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

TL;DR: This paper adapts automotive LiDAR perception for bicycle safety by creating a domain-specific dataset (BikeScenes-lidarseg) and showing that fine-tuning on this dataset significantly improves 3D LiDAR segmentation performance for bicycle-mounted systems.


<details>
  <summary>Details</summary>
Motivation: Cyclist vulnerability is increasing due to the popularity of faster e-bikes, creating a need to adapt automotive perception technologies specifically for bicycle safety applications.

Method: Developed a multi-sensor 'SenseBike' research platform and created the BikeScenes-lidarseg dataset with 3021 consecutive LiDAR scans annotated for 29 classes. Used this dataset to fine-tune 3D LiDAR segmentation models.

Result: Fine-tuning on the BikeScenes dataset achieved 63.6% mIoU, significantly outperforming the 13.8% mIoU obtained with SemanticKITTI pre-training alone, demonstrating the effectiveness of domain-specific training.

Conclusion: Domain-specific training is essential for bicycle-mounted perception systems, and the BikeScenes dataset provides a valuable resource for advancing cyclist-centric LiDAR segmentation research.

Abstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [4] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: Machine learning approach for STM image repair and super-resolution using physics-informed synthetic data, enabling faster image acquisition and reduced tip conditioning.


<details>
  <summary>Details</summary>
Motivation: STM limitations include tip degradation, slow serial data acquisition, and complexity in tip fabrication due to voltage-induced shape changes requiring conditioning.

Method: Used physics-informed synthetic data generation pipeline with only 36 pristine experimental images to train flow-matching and diffusion models for image restoration and super-resolution.

Result: Models effectively restored images with 2-4x reduction in acquisition time by reconstructing from sparse data, validated by CLIP Maximum Mean Discrepancy and structural similarity metrics.

Conclusion: The framework can significantly increase STM experimental throughput by reducing tip-conditioning frequency and enhancing frame rates in high-speed STM systems.

Abstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [5] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

TL;DR: Proposes a flow decomposition-and-aggregation framework for image editing that semantically decomposes target prompts into sub-prompts, computes independent flows for each, and aggregates them with adaptive weighting to address inversion and gradient entanglement issues in rectified flow models.


<details>
  <summary>Details</summary>
Motivation: Rectified flow models have limitations in image editing tasks including inaccurate inversion processes and gradient entanglement issues that result in outputs not faithfully reflecting target prompts. Existing ODE-based approaches without inversion still yield suboptimal editing quality.

Method: Semantically decomposes target prompt into multiple sub-prompts, computes independent flow for each, and aggregates them using projection and soft-aggregation mechanism inspired by gradient conflict resolution in multi-task learning. This adaptively weights sub-target velocity fields to suppress semantic redundancy while emphasizing distinct directions.

Result: Outperforms existing zero-shot editing approaches in terms of semantic fidelity and attribute disentanglement. The method enhances diversity in target space while maintaining consistent guidance toward full target prompt.

Conclusion: The proposed flow decomposition-and-aggregation framework effectively addresses limitations of rectified flow models in image editing by preserving both diversity and consistency in final edited outputs through adaptive weighting of semantic sub-flows.

Abstract: Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [6] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

TL;DR: Brain-IT is a brain-inspired method that uses a Brain Interaction Transformer to reconstruct images from fMRI data by enabling interactions between functionally-similar brain-voxel clusters and predicting complementary semantic and structural image features.


<details>
  <summary>Details</summary>
Motivation: Current methods for reconstructing images from fMRI brain recordings often lack faithfulness to the actual seen images, despite recent progress with diffusion models.

Method: Uses a Brain Interaction Transformer (BIT) that allows interactions between clusters of functionally-similar brain-voxels. Predicts two complementary localized patch-level image features: high-level semantic features and low-level structural features to guide diffusion model reconstruction.

Result: Achieves faithful image reconstructions from fMRI that surpass current state-of-the-art approaches both visually and by standard objective metrics. With only 1-hour of fMRI data from a new subject, achieves results comparable to methods trained on full 40-hour recordings.

Conclusion: Brain-IT's brain-inspired approach with functional clusters and direct information flow from brain-voxel clusters to image features enables efficient and faithful image reconstruction from limited fMRI data.

Abstract: Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [7] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: The paper presents a real-time tumor tracking method for cine-MRI using SAM 2.1 foundation model with mask-based prompts, achieving 0.8794 Dice score in TrackRAD2025 challenge while operating under 1-second runtime constraint.


<details>
  <summary>Details</summary>
Motivation: To address the TrackRAD2025 challenge of real-time tumor tracking in thoracic and abdominal cine-MRI sequences under strong data scarcity constraints, requiring methods that can work with limited labeled data.

Method: Used SAM 2.1 foundation model with mask-based prompts from first annotated slice, fine-tuned only on small labeled subset. Training used 1024x1024 patches, standard augmentations, balanced Dice+IoU loss, and low uniform learning rate (0.0001) across all modules to prevent overfitting.

Result: Achieved Dice score of 0.8794 on hidden test set, ranking 6th overall in TrackRAD2025 challenge. The method successfully operated within the 1-second runtime constraint and generalized across different anatomical sites and MRI field strengths.

Conclusion: Foundation models like SAM 2.1 show strong potential for accurate real-time tumor tracking in MRI-guided radiotherapy, particularly effective under data scarcity conditions while maintaining real-time performance requirements.

Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [8] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

TL;DR: Enhanced Mamba framework with Hilbert Selective Scan mechanism increases Hausdorff dimension for better feature exploration, improving low-light image enhancement with higher quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address information inconsistencies and improve spatial locality in Mamba-based methods while maintaining long-range dependency handling capabilities.

Method: Proposed Hilbert Selective Scan mechanism that increases the Hausdorff dimension of scanning patterns for more effective feature space exploration.

Result: Significantly improved quantitative metrics and qualitative visual fidelity on benchmarks, with reduced computational resources and shorter inference time.

Conclusion: The refined strategy advances low-light image enhancement state-of-the-art and shows promise for broader Mamba-based applications.

Abstract: We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [9] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

TL;DR: CAVE is the first benchmark for real-world visual anomalies with three tasks: description, explanation, and justification, using fine-grained annotations inspired by human cognitive processes.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection in computer vision is limited to industrial defects or synthetic anomalies, failing to capture the richness and unpredictability of real-world anomalies that humans can naturally identify and explain.

Method: Introduces CAVE benchmark with fine-grained annotations for visual grounding and categorizing anomalies based on visual manifestations, complexity, severity, and commonness, drawing from cognitive science research on human anomaly perception.

Result: State-of-the-art Vision-Language Models (VLMs) struggle with visual anomaly perception and commonsense reasoning, even with advanced prompting strategies.

Conclusion: CAVE serves as a valuable resource for advancing research in anomaly detection and commonsense reasoning in VLMs by providing a realistic and cognitively grounded benchmark.

Abstract: Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [10] [Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning](https://arxiv.org/abs/2510.26017)
*Bilal Hassan,Areg Karapetyan,Aaron Chung Hin Chow,Samer Madanat*

Main category: cs.CV

TL;DR: A lightweight CNN model predicts coastal flooding under sea-level rise scenarios, outperforming state-of-the-art methods by reducing MAE by 20% and generalizing across Abu Dhabi and San Francisco.


<details>
  <summary>Details</summary>
Motivation: Climate change and sea-level rise threaten coastal cities, but traditional physics-based simulators are computationally expensive for city-scale planning. Deep learning offers alternatives but faces data scarcity and high-dimensional output challenges.

Method: Developed a novel lightweight CNN-based model using a vision-based, low-resource DL framework to predict coastal flooding under variable SLR projections and shoreline adaptation scenarios.

Result: The model significantly outperforms state-of-the-art methods, reducing mean absolute error in predicted flood depth maps by nearly 20% on average, and demonstrates generalization across diverse geographical contexts.

Conclusion: The approach serves as a scalable and practical tool for coastal flood management, empowering decision-makers to develop effective mitigation strategies against climate change impacts.

Abstract: Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/

</details>


### [11] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

TL;DR: Proposes STAVEQ2, a Video-LLM architecture with stacked temporal attention modules in the vision encoder to improve temporal understanding in videos, achieving up to +5.5% improvement on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Video-LLMs struggle with complex temporal dynamics and understanding action sequences in videos, limiting their performance in video question answering tasks.

Method: Introduces stacked temporal attention modules directly within the vision encoder to capture temporal progression and relationships between frames before passing visual tokens to the LLM.

Result: Significantly improves temporal reasoning and outperforms existing models on VITATECS, MVBench, and Video-MME benchmarks by up to +5.5%, particularly in action recognition tasks.

Conclusion: Enhancing the vision encoder with temporal structure addresses a critical gap in video understanding for Video-LLMs, enabling better comprehension of temporal dynamics.

Abstract: Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [12] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

TL;DR: FlexICL is a flexible in-context learning framework for segmenting bony regions in pediatric elbow and wrist ultrasound images, achieving robust performance with only 5% labeled training data.


<details>
  <summary>Details</summary>
Motivation: Automatic segmentation of musculoskeletal structures in ultrasound can improve diagnostic accuracy for pediatric fractures, but pixel-wise expert annotations are time-consuming and costly.

Method: Proposed FlexICL framework using in-context learning for intra-video segmentation, with novel image concatenation techniques and training strategies that enhance performance with limited labeled data.

Result: Outperforms state-of-the-art visual ICL models (Painter, MAE-VQGAN) and conventional segmentation models (U-Net, TransUNet) by 1-27% Dice coefficient on 1,252 US sweeps.

Conclusion: FlexICL shows potential as an efficient and scalable solution for ultrasound image segmentation in medical imaging where labeled data is scarce.

Abstract: Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [13] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

TL;DR: Dynamic negative prompting using VLMs to generate adaptive negative prompts during denoising process


<details>
  <summary>Details</summary>
Motivation: Traditional negative prompting uses fixed prompts, which may not be contextually appropriate throughout the denoising process

Method: Generate intermediate image predictions at specific denoising steps and query VLM to produce context-aware negative prompts

Result: Evaluated on benchmark datasets, showing trade-offs between negative guidance strength and text-image alignment

Conclusion: Proposed approach enables adaptive negative prompting that responds to image context during generation

Abstract: We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [14] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

TL;DR: PReMA is a novel adversarial attack that manipulates multi-modal diffusion models by modifying input images while keeping prompts fixed, enabling generation of inappropriate content without changing text inputs.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal diffusion models have inadequate alignment between text and image modalities, creating security risks for generating NSFW content. Current attacks mainly focus on adversarial prompts, leaving image-based manipulation underexplored.

Method: Proposes Prompt-Restricted Multi-modal Attack (PReMA) that creates adversarial images to manipulate model outputs while keeping prompts unchanged. It targets image-editing applications with fixed prompts.

Result: Comprehensive evaluations on image inpainting and style transfer tasks across various models demonstrate PReMA's potent efficacy in manipulating generated content.

Conclusion: PReMA poses a novel threat to multi-modal diffusion model integrity, particularly in applications with fixed prompts, highlighting the need for better text-image alignment and security measures.

Abstract: Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [15] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: The paper introduces EgoExo-Con, a benchmark for evaluating Video-LLMs' temporal understanding consistency across egocentric and exocentric viewpoints, and proposes View-GRPO to improve cross-view consistency.


<details>
  <summary>Details</summary>
Motivation: To address the limitation that existing Video-LLMs fail to maintain consistent temporal understanding when videos capture the same event from different viewpoints (egocentric vs exocentric).

Method: Proposes View-GRPO, a novel reinforcement learning framework that strengthens view-specific temporal reasoning while encouraging consistent comprehension across viewpoints.

Result: View-GRPO demonstrates superiority over naive supervised fine-tuning (SFT) and GRPO, especially for improving cross-view consistency in temporal understanding tasks.

Conclusion: The proposed View-GRPO effectively addresses the consistency limitations of Video-LLMs across different viewpoints, outperforming existing methods in maintaining temporal understanding consistency.

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [16] [OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research](https://arxiv.org/abs/2510.26114)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,Xu Peng,Taisong Jin,Yongge Liu,Shengwei Han,Jing Yang,Xiaoping He,Feng Gao,AndyPian Wu,SevenShu,Chaoyang Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: OracleAgent is the first agent system for Oracle Bone Script (OBS) research that integrates multiple analysis tools with LLMs and uses a comprehensive multimodal knowledge base to address workflow complexity and retrieval inefficiency challenges.


<details>
  <summary>Details</summary>
Motivation: Current OBS research faces challenges: (1) complex interpretation workflow with multiple serial/parallel sub-tasks, and (2) inefficient information organization and retrieval that requires substantial effort from scholars.

Method: Developed OracleAgent system that integrates multiple OBS analysis tools empowered by LLMs and flexibly orchestrates them. Built a comprehensive domain-specific multimodal knowledge base with over 1.4M single-character rubbing images and 80K interpretation texts through multi-year data collection, cleaning, and expert annotation.

Result: OracleAgent achieves superior performance across multimodal reasoning and generation tasks, surpassing leading MLLMs like GPT-4o. Case studies show it significantly reduces time cost for OBS research and effectively assists domain experts in retrieval tasks.

Conclusion: OracleAgent represents a significant step toward practical deployment of OBS-assisted research and automated interpretation systems, demonstrating effective integration of multimodal tools and knowledge resources.

Abstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.

</details>


### [17] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

TL;DR: A unified framework that jointly optimizes 3D Gaussian points and camera poses without requiring pre-calibrated camera inputs, eliminating the need for external pose estimation tools like COLMAP.


<details>
  <summary>Details</summary>
Motivation: Traditional novel view synthesis methods rely on external camera pose estimation tools (e.g., COLMAP) which introduce computational bottlenecks and propagate errors, limiting performance in challenging scenarios.

Method: Joint optimization framework with interleaved phases: 1) update 3D Gaussian parameters via differentiable rendering with fixed poses, 2) refine camera poses using customized 3D optical flow algorithm with geometric and photometric constraints.

Result: Significantly outperforms existing COLMAP-free techniques in reconstruction quality and surpasses standard COLMAP-based baseline in general performance, particularly in challenging scenarios with large viewpoint variations and sparse features.

Conclusion: The proposed co-optimization strategy effectively eliminates dependency on external pose estimation tools while achieving superior scene reconstruction fidelity and pose accuracy through progressive error reduction.

Abstract: Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [18] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

TL;DR: WOD-E2E is a new dataset for end-to-end driving focusing on challenging long-tail scenarios, with a novel Rater Feedback Score metric for better evaluation.


<details>
  <summary>Details</summary>
Motivation: Current E2E driving benchmarks lack challenging long-tail scenarios and existing metrics don't effectively evaluate multi-modal driving performance in rare situations.

Method: Created WOD-E2E dataset with 4,021 driving segments (12 hours) of rare scenarios (<0.03% frequency), including routing info, ego states, and 360-degree camera views. Introduced Rater Feedback Score metric that measures trajectory preference alignment.

Result: Developed comprehensive dataset and evaluation framework specifically designed for testing E2E driving systems on challenging long-tail scenarios that are rare in daily driving.

Conclusion: WOD-E2E aims to advance research in generalizable, robust, and safe end-to-end autonomous driving by providing better benchmarks and evaluation methods for complex real-world situations.

Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [19] [Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM](https://arxiv.org/abs/2510.26131)
*Ali Caglayan,Nevrez Imamoglu,Oguzhan Guclu,Ali Osman Serhatoglu,Ahmet Burak Can,Ryosuke Nakamura*

Main category: cs.CV

TL;DR: The paper proposes integrating gradient-based attention information with CNN features to improve RGB-D indoor SLAM performance, particularly for large environments.


<details>
  <summary>Details</summary>
Motivation: Existing gradient-based attention methods provide visual insights but are not effectively integrated into CNN representations for semantic object understanding, which could benefit visual tasks like SLAM.

Method: Integrate layer-wise attention information derived from network gradients with CNN feature representations to enhance frame association in RGB-D indoor SLAM.

Result: Experimental results show improved performance compared to baseline methods, especially for large environments.

Conclusion: Integrating task-specific network attention with CNN features effectively enhances SLAM performance, demonstrating the value of attention mechanisms in visual tasks.

Abstract: Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.

</details>


### [20] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

TL;DR: FullPart combines implicit and explicit paradigms for 3D part generation, using implicit diffusion for bounding box layout and full-resolution voxel grids for detailed part generation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Previous methods either use implicit representations with insufficient geometric details or explicit voxel representations where small parts occupy too few voxels, leading to degraded quality.

Method: Uses implicit diffusion for bounding box layout and generates each part in its own fixed full-resolution voxel grid. Introduces center-point encoding to address misalignment between parts of different sizes.

Result: Achieves state-of-the-art results in 3D part generation, enabling synthesis of intricate details even for small parts.

Conclusion: FullPart effectively combines implicit and explicit paradigms for high-quality 3D part generation and introduces PartVerse-XL, the largest human-annotated 3D part dataset.

Abstract: Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [21] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: BasicAVSR is a strong baseline for arbitrary-scale video super-resolution that integrates adaptive multi-scale frequency priors, flow-guided propagation, second-order motion compensation, and hyper-upsampling to achieve superior performance across different scenarios.


<details>
  <summary>Details</summary>
Motivation: Arbitrary-scale video super-resolution faces challenges in spatial detail reproduction, temporal consistency, and computational complexity. The paper aims to create a versatile baseline that can handle various scaling factors while maintaining high quality and efficiency.

Method: Proposes BasicAVSR with four key components: 1) adaptive multi-scale frequency priors from image Laplacian pyramids, 2) flow-guided propagation unit for spatiotemporal aggregation, 3) second-order motion compensation for accurate frame alignment, and 4) hyper-upsampling unit for scale-aware upsampling. Three propagation variants are instantiated for different scenarios.

Result: BasicAVSR significantly outperforms existing methods in super-resolution quality, generalization ability, and inference speed. The model demonstrates effectiveness and adaptability across different scenarios including online inference, limited lookahead, and offline tasks.

Conclusion: The work advances state-of-the-art in arbitrary-scale video super-resolution and extends core components to multiple frameworks for diverse scenarios, providing a strong baseline for future research.

Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [22] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: A novel Multi-View Mammography and Language Model (MV-MLM) that uses synthetic radiology reports and multi-view supervision for breast cancer classification and risk prediction, achieving state-of-the-art performance without needing real radiology reports.


<details>
  <summary>Details</summary>
Motivation: Large annotated datasets for breast cancer CAD models are costly and time-consuming to acquire. Vision-Language Models offer a promising solution for enhancing robustness and data efficiency in medical imaging.

Method: MV-MLM uses multi-view supervision and cross-modal self-supervision across image-text pairs, including multiple mammogram views and corresponding synthetic radiology reports. A joint visual-textual learning strategy enhances generalization across different data types and tasks.

Result: The model achieves state-of-the-art performance in three classification tasks: malignancy classification, subtype classification, and image-based cancer risk prediction. It demonstrates strong data efficiency, outperforming fully supervised and VLM baselines.

Conclusion: MV-MLM effectively leverages synthetic text reports and multi-view learning for robust breast cancer analysis without requiring actual radiology reports, showing promising data efficiency and performance.

Abstract: Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [23] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: This paper presents a YOLOv8-based real-time object detection system for automatically identifying auto-rickshaws in traffic images, achieving 83.447% mAP50 and over 78% precision/recall.


<details>
  <summary>Details</summary>
Motivation: Auto-rickshaws in South Asian countries like Bangladesh need monitoring due to traffic restrictions, but existing surveillance systems struggle to distinguish them from similar vehicles like non-auto rickshaws, and manual video analysis is too slow.

Method: Used YOLOv8 model for real-time object detection, trained on a custom dataset of 1,730 annotated images captured under various traffic conditions.

Result: The model achieved 83.447% mAP50 and binary precision and recall values above 78%, demonstrating effectiveness in both dense and sparse traffic scenarios.

Conclusion: The proposed machine learning approach successfully enables automated auto-rickshaw detection in real-time traffic monitoring, with the dataset publicly released for further research.

Abstract: Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [24] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

TL;DR: CRAG-MM is a comprehensive benchmark for multi-modal RAG with 6.5K image-question-answer triplets and 2K multi-turn conversations across 13 domains, designed for wearable device scenarios with egocentric images.


<details>
  <summary>Details</summary>
Motivation: There is no comprehensive benchmark for multi-modal RAG in wearable scenarios, despite the growing importance of smart glasses and similar devices that enable visual-based information seeking.

Method: Created a diverse dataset with 6.5K triplets and 2K multi-turn conversations across 13 domains, featuring 6.2K egocentric images, various question types, image quality issues, entity popularity levels, and conversation turns. Designed three tasks with retrieval corpus and APIs.

Result: Current RAG approaches achieve only 32-43% truthfulness, while industry solutions show similar performance (32-45%), indicating significant room for improvement. The benchmark attracted 1K participants and 5K submissions in KDD Cup 2025.

Conclusion: CRAG-MM fills a critical gap in multi-modal RAG benchmarking and demonstrates substantial performance improvement potential, with winning solutions already showing 28% improvement over baselines.

Abstract: Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [25] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

TL;DR: MoTDiff is a high-resolution motion trajectory estimation framework using diffusion models that extracts fine-grained motion information from single motion-blurred images, outperforming state-of-the-art methods in blind image deblurring and coded exposure photography.


<details>
  <summary>Details</summary>
Motivation: Existing motion representations from single blurred images are often low quality (coarse-grained and inaccurate), limiting performance in computational imaging and computer vision applications that require precise motion information.

Method: Proposes MoTDiff framework with: 1) conditional diffusion using multi-scale feature maps from blurred images as condition, and 2) training method promoting fine-grained trajectory identification, consistent shape/position estimation, and pixel connectivity.

Result: Outperforms state-of-the-art methods in both blind image deblurring and coded exposure photography applications.

Conclusion: MoTDiff successfully enables high-quality, high-resolution motion trajectory estimation from single motion-blurred images using diffusion models, demonstrating superior performance over existing approaches.

Abstract: Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [26] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

TL;DR: ConceptScope is an automated framework that uses Sparse Autoencoders on vision foundation models to discover and quantify human-interpretable concepts in datasets, enabling systematic bias identification and dataset analysis without requiring fine-grained annotations.


<details>
  <summary>Details</summary>
Motivation: Dataset bias is widespread in machine learning but challenging to identify without costly manual annotations. There's a need for scalable, automated methods to systematically discover and analyze these biases.

Method: Uses Sparse Autoencoders trained on representations from vision foundation models to discover interpretable concepts. Categorizes concepts into target, context, and bias types based on semantic relevance and statistical correlation to class labels. Enables class-level dataset characterization and concept-based subgrouping for robustness evaluation.

Result: Validated to capture diverse visual concepts (objects, textures, backgrounds, facial attributes, emotions, actions). Concept activations produce spatial attributions aligned with meaningful image regions. Successfully detected known biases (e.g., background bias in Waterbirds) and uncovered previously unannotated biases (e.g., co-occurring objects in ImageNet).

Conclusion: ConceptScope provides a practical tool for automated dataset auditing and model diagnostics, offering scalable bias detection without requiring expensive manual annotations.

Abstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [27] [Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction](https://arxiv.org/abs/2510.26196)
*Li Wang,Yiyu Zhuang,Yanwen Wang,Xun Cao,Chuan Guo,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: Proposes a novel approach for 3D human pose estimation from sketches using a "learn from synthesis" strategy with diffusion models to create synthetic sketch-pose data, achieving superior accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in sketch-to-3D pose estimation due to abstract and disproportionate sketch nature, overcoming limitations of previous optimization-based methods that lack large-scale annotated data.

Method: Uses diffusion model to synthesize sketches from 2D poses, creates SKEP-120K synthetic dataset, combines 2D pose detectors with diffusion priors and feed-forward network, incorporates heuristic loss functions for geometric coherence.

Result: Qualitative, quantitative, and subjective evaluations show substantial improvements in both estimation accuracy and speed compared to previous methods.

Conclusion: The proposed data-driven framework effectively handles diverse sketch styles and outperforms existing sketch-to-pose estimation approaches.

Abstract: 3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.

</details>


### [28] [Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management](https://arxiv.org/abs/2510.26203)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.CV

TL;DR: A novel Chebyshev ensemble geometric network (Ch-EGN) is proposed for supply chain risk management and sustainability, achieving high accuracy in delivery status prediction (98.95%), product classification (100%), and relationship classification (98.07%-92.37%).


<details>
  <summary>Details</summary>
Motivation: Supply chain sustainability requires effective risk management and product classification to optimize performance. Deep learning approaches can leverage information dependencies in supply chain data to identify invisible states and patterns.

Method: Proposed Chebyshev ensemble geometric network (Ch-EGN) - a hybrid convolutional and geometric deep learning approach that captures information dependencies in supply chain networks. Evaluated on SupplyGraph Dataset and DataCo databases.

Result: Achieved 98.95% accuracy for delivery status prediction (risk management), 100% accuracy for 5 product group classification, 98.07% for 4 product relation classification, and 92.37% for 25 company relation classification. Outperformed state-of-the-art methods.

Conclusion: The proposed Ch-EGN ensemble network effectively enhances supply chain sustainability through accurate risk management and classification tasks, demonstrating superior performance compared to existing approaches.

Abstract: The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.

</details>


### [29] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

TL;DR: The paper introduces OmniLayout-1M, a million-scale diverse document layout dataset, and OmniLayout-LLM, a 0.5B model with coarse-to-fine learning for document layout generation.


<details>
  <summary>Details</summary>
Motivation: Document layout generation is underexplored compared to layout analysis, with existing datasets dominated by academic papers and lacking diversity in open-world genres like newspapers and magazines.

Method: Two-stage coarse-to-fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse categories, 2) transferring knowledge to specific domains with fine-grained annotations.

Result: Achieves strong performance on multiple domains in M$^{6}$Doc dataset, substantially surpassing existing layout generation experts and latest general-purpose LLMs.

Conclusion: The proposed approach effectively addresses the scarcity of diverse layouts and improves document layout generation across multiple domains.

Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [30] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

TL;DR: VLMs struggle with temporal reasoning, performing poorly on judging video playback direction (forward/backward) compared to humans, especially for irreversible physical processes and causal actions.


<details>
  <summary>Details</summary>
Motivation: To evaluate and expose the weak temporal understanding in modern vision-language models (VLMs) by testing their ability to judge the arrow of time in videos, using human behavioral baselines.

Method: Created AoT-PsyPhyBENCH benchmark with psychophysically validated stimuli to test VLMs' temporal direction inference in natural videos, evaluating both open-weight and proprietary models.

Result: Most VLMs performed near chance level, with even the best models significantly lagging behind human accuracy on physically irreversible processes and causal manual actions.

Conclusion: Current VLMs lack essential inductive biases for temporal continuity and causal understanding despite capturing visual-semantic correlations, revealing a fundamental gap in multimodal systems.

Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [31] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

TL;DR: HCLFuse is a novel infrared and visible image fusion method inspired by human cognitive laws, using multi-scale mask-regulated variational bottleneck encoder and diffusion model with physical guidance to achieve state-of-the-art fusion performance.


<details>
  <summary>Details</summary>
Motivation: Existing fusion methods struggle with balancing modal information and lack interpretability in modal selection, affecting reliability in complex scenarios. Generative methods have limited capabilities.

Method: Uses multi-scale mask-regulated variational bottleneck encoder for information decomposition and diffusion model with time-varying physical guidance mechanism to regulate generation process.

Result: Achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets, significantly improving semantic segmentation metrics.

Conclusion: The human cognition-inspired generative fusion method enhances structural consistency and detail quality, demonstrating advantages in reliable image fusion.

Abstract: Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [32] [Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances](https://arxiv.org/abs/2510.26282)
*Fernando Alonso-Fernandez,Kevin Hernandez Diaz,Jose M. Buades,Kiran Raja,Josef Bigun*

Main category: cs.CV

TL;DR: Analysis of CNN complementarity for periocular verification at different distances, showing that fusion of multiple architectures (SqueezeNet, MobileNetv2, ResNet50) achieves state-of-the-art performance on UBIPr database.


<details>
  <summary>Details</summary>
Motivation: To study how different CNN architectures complement each other for periocular verification, particularly at varying distances, and leverage their complementary strengths through fusion techniques.

Method: Trained three CNN architectures (SqueezeNet, MobileNetv2, ResNet50) on eye crops from VGGFace2, analyzed performance with cosine and chi2 metrics, compared network initializations, applied score-level fusion via logistic regression, and used LIME heatmaps and Jensen-Shannon divergence to compare attention patterns.

Result: ResNet50 performed best individually, but fusion of all three networks provided substantial performance gains. Heatmaps revealed that networks focus on distinct regions of images, explaining their complementarity. The method significantly outperformed previous works on UBIPr database.

Conclusion: Combining multiple CNN architectures through fusion exploits their complementary attention patterns and significantly improves periocular verification performance, achieving new state-of-the-art results on UBIPr database.

Abstract: We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.

</details>


### [33] [Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving](https://arxiv.org/abs/2510.26292)
*Lin Liu,Guanyi Yu,Ziying Song,Junqiao Li,Caiyan Jia,Feiyang Jia,Peiliang Wu,Yandan Luo*

Main category: cs.CV

TL;DR: CATG is a novel autonomous driving planning framework that uses Constrained Flow Matching to generate diverse trajectories while incorporating safety and kinematic constraints directly into the generative process, avoiding mode collapse and eliminating the need for post-processing optimization.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning methods suffer from mode collapse and fail to produce diverse trajectory hypotheses, while generative approaches cannot directly incorporate safety and physical constraints during generation, requiring additional optimization stages.

Method: The framework leverages Constrained Flow Matching to explicitly model the flow matching process, allowing flexible guidance from conditioning signals. It imposes explicit constraints directly within flow matching to ensure trajectory adherence to safety and kinematic rules, and parameterizes driving aggressiveness as a control signal.

Result: On the NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and received the Innovation Award.

Conclusion: CATG successfully addresses the limitations of existing planning methods by enabling diverse trajectory generation while directly incorporating safety constraints during the generative process, without requiring post-processing optimization.

Abstract: Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.

</details>


### [34] [Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping](https://arxiv.org/abs/2510.26294)
*Fernando Alonso-Fernandez,Kevin Hernandez-Diaz,Jose Maria Buades Rubio,Josef Bigun*

Main category: cs.CV

TL;DR: This paper evaluates three CNN architectures for periocular recognition using large-scale training data from VGGFace2, achieving state-of-the-art performance on the UFPR-Periocular dataset with 1-2% EER.


<details>
  <summary>Details</summary>
Motivation: To leverage the periocular region's high discrimination capability and minimal acquisition constraints for biometric recognition, addressing limitations of small-scale datasets in existing works.

Method: Three CNN architectures of varying depth/complexity trained on 1,907,572 ocular crops from VGGFace2, evaluated on VGGFace2-Pose and UFPR-Periocular datasets under different acquisition conditions.

Result: VGGFace2-Pose yielded 9-15% EER due to uncontrolled conditions, while UFPR-Periocular achieved 1-2% EER - the lowest reported on this dataset, demonstrating the impact of image quality and consistent acquisition protocols.

Conclusion: Large-scale training data significantly improves periocular recognition performance, with controlled acquisition conditions enabling state-of-the-art results, highlighting the periocular region's potential as a robust biometric modality.

Abstract: We focus on ocular biometrics, specifically the periocular region (the area
around the eye), which offers high discrimination and minimal acquisition
constraints. We evaluate three Convolutional Neural Network architectures of
varying depth and complexity to assess their effectiveness for periocular
recognition. The networks are trained on 1,907,572 ocular crops extracted from
the large-scale VGGFace2 database. This significantly contrasts with existing
works, which typically rely on small-scale periocular datasets for training
having only a few thousand images. Experiments are conducted with ocular images
from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,
and the UFPR-Periocular database, which consists of selfies captured via mobile
devices with user guidance on the screen. Due to the uncontrolled conditions of
VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from
9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In
contrast, UFPR-Periocular yields significantly better performance (EERs of
1-2%), thanks to higher image quality and more consistent acquisition
protocols. To the best of our knowledge, these are the lowest reported EERs on
the UFPR dataset to date.

</details>


### [35] [Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology](https://arxiv.org/abs/2510.26297)
*Luting Wang,Yinghao Xiang,Hongliang Huang,Dongjun Li,Chen Gao,Si Liu*

Main category: cs.CV

TL;DR: A unified framework for Agile Earth Observation Satellites scheduling featuring AEOS-Bench benchmark suite with 16,410 scenarios and AEOS-Former Transformer model with constraint-aware attention.


<details>
  <summary>Details</summary>
Motivation: Existing AEOS scheduling methods simplify complexities, limiting real-world performance under large-scale scenarios, dynamic environments, and stringent constraints.

Method: Developed AEOS-Bench benchmark suite with 3,907 satellite assets and 16,410 scenarios, plus AEOS-Former Transformer model with constraint-aware attention mechanism and internal constraint module for physical/operational limits.

Result: AEOS-Former outperforms baseline models in task completion and energy efficiency, with ablation studies confirming component contributions.

Conclusion: The framework provides robust AEOS constellation scheduling solution, with code and data available for community use.

Abstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.

</details>


### [36] [Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG](https://arxiv.org/abs/2510.26304)
*Jelizaveta Jankowska,Bożena Kostek,Fernando Alonso-Fernandez,Prayag Tiwari*

Main category: cs.CV

TL;DR: Study examines how different music genres affect human emotions using EEG measurements and surveys, finding connections between brain activity and emotional responses.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of different music genres on human emotions and demonstrate measurable brain activity patterns associated with emotional responses to music.

Method: Used EEG helmet to measure brain activity while participants listened to different music genres, combined with subjective surveys from a diverse group of participants with varying gender and musical preferences.

Result: Analysis revealed connections between emotions and observed brain activity, showing that different music genres elicit distinct emotional responses measurable through EEG signals.

Conclusion: Different types of music significantly affect human emotions, and these emotional responses can be detected and correlated with specific brain activity patterns measured by EEG.

Abstract: The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.

</details>


### [37] [A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading](https://arxiv.org/abs/2510.26315)
*Junlai Qiu,Yunzhu Chen,Hao Zheng,Yawen Huang,Yuexiang Li*

Main category: cs.CV

TL;DR: Proposes an evidential fusion paradigm to combine CNN and ViT backbones for diabetic retinopathy grading, leveraging local and global feature strengths respectively.


<details>
  <summary>Details</summary>
Motivation: Existing DR diagnosis systems using single-type backbones (CNN or ViT) have performance bottlenecks due to their respective limitations. Integrating both can leverage CNN's local feature extraction and ViT's global feature capturing capabilities.

Method: Uses deep evidential networks to transform features from different backbones into supporting evidences, forming aggregated opinions to adaptively tune fusion patterns between CNN and ViT backbones.

Result: Experimental results on two DR grading datasets show improved accuracy compared to state-of-the-art frameworks and provide excellent interpretability for feature fusion and decision-making.

Conclusion: The proposed evidential fusion paradigm effectively combines CNN and ViT strengths, boosting DR grading performance while offering interpretable feature fusion and decision processes.

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged
and elderly people, which significantly impacts their daily lives and mental
health. To improve the efficiency of clinical screening and enable the early
detection of DR, a variety of automated DR diagnosis systems have been recently
established based on convolutional neural network (CNN) or vision Transformer
(ViT). However, due to the own shortages of CNN / ViT, the performance of
existing methods using single-type backbone has reached a bottleneck. One
potential way for the further improvements is integrating different kinds of
backbones, which can fully leverage the respective strengths of them
(\emph{i.e.,} the local feature extraction capability of CNN and the global
feature capturing ability of ViT). To this end, we propose a novel paradigm to
effectively fuse the features extracted by different backbones based on the
theory of evidence. Specifically, the proposed evidential fusion paradigm
transforms the features from different backbones into supporting evidences via
a set of deep evidential networks. With the supporting evidences, the
aggregated opinion can be accordingly formed, which can be used to adaptively
tune the fusion pattern between different backbones and accordingly boost the
performance of our hybrid model. We evaluated our method on two publicly
available DR grading datasets. The experimental results demonstrate that our
hybrid model not only improves the accuracy of DR grading, compared to the
state-of-the-art frameworks, but also provides the excellent interpretability
for feature fusion and decision-making.

</details>


### [38] [GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?](https://arxiv.org/abs/2510.26339)
*Mingyu Sung,Seungjae Ham,Kangwoo Kim,Yeokyoung Yoon,Sangseok Yun,Il-Min Kim,Jae-Mo Kang*

Main category: cs.CV

TL;DR: GLYPH-SR is a vision-language-guided diffusion framework for image super-resolution that specifically optimizes for both text legibility and perceptual quality in natural scenes, addressing the limitations of previous SR methods that treat scene-text as generic texture.


<details>
  <summary>Details</summary>
Motivation: Scene-text in natural images carries actionable information but current SR methods optimized for distortion or perceptual metrics are insensitive to character-level errors, causing OCR failures even when the rest of the image appears sharp.

Method: GLYPH-SR uses a Text-SR Fusion ControlNet guided by OCR data and a ping-pong scheduler that alternates between text- and scene-centric guidance, trained on synthetic corpus while keeping the main SR branch frozen.

Result: Across SVT, SCUT-CTW1500, and CUTE80 datasets at x4 and x8 scales, GLYPH-SR improves OCR F1 by up to +15.18 percentage points over diffusion/GAN baselines while maintaining competitive perceptual quality metrics (MANIQA, CLIP-IQA, MUSIQ).

Conclusion: GLYPH-SR simultaneously achieves high text readability and visual realism, delivering super-resolution that both looks right and reads right for practical vision system deployments.

Abstract: Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.

</details>


### [39] [EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models](https://arxiv.org/abs/2510.26391)
*Igor Abramov,Ilya Makarov*

Main category: cs.CV

TL;DR: A dual-conditioning framework combining EEG embeddings with spatial saliency maps improves EEG-driven image reconstruction by enhancing fidelity and semantic coherence through attentional priors.


<details>
  <summary>Details</summary>
Motivation: Existing EEG-driven image reconstruction methods lack spatial attention mechanisms, limiting fidelity and semantic coherence in generated images.

Method: Proposes a dual-conditioning framework using Adaptive Thinking Mapper (ATM) for EEG feature extraction, fine-tunes Stable Diffusion 2.1 via LoRA for neural-visual alignment, and employs ControlNet for saliency map conditioning.

Result: Achieves significant improvement in quality of low- and high-level image features on THINGS-EEG dataset, with strong alignment to human visual attention.

Conclusion: Attentional priors resolve EEG ambiguities, enabling high-fidelity reconstructions for medical diagnostics and neuroadaptive interfaces through efficient adaptation of pre-trained diffusion models.

Abstract: Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.

</details>


### [40] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: LoCoT2V-Bench is a new benchmark for evaluating long video generation (LVG) that addresses limitations in existing benchmarks by using complex prompts and multi-dimensional evaluation metrics including event-level alignment, temporal consistency, and abstract narrative attributes.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video benchmarks rely on simplified prompts and focus on low-level metrics, overlooking fine-grained alignment with prompts and abstract dimensions like narrative coherence and thematic expression in long-form video generation.

Method: The authors propose LoCoT2V-Bench based on real-world videos with realistic complex prompts containing scene transitions and event dynamics. They construct a multi-dimensional evaluation framework with new metrics including event-level alignment, fine-grained temporal consistency, content clarity, and Human Expectation Realization Degree (HERD) for narrative flow and emotional response.

Result: Evaluation of nine representative LVG models shows that while current methods perform well on basic visual and temporal aspects, they struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence.

Conclusion: LoCoT2V-Bench provides a comprehensive platform for evaluating long-form complex text-to-video generation and highlights critical directions for future method improvement, particularly in handling complex narrative structures and thematic consistency.

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


### [41] [A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2510.26441)
*Shihab Aaqil Ahamed,Udaya S. K. P. Miriya Thanthrige,Ranga Rodrigo,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: A-TPT is a test-time prompt tuning framework that improves vision-language model calibration by maximizing angular diversity between class-wise textual features on the unit hypersphere.


<details>
  <summary>Details</summary>
Motivation: Current TPT methods lack optimal angular separation between class-wise textual features, which hurts calibration performance and raises concerns about VLMs' reliability and safety.

Method: Introduces angular diversity by maximizing the minimum pairwise angular distance between normalized textual features on the unit hypersphere to achieve uniform distribution of features.

Result: Consistently surpasses state-of-the-art TPT methods in reducing aggregate average calibration error while maintaining comparable accuracy, with superior zero-shot calibration on distribution shifts and medical datasets.

Conclusion: Promoting angular diversity achieves well-dispersed textual features, significantly improving VLM calibration during test-time adaptation.

Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.

</details>


### [42] [PointSt3R: Point Tracking through 3D Grounded Correspondence](https://arxiv.org/abs/2510.26443)
*Rhodri Guerrier,Adam W. Harley,Dima Damen*

Main category: cs.CV

TL;DR: The paper adapts foundational 3D reconstruction models (DUSt3R/MASt3R) for point tracking using 3D grounded correspondence, achieving competitive results on multiple benchmarks while requiring only frame pairs without temporal context.


<details>
  <summary>Details</summary>
Motivation: To leverage recent advances in 3D reconstruction models for point tracking tasks, addressing limitations of existing methods and improving performance on both static and dynamic point tracking.

Method: Fine-tune MASt3R for point tracking using synthetic data, combine reconstruction loss with dynamic correspondence training, add visibility head, and train/evaluate on frame pairs without temporal context.

Result: Achieves competitive or superior performance on four datasets: TAP-Vid-DAVIS (73.8 δ_avg / 85.8% occlusion acc), significantly outperforms CoTracker3 on EgoPoints (61.3 vs 54.2) and RGB-S (87.0 vs 82.8).

Conclusion: 3D reconstruction models can be effectively adapted for point tracking, achieving strong performance with minimal temporal context and showing particular strength on static points (+33.5% on EgoPoints).

Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.

</details>


### [43] [Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.26464)
*Yuanting Fan,Jun Liu,Xiaochen Chen,Bin-Bin Gao,Jian Li,Yong Liu,Jinlong Peng,Chengjie Wang*

Main category: cs.CV

TL;DR: FineGrainedAD improves few-shot anomaly detection by providing multi-level fine-grained textual descriptions to address semantic misalignment between image descriptions and patch-level visual anomalies.


<details>
  <summary>Details</summary>
Motivation: Existing FSAD methods suffer from semantic misalignment due to lack of detailed textual descriptions, leading to sub-optimal localization performance when using pre-defined image-level descriptions.

Method: Proposes Multi-Level Fine-Grained Semantic Caption (MFSC) with automatic construction pipeline, and a framework with Multi-Level Learnable Prompt (MLLP) for fine-grained semantics and Multi-Level Semantic Alignment (MLSA) for better visual-text alignment.

Result: Achieves superior overall performance in few-shot settings on MVTec-AD and VisA datasets compared to existing methods.

Conclusion: Fine-grained multi-level textual descriptions significantly improve anomaly localization performance in few-shot anomaly detection by addressing semantic misalignment issues.

Abstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.

</details>


### [44] [Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition](https://arxiv.org/abs/2510.26466)
*Pei Peng,MingKun Xie,Hang Hao,Tong Jin,ShengJun Huang*

Main category: cs.CV

TL;DR: A causal inference approach to address object-context shortcuts in vision-language models by synthesizing counterfactual embeddings and estimating Total Direct Effect to improve zero-shot reliability.


<details>
  <summary>Details</summary>
Motivation: Object-context shortcuts undermine zero-shot reliability in vision-language models when test scenes differ from training co-occurrences, creating biased predictions.

Method: Estimate object and background expectations in CLIP's representation space, synthesize counterfactual embeddings by recombining object features with diverse alternative contexts, and use Total Direct Effect to subtract background-only activation.

Result: Substantially improves both worst-group and average accuracy on context-sensitive benchmarks, establishing new zero-shot state of the art without retraining or prompt design.

Conclusion: Provides a lightweight representation-level counterfactual approach for debiased and reliable multimodal reasoning through causal inference.

Abstract: Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.

</details>


### [45] [Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing](https://arxiv.org/abs/2510.26474)
*Xin Guo,Zhiheng Xi,Yiwen Ding,Yitao Zhai,Xiaowei Shi,Xunliang Cai,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: The paper identifies a "Matthew effect" in self-improvement of large vision-language models where models excel at simple queries but struggle with complex ones, leading to imbalanced optimization. It proposes distribution-reshaping and trajectory-resampling strategies to re-balance head-tail data and improve visual reasoning.


<details>
  <summary>Details</summary>
Motivation: Current self-improvement paradigms for LVLMs create an imbalance where models prioritize simple reasoning tasks over complex ones, leading to performance bottlenecks and hindered improvement over iterations.

Method: Four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, designed to achieve head-tail re-balancing during the exploration-and-learning self-improvement process.

Result: Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models show consistent improvement in visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.

Conclusion: The proposed head-tail re-balancing strategies effectively counteract the Matthew effect in self-improvement, enabling better optimization across both simple and complex reasoning tasks.

Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.

</details>


### [46] [Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm](https://arxiv.org/abs/2510.26509)
*Vinícius Ferraria,Eurico Ruivo*

Main category: cs.CV

TL;DR: An adaptable edge detector using cellular automata optimized by meta-heuristics with transfer learning was developed, but expanding the search space didn't improve results and transfer learning showed no significant benefits.


<details>
  <summary>Details</summary>
Motivation: To address weaknesses in edge detection like difficulty detecting loose edges and lack of context, and to create a detector adaptable to specific image properties.

Method: Developed an adaptable detector using two-dimensional cellular automaton optimized by meta-heuristic combined with transfer learning techniques.

Result: Expanding the search space in optimization phase was ineffective for the chosen image set, and transfer learning techniques showed no significant improvements despite the model's adaptability.

Conclusion: The model demonstrated adaptability to input images, but neither search space expansion nor transfer learning provided meaningful improvements for edge detection performance.

Abstract: The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.

</details>


### [47] [SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging](https://arxiv.org/abs/2510.26568)
*Hao Xie,Zixun Huang,Yushen Zuo,Yakun Ju,Frank H. F. Leung,N. F. Law,Kin-Man Lam,Yong-Ping Zheng,Sai Ho Ling*

Main category: cs.CV

TL;DR: SA²Net is a scale-adaptive structure-aware network for spine segmentation in ultrasound images that addresses challenges in learning global contextual knowledge and encoding structural bone features through cross-dimensional correlation learning and structure-affinity transformation.


<details>
  <summary>Details</summary>
Motivation: Spine segmentation from ultrasound VPI is crucial for scoliosis diagnosis but faces challenges: poor learning of global contextual knowledge due to neglected spatial correlation of bone features, and insufficient encoding of rich structural knowledge about bone shapes and positions.

Method: Proposes SA²Net with: 1) scale-adaptive complementary strategy for cross-dimensional long-distance correlation features, 2) structure-affinity transformation that combines semantic features with class-specific affinity using Transformer decoder for structure-aware reasoning, and 3) feature mixing loss aggregation for enhanced training.

Result: SA²Net achieves superior segmentation performance compared to state-of-the-art methods, demonstrating improved robustness and accuracy in spine segmentation.

Conclusion: SA²Net shows strong potential as a promising tool for advanced scoliosis diagnosis through intelligent spinal image analysis, with adaptability to various backbones enhancing its practical utility.

Abstract: Spine segmentation, based on ultrasound volume projection imaging (VPI),
plays a vital role for intelligent scoliosis diagnosis in clinical
applications. However, this task faces several significant challenges. Firstly,
the global contextual knowledge of spines may not be well-learned if we neglect
the high spatial correlation of different bone features. Secondly, the spine
bones contain rich structural knowledge regarding their shapes and positions,
which deserves to be encoded into the segmentation process. To address these
challenges, we propose a novel scale-adaptive structure-aware network
(SA$^{2}$Net) for effective spine segmentation. First, we propose a
scale-adaptive complementary strategy to learn the cross-dimensional
long-distance correlation features for spinal images. Second, motivated by the
consistency between multi-head self-attention in Transformers and semantic
level affinity, we propose structure-affinity transformation to transform
semantic features with class-specific affinity and combine it with a
Transformer decoder for structure-aware reasoning. In addition, we adopt a
feature mixing loss aggregation method to enhance model training. This method
improves the robustness and accuracy of the segmentation process. The
experimental results demonstrate that our SA$^{2}$Net achieves superior
segmentation performance compared to other state-of-the-art methods. Moreover,
the adaptability of SA$^{2}$Net to various backbones enhances its potential as
a promising tool for advanced scoliosis diagnosis using intelligent spinal
image analysis. The code and experimental demo are available at
https://github.com/taetiseo09/SA2Net.

</details>


### [48] [AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping](https://arxiv.org/abs/2510.26569)
*Wen Xie,Yanjun Zhu,Gijs Overgoor,Yakov Bart,Agata Lapedriza Garcia,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: Automated video ad clipping framework using audio-visual fusion for shot selection, outperforming existing methods on multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Manual creation of multiple ad versions at different durations is labor-intensive; need automated solution specifically for advertising context.

Method: Two-stream audio-visual fusion model that predicts frame importance based on likelihood of selection in firm-produced ads; frames video clipping as shot selection problem.

Result: Outperforms state-of-the-art methods across Average Precision, Area Under Curve, Spearman, and Kendall metrics.

Conclusion: Audio plays critical role in advertising; proposed framework effectively automates video ad clipping with superior performance.

Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.

</details>


### [49] [Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios](https://arxiv.org/abs/2510.26580)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: A Dynamic Context-Aware Scene Reasoning framework that uses Vision-Language Alignment for zero-shot scene understanding in unfamiliar real-world environments without labeled data.


<details>
  <summary>Details</summary>
Motivation: AI systems struggle with unfamiliar scenarios lacking labeled data, limiting deployment in dynamic, unstructured settings. Conventional models cannot generalize across unseen contexts.

Method: Integrates pre-trained vision transformers and large language models to align visual semantics with natural language descriptions. Uses a dynamic reasoning module that combines global scene cues and object-level interactions guided by linguistic priors.

Result: Achieves up to 18% improvement in scene understanding accuracy over baseline models on zero-shot benchmarks (COCO, Visual Genome, Open Images). Shows robust performance in ambiguous or cluttered scenes through synergistic vision-language fusion.

Conclusion: Provides a scalable and interpretable approach for context-aware reasoning, advancing zero-shot generalization in dynamic real-world settings.

Abstract: In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.

</details>


### [50] [CATCH: A Modular Cross-domain Adaptive Template with Hook](https://arxiv.org/abs/2510.26582)
*Xinjin Li,Yulie Lu,Jinghan Cao,Yu Ma,Zhenglin Li,Yeyang Zhou*

Main category: cs.CV

TL;DR: CATCH is a plug-and-play framework for cross-domain VQA adaptation that uses lightweight domain classification and dual adapters to improve generalization without retraining backbone models.


<details>
  <summary>Details</summary>
Motivation: Current VQA models like LLaVA perform well on natural images but degrade significantly in out-of-domain scenarios (remote sensing, medical imaging, math diagrams) due to distribution shifts and lack of effective domain adaptation mechanisms.

Method: Decouples visual and linguistic adaptation using two lightweight modules: a domain classifier to identify input image type, and dual adapters (Prompt Adapter for language modulation and Visual Adapter for vision feature adjustment) dynamically injected via unified hook interface.

Result: Achieves consistent performance gains across four domain-specific VQA benchmarks: +2.3 BLEU on MathVQA, +2.6 VQA on MedVQA-RAD, and +3.1 ROUGE on ChartQA without retraining the backbone model.

Conclusion: CATCH provides a scalable and extensible approach to multi-domain VQA, enabling practical deployment across diverse application domains with minimal architectural changes.

Abstract: Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.

</details>


### [51] [Emu3.5: Native Multimodal Models are World Learners](https://arxiv.org/abs/2510.26583)
*Yufeng Cui,Honghao Chen,Haoge Deng,Xu Huang,Xinghang Li,Jirong Liu,Yang Liu,Zhuoyan Luo,Jinsheng Wang,Wenxuan Wang,Yueze Wang,Chengyuan Wang,Fan Zhang,Yingli Zhao,Ting Pan,Xianduo Li,Zecheng Hao,Wenxuan Ma,Zhuo Chen,Yulong Ao,Tiejun Huang,Zhongyuan Wang,Xinlong Wang*

Main category: cs.CV

TL;DR: Emu3.5 is a large-scale multimodal world model that predicts next states across vision and language using unified next-token prediction on 10+ trillion tokens, enhanced with reinforcement learning and accelerated inference via Discrete Diffusion Adaptation.


<details>
  <summary>Details</summary>
Motivation: To create a unified multimodal world model that can naturally handle interleaved vision-language inputs and outputs, enabling long-horizon generation and world-modeling capabilities across diverse scenarios.

Method: End-to-end pre-training with unified next-token prediction on vision-language interleaved data, followed by large-scale reinforcement learning post-training, and Discrete Diffusion Adaptation for efficient inference.

Result: Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image on generation tasks, shows superior results on interleaved generation tasks, and enables 20x faster inference without performance loss.

Conclusion: Emu3.5 demonstrates strong multimodal capabilities including world exploration and embodied manipulation, and is open-sourced to support community research.

Abstract: We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language. Emu3.5 is pre-trained
end-to-end with a unified next-token prediction objective on a corpus of
vision-language interleaved data containing over 10 trillion tokens, primarily
derived from sequential frames and transcripts of internet videos. The model
naturally accepts interleaved vision-language inputs and generates interleaved
vision-language outputs. Emu3.5 is further post-trained with large-scale
reinforcement learning to enhance multimodal reasoning and generation. To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.
Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
vision-language generation, any-to-image (X2I) generation, and complex
text-rich image generation. It also exhibits generalizable world-modeling
abilities, enabling spatiotemporally consistent world exploration and
open-world embodied manipulation across diverse scenarios and tasks. For
comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
(Nano Banana) on image generation and editing tasks and demonstrates superior
results on a suite of interleaved generation tasks. We open-source Emu3.5 at
https://github.com/baaivision/Emu3.5 to support community research.

</details>


### [52] [ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching](https://arxiv.org/abs/2510.26601)
*Anirban Ray,Vera Galinova,Florian Jug*

Main category: cs.CV

TL;DR: ResMatching is a novel computational super-resolution method using guided conditional flow matching that achieves the best trade-off between data fidelity and perceptual realism, particularly effective in noisy conditions and providing uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Computational super-resolution in fluorescence microscopy is an ill-posed problem that requires strong priors to extrapolate missing frequencies. With advances in machine learning, better data-driven priors can be learned to improve CSR results.

Method: ResMatching uses guided conditional flow matching to learn improved data priors for computational super-resolution. The method can sample from an implicitly learned posterior distribution and provides pixel-wise uncertainty estimation.

Result: ResMatching consistently achieved competitive results on 4 diverse biological structures from BioSR dataset against 7 baselines, showing the best trade-off between data fidelity and perceptual realism. It performs particularly well with noisy low-resolution images and provides calibrated uncertainty estimates.

Conclusion: ResMatching is an effective CSR method that learns strong data priors through flow matching, delivers high-quality super-resolution with uncertainty quantification, and excels in challenging scenarios with noisy inputs.

Abstract: Computational Super-Resolution (CSR) in fluorescence microscopy has, despite
being an ill-posed problem, a long history. At its very core, CSR is about
finding a prior that can be used to extrapolate frequencies in a micrograph
that have never been imaged by the image-generating microscope. It stands to
reason that, with the advent of better data-driven machine learning techniques,
stronger prior can be learned and hence CSR can lead to better results. Here,
we present ResMatching, a novel CSR method that uses guided conditional flow
matching to learn such improved data-priors. We evaluate ResMatching on 4
diverse biological structures from the BioSR dataset and compare its results
against 7 baselines. ResMatching consistently achieves competitive results,
demonstrating in all cases the best trade-off between data fidelity and
perceptual realism. We observe that CSR using ResMatching is particularly
effective in cases where a strong prior is hard to learn, e.g. when the given
low-resolution images contain a lot of noise. Additionally, we show that
ResMatching can be used to sample from an implicitly learned posterior
distribution and that this distribution is calibrated for all tested use-cases,
enabling our method to deliver a pixel-wise data-uncertainty term that can
guide future users to reject uncertain predictions.

</details>


### [53] [CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing](https://arxiv.org/abs/2510.26609)
*Shayan Nejadshamsi,Yuanyuan Zhang,Shadi Zaki,Brock Porth,Lysa Porth,Vahab Khoshdel*

Main category: cs.CV

TL;DR: CYPRESS is a deep learning model for high-resolution canola yield prediction using satellite imagery and foundation models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional crop yield prediction methods lack scalability and granularity needed for precision farming, creating a need for more detailed agricultural monitoring tools.

Method: Fine-tunes Prithvi-EO-2.0-600M geospatial foundation model for continuous regression, transforming multi-temporal satellite imagery into pixel-level yield maps.

Result: Demonstrates superior performance over existing deep learning models on Canadian Prairies dataset, providing continuous high-resolution yield predictions.

Conclusion: Validates approach of bridging large-scale Earth observation with on-farm decision-making, offering scalable solution for precision agriculture.

Abstract: Accurate and timely crop yield prediction is crucial for global food security
and modern agricultural management. Traditional methods often lack the
scalability and granularity required for precision farming. This paper
introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder
for Satellite Sensing), a deep learning model designed for high-resolution,
intra-field canola yield prediction. CYPRESS leverages a pre-trained,
large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for
a continuous regression task, transforming multi-temporal satellite imagery
into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from
the Canadian Prairies, CYPRESS demonstrates superior performance over existing
deep learning-based yield prediction models, highlighting the effectiveness of
fine-tuning foundation models for specialized agricultural applications. By
providing a continuous, high-resolution output, CYPRESS offers a more
actionable tool for precision agriculture than conventional classification or
county-level aggregation methods. This work validates a novel approach that
bridges the gap between large-scale Earth observation and on-farm
decision-making, offering a scalable solution for detailed agricultural
monitoring.

</details>


### [54] [Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras](https://arxiv.org/abs/2510.26614)
*Christoffer Koo Øhrstrøm,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: Spiking Patches tokenizer preserves event camera properties (asynchronous, sparse) while matching or exceeding frame/voxel accuracy with 3.4-10.4x faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing event representations (frames, voxels) lose the asynchronous and spatially sparse properties of event cameras, which are their key advantages.

Method: Propose Spiking Patches tokenizer that converts event streams into tokens preserving asynchronous and sparse properties, evaluated with GNN, PCN, and Transformer on gesture recognition and object detection.

Result: 3.4x faster than voxels, 10.4x faster than frames while matching accuracy; absolute improvements up to 3.8% for gesture recognition and 1.4% for object detection.

Conclusion: Tokenization is a novel direction for event-based vision that preserves event camera properties without sacrificing accuracy.

Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.

</details>


### [55] [PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus](https://arxiv.org/abs/2510.26630)
*Bingcong Huo,Zhiming Wang*

Main category: cs.CV

TL;DR: PT-DETR improves UAV small object detection by enhancing feature extraction with PADF and MFFF modules, and using Focaler-SIoU for better bounding box matching, achieving higher mAP with lower complexity.


<details>
  <summary>Details</summary>
Motivation: Address challenges in UAV object detection including complex backgrounds, severe occlusion, dense small objects, and varying lighting conditions.

Method: Based on RT-DETR, introduces Partially-Aware Detail Focus (PADF) Module for enhanced small object feature extraction, Median-Frequency Feature Fusion (MFFF) module for better detail and context capture, and Focaler-SIoU for improved bounding box matching.

Result: Achieves mAP improvements of 1.6% and 1.7% on VisDrone2019 dataset compared to RT-DETR, with lower computational complexity and fewer parameters.

Conclusion: PT-DETR demonstrates robustness and feasibility for small-object detection tasks in UAV imagery.

Abstract: To address the challenges in UAV object detection, such as complex
backgrounds, severe occlusion, dense small objects, and varying lighting
conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection
algorithm specifically designed for small objects in UAV imagery. In the
backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module
to enhance feature extraction for small objects. Additionally,we design the
Median-Frequency Feature Fusion (MFFF) module,which effectively improves the
model's ability to capture small-object details and contextual information.
Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box
matching capability and increase its sensitivity to small-object features,
thereby further enhancing detection accuracy and robustness. Compared with
RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the
VisDrone2019 dataset with lower computational complexity and fewer parameters,
demonstrating its robustness and feasibility for small-object detection tasks.

</details>


### [56] [All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles](https://arxiv.org/abs/2510.26641)
*Sayed Pedram Haeri Boroujeni,Niloufar Mehrabi,Hazim Alzorgan,Ahmad Sarlak,Mahlagha Fazeli,Abolfazl Razi*

Main category: cs.CV

TL;DR: This survey provides a forward-looking analysis of object detection in Autonomous Vehicles, focusing on emerging AI paradigms like Vision-Language Models, Large Language Models, and Generative AI rather than outdated techniques.


<details>
  <summary>Details</summary>
Motivation: Knowledge in autonomous vehicle object detection remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence, creating a need to bridge this gap with a comprehensive survey.

Method: Systematic review of AV sensors and fusion strategies, structured categorization of AV datasets (ego-vehicle, infrastructure-based, cooperative), and analysis of cutting-edge detection methodologies including 2D/3D pipelines and transformer-driven approaches.

Result: The survey synthesizes perspectives on current capabilities, open challenges, and future opportunities in AV object detection, with particular attention to emerging transformer-based methods and multimodal integration.

Conclusion: The paper delivers a clear roadmap for advancing object detection in autonomous vehicles by integrating recent AI advances with traditional sensor fusion approaches.

Abstract: Autonomous Vehicles (AVs) are transforming the future of transportation
through advances in intelligent perception, decision-making, and control
systems. However, their success is tied to one core capability, reliable object
detection in complex and multimodal environments. While recent breakthroughs in
Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
progress, the field still faces a critical challenge as knowledge remains
fragmented across multimodal perception, contextual reasoning, and cooperative
intelligence. This survey bridges that gap by delivering a forward-looking
analysis of object detection in AVs, emphasizing emerging paradigms such as
Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
rather than re-examining outdated techniques. We begin by systematically
reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
and Radar) and their fusion strategies, highlighting not only their
capabilities and limitations in dynamic driving environments but also their
potential to integrate with recent advances in LLM/VLM-driven perception
frameworks. Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics. Ultimately, we analyze
cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
hybrid sensor fusion, with particular attention to emerging transformer-driven
approaches powered by Vision Transformers (ViTs), Large and Small Language
Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
delivers a clear roadmap of current capabilities, open challenges, and future
opportunities.

</details>


### [57] [Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2](https://arxiv.org/abs/2510.26653)
*Daniela Martin,Joseph Gallego*

Main category: cs.CV

TL;DR: Deep learning optical flow models achieve sub-kilometer accuracy for sea ice drift estimation from SAR imagery, outperforming classical methods and providing spatially continuous motion fields.


<details>
  <summary>Details</summary>
Motivation: Accurate sea ice drift estimation is critical for Arctic navigation and climate research, but traditional optical flow methods have limitations in complex scenarios. Deep learning approaches have shown superior performance in computer vision, motivating their application to satellite SAR imagery.

Method: Conducted the first large-scale benchmark of 48 deep learning optical flow models on RADARSAT-2 ScanSAR sea ice imagery, evaluated using endpoint error (EPE) and F1-all metrics against GNSS-tracked buoys.

Result: Several models achieved sub-kilometer accuracy (EPE 6-8 pixels, 300-400m), which is small relative to spatial scales of sea ice motion. Models captured consistent regional drift patterns and demonstrated substantial improvement over classical methods.

Conclusion: Deep learning optical flow methods can be effectively transferred to polar remote sensing, providing spatially continuous drift fields for every image pixel rather than sparse buoy locations, offering new opportunities for navigation and climate modeling.

Abstract: Accurate estimation of sea ice drift is critical for Arctic navigation,
climate research, and operational forecasting. While optical flow, a computer
vision technique for estimating pixel wise motion between consecutive images,
has advanced rapidly in computer vision, its applicability to geophysical
problems and to satellite SAR imagery remains underexplored. Classical optical
flow methods rely on mathematical models and strong assumptions about motion,
which limit their accuracy in complex scenarios. Recent deep learning based
approaches have substantially improved performance and are now the standard in
computer vision, motivating their application to sea ice drift estimation. We
present the first large scale benchmark of 48 deep learning optical flow models
on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and
Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer
accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the
spatial scales of sea ice motion and typical navigation requirements in the
Arctic. Our results demonstrate that the models are capable of capturing
consistent regional drift patterns and that recent deep learning based optical
flow methods, which have substantially improved motion estimation accuracy
compared to classical methods, can be effectively transferred to polar remote
sensing. Optical flow produces spatially continuous drift fields, providing
motion estimates for every image pixel rather than at sparse buoy locations,
offering new opportunities for navigation and climate modeling.

</details>


### [58] [Improving Classification of Occluded Objects through Scene Context](https://arxiv.org/abs/2510.26681)
*Courtney M. King,Daniel D. Leeds,Damian Lyons,George Kalaitzis*

Main category: cs.CV

TL;DR: This paper presents two scene-based information fusion techniques to improve object detection robustness against occlusions in RPN-DCNN networks, showing improved recall and precision on challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Occlusions pose significant challenges to object recognition algorithms, and scene context can provide valuable additional information to reduce errors caused by occlusions.

Method: Two distinct scene-based fusion techniques: 1) selecting custom object networks based on identified background scene (pre-prediction), and 2) fusing scene knowledge into initial object scores from RPN (post-detection). Also explores training methodologies using both occluded and unoccluded images.

Result: Demonstrated overall improvement in both recall and precision against baseline methods on challenging datasets with partial occlusions. Training on combination of occluded and unoccluded images showed improvement over other training approaches.

Conclusion: The method is interpretable, easily adaptable to other datasets, and offers promising directions for future research and practical applications in occlusion-robust object detection.

Abstract: The presence of occlusions has provided substantial challenges to
typically-powerful object recognition algorithms. Additional sources of
information can be extremely valuable to reduce errors caused by occlusions.
Scene context is known to aid in object recognition in biological vision. In
this work, we attempt to add robustness into existing Region Proposal
Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks
through two distinct scene-based information fusion techniques. We present one
algorithm under each methodology: the first operates prior to prediction,
selecting a custom object network to use based on the identified background
scene, and the second operates after detection, fusing scene knowledge into
initial object scores output by the RPN. We demonstrate our algorithms on
challenging datasets featuring partial occlusions, which show overall
improvement in both recall and precision against baseline methods. In addition,
our experiments contrast multiple training methodologies for occlusion
handling, finding that training on a combination of both occluded and
unoccluded images demonstrates an improvement over the others. Our method is
interpretable and can easily be adapted to other datasets, offering many future
directions for research and practical applications.

</details>


### [59] [Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill](https://arxiv.org/abs/2510.26684)
*Vaibhav Kurrey,Sivakalyan Pujari,Gagan Raj Gupta*

Main category: cs.CV

TL;DR: A machine vision system using deep learning for real-time anomaly detection in steel rolling mills, integrating camera data with sensor inputs to predict equipment failures and enable proactive maintenance.


<details>
  <summary>Details</summary>
Motivation: To reduce unplanned breakdown costs and improve operational reliability in industrial manufacturing by enabling early prediction of equipment failures and process interruptions.

Method: Integration of industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time. Live video streams are processed on a centralized video server using deep learning models, with joint analysis of sensor data from data acquisition systems and visual inputs.

Result: The system enables early prediction of equipment failures, identifies location and root causes of failures, reduces computational load on industrial PLCs, and supports scalable deployment across production lines with minimal additional resources.

Conclusion: This integrated machine vision approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments by providing actionable insights for proactive maintenance.

Abstract: We present a long-term deployment study of a machine vision-based anomaly
detection system for failure prediction in a steel rolling mill. The system
integrates industrial cameras to monitor equipment operation, alignment, and
hot bar motion in real time along the process line. Live video streams are
processed on a centralized video server using deep learning models, enabling
early prediction of equipment failures and process interruptions, thereby
reducing unplanned breakdown costs. Server-based inference minimizes the
computational load on industrial process control systems (PLCs), supporting
scalable deployment across production lines with minimal additional resources.
By jointly analyzing sensor data from data acquisition systems and visual
inputs, the system identifies the location and probable root causes of
failures, providing actionable insights for proactive maintenance. This
integrated approach enhances operational reliability, productivity, and
profitability in industrial manufacturing environments.

</details>


### [60] [The Impact and Outlook of 3D Gaussian Splatting](https://arxiv.org/abs/2510.26694)
*Bernhard Kerbl*

Main category: cs.CV

TL;DR: A comprehensive overview of 3D Gaussian Splatting's evolution, covering efficiency improvements, dynamic representations, mathematical foundations, mobile/VR applications, large-scale environments, and fast reconstruction methods.


<details>
  <summary>Details</summary>
Motivation: To summarize the rapid development and diverse research directions that have emerged following the introduction of 3D Gaussian Splatting as a transformative 3D scene representation method.

Method: Survey and analysis of key research directions including: resource-efficient training/rendering, dynamic 4D representations, mathematical foundations exploration, mobile/VR platform adaptation, large-scale environment extension, and fast reconstruction techniques.

Result: Identified major advancements that have transformed 3DGS from a breakthrough representation into a versatile foundational tool for 3D vision and graphics applications.

Conclusion: 3D Gaussian Splatting has evolved significantly across multiple dimensions, establishing itself as a foundational and versatile tool in 3D vision and graphics with broad applicability and ongoing innovation.

Abstract: Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed
the landscape of 3D scene representations, inspiring an extensive body of
associated research. Follow-up work includes analyses and contributions that
enhance the efficiency, scalability, and real-world applicability of 3DGS. In
this summary, we present an overview of several key directions that have
emerged in the wake of 3DGS. We highlight advances enabling resource-efficient
training and rendering, the evolution toward dynamic (or four-dimensional,
4DGS) representations, and deeper exploration of the mathematical foundations
underlying its appearance modeling and rendering process. Furthermore, we
examine efforts to bring 3DGS to mobile and virtual reality platforms, its
extension to massive-scale environments, and recent progress toward
near-instant radiance field reconstruction via feed-forward or distributed
computation. Collectively, these developments illustrate how 3DGS has evolved
from a breakthrough representation into a versatile and foundational tool for
3D vision and graphics.

</details>


### [61] [SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models](https://arxiv.org/abs/2510.26769)
*Anushka Sivakumar,Andrew Zhang,Zaber Hakim,Chris Thomas*

Main category: cs.CV

TL;DR: SteerVLM introduces a lightweight steering module that guides Vision-Language Models to better follow instructions by dynamically adjusting activations between language and image modalities, requiring only 0.14% of the original model's parameters.


<details>
  <summary>Details</summary>
Motivation: To enable fine-grained control over VLM outputs during inference without modifying model weights, while preserving performance on other tasks and avoiding manual intervention tuning.

Method: Learns from latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting language modality with image context, using dimension-wise activation modulation and adaptive steering across layers.

Result: Outperforms existing intervention techniques on steering and hallucination mitigation benchmarks, and introduces VNIA dataset for VLM steering evaluation.

Conclusion: Proposes a robust solution for multimodal model control through activation engineering that enables inference-time control over complex output semantics while maintaining off-target task performance.

Abstract: This work introduces SteerVLM, a lightweight steering module designed to
guide Vision-Language Models (VLMs) towards outputs that better adhere to
desired instructions. Our approach learns from the latent embeddings of paired
prompts encoding target and converse behaviors to dynamically adjust
activations connecting the language modality with image context. This allows
for fine-grained, inference-time control over complex output semantics without
modifying model weights while preserving performance on off-target tasks. Our
steering module requires learning parameters equal to 0.14% of the original
VLM's size. Our steering module gains model control through dimension-wise
activation modulation and adaptive steering across layers without requiring
pre-extracted static vectors or manual tuning of intervention points.
Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a
multimodal dataset specifically created to facilitate the development and
evaluation of VLM steering techniques. Our method outperforms existing
intervention techniques on steering and hallucination mitigation benchmarks for
VLMs and proposes a robust solution for multimodal model control through
activation engineering.

</details>


### [62] [Surpassing state of the art on AMD area estimation from RGB fundus images through careful selection of U-Net architectures and loss functions for class imbalance](https://arxiv.org/abs/2510.26778)
*Valentyna Starodub,Mantas Lukoševičius*

Main category: cs.CV

TL;DR: This paper presents an AMD lesion detection framework using semantic segmentation on RGB fundus images, achieving state-of-the-art performance on the ADAM challenge benchmark.


<details>
  <summary>Details</summary>
Motivation: AMD is a leading cause of irreversible vision impairment in elderly people, and there's a need for effective detection methods using non-invasive, cost-effective RGB fundus imaging.

Method: Used U-Net as base framework, evaluated various improvements including pre-processing techniques, different encoder backbone networks of varying complexity, and specialized loss functions to handle class imbalances at image and pixel levels.

Result: The final framework configuration outperformed all prior ADAM challenge submissions for multi-class segmentation of different AMD lesion types in RGB fundus images.

Conclusion: The proposed AMD detection framework achieves superior performance for lesion segmentation in non-invasive fundus images, with source code made publicly available.

Abstract: Age-related macular degeneration (AMD) is one of the leading causes of
irreversible vision impairment in people over the age of 60. This research
focuses on semantic segmentation for AMD lesion detection in RGB fundus images,
a non-invasive and cost-effective imaging technique. The results of the ADAM
challenge - the most comprehensive AMD detection from RGB fundus images
research competition and open dataset to date - serve as a benchmark for our
evaluation. Taking the U-Net connectivity as a base of our framework, we
evaluate and compare several approaches to improve the segmentation model's
architecture and training pipeline, including pre-processing techniques,
encoder (backbone) deep network types of varying complexity, and specialized
loss functions to mitigate class imbalances on image and pixel levels. The main
outcome of this research is the final configuration of the AMD detection
framework, which outperforms all the prior ADAM challenge submissions on the
multi-class segmentation of different AMD lesion types in non-invasive RGB
fundus images. The source code used to conduct the experiments presented in
this paper is made freely available.

</details>


### [63] [ChartAB: A Benchmark for Chart Grounding & Dense Alignment](https://arxiv.org/abs/2510.26781)
*Aniruddh Bansal,Davit Soselia,Dang Nguyen,Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces ChartAlign Benchmark (ChartAB) to evaluate vision-language models' chart grounding capabilities, including data extraction, element localization, and attribute recognition from diverse charts.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack accurate perception of details and struggle with fine-grained structure extraction from charts, which limits their ability to compare multiple charts and reason over them.

Method: Developed a comprehensive benchmark with JSON template for evaluation metrics, and incorporated a two-stage inference workflow to evaluate VLMs' capability to align and compare elements across charts.

Result: Evaluation of recent VLMs revealed perception biases, weaknesses, robustness issues, and hallucinations in chart understanding, highlighting fine-grained discrepancies among models.

Conclusion: The findings identify specific skills that need strengthening in current VLMs for chart understanding tasks and provide insights into their limitations.

Abstract: Charts play an important role in visualization, reasoning, data analysis, and
the exchange of ideas among humans. However, existing vision-language models
(VLMs) still lack accurate perception of details and struggle to extract
fine-grained structures from charts. Such limitations in chart grounding also
hinder their ability to compare multiple charts and reason over them. In this
paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a
comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting
tabular data, localizing visualization elements, and recognizing various
attributes from charts of diverse types and complexities. We design a JSON
template to facilitate the calculation of evaluation metrics specifically
tailored for each grounding task. By incorporating a novel two-stage inference
workflow, the benchmark can further evaluate VLMs' capability to align and
compare elements/attributes across two charts. Our analysis of evaluations on
several recent VLMs reveals new insights into their perception biases,
weaknesses, robustness, and hallucinations in chart understanding. These
findings highlight the fine-grained discrepancies among VLMs in chart
understanding tasks and point to specific skills that need to be strengthened
in current models.

</details>


### [64] [HEIR: Learning Graph-Based Motion Hierarchies](https://arxiv.org/abs/2510.26786)
*Cheng Zheng,William Koch,Baiang Li,Felix Heide*

Main category: cs.CV

TL;DR: A general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data using graph-based hierarchies and differentiable graph learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting generalizability across different tasks. There's a need for adaptable, data-driven hierarchical modeling.

Method: Represents motions using graph-based hierarchies that decompose global absolute motions into parent-inherited patterns and local motion residuals. Formulates hierarchy inference as a differentiable graph learning problem using graph neural networks.

Result: Successfully reconstructs intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to baseline on dynamic 3D Gaussian splatting scenes.

Conclusion: Provides an adaptable, data-driven hierarchical modeling paradigm applicable to a broad range of motion-centric tasks across computer vision, graphics, and robotics.

Abstract: Hierarchical structures of motion exist across research fields, including
computer vision, graphics, and robotics, where complex dynamics typically arise
from coordinated interactions among simpler motion components. Existing methods
to model such dynamics typically rely on manually-defined or heuristic
hierarchies with fixed motion primitives, limiting their generalizability
across different tasks. In this work, we propose a general hierarchical motion
modeling method that learns structured, interpretable motion relationships
directly from data. Our method represents observed motions using graph-based
hierarchies, explicitly decomposing global absolute motions into
parent-inherited patterns and local motion residuals. We formulate hierarchy
inference as a differentiable graph learning problem, where vertices represent
elemental motions and directed edges capture learned parent-child dependencies
through graph neural networks. We evaluate our hierarchical reconstruction
approach on three examples: 1D translational motion, 2D rotational motion, and
dynamic 3D scene deformation via Gaussian splatting. Experimental results show
that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,
and produces more realistic and interpretable deformations compared to the
baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,
data-driven hierarchical modeling paradigm, our method offers a formulation
applicable to a broad range of motion-centric tasks. Project Page:
https://light.princeton.edu/HEIR/

</details>


### [65] [The Quest for Generalizable Motion Generation: Data, Model, and Evaluation](https://arxiv.org/abs/2510.26794)
*Jing Lin,Ruisi Wang,Junzhe Lu,Ziqi Huang,Guorui Song,Ailing Zeng,Xian Liu,Chen Wei,Wanqi Yin,Qingping Sun,Zhongang Cai,Lei Yang,Ziwei Liu*

Main category: cs.CV

TL;DR: A framework that transfers knowledge from video generation to 3D human motion generation through large-scale dataset creation, unified modeling, and comprehensive evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing 3D motion generation models face generalization bottlenecks, while video generation has shown remarkable generalization in modeling human behaviors, suggesting transferable insights.

Method: Created ViMoGen-228K dataset with 228K motion samples, proposed ViMoGen (flow-matching diffusion transformer) with gated multimodal conditioning, and developed ViMoGen-light distilled variant and MBench evaluation benchmark.

Result: Significantly outperforms existing approaches in both automatic and human evaluations, demonstrating improved generalization capability.

Conclusion: The framework successfully bridges video generation and motion generation domains, providing comprehensive solutions for data, modeling, and evaluation to address generalization challenges.

Abstract: Despite recent advances in 3D human motion generation (MoGen) on standard
benchmarks, existing models still face a fundamental bottleneck in their
generalization capability. In contrast, adjacent generative fields, most
notably video generation (ViGen), have demonstrated remarkable generalization
in modeling human behaviors, highlighting transferable insights that MoGen can
leverage. Motivated by this observation, we present a comprehensive framework
that systematically transfers knowledge from ViGen to MoGen across three key
pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a
large-scale dataset comprising 228,000 high-quality motion samples that
integrates high-fidelity optical MoCap data with semantically annotated motions
from web videos and synthesized samples generated by state-of-the-art ViGen
models. The dataset includes both text-motion pairs and text-video-motion
triplets, substantially expanding semantic diversity. Second, we propose
ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
MoCap data and ViGen models through gated multimodal conditioning. To enhance
efficiency, we further develop ViMoGen-light, a distilled variant that
eliminates video generation dependencies while preserving strong
generalization. Finally, we present MBench, a hierarchical benchmark designed
for fine-grained evaluation across motion quality, prompt fidelity, and
generalization ability. Extensive experiments show that our framework
significantly outperforms existing approaches in both automatic and human
evaluations. The code, data, and benchmark will be made publicly available.

</details>


### [66] [Scaling Image Geo-Localization to Continent Level](https://arxiv.org/abs/2510.26795)
*Philipp Lindenberger,Paul-Edouard Sarlin,Jan Hosang,Matteo Balice,Marc Pollefeys,Simon Lynen,Eduard Trulls*

Main category: cs.CV

TL;DR: A hybrid approach for fine-grained image geo-localization at continental scale using proxy classification and aerial imagery embeddings to achieve precise location within 200m for 68% of queries in Europe.


<details>
  <summary>Details</summary>
Motivation: Standard image retrieval fails at global scale due to large data volumes and insufficient coverage, while existing scalable solutions trade off between coarse classification results and domain gap issues in cross-view retrieval.

Method: Uses proxy classification during training to learn rich feature representations encoding location information, then combines these learned prototypes with aerial imagery embeddings to handle ground-level data sparsity.

Result: Achieves fine-grained geo-localization across continental scale, localizing within 200m for more than 68% of queries on a dataset covering large parts of Europe.

Conclusion: The hybrid approach enables direct, fine-grained retrieval over multi-country areas, demonstrating effective scaling of geo-localization while maintaining precision.

Abstract: Determining the precise geographic location of an image at a global scale
remains an unsolved challenge. Standard image retrieval techniques are
inefficient due to the sheer volume of images (>100M) and fail when coverage is
insufficient. Scalable solutions, however, involve a trade-off: global
classification typically yields coarse results (10+ kilometers), while
cross-view retrieval between ground and aerial imagery suffers from a domain
gap and has been primarily studied on smaller regions. This paper introduces a
hybrid approach that achieves fine-grained geo-localization across a large
geographic expanse the size of a continent. We leverage a proxy classification
task during training to learn rich feature representations that implicitly
encode precise location information. We combine these learned prototypes with
embeddings of aerial imagery to increase robustness to the sparsity of
ground-level data. This enables direct, fine-grained retrieval over areas
spanning multiple countries. Our extensive evaluation demonstrates that our
approach can localize within 200m more than 68\% of queries of a dataset
covering a large part of Europe. The code is publicly available at
https://scaling-geoloc.github.io.

</details>


### [67] [SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting](https://arxiv.org/abs/2510.26796)
*Dongyue Lu,Ao Liang,Tianxin Huang,Xiao Fu,Yuyang Zhao,Baorui Ma,Liang Pan,Wei Yin,Lingdong Kong,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: SEE4D is a pose-free video-to-4D method that uses virtual cameras and view-conditional inpainting to synthesize spatiotemporal content from casual videos without 3D supervision, outperforming trajectory-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing video-to-4D methods require manually annotated camera poses, which are labor-intensive and brittle for in-the-wild footage. Warp-then-inpaint approaches still entangle camera motion with scene dynamics, complicating modeling and inference.

Method: Proposes trajectory-to-camera framework with fixed virtual cameras, separating camera control from scene modeling. Uses view-conditional video inpainting trained to denoise warped images and inpaint missing regions across viewpoints. Implements spatiotemporal autoregressive inference with virtual-camera splines and overlapping windows.

Result: Achieves superior generalization and improved performance on cross-view video generation and sparse reconstruction benchmarks compared to pose- or trajectory-conditioned baselines, as validated by quantitative metrics and qualitative assessments.

Conclusion: SEE4D advances practical 4D world modeling from casual videos by eliminating the need for explicit 3D annotations and providing a more robust framework for synthesizing spatiotemporal content.

Abstract: Immersive applications call for synthesizing spatiotemporal 4D content from
casual videos without costly 3D supervision. Existing video-to-4D methods
typically rely on manually annotated camera poses, which are labor-intensive
and brittle for in-the-wild footage. Recent warp-then-inpaint approaches
mitigate the need for pose labels by warping input frames along a novel camera
trajectory and using an inpainting model to fill missing regions, thereby
depicting the 4D scene from diverse viewpoints. However, this
trajectory-to-trajectory formulation often entangles camera motion with scene
dynamics and complicates both modeling and inference. We introduce SEE4D, a
pose-free, trajectory-to-camera framework that replaces explicit trajectory
prediction with rendering to a bank of fixed virtual cameras, thereby
separating camera control from scene modeling. A view-conditional video
inpainting model is trained to learn a robust geometry prior by denoising
realistically synthesized warped images and to inpaint occluded or missing
regions across virtual viewpoints, eliminating the need for explicit 3D
annotations. Building on this inpainting core, we design a spatiotemporal
autoregressive inference pipeline that traverses virtual-camera splines and
extends videos with overlapping windows, enabling coherent generation at
bounded per-step complexity. We validate See4D on cross-view video generation
and sparse reconstruction benchmarks. Across quantitative metrics and
qualitative assessments, our method achieves superior generalization and
improved performance relative to pose- or trajectory-conditioned baselines,
advancing practical 4D world modeling from casual videos.

</details>


### [68] [Masked Diffusion Captioning for Visual Feature Learning](https://arxiv.org/abs/2510.26799)
*Chao Feng,Zihao Wei,Andrew Owens*

Main category: cs.CV

TL;DR: Masked diffusion captioning (MDC) learns visual features by reconstructing masked text tokens from images using diffusion models, achieving competitive performance with autoregressive and contrastive methods.


<details>
  <summary>Details</summary>
Motivation: To develop a visual feature learning approach that doesn't depend on token position like autoregressive methods and reduces the need for auxiliary objectives.

Method: Train an image-conditioned masked diffusion language model where text tokens in image-caption pairs are randomly masked, and a visual-conditioned decoder reconstructs the original text.

Result: Linear probing experiments show MDC produces visual features competitive with autoregressive and contrastive approaches across various models and datasets.

Conclusion: MDC provides an effective alternative for visual feature learning that decouples visual signal strength from token position and reduces auxiliary objective requirements.

Abstract: We learn visual features by captioning images with an image-conditioned
masked diffusion language model, a formulation we call masked diffusion
captioning (MDC). During training, text tokens in each image-caption pair are
masked at a randomly chosen ratio, and a decoder conditioned on visual features
is trained to reconstruct the original text. After training, the learned visual
features can be applied to downstream vision tasks. Unlike autoregressive
captioning, the strength of the visual learning signal in MDC does not depend
on each token's position in the sequence, reducing the need for auxiliary
objectives. Linear probing experiments across a variety of academic-scale
models and datasets show that the learned visual features are competitive with
those produced by autoregressive and contrastive approaches.

</details>


### [69] [OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes](https://arxiv.org/abs/2510.26800)
*Yukun Huang,Jiwen Yu,Yanning Zhou,Jianan Wang,Xintao Wang,Pengfei Wan,Xihui Liu*

Main category: cs.CV

TL;DR: OmniX is a unified framework that repurposes 2D generative models for panoramic perception of geometry, textures, and PBR materials to create graphics-ready 3D scenes suitable for physically based rendering, relighting, and simulation.


<details>
  <summary>Details</summary>
Motivation: To advance panorama-based 2D lifting techniques beyond appearance generation and enable the creation of physically realistic 3D environments with intrinsic properties like geometry and materials.

Method: Uses a lightweight cross-modal adapter structure to reuse 2D generative priors for panoramic vision tasks including perception, generation, and completion. Built a large-scale synthetic panorama dataset with multimodal panoramas from diverse scenes.

Result: Extensive experiments demonstrate effectiveness in panoramic visual perception and graphics-ready 3D scene generation, enabling physically realistic virtual world creation.

Conclusion: OmniX opens new possibilities for immersive and physically realistic virtual world generation by unifying panoramic perception and generation capabilities.

Abstract: There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.

</details>


### [70] [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802)
*Ziyu Guo,Xinyan Chen,Renrui Zhang,Ruichuan An,Yu Qi,Dongzhi Jiang,Xiangtai Li,Manyuan Zhang,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: Video generation models like Veo-3 show emerging visual reasoning capabilities but are not yet reliable as standalone zero-shot reasoners, exhibiting strengths in short-horizon spatial coherence but limitations in long-horizon causal reasoning.


<details>
  <summary>Details</summary>
Motivation: To investigate whether current video generation models can serve as zero-shot reasoners in challenging visual reasoning scenarios, given their demonstrated capabilities in producing high-fidelity, temporally coherent videos.

Method: Conducted empirical study evaluating Veo-3 across 12 reasoning dimensions (spatial, geometric, physical, temporal, embodied logic) using MME-CoF benchmark for Chain-of-Frame reasoning assessment.

Result: Models show promising reasoning in short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, but struggle with long-horizon causal reasoning, strict geometric constraints, and abstract logic.

Conclusion: Current video models are not reliable as standalone zero-shot reasoners but show potential as complementary visual engines alongside dedicated reasoning models.

Abstract: Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io

</details>
