<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 71]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

TL;DR: An explainable AI system that combines a lightweight CNN classifier with a Vision-Language Model to detect, localize, and explain artifacts in 32x32 images with 96.5% accuracy and fast inference.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of verifying visual authenticity due to increasing realism of AI-generated imagery.

Method: Combines Faster-Than-Lies CNN classifier with Qwen2-VL-7B Vision-Language Model, using autoencoder-based reconstruction error maps for artifact localization heatmaps.

Result: Achieves 96.5% accuracy on extended CiFAKE dataset with adversarial perturbations, maintains 175ms inference time on 8-core CPUs, and generates explainable text for 70 categorized artifact types.

Conclusion: Demonstrates feasibility of combining visual and linguistic reasoning for interpretable authenticity detection in low-resolution imagery with cross-domain applications.

Abstract: The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [2] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

TL;DR: CountFormer is a transformer-based framework for class-agnostic object counting that uses DINOv2 features and positional embeddings to recognize visual repetition and structural relationships, achieving state-of-the-art performance on complex scenes.


<details>
  <summary>Details</summary>
Motivation: Humans can count diverse objects by perceiving visual repetition and structural relationships, but existing counting models often miscount on objects with complex shapes, internal symmetry, or overlapping components.

Method: Built on CounTR architecture, replaces visual encoder with self-supervised DINOv2 foundation model, incorporates positional embedding fusion, and uses lightweight convolutional decoder to generate density maps.

Result: Achieves performance comparable to state-of-the-art methods on FSC-147 dataset, with superior accuracy on structurally intricate or densely packed scenes.

Conclusion: Integrating foundation models like DINOv2 enables counting systems to approach human-like structural perception, advancing toward truly general and exemplar-free counting.

Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [3] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Cl√©ment Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

TL;DR: A novel framework using fixed cameras and deep learning for continuous monitoring of floating debris in rivers, with geometric modeling for object size estimation.


<details>
  <summary>Details</summary>
Motivation: Floating anthropogenic debris in rivers negatively impacts biodiversity, water quality, navigation, and recreation, requiring effective monitoring solutions.

Method: Uses fixed in-situ cameras with deep learning models for debris detection and quantification, plus geometric modeling using camera characteristics for size estimation.

Result: Demonstrated feasibility of automated monitoring, identified optimal deep learning models for accuracy and speed, and highlighted importance of dataset protocols.

Conclusion: The approach enables development of robust, low-cost automated monitoring systems for urban aquatic environments using projective geometry and regression corrections.

Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [4] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

TL;DR: RareFlow is a physics-aware super-resolution framework for remote sensing imagery that addresses out-of-distribution robustness through dual-conditioning architecture, uncertainty quantification, and multifaceted loss functions.


<details>
  <summary>Details</summary>
Motivation: Super-resolution for remote sensing imagery often fails under out-of-distribution conditions, producing visually plausible but physically inaccurate results, especially for rare geomorphic features captured by diverse sensors.

Method: Uses dual-conditioning architecture with Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Introduces multifaceted loss function for spectral/radiometric consistency and employs stochastic forward pass for uncertainty quantification.

Result: In blind evaluations by geophysical experts, outputs approached ground truth fidelity, significantly outperforming state-of-the-art baselines. Quantitative gains include nearly 40% reduction in FID.

Conclusion: RareFlow provides a robust framework for high-fidelity synthesis in data-scarce scientific domains and offers a new paradigm for controlled generation under severe domain shift.

Abstract: Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [5] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

TL;DR: A training-free method for 3D scene generation that repurposes text-to-3D object diffusion models as modular tile generators, enabling scalable synthesis of large, coherent scenes through multi-tile denoising and seamless blending.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene generation methods are limited to single objects, require domain-specific training, or lack full 360-degree viewability, creating a need for more flexible and scalable approaches.

Method: Reformulates scene generation as a multi-tile denoising problem where overlapping 3D regions are independently generated using text-to-3D object diffusion models and seamlessly blended via weighted averaging.

Result: The approach enables scalable synthesis of large, coherent scenes while preserving local semantic control, supports diverse scene layouts, efficient generation, and flexible editing without requiring scene-level datasets or retraining.

Conclusion: Establishes a simple yet powerful foundation for general-purpose, language-driven 3D scene construction by leveraging object-level priors and eliminating the need for scene-specific training.

Abstract: Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [6] [Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2510.23894)
*Jinxin Zhou,Jiachen Jiang,Zhihui Zhu*

Main category: cs.CV

TL;DR: LHT-CLIP is a training-free framework that improves CLIP for semantic segmentation by exploiting visual discriminability across layers, attention heads, and tokens through three techniques: semantic-spatial reweighting, selective head enhancement, and abnormal token replacement.


<details>
  <summary>Details</summary>
Motivation: CLIP models struggle with semantic segmentation due to misalignment between image-level pre-training and pixel-level dense prediction requirements. Prior methods inherit global alignment bias from earlier layers, leading to suboptimal segmentation performance.

Method: The framework uses three complementary techniques: semantic-spatial reweighting to restore visual discriminability, selective head enhancement to leverage consistently discriminative attention heads, and abnormal token replacement to handle anomalous tokens with sparse activation patterns.

Result: Extensive experiments on 8 semantic segmentation benchmarks show LHT-CLIP achieves state-of-the-art performance across diverse scenarios without additional training, auxiliary networks, or extensive hyperparameter tuning.

Conclusion: LHT-CLIP effectively restores visual discriminability in CLIP models for semantic segmentation, demonstrating superior performance and practical deployment value through systematic exploitation of layer, head, and token-level characteristics.

Abstract: Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.

</details>


### [7] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: DynaStride generates coherent scene-level captions for instructional videos using adaptive frame sampling, multimodal windowing, and a dynamic stride selection algorithm that balances temporal context and redundancy.


<details>
  <summary>Details</summary>
Motivation: Scene-level captioning in instructional videos enhances learning by understanding visual cues and temporal structure, but current captions often lack coherence and quality, undermining educational intent.

Method: Uses adaptive frame sampling and multimodal windowing to capture key transitions, employs multimodal chain-of-thought for action-object pairs, and refines them with dynamic stride window selection algorithm.

Result: Outperforms strong baselines (VLLaMA3, GPT-4o) on both N-gram metrics (BLEU, METEOR) and semantic similarity measures (BERTScore, CLIPScore), producing more temporally coherent and informative captions.

Conclusion: DynaStride shows promising direction for improving AI-powered instructional content generation by integrating visual semantics and temporal reasoning in coherent scene-level captions.

Abstract: Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [8] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

TL;DR: TurboPortrait3D is a low-latency method for novel-view synthesis of human portraits that combines image-to-avatar generation with diffusion models to enhance quality while maintaining 3D consistency.


<details>
  <summary>Details</summary>
Motivation: Existing image-to-3D models for portraits produce visual artifacts, lack detail, and fail to preserve subject identity, while diffusion models are computationally expensive and not 3D-aware.

Method: Uses a feedforward image-to-avatar pipeline to get initial 3D representation and noisy renders, then refines them with a single-step diffusion model conditioned on input images and trained for multi-view consistency.

Result: Qualitatively and quantitatively outperforms current state-of-the-art methods for portrait novel-view synthesis while being efficient in time.

Conclusion: Image-space diffusion models can effectively enhance existing image-to-avatar methods, maintaining 3D-awareness and running with low latency for high-quality portrait novel-view synthesis.

Abstract: We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [9] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

TL;DR: PlanarGS enhances 3D Gaussian Splatting for indoor scenes by incorporating planar and geometric priors to overcome limitations in low-texture regions, achieving superior 3D surface reconstruction.


<details>
  <summary>Details</summary>
Motivation: 3DGS struggles with ambiguous geometry in large, low-texture indoor scenes due to reliance on photometric loss alone.

Method: Uses Language-Prompted Planar Priors (LP3) pipeline with vision-language segmentation and geometric priors, plus planar and geometric supervision terms for Gaussian optimization.

Result: Outperforms state-of-the-art methods by large margins on standard indoor benchmarks, reconstructing accurate and detailed 3D surfaces.

Conclusion: PlanarGS effectively addresses 3DGS limitations in indoor scenes through planar and geometric priors, enabling high-fidelity reconstruction.

Abstract: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [10] [Adaptive Training of INRs via Pruning and Densification](https://arxiv.org/abs/2510.23943)
*Diana Aldana,Jo√£o Paulo Lima,Daniel Csillag,Daniel Perazzo,Haoan Feng,Luiz Velho,Tiago Novello*

Main category: cs.CV

TL;DR: AIRe is an adaptive training scheme for implicit neural representations that uses neuron pruning and input frequency densification to optimize network architecture during training, achieving better size-quality trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current methods for implicit neural representations require manual selection of input frequencies and architectures, leading to parameter redundancy and heavy hyperparameter optimization.

Method: AIRe uses neuron pruning to remove redundant neurons by identifying less-contributory neurons and applying targeted weight decay, followed by structured pruning. It then densifies input frequencies in spectrum regions where the signal underfits.

Result: Experiments on images and SDFs show that AIRe reduces model size while preserving or improving reconstruction quality.

Conclusion: AIRe provides an effective adaptive training scheme that automatically refines INR architecture during optimization, eliminating the need for manual hyperparameter tuning.

Abstract: Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.

</details>


### [11] [Neural USD: An object-centric framework for iterative editing and control](https://arxiv.org/abs/2510.23956)
*Alejandro Escontrela,Shrinu Kushagra,Sjoerd van Steenkiste,Yulia Rubanova,Aleksander Holynski,Kelsey Allen,Kevin Murphy,Thomas Kipf*

Main category: cs.CV

TL;DR: Neural USD introduces a hierarchical scene representation framework inspired by computer graphics standards, enabling precise per-object editing in generative models without unintended global changes.


<details>
  <summary>Details</summary>
Motivation: Current controllable generative models often cause unintended global changes when trying to edit specific objects, lacking precise and iterative object editing capabilities.

Method: Proposes Neural USD framework with structured hierarchical scene representation, applying fine-tuning to disentangle control signals for appearance, geometry, and pose per object.

Result: The framework enables iterative and incremental workflows with precise per-object control, minimizing model-specific constraints while accommodating diverse signals.

Conclusion: Neural USD successfully addresses challenges in precise object editing by providing structured scene representation that supports disentangled control over individual scene elements.

Abstract: Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .

</details>


### [12] [SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability](https://arxiv.org/abs/2510.23960)
*Peiyang Xu,Minzhou Pan,Zhaorun Chen,Shuang Yang,Chaowei Xiao,Bo Li*

Main category: cs.CV

TL;DR: SafeVision is a novel image guardrail system that uses human-like reasoning to detect unsafe content, dynamically adapts to new safety policies without retraining, and provides transparent explanations.


<details>
  <summary>Details</summary>
Motivation: Traditional image guardrail models have limitations: they misclassify content due to lack of semantic reasoning, struggle with emerging threats requiring costly retraining, and lack transparency in decision-making.

Method: The approach includes: effective data collection/generation framework, policy-following training pipeline, customized loss function, and diverse QA generation/training strategy. Also introduces VisionHarm dataset with comprehensive harmful categories.

Result: SafeVision achieves state-of-the-art performance, outperforming GPT-4o by 8.6% on VisionHarm-T and 15.5% on VisionHarm-C, while being over 16x faster.

Conclusion: SafeVision establishes a comprehensive, policy-following, and explainable image guardrail system with dynamic adaptation to emerging threats, addressing key limitations of traditional approaches.

Abstract: With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.

</details>


### [13] [Reasoning Visual Language Model for Chest X-Ray Analysis](https://arxiv.org/abs/2510.23968)
*Andriy Myronenko,Dong Yang,Baris Turkbey,Mariam Aboian,Sena Azamat,Esra Akcicek,Hongxu Yin,Pavlo Molchanov,Marc Edgar,Yufan He,Pengfei Guo,Yucheng Tang,Daguang Xu*

Main category: cs.CV

TL;DR: A framework that brings chain-of-thought reasoning to chest X-ray interpretation, enabling transparent stepwise reasoning that aligns with radiologists' workflow and supports clinical auditability.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models for medical image analysis are opaque and lack the transparent, stepwise reasoning that clinicians rely on for trust and verification.

Method: Couples high-fidelity visual encoding with two-stage training: reasoning-style supervised fine-tuning followed by reinforcement learning using verifiable rewards over X-ray abnormalities.

Result: Achieves competitive multi-label classification in out-of-distribution evaluation while improving interpretability. Reader study showed reasoning traces increased radiologist confidence, supported error auditing, and reduced report finalization time.

Conclusion: The approach enables trustworthy, explainable AI in medical imaging where reasoning quality is as critical as prediction quality, supporting safer human-AI collaboration.

Abstract: Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.

</details>


### [14] [Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints](https://arxiv.org/abs/2510.23978)
*Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: Proposes joint prediction of multiple Fourier components for better quality and efficiency in arbitrary-scale super-resolution, addressing limitations of existing recurrent neural network methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods predict Fourier components one by one using recurrent neural networks, leading to performance degradation and inefficiency due to independent prediction.

Method: Predicting multiple Fourier components jointly instead of one by one.

Result: Improved both quality and efficiency in arbitrary-scale super-resolution.

Conclusion: Joint prediction of multiple components is more effective than independent sequential prediction for Fourier-based super-resolution.

Abstract: Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.

</details>


### [15] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo is a comprehensive benchmark for egocentric AI assistants that evaluates memory, understanding, and cross-memory reasoning across long-duration, streaming, multi-modal data in realistic daily contexts.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks evaluate AI assistant capabilities in isolation, lack realistic streaming scenarios, or only support short-term tasks, failing to capture the real-world requirements of egocentric AI assistants.

Method: Created a dataset with over 14 hours per participant of synchronized egocentric video, audio, and text across four domains, aligned on a unified timeline with human-refined visual narrations and speech transcripts. Defined 12 diagnostic subtasks across three core capabilities with 3,291 human-verified QA items.

Result: Developed a benchmark with comprehensive evaluation metrics (Real-Time Accuracy and Memory Persistence Time) that assesses correctness, temporal responsiveness, and long-term retention in streaming settings.

Conclusion: TeleEgo provides a realistic and comprehensive evaluation framework to advance the development of practical AI assistants capable of processing multi-modal inputs, responding in real time, and retaining evolving long-term memory.

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [16] [AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization](https://arxiv.org/abs/2510.24000)
*Heethanjan Kanagalingam,Thenukan Pathmanathan,Mokeeshan Vathanakumar,Tharmakulasingam Mukunthan*

Main category: cs.CV

TL;DR: Proposes AdvBlur, a novel diabetic retinopathy classification method that uses adversarial blurred images and dual-loss functions to improve domain generalization across different datasets and imaging conditions.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of existing deep learning models in maintaining robustness against distributional variations caused by different acquisition devices, demographic disparities, and imaging conditions in diabetic retinopathy detection.

Method: Integrates adversarial blurred images into the dataset and employs a dual-loss function framework to enhance domain generalization capabilities.

Result: Achieves competitive performance compared to state-of-the-art domain generalization DR models on unseen external datasets, with comprehensive evaluations across multiple datasets.

Conclusion: The AdvBlur method effectively mitigates the impact of unseen distributional variations and demonstrates strong domain generalization capabilities for diabetic retinopathy classification.

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.

</details>


### [17] [Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge](https://arxiv.org/abs/2510.24009)
*Yuan Jin,Antonio Pepe,Gian Marco Melito,Yuxuan Chen,Yunsu Byeon,Hyeseong Kim,Kyungwon Kim,Doohyun Park,Euijoon Choi,Dosik Hwang,Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu,Ayman El-Ghotni,Mohamed Nabil,Hossam El-Kady,Ahmed Ayyad,Amr Nasr,Marek Wodzinski,Henning M√ºller,Hyeongyu Kim,Yejee Shin,Abbas Khan,Muhammad Asad,Alexander Zolotarev,Caroline Roney,Anthony Mathur,Martin Benning,Gregory Slabaugh,Theodoros Panagiotis Vagenas,Konstantinos Georgas,George K. Matsopoulos,Jihan Zhang,Zhen Zhang,Liqin Huang,Christian Mayer,Heinrich M√§chler,Jan Egger*

Main category: cs.CV

TL;DR: The SEG.A challenge introduced a large public dataset for aortic vessel tree segmentation, benchmarking deep learning algorithms and revealing that ensemble methods outperform individual models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of shared, high-quality data that has impeded development of automated aortic vessel tree analysis from CT angiography.

Method: Launched a multi-institutional challenge with a large public dataset, benchmarked automated algorithms on hidden test sets, and included optional surface meshing tasks for computational simulations.

Result: 3D U-Net architectures dominated top submissions, ensemble methods significantly outperformed individual models, and performance was strongly linked to algorithmic design and training data characteristics.

Conclusion: The initiative establishes a new performance benchmark and provides a lasting resource to drive future innovation toward robust, clinically translatable tools.

Abstract: The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.

</details>


### [18] [Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks](https://arxiv.org/abs/2510.24010)
*Mirali Purohit,Bimal Gajera,Vatsal Malaviya,Irish Mehta,Kunal Kasodekar,Jacob Adler,Steven Lu,Umaa Rebbapragada,Hannah Kerner*

Main category: cs.CV

TL;DR: Mars-Bench is the first standardized benchmark for evaluating foundation models on Mars-related tasks using orbital and surface imagery across 20 datasets for classification, segmentation, and object detection.


<details>
  <summary>Details</summary>
Motivation: Mars science lacks standardized benchmarks and evaluation frameworks that have enabled progress in other domains like Earth Observation, limiting the development of foundation models for Martian tasks.

Method: Created Mars-Bench with 20 datasets spanning classification, segmentation, and object detection tasks focused on key geologic features. Provided standardized datasets and baseline evaluations using models pre-trained on natural images, Earth satellite data, and vision-language models.

Result: Results suggest Mars-specific foundation models may offer advantages over general-domain counterparts, indicating the potential benefits of domain-adapted pre-training for Martian tasks.

Conclusion: Mars-Bench establishes a standardized foundation for developing and comparing machine learning models for Mars science, with data, models, and code made publicly available.

Abstract: Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.

</details>


### [19] [AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://arxiv.org/abs/2510.24034)
*Yufan Liu,Wanqian Zhang,Huashan Chen,Lin Wang,Xiaojun Jia,Zheng Lin,Weiping Wang*

Main category: cs.CV

TL;DR: APT is a black-box framework that uses LLMs to generate human-readable adversarial suffixes for text-to-image models, bypassing safety filters through dual-evasion strategy and achieving high transferability.


<details>
  <summary>Details</summary>
Motivation: Current red-teaming methods for T2I models require white-box access, use inefficient per-prompt optimization, and generate meaningless prompts that are easily blocked by filters.

Method: Alternating optimization-finetuning pipeline between adversarial suffix optimization and LLM fine-tuning, with dual-evasion strategy using perplexity scoring and banned-token penalties.

Result: Excellent red-teaming performance with human-readable, filter-resistant prompts, and superior zero-shot transferability to unseen prompts and commercial APIs.

Conclusion: APT effectively exposes critical vulnerabilities in T2I models through human-readable adversarial prompts that bypass safety mechanisms.

Abstract: Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).

</details>


### [20] [ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning](https://arxiv.org/abs/2510.24036)
*Xingyu Liu,Kun Ming Goh*

Main category: cs.CV

TL;DR: ResNet uses skip connections to solve vanishing gradient problem, enabling training of very deep networks with better accuracy and faster convergence.


<details>
  <summary>Details</summary>
Motivation: To overcome the vanishing gradient problem that makes training very deep convolutional neural networks challenging.

Method: Uses residual blocks with skip connections that allow gradients to flow directly through shortcut connections bypassing intermediate layers.

Result: ResNet-18 achieved 89.9% accuracy on CIFAR-10 vs 84.1% for traditional deep CNN of similar depth, with faster convergence and more stable training.

Conclusion: Residual Networks successfully enable training of very deep networks by addressing vanishing gradients through skip connections, achieving superior performance.

Abstract: Convolutional Neural Networks (CNNs) has revolutionized computer vision, but
training very deep networks has been challenging due to the vanishing gradient
problem. This paper explores Residual Networks (ResNet), introduced by He et
al. (2015), which overcomes this limitation by using skip connections. ResNet
enables the training of networks with hundreds of layers by allowing gradients
to flow directly through shortcut connections that bypass intermediate layers.
In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%
accuracy compared to 84.1% for a traditional deep CNN of similar depth, while
also converging faster and training more stably.

</details>


### [21] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: SNELLA is a one-stage parameter-efficient fine-tuning method that uses nonlinear low-rank decomposition and adaptive bi-level sparsity allocation to achieve state-of-the-art performance with significantly reduced memory usage compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current sparse tuning methods have limitations: they use a two-stage approach that overlooks parameter adjustments during fine-tuning, and they suffer from high memory usage due to storing all weight matrices in the optimizer.

Method: SNELLA uses a one-stage approach with selective weight updates via sparse matrix addition to low-rank learnable matrices, enhanced by nonlinear kernel functions to increase rank and prevent interdependency. It also employs adaptive bi-level sparsity allocation that allows weights to compete across and within layers based on importance scores in an end-to-end manner.

Result: SNELLA achieves SOTA performance with 1.8% higher Top-1 accuracy (91.9% vs 90.1%) on FGVC benchmark compared to SPT-LoRA, and reduces memory usage by 31.1%-39.9% across models ranging from 86M to 632M parameters.

Conclusion: SNELLA successfully overcomes the limitations of existing sparse tuning methods by providing a more efficient one-stage approach that delivers superior performance with significantly reduced memory consumption across various vision tasks.

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [22] [Enhancing CLIP Robustness via Cross-Modality Alignment](https://arxiv.org/abs/2510.24038)
*Xingyu Zhu,Beier Zhu,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: COLA is a training-free framework that uses optimal transport to align image and text features in CLIP models, improving adversarial robustness by 6.7% on average while maintaining clean accuracy.


<details>
  <summary>Details</summary>
Motivation: CLIP models are vulnerable to adversarial attacks due to misalignment between image and text features, which worsens under perturbations. Existing methods overlook this feature gap problem.

Method: COLA projects adversarial image embeddings onto class text feature subspace to filter distortions, then uses optimal transport to align images and texts as discrete distributions over multiple augmented views.

Result: Extensive evaluations on 14 benchmarks show COLA improves zero-shot classification by 6.7% on average under PGD attacks on ImageNet variants while maintaining high clean accuracy.

Conclusion: COLA effectively addresses adversarial misalignment in CLIP through optimal transport-based cross-modality alignment, providing training-free robustness enhancement compatible with existing models.

Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.

</details>


### [23] [Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification](https://arxiv.org/abs/2510.24078)
*William Yang,Xindi Wu,Zhiwei Deng,Esin Tureci,Olga Russakovsky*

Main category: cs.CV

TL;DR: BOB is a fine-tuning strategy for T2I models that extracts class-agnostic attributes (background, pose) from real examples, conditions on them during fine-tuning, and marginalizes them out during generation to improve synthetic data quality for fine-grained classification.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning T2I models with few real examples for synthetic dataset generation can cause overfitting and reduce diversity, limiting effectiveness for classification tasks.

Method: Extract class-agnostic attributes from real examples, condition T2I model fine-tuning on these attributes, then marginalize them out during generation to preserve generative prior and reduce overfitting.

Result: BOB achieves SOTA performance in low-shot fine-grained classification, outperforming DataDream by 7.4% on Aircraft dataset and beating prior art in 18 of 24 experimental settings with 2+% accuracy improvements in 14 settings.

Conclusion: BOB effectively mitigates overfitting in T2I fine-tuning for synthetic data generation, enabling better performance than using more real images in fine-grained classification tasks.

Abstract: Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.

</details>


### [24] [OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation](https://arxiv.org/abs/2510.24093)
*Agus Gunawan,Samuel Teodoro,Yun Chen,Soo Ye Kim,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: OmniText is a training-free generalist framework for Text Image Manipulation (TIM) that addresses limitations of existing text inpainting methods by enabling text removal, style control, and preventing duplicated letters through cross- and self-attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based text inpainting methods have three key limitations: inability to remove text, lack of style control over rendered text, and tendency to generate duplicated letters, which hinders their applicability to broader TIM tasks.

Method: Proposes OmniText using cross- and self-attention mechanisms: self-attention inversion for text removal to reduce hallucinations, cross-attention redistribution to reduce text hallucination, and novel loss functions in latent optimization for controllable inpainting (cross-attention content loss for accuracy and self-attention style loss for style customization).

Result: OmniText achieves state-of-the-art performance across multiple TIM tasks and metrics, comparable with specialist methods. Also introduces OmniText-Bench benchmark dataset for evaluating diverse TIM tasks.

Conclusion: OmniText is the first generalist method capable of performing diverse TIM tasks including text removal, rescaling, repositioning, and insertion/editing with various styles, overcoming limitations of existing text inpainting approaches.

Abstract: Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.

</details>


### [25] [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://arxiv.org/abs/2510.24105)
*Shufan Shen,Zhaobo Qi,Junshu Sun,Qingming Huang,Qi Tian,Shuhui Wang*

Main category: cs.CV

TL;DR: The paper discovers a positive correlation between interpretability and classifiability in pre-trained visual models, proposes an Inherent Interpretability Score (IIS) to quantify representation interpretability, and shows that improving classifiability also enhances interpretability.


<details>
  <summary>Details</summary>
Motivation: To investigate whether pre-trained visual representations can simultaneously achieve high interpretability and classifiability, as current models prioritize classifiability but applications increasingly require interpretable representations.

Method: Proposed Inherent Interpretability Score (IIS) that quantifies representation interpretability by measuring the ratio of interpretable semantics through information loss analysis when interpretations capture only interpretable parts.

Result: Found positive correlation between interpretability and classifiability - representations with higher classifiability contain more interpretable semantics. Fine-tuning with interpretability maximization further improves classifiability, and predictions based on interpretations show less accuracy degradation.

Conclusion: Practitioners can unify improvements in both interpretability and classifiability for pre-trained vision models, as these two properties are positively correlated rather than conflicting objectives.

Abstract: The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.

</details>


### [26] [UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations](https://arxiv.org/abs/2510.24116)
*Fengming Yu,Haiwei Pan,Kejia Zhang,Jian Guan,Haiying Jiang*

Main category: cs.CV

TL;DR: UHKD is a knowledge distillation framework that uses frequency-domain analysis to transfer knowledge between heterogeneous teacher-student models, addressing architectural discrepancies that hinder traditional distillation methods.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge distillation methods work well for homogeneous models but degrade in heterogeneous scenarios due to architectural differences and semantic discrepancies in intermediate representations. Most methods focus only on logits space, underutilizing semantic information in intermediate layers.

Method: Proposes Unified Heterogeneous Knowledge Distillation (UHKD) using Fourier transform to capture global feature information in frequency domain. Includes Feature Transformation Module (FTM) for compact frequency representations and Feature Alignment Module (FAM) for multi-level feature matching. Uses joint objective combining MSE on intermediate features and KL divergence on logits.

Result: Achieves gains of 5.59% on CIFAR-100 and 0.83% on ImageNet-1K over the latest method, demonstrating effectiveness in unifying heterogeneous representations.

Conclusion: UHKD effectively addresses architectural heterogeneity in knowledge distillation by leveraging frequency-domain analysis, enabling efficient utilization of visual knowledge across different model architectures.

Abstract: Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge

</details>


### [27] [DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery](https://arxiv.org/abs/2510.24117)
*Zan Wang,Siyu Chen,Luya Mo,Xinfeng Gao,Yuxin Shen,Lebin Ding,Wei Liang*

Main category: cs.CV

TL;DR: DogMo is a large-scale multi-view RGB-D video dataset of canine movements, addressing limitations of existing datasets with 1.2k sequences from 10 dogs, and introduces a three-stage optimization pipeline for accurate motion recovery.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing dog motion datasets, including lack of multi-view and real 3D data, limited scale, and insufficient diversity in motion and breed.

Method: A three-stage, instance-specific optimization pipeline that progressively refines body shape and pose through coarse alignment, dense correspondence supervision, and temporal regularization, fitting the SMAL model to motion sequences.

Result: Established four motion recovery benchmark settings supporting systematic evaluation across monocular and multi-view, RGB and RGB-D inputs, providing a foundation for accurate canine motion recovery.

Conclusion: DogMo dataset and the proposed method provide a principled foundation for advancing research in dog motion recovery and open new directions in computer vision, graphics, and animal behavior modeling.

Abstract: We present DogMo, a large-scale multi-view RGB-D video dataset capturing
diverse canine movements for the task of motion recovery from images. DogMo
comprises 1.2k motion sequences collected from 10 unique dogs, offering rich
variation in both motion and breed. It addresses key limitations of existing
dog motion datasets, including the lack of multi-view and real 3D data, as well
as limited scale and diversity. Leveraging DogMo, we establish four motion
recovery benchmark settings that support systematic evaluation across monocular
and multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,
we further introduce a three-stage, instance-specific optimization pipeline
that fits the SMAL model to the motion sequences. Our method progressively
refines body shape and pose through coarse alignment, dense correspondence
supervision, and temporal regularization. Our dataset and method provide a
principled foundation for advancing research in dog motion recovery and open up
new directions at the intersection of computer vision, computer graphics, and
animal behavior modeling.

</details>


### [28] [ETC: training-free diffusion models acceleration with Error-aware Trend Consistency](https://arxiv.org/abs/2510.24129)
*Jiajian Xie,Hubery Yin,Chen Li,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: ETC is a training-free framework that accelerates diffusion models by reusing model outputs while maintaining trajectory consistency through trend prediction and error tolerance control.


<details>
  <summary>Details</summary>
Motivation: Current training-free acceleration methods for diffusion models ignore denoising trends and lack error control, leading to trajectory deviations and result inconsistencies when reusing model outputs across multiple steps.

Method: ETC introduces (1) a consistent trend predictor that projects historical denoising patterns into stable future directions across multiple approximation steps, and (2) a model-specific error tolerance search mechanism that identifies transition points from semantic planning to quality refinement to derive corrective thresholds.

Result: ETC achieves 2.65x acceleration over FLUX with only -0.074 SSIM score degradation in consistency, demonstrating significant speedup while maintaining output quality.

Conclusion: The ETC framework effectively accelerates diffusion models by leveraging denoising trend consistency and model-specific error tolerance, enabling faster sampling without compromising generative quality.

Abstract: Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.

</details>


### [29] [Compositional Image Synthesis with Inference-Time Scaling](https://arxiv.org/abs/2510.24133)
*Minsuk Ji,Sanghyeok Lee,Namhyuk Ahn*

Main category: cs.CV

TL;DR: Training-free framework combining object-centric approach with self-refinement to improve text-to-image model compositionality by using LLMs for layout synthesis and VLM-based reranking.


<details>
  <summary>Details</summary>
Motivation: Modern text-to-image models struggle with compositionality, often failing to render accurate object counts, attributes, and spatial relations.

Method: Leverage LLMs to synthesize explicit layouts from prompts, inject layouts into generation process, and use object-centric VLM to rerank candidates iteratively for prompt alignment.

Result: Achieves stronger scene alignment with prompts compared to recent text-to-image models while preserving aesthetic quality.

Conclusion: Unifying explicit layout-grounding with self-refine-based inference-time scaling improves layout faithfulness in text-to-image generation.

Abstract: Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.

</details>


### [30] [VC4VG: Optimizing Video Captions for Text-to-Video Generation](https://arxiv.org/abs/2510.24134)
*Yang Du,Zhuoran Lin,Kaiqiang Song,Biao Wang,Zhicheng Zheng,Tiezheng Ge,Bo Zheng,Qin Jin*

Main category: cs.CV

TL;DR: VC4VG is a framework for optimizing video captions specifically for text-to-video generation training, with a new benchmark showing improved caption quality leads to better video generation performance.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video generation relies on high-quality video-text pairs, but strategies for optimizing video captions specifically for T2V training are underexplored.

Method: Proposed VC4VG framework that analyzes caption content from T2V perspective, decomposes essential elements for video reconstruction, and provides principled caption design methodology. Created VC4VG-Bench benchmark with fine-grained, multi-dimensional metrics aligned with T2V requirements.

Result: Extensive T2V fine-tuning experiments show strong correlation between improved caption quality and video generation performance, validating the effectiveness of the approach.

Conclusion: The VC4VG framework successfully addresses the gap in caption optimization for T2V training, with benchmark results demonstrating that better captions lead to improved video generation quality.

Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.

</details>


### [31] [Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning](https://arxiv.org/abs/2510.24152)
*Aodi Wu,Xubo Luo*

Main category: cs.CV

TL;DR: A systematic framework for Vision-Language Models in autonomous driving that uses Mixture-of-Prompts routing, task-specific prompts with spatial reasoning, visual assembly, and optimized inference parameters to achieve state-of-the-art performance on the RoboSense Challenge.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of using Vision-Language Models for comprehensive autonomous driving scene understanding across perception, prediction, planning, and corruption detection tasks in the RoboSense Challenge at IROS 2025.

Method: Four-component framework: 1) Mixture-of-Prompts router for question classification and expert prompt dispatch, 2) Task-specific prompts with coordinate systems, spatial reasoning, role-playing, Chain/Tree-of-Thought reasoning, and few-shot examples, 3) Visual assembly module for multi-view image composition with object crops and historical frames, 4) Task-specific inference parameter optimization.

Result: Achieved 70.87% average accuracy on Phase-1 (clean data) and 72.85% on Phase-2 (corrupted data) using Qwen2.5-VL-72B, demonstrating substantial performance enhancement on safety-critical autonomous driving tasks.

Conclusion: Structured prompting and spatial grounding significantly improve VLM performance on autonomous driving tasks, showing the effectiveness of the proposed systematic framework for safety-critical applications.

Abstract: This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.

</details>


### [32] [Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2](https://arxiv.org/abs/2510.24195)
*Ziqi Zhou,Yifan Hu,Yufei Song,Zijing Li,Shengshan Hu,Leo Yu Zhang,Dezhong Yao,Long Zheng,Hai Jin*

Main category: cs.CV

TL;DR: First cross-prompt universal adversarial attack (UAP-SAM2) against SAM2 video segmentation model that addresses architectural differences and semantic entanglement across frames.


<details>
  <summary>Details</summary>
Motivation: SAM2's robustness remains unexplored despite its strong video segmentation capabilities, and existing SAM attacks cannot be directly transferred due to architectural differences involving prompt guidance and frame-to-frame semantic entanglement.

Method: Proposed UAP-SAM2 with target-scanning strategy that divides frames into regions with random prompts to reduce prompt dependency, and dual semantic deviation framework that distorts semantics within frames and disrupts consistency across consecutive frames.

Result: Extensive experiments on six datasets across two segmentation tasks show UAP-SAM2 significantly outperforms state-of-the-art attacks by a large margin.

Conclusion: The proposed UAP-SAM2 effectively addresses the unique challenges of attacking SAM2 and demonstrates superior performance compared to existing methods.

Abstract: Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

</details>


### [33] [CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation](https://arxiv.org/abs/2510.24202)
*Anshul Kaushal,Kunal Jangid,Vinod K. Kurmi*

Main category: cs.CV

TL;DR: CLFSeg is an encoder-decoder framework that combines fuzzy logic with convolutional layers to improve medical image segmentation for polyp and cardiac analysis, addressing uncertainty and boundary ambiguity while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional CNN models have limited generalizability, robustness, and inability to handle uncertainty in medical image segmentation, which affects performance for early cancer detection and treatment planning.

Method: Proposes CLFSeg framework with Fuzzy-Convolutional (FC) module that aggregates convolutional layers and fuzzy logic to identify local/global features while minimizing uncertainty and noise. Uses binary cross-entropy with dice loss to handle class imbalance and focus on tiny boundary regions.

Result: Exceptional performance on four public datasets (CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, ACDC), surpassing existing SOTA methods while focusing on relevant anatomical regions of interest.

Conclusion: CLFSeg improves segmentation performance while ensuring computational efficiency, making it a potential solution for real-world medical diagnostic scenarios.

Abstract: Accurate polyp and cardiac segmentation for early detection and treatment is
essential for the diagnosis and treatment planning of cancer-like diseases.
Traditional convolutional neural network (CNN) based models have represented
limited generalizability, robustness, and inability to handle uncertainty,
which affects the segmentation performance. To solve these problems, this paper
introduces CLFSeg, an encoder-decoder based framework that aggregates the
Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy
logic. This module enhances the segmentation performance by identifying local
and global features while minimizing the uncertainty, noise, and ambiguity in
boundary regions, ensuring computing efficiency. In order to handle class
imbalance problem while focusing on the areas of interest with tiny and
boundary regions, binary cross-entropy (BCE) with dice loss is incorporated.
Our proposed model exhibits exceptional performance on four publicly available
datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.
Extensive experiments and visual studies show CLFSeg surpasses the existing
SOTA performance and focuses on relevant regions of interest in anatomical
structures. The proposed CLFSeg improves performance while ensuring computing
efficiency, which makes it a potential solution for real-world medical
diagnostic scenarios. Project page is available at
https://visdomlab.github.io/CLFSeg/

</details>


### [34] [MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration](https://arxiv.org/abs/2510.24211)
*Junhyuk So,Hyunho Kook,Chaeyeon Jang,Eunhyeok Park*

Main category: cs.CV

TL;DR: MC-SJD is a training-free, lossless parallel decoding framework that accelerates autoregressive visual generation by improving token stability across iterations through coupling-based sampling, achieving up to 4.2x image generation and 13.3x video generation speedups.


<details>
  <summary>Details</summary>
Motivation: Autoregressive modeling for visual generation suffers from slow inference speed due to per-token generation requiring thousands of steps, limiting practical adoption despite its strong performance.

Method: Extends Speculative Jacobi Decoding (SJD) with MC-SJD, an information-theoretic approach using coupling to maximize probability of sampling identical draft tokens across iterations while preserving lossless property. Requires only single-line modification to existing algorithm.

Result: Achieves substantial performance gains: ~4.2x acceleration in image generation and ~13.3x acceleration in video generation compared to standard AR decoding, with no degradation in output quality.

Conclusion: MC-SJD provides a simple yet effective solution to accelerate autoregressive visual generation by addressing token instability in SJD, enabling practical adoption through significant speed improvements while maintaining lossless generation quality.

Abstract: While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.

</details>


### [35] [Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization](https://arxiv.org/abs/2510.24213)
*Haoxin Yang,Yihong Lin,Jingdan Kang,Xuemiao Xu,Yue Li,Cheng Xu,Shengfeng He*

Main category: cs.CV

TL;DR: ID¬≤Face is a training-centric face anonymization framework that uses structured latent space disentanglement to separate identity and non-identity features, enabling direct anonymization without inference-time optimization.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based anonymization methods rely on inference-time interventions that cause distribution shifts and entangle identity with non-identity attributes, degrading visual quality and data utility.

Method: Uses a conditional diffusion model with identity-masked learning, Identity-Decoupled Latent Recomposer with Identity VAE for identity features, bidirectional latent alignment for non-identity attributes, and Identity-Guided Latent Harmonizer with soft-gating. Includes Orthogonal Identity Mapping to suppress identity leakage.

Result: Outperforms existing methods in visual quality, identity suppression, and utility preservation.

Conclusion: ID¬≤Face provides effective face anonymization through explicit disentanglement of identity and non-identity features in a training-centric approach, eliminating the need for inference-time optimization.

Abstract: Face anonymization aims to conceal identity information while preserving
non-identity attributes. Mainstream diffusion models rely on inference-time
interventions such as negative guidance or energy-based optimization, which are
applied post-training to suppress identity features. These interventions often
introduce distribution shifts and entangle identity with non-identity
attributes, degrading visual fidelity and data utility. To address this, we
propose \textbf{ID\textsuperscript{2}Face}, a training-centric anonymization
framework that removes the need for inference-time optimization. The rationale
of our method is to learn a structured latent space where identity and
non-identity information are explicitly disentangled, enabling direct and
controllable anonymization at inference. To this end, we design a conditional
diffusion model with an identity-masked learning scheme. An Identity-Decoupled
Latent Recomposer uses an Identity Variational Autoencoder to model identity
features, while non-identity attributes are extracted from same-identity pairs
and aligned through bidirectional latent alignment. An Identity-Guided Latent
Harmonizer then fuses these representations via soft-gating conditioned on
noisy feature prediction. The model is trained with a recomposition-based
reconstruction loss to enforce disentanglement. At inference, anonymization is
achieved by sampling a random identity vector from the learned identity space.
To further suppress identity leakage, we introduce an Orthogonal Identity
Mapping strategy that enforces orthogonality between sampled and source
identity vectors. Experiments demonstrate that ID\textsuperscript{2}Face
outperforms existing methods in visual quality, identity suppression, and
utility preservation.

</details>


### [36] [SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs](https://arxiv.org/abs/2510.24214)
*Jinhong Deng,Wen Li,Joey Tianyi Zhou,Yang He*

Main category: cs.CV

TL;DR: SCOPE is a visual token pruning method for MLLMs that jointly considers saliency and coverage to preserve semantic completeness while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing visual token pruning methods focus only on saliency, leading to semantic incompleteness in selected tokens, while many visual tokens in MLLMs are redundant.

Method: Proposes SCOPE score that integrates saliency with token-coverage gain, iteratively selecting tokens with highest SCOPE score based on set-coverage and token relationships.

Result: Outperforms prior approaches on multiple vision-language benchmarks using LLaVA-1.5 and LLaVA-Next models.

Conclusion: SCOPE effectively reduces computational overhead while maintaining semantic completeness through joint modeling of saliency and coverage.

Abstract: Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.

</details>


### [37] [Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation](https://arxiv.org/abs/2510.24231)
*Waseem Shariff,Timothy Hanley,Maciej Stec,Hossein Javidnia,Peter Corcoran*

Main category: cs.CV

TL;DR: This paper introduces the first event-based microsaccade dataset using simulated eye movements and evaluates spiking neural networks for microsaccade classification, achieving around 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional microsaccade studies using eye trackers or frame-based analysis are costly and limited in scalability and temporal resolution. Event-based sensing offers a high-speed, low-latency alternative for studying small eye movements.

Method: Used Blender to render high-fidelity eye movement scenarios with microsaccades (0.5-2.0 degrees angular displacement), converted to event streams using v2e. Evaluated with Spiking-VGG11/13/16 models and proposed Spiking-VGG16Flow with optical flow enhancement.

Result: Models achieved around 90% average accuracy in classifying microsaccades by angular displacement, independent of event count or duration. Successfully demonstrated spiking neural networks' capability for fine motion recognition.

Conclusion: The work establishes a benchmark for event-based vision research and demonstrates the potential of spiking neural networks for microsaccade analysis. The dataset, code, and models will be publicly available.

Abstract: Microsaccades are small, involuntary eye movements vital for visual
perception and neural processing. Traditional microsaccade studies typically
use eye trackers or frame-based analysis, which, while precise, are costly and
limited in scalability and temporal resolution. Event-based sensing offers a
high-speed, low-latency alternative by capturing fine-grained spatiotemporal
changes efficiently. This work introduces a pioneering event-based microsaccade
dataset to support research on small eye movement dynamics in cognitive
computing. Using Blender, we render high-fidelity eye movement scenarios and
simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
divided into seven distinct classes. These are converted to event streams using
v2e, preserving the natural temporal dynamics of microsaccades, with durations
ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
around 90 percent average accuracy, successfully classifying microsaccades by
angular displacement, independent of event count or duration. These results
demonstrate the potential of spiking neural networks for fine motion
recognition and establish a benchmark for event-based vision research. The
dataset, code, and trained models will be publicly available at
https://waseemshariff126.github.io/microsaccades/ .

</details>


### [38] [Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy](https://arxiv.org/abs/2510.24232)
*Qing Zhao,Weijian Deng,Pengxu Wei,ZiYi Dong,Hannan Lu,Xiangyang Ji,Liang Lin*

Main category: cs.CV

TL;DR: The paper proposes Lipschitz-regularized object detection (LROD) to address functional mismatch between image restoration and detection networks, improving detection robustness in adverse conditions.


<details>
  <summary>Details</summary>
Motivation: Image restoration is commonly used as pre-processing for detection in adverse conditions, but functional mismatch between restoration and detection networks introduces instability and hinders effective integration.

Method: Propose Lipschitz-regularized object detection (LROD) that integrates image restoration directly into detector's feature learning, harmonizing Lipschitz continuity of both tasks. Implement as LR-YOLO framework.

Result: Extensive experiments on haze and low-light benchmarks show LR-YOLO consistently improves detection stability, optimization smoothness, and overall accuracy.

Conclusion: LROD framework effectively addresses functional mismatch between restoration and detection networks, providing stable and accurate object detection in adverse conditions.

Abstract: To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector's feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.

</details>


### [39] [DeshadowMamba: Deshadowing as 1D Sequential Similarity](https://arxiv.org/abs/2510.24260)
*Zhaotong Yang,Yi Chen,Yanying Li,Shengfeng He,Yangyang Xu,Junyu Dong,Jian Yang,Yong Du*

Main category: cs.CV

TL;DR: Proposes DeshadowMamba, a shadow removal method using Mamba's selective state space model with CrossGate modulation and ColorShift regularization to improve structural and color consistency.


<details>
  <summary>Details</summary>
Motivation: Existing attention-based shadow removal models mix irrelevant illumination cues, causing distorted structures and inconsistent colors. Mamba's directional state transitions offer better global context while preserving positional continuity.

Method: Uses Mamba's selective state space model with CrossGate modulation to inject shadow-aware similarity into input gates, and ColorShift regularization with contrastive learning to suppress color contamination.

Result: Achieves state-of-the-art visual quality and strong quantitative performance on public benchmarks.

Conclusion: The proposed approach effectively adapts sequence modeling to shadow removal requirements, ensuring structural integrity and chromatic consistency through directional modulation and color regularization.

Abstract: Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.

</details>


### [40] [UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation](https://arxiv.org/abs/2510.24262)
*Jiyu Guo,Shuo Yang,Yiming Huang,Yancheng Long,Xiaobo Xia,Xiu Su,Bo Zhao,Zeke Xie,Liqiang Nie*

Main category: cs.CV

TL;DR: UtilGen is a utility-centric data augmentation framework that optimizes synthetic data generation using downstream task feedback, achieving better performance than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing data augmentation methods focus on visual quality metrics like fidelity and diversity but neglect task-specific requirements, which vary across different tasks and architectures.

Method: UtilGen uses a weight allocation network to evaluate task-specific utility of synthetic samples, then employs dual-level optimization: model-level optimization tailors the generative model to the downstream task, and instance-level optimization adjusts generation policies like prompt embeddings and initial noise.

Result: Extensive experiments on eight benchmark datasets show UtilGen consistently achieves superior performance with an average accuracy improvement of 3.87% over previous state-of-the-art methods.

Conclusion: UtilGen validates the effectiveness of shifting from visual characteristics-centric to task utility-centric data augmentation, producing more impactful and task-relevant synthetic data.

Abstract: Data augmentation using generative models has emerged as a powerful paradigm
for enhancing performance in computer vision tasks. However, most existing
augmentation approaches primarily focus on optimizing intrinsic data attributes
-- such as fidelity and diversity -- to generate visually high-quality
synthetic data, while often neglecting task-specific requirements. Yet, it is
essential for data generators to account for the needs of downstream tasks, as
training data requirements can vary significantly across different tasks and
network architectures. To address these limitations, we propose UtilGen, a
novel utility-centric data augmentation framework that adaptively optimizes the
data generation process to produce task-specific, high-utility training data
via downstream task feedback. Specifically, we first introduce a weight
allocation network to evaluate the task-specific utility of each synthetic
sample. Guided by these evaluations, UtilGen iteratively refines the data
generation process using a dual-level optimization strategy to maximize the
synthetic data utility: (1) model-level optimization tailors the generative
model to the downstream task, and (2) instance-level optimization adjusts
generation policies -- such as prompt embeddings and initial noise -- at each
generation round. Extensive experiments on eight benchmark datasets of varying
complexity and granularity demonstrate that UtilGen consistently achieves
superior performance, with an average accuracy improvement of 3.87% over
previous SOTA. Further analysis of data influence and distribution reveals that
UtilGen produces more impactful and task-relevant synthetic data, validating
the effectiveness of the paradigm shift from visual characteristics-centric to
task utility-centric data augmentation.

</details>


### [41] [Training-free Source Attribution of AI-generated Images via Resynthesis](https://arxiv.org/abs/2510.24278)
*Pietro Bongini,Valentina Molinari,Andrea Costanzo,Benedetta Tondi,Mauro Barni*

Main category: cs.CV

TL;DR: A training-free one-shot attribution method using image resynthesis outperforms existing techniques in few-shot scenarios for synthetic image source attribution.


<details>
  <summary>Details</summary>
Motivation: Synthetic image source attribution is challenging under data scarcity conditions requiring few-shot or zero-shot classification capabilities.

Method: Training-free one-shot attribution based on image resynthesis: generate a prompt describing the image, resynthesize with candidate sources, and attribute to model producing closest resynthesis in feature space.

Result: Proposed resynthesis method outperforms state-of-the-art few-shot approaches and other baselines when only few samples are available for training or fine-tuning.

Conclusion: The new dataset provides a challenging attribution framework and valuable benchmark for developing future few-shot and zero-shot methods.

Abstract: Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.

</details>


### [42] [ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model](https://arxiv.org/abs/2510.24285)
*Juntian Zhang,Song Jin,Chuanqi Cheng,Yuhan Liu,Yankai Lin,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CV

TL;DR: ViPER is a self-bootstrapping framework that enhances fine-grained visual perception in VLMs through a two-stage reinforcement learning approach with self-critiquing and self-prediction.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck of limited fine-grained visual perception in VLMs, which is challenging due to data scarcity and limitations of existing methods like SFT compromising general capabilities and RFT prioritizing textual reasoning over visual perception.

Method: Proposes a two-stage task structuring visual perception as coarse-to-fine progressive process, using self-bootstrapping framework with image-level and instance-level reconstruction integrated with two-stage reinforcement learning strategy.

Result: Applied to Qwen2.5-VL family, produces Qwen-Viper series with average gain of 1.7% on seven benchmarks and up to 6.0% on fine-grained perception, demonstrating superior performance across vision-language scenarios while maintaining generalizability.

Conclusion: ViPER enables self-improvement in perceptual capabilities and provides evidence for reciprocal relationship between generation and understanding, representing a breakthrough for developing more autonomous and capable VLMs.

Abstract: The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.

</details>


### [43] [Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning](https://arxiv.org/abs/2510.24321)
*Ivica Dimitrovski,Vlatko Spasev,Ivan Kitanovski*

Main category: cs.CV

TL;DR: Prompt learning methods outperform zero-shot CLIP and linear probing for few-shot remote sensing scene classification, with self-regulating constraints showing the best cross-domain performance.


<details>
  <summary>Details</summary>
Motivation: Address the domain gap between general vision-language models like CLIP and remote sensing imagery, and overcome the scarcity of labeled data in remote sensing applications.

Method: Systematically evaluate four prompt learning methods (Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, Prompting with Self-Regulating Constraints) against zero-shot CLIP with hand-crafted prompts and linear probe baselines.

Result: Prompt learning consistently outperforms both baselines in few-shot scenarios across multiple benchmark datasets, with Prompting with Self-Regulating Constraints achieving the most robust cross-domain performance.

Conclusion: Prompt learning provides a scalable and efficient solution for bridging domain gaps in satellite and aerial imagery, offering a strong foundation for future remote sensing research.

Abstract: Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.

</details>


### [44] [Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2510.24366)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Vi Vu,Bach X. Nguyen,Jianhua Xing,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: A novel switching Dual-Student architecture with Loss-Aware Exponential Moving Average for semi-supervised medical image segmentation, improving collaboration and preventing error reinforcement.


<details>
  <summary>Details</summary>
Motivation: Teacher-student frameworks in semi-supervised medical image segmentation are limited by strong correlation and unreliable knowledge transfer between teacher and student networks.

Method: Switching Dual-Student architecture that selects the most reliable student at each iteration, plus Loss-Aware Exponential Moving Average to dynamically ensure teacher absorbs meaningful information from students.

Result: Outperforms state-of-the-art semi-supervised methods on 3D medical image segmentation datasets, improving segmentation accuracy under limited supervision.

Conclusion: The plug-and-play framework effectively enhances dual-student collaboration and prevents error reinforcement, demonstrating strong performance in semi-supervised medical image segmentation.

Abstract: Teacher-student frameworks have emerged as a leading approach in
semi-supervised medical image segmentation, demonstrating strong performance
across various tasks. However, the learning effects are still limited by the
strong correlation and unreliable knowledge transfer process between teacher
and student networks. To overcome this limitation, we introduce a novel
switching Dual-Student architecture that strategically selects the most
reliable student at each iteration to enhance dual-student collaboration and
prevent error reinforcement. We also introduce a strategy of Loss-Aware
Exponential Moving Average to dynamically ensure that the teacher absorbs
meaningful information from students, improving the quality of pseudo-labels.
Our plug-and-play framework is extensively evaluated on 3D medical image
segmentation datasets, where it outperforms state-of-the-art semi-supervised
methods, demonstrating its effectiveness in improving segmentation accuracy
under limited supervision.

</details>


### [45] [Decoupling What to Count and Where to See for Referring Expression Counting](https://arxiv.org/abs/2510.24374)
*Yuda Zou,Zijian Zhang,Yongchao Xu*

Main category: cs.CV

TL;DR: W2-Net addresses the overlooked challenge in Referring Expression Counting where annotation points focus on class-representative locations, causing models to neglect attribute information. It introduces dual-query mechanism and Subclass Separable Matching to improve subclass discrimination.


<details>
  <summary>Details</summary>
Motivation: Current REC models suffer from focusing on class-level features due to annotation points being placed on class-representative locations (e.g., heads), which neglects attribute information from other visual regions (e.g., legs for "walking").

Method: W2-Net uses a dual-query mechanism with what-to-count (w2c) queries for object localization and where-to-see (w2s) queries for attribute-specific feature extraction. It also introduces Subclass Separable Matching (SSM) with repulsive force for better inter-subclass separability.

Result: W2-Net significantly outperforms state-of-the-art on REC-8K dataset, reducing counting error by 22.5% (validation) and 18.0% (test), and improving localization F1 by 7% and 8%, respectively.

Conclusion: The proposed W2-Net framework effectively addresses the annotation bias in REC by decoupling object counting into "what to count" and "where to see", achieving substantial improvements in both counting accuracy and localization performance.

Abstract: Referring Expression Counting (REC) extends class-level object counting to
the fine-grained subclass-level, aiming to enumerate objects matching a textual
expression that specifies both the class and distinguishing attribute. A
fundamental challenge, however, has been overlooked: annotation points are
typically placed on class-representative locations (e.g., heads), forcing
models to focus on class-level features while neglecting attribute information
from other visual regions (e.g., legs for "walking"). To address this, we
propose W2-Net, a novel framework that explicitly decouples the problem into
"what to count" and "where to see" via a dual-query mechanism. Specifically,
alongside the standard what-to-count (w2c) queries that localize the object, we
introduce dedicated where-to-see (w2s) queries. The w2s queries are guided to
seek and extract features from attribute-specific visual regions, enabling
precise subclass discrimination. Furthermore, we introduce Subclass Separable
Matching (SSM), a novel matching strategy that incorporates a repulsive force
to enhance inter-subclass separability during label assignment. W2-Net
significantly outperforms the state-of-the-art on the REC-8K dataset, reducing
counting error by 22.5% (validation) and 18.0% (test), and improving
localization F1 by 7% and 8%, respectively. Code will be available.

</details>


### [46] [Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool](https://arxiv.org/abs/2510.24378)
*Yann Kerverdo,Florent Leray,Youwan Mah√©,St√©phanie Leplaideur,Francesca Galassi*

Main category: cs.CV

TL;DR: StrokeSeg is a lightweight, modular framework that converts research-grade stroke lesion segmentation models into deployable clinical applications with equivalent performance to original PyTorch pipelines.


<details>
  <summary>Details</summary>
Motivation: Current deep learning frameworks like nnU-Net achieve state-of-the-art performance but are difficult to deploy clinically due to heavy dependencies and monolithic design.

Method: Decouples preprocessing (using Anima toolbox with BIDS-compliant outputs), inference (using ONNX Runtime with Float16 quantization), and postprocessing. Provides both graphical and command-line interfaces, distributed as Python scripts and standalone Windows executable.

Result: On 300 sub-acute and chronic stroke subjects, segmentation performance was equivalent to original PyTorch pipeline (Dice difference <10^-3), with model size reduced by about 50% through quantization.

Conclusion: High-performing research pipelines can be successfully transformed into portable, clinically usable tools without sacrificing segmentation accuracy.

Abstract: Deep learning frameworks such as nnU-Net achieve state-of-the-art performance
in brain lesion segmentation but remain difficult to deploy clinically due to
heavy dependencies and monolithic design. We introduce \textit{StrokeSeg}, a
modular and lightweight framework that translates research-grade stroke lesion
segmentation models into deployable applications. Preprocessing, inference, and
postprocessing are decoupled: preprocessing relies on the Anima toolbox with
BIDS-compliant outputs, and inference uses ONNX Runtime with \texttt{Float16}
quantisation, reducing model size by about 50\%. \textit{StrokeSeg} provides
both graphical and command-line interfaces and is distributed as Python scripts
and as a standalone Windows executable. On a held-out set of 300 sub-acute and
chronic stroke subjects, segmentation performance was equivalent to the
original PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that
high-performing research pipelines can be transformed into portable, clinically
usable tools.

</details>


### [47] [A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset](https://arxiv.org/abs/2510.24379)
*Zhuangfan Huang,Xiaosong Li,Gao Wang,Tao Ye,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: A luminance-aware multi-scale network (MLSN) for polarization image fusion that addresses contrast differences in polarized images and adapts to complex lighting through dynamic luminance injection and brightness enhancement.


<details>
  <summary>Details</summary>
Motivation: Polarization image fusion combines S0 and DOLP images to reveal surface properties, but faces challenges with inherent contrast differences and complex lighting environments that affect texture feature integration.

Method: Proposes MLSN with multi-scale spatial weight matrix for dynamic luminance injection, global-local feature fusion with windowed self-attention, and Brightness-Enhancement module for nonlinear luminance correction. Also introduces MSP dataset with 1000 polarized image pairs.

Result: Outperforms state-of-the-art methods on MSP, PIF and GAND datasets, with MS-SSIM and SD metrics 8.57%-63.53% higher than average values of other methods.

Conclusion: MLSN effectively handles complex luminance environments in polarization image fusion and the new MSP dataset addresses data scarcity in this field.

Abstract: Polarization image fusion combines S0 and DOLP images to reveal surface
roughness and material properties through complementary texture features, which
has important applications in camouflage recognition, tissue pathology
analysis, surface defect detection and other fields. To intergrate
coL-Splementary information from different polarized images in complex
luminance environment, we propose a luminance-aware multi-scale network (MLSN).
In the encoder stage, we propose a multi-scale spatial weight matrix through a
brightness-branch , which dynamically weighted inject the luminance into the
feature maps, solving the problem of inherent contrast difference in polarized
images. The global-local feature fusion mechanism is designed at the bottleneck
layer to perform windowed self-attention computation, to balance the global
context and local details through residual linking in the feature dimension
restructuring stage. In the decoder stage, to further improve the adaptability
to complex lighting, we propose a Brightness-Enhancement module, establishing
the mapping relationship between luminance distribution and texture features,
realizing the nonlinear luminance correction of the fusion result. We also
present MSP, an 1000 pairs of polarized images that covers 17 types of indoor
and outdoor complex lighting scenes. MSP provides four-direction polarization
raw maps, solving the scarcity of high-quality datasets in polarization image
fusion. Extensive experiment on MSP, PIF and GAND datasets verify that the
proposed MLSN outperms the state-of-the-art methods in subjective and objective
evaluations, and the MS-SSIM and SD metircs are higher than the average values
of other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,
respectively. The source code and dataset is avalable at
https://github.com/1hzf/MLS-UNet.

</details>


### [48] [When are radiology reports useful for training medical image classifiers?](https://arxiv.org/abs/2510.24385)
*Herman Bergstr√∂m,Zhongqi Yue,Fredrik D. Johansson*

Main category: cs.CV

TL;DR: This paper systematically studies how radiology reports can be leveraged during training to improve medical image classification, examining both pre-training and fine-tuning approaches across different task types and dataset sizes.


<details>
  <summary>Details</summary>
Motivation: Medical images often come with radiology reports containing expert annotations, but using these reports as inputs requires manual work. The research aims to determine when and how these reports can be leveraged during training to improve image-only classification, especially for tasks where labels are weakly associated with the text.

Method: The study systematically evaluates how radiology reports can be used during both pre-training and fine-tuning phases, across diagnostic and prognostic tasks (e.g., 12-month readmission), and under varying training set sizes. It compares different approaches including explicit image-text alignment.

Result: Key findings: (1) Leveraging reports during pre-training benefits downstream classification when labels are well-represented in text, but explicit image-text alignment can be detrimental when they're not; (2) Fine-tuning with reports can lead to significant improvements and sometimes has larger impact than pre-training method.

Conclusion: The study provides actionable insights into when and how to leverage privileged text data for training medical image classifiers, while identifying gaps in current research regarding optimal use of radiology reports across different task types and settings.

Abstract: Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.

</details>


### [49] [Unsupervised Detection of Post-Stroke Brain Abnormalities](https://arxiv.org/abs/2510.24398)
*Youwan Mah√©,Elise Bannier,St√©phanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: REFLECT, a flow-based generative model, detects both focal and non-lesional abnormalities in post-stroke MRI better when trained on healthy controls rather than stroke patients.


<details>
  <summary>Details</summary>
Motivation: Post-stroke MRI shows secondary structural changes like atrophy and ventricular enlargement, which are imaging biomarkers of recovery but poorly captured by supervised segmentation methods.

Method: Used REFLECT, a flow-based generative model, for unsupervised detection of abnormalities. Trained two models on lesion-free slices from stroke patients (ATLAS) and on healthy controls (IXI), evaluated with dual-expert annotations and Free-Response ROC analysis.

Result: IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43) compared to ATLAS-trained model.

Conclusion: Training on fully healthy anatomy improves modeling of normal variability, enabling broader and more reliable detection of structural abnormalities in post-stroke patients.

Abstract: Post-stroke MRI not only delineates focal lesions but also reveals secondary
structural changes, such as atrophy and ventricular enlargement. These
abnormalities, increasingly recognised as imaging biomarkers of recovery and
outcome, remain poorly captured by supervised segmentation methods. We evaluate
REFLECT, a flow-based generative model, for unsupervised detection of both
focal and non-lesional abnormalities in post-stroke patients. Using dual-expert
central-slice annotations on ATLAS data, performance was assessed at the object
level with Free-Response ROC analysis for anomaly maps. Two models were trained
on lesion-free slices from stroke patients (ATLAS) and on healthy controls
(IXI) to test the effect of training data. On ATLAS test subjects, the
IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and
improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).
Training on fully healthy anatomy improves the modelling of normal variability,
enabling broader and more reliable detection of structural abnormalities.

</details>


### [50] [GenTrack: A New Generation of Multi-Object Tracking](https://arxiv.org/abs/2510.24399)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: GenTrack is a novel multi-object tracking method that uses hybrid stochastic-deterministic tracking with PSO optimization and social interactions to handle varying target numbers, maintain ID consistency, and work effectively with weak detectors.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multi-object tracking including handling unknown and time-varying numbers of targets, maintaining target identity consistency, managing nonlinear dynamics, and working effectively with weak and noisy object detectors.

Method: Hybrid tracking approach combining stochastic and deterministic methods, using PSO with fitness measures to guide particles, integrating social interactions among targets, and implementing a comprehensive state and observation model with space consistency, appearance, detection confidence, track penalties, and social scores.

Result: Superior performance on standard benchmarks and real-world scenarios compared to state-of-the-art trackers, with reduced ID switches and track loss especially during occlusions.

Conclusion: GenTrack provides an effective solution for robust multi-object tracking with three variants available, and the first publicly available source-code implementation facilitates further research and development in this area.

Abstract: This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack

</details>


### [51] [A Hybrid Approach for Visual Multi-Object Tracking](https://arxiv.org/abs/2510.24410)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: A visual multi-object tracking method combining stochastic particle filtering with deterministic association to maintain identifier consistency for varying numbers of targets under nonlinear dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of tracking unknown and time-varying numbers of targets with nonlinear dynamics while ensuring identifier consistency, especially during interactions and occlusions.

Method: Uses stochastic particle filter with PSO optimization for nonlinear dynamics, deterministic association with cost matrix, smooth state updating scheme, and velocity regression for trend-seed velocities.

Result: Superior performance compared to state-of-the-art trackers, with flexible operation for both pre-recorded videos and live camera streams.

Conclusion: The proposed hybrid stochastic-deterministic approach effectively maintains tracking consistency and handles complex scenarios like interactions and occlusions.

Abstract: This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2

</details>


### [52] [50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon](https://arxiv.org/abs/2510.24413)
*Ali Ahmad Faour,Nabil Amacha,Ali J. Ghandour*

Main category: cs.CV

TL;DR: A sensor-free approach using satellite imagery and machine learning to monitor reservoir volume in Lebanon, achieving 95% shoreline accuracy and under 1.5% error.


<details>
  <summary>Details</summary>
Motivation: Sustainable management of Qaraaoun Reservoir requires reliable monitoring despite sensor malfunctions and limited maintenance capacity in Lebanon.

Method: Integration of Sentinel-2/Landsat imagery with new water segmentation index and Support Vector Regression model trained on bathymetry data to estimate volume from surface area alone.

Result: 95% shoreline alignment with ground truth, optimized SVR with under 1.5% error of full capacity, and R¬≤ exceeding 0.98.

Conclusion: The method provides robust, cost-effective sensor-independent monitoring that can be replicated for other water bodies and supports climate change research.

Abstract: The sustainable management of the Qaraaoun Reservoir, the largest surface
water body in Lebanon located in the Bekaa Plain, depends on reliable
monitoring of its storage volume despite frequent sensor malfunctions and
limited maintenance capacity. This study introduces a sensor-free approach that
integrates open-source satellite imagery, advanced water-extent segmentation,
and machine learning to estimate the reservoir surface area and volume in near
real time. Sentinel-2 and Landsat images are processed, where surface water is
delineated using a newly proposed water segmentation index. A machine learning
model based on Support Vector Regression (SVR) is trained on a curated dataset
that includes water surface area, water level, and water volume calculations
using a reservoir bathymetry survey. The model is then able to estimate
reservoir volume relying solely on surface area extracted from satellite
imagery, without the need for ground measurements. Water segmentation using the
proposed index aligns with ground truth for more than 95 percent of the
shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR
performance with error under 1.5 percent of full reservoir capacity and
coefficients of determination exceeding 0.98. These results demonstrate the
robustness and cost-effectiveness of the method, offering a practical solution
for continuous, sensor-independent monitoring of reservoir storage. The
proposed methodology can be replicated for other water bodies, and the
resulting 50 years of time-series data is valuable for research on climate
change and environmental patterns.

</details>


### [53] [XAI Evaluation Framework for Semantic Segmentation](https://arxiv.org/abs/2510.24414)
*Reem Hammoud,Abdul karim Gizzini,Ali J. Ghandour*

Main category: cs.CV

TL;DR: This paper introduces a comprehensive evaluation framework for explainable AI (XAI) methods in semantic segmentation, addressing the gap in evaluation strategies for this specific task compared to classification.


<details>
  <summary>Details</summary>
Motivation: As AI models are increasingly used in safety-critical domains, ensuring transparency and trust is essential. While XAI has emerged to address this, rigorous evaluation methods for semantic segmentation remain underexplored compared to classification tasks.

Method: The paper proposes a systematic evaluation framework specifically designed for XAI in semantic segmentation, accounting for both spatial and contextual complexities. It employs pixel-level evaluation strategies and carefully designed metrics for fine-grained interpretability insights.

Result: Simulation results using class activation mapping (CAM)-based XAI schemes demonstrate the efficiency, robustness, and reliability of the proposed methodology.

Conclusion: The findings contribute to advancing transparent, trustworthy, and accountable semantic segmentation models by providing a comprehensive evaluation framework for XAI methods in this domain.

Abstract: Ensuring transparency and trust in artificial intelligence (AI) models is
essential, particularly as they are increasingly applied in safety-critical and
high-stakes domains. Explainable AI (XAI) has emerged as a promising approach
to address this challenge, yet the rigorous evaluation of XAI methods remains
crucial for optimizing the trade-offs between model complexity, predictive
performance, and interpretability. While extensive progress has been achieved
in evaluating XAI techniques for classification tasks, evaluation strategies
tailored to semantic segmentation remain relatively underexplored. This work
introduces a comprehensive and systematic evaluation framework specifically
designed for assessing XAI in semantic segmentation, explicitly accounting for
both spatial and contextual task complexities. The framework employs
pixel-level evaluation strategies and carefully designed metrics to provide
fine-grained interpretability insights. Simulation results using recently
adapted class activation mapping (CAM)-based XAI schemes demonstrate the
efficiency, robustness, and reliability of the proposed methodology. These
findings contribute to advancing transparent, trustworthy, and accountable
semantic segmentation models.

</details>


### [54] [Deeply-Conditioned Image Compression via Self-Generated Priors](https://arxiv.org/abs/2510.24437)
*Zhineng Zhao,Zhihai He,Zikun Zhou,Siwei Ma,Yaowei Wang*

Main category: cs.CV

TL;DR: DCIC-sgp is a learned image compression framework that uses self-generated priors to separate global structures from local textures, reducing geometric deformation at low bitrates and achieving significant BD-rate reductions against VVC.


<details>
  <summary>Details</summary>
Motivation: Current learned image compression methods struggle to model complex correlation structures in natural images, particularly the entanglement of invariant global structures with transient local textures, leading to severe geometric deformation at low bitrates.

Method: The framework uses functional decomposition with self-generated priors that encapsulate the image's structural backbone. This prior holistically modulates the entire compression pipeline, especially the analysis transform, allowing it to focus on residual high-entropy details through hierarchical, dependency-driven information disentanglement.

Result: The method substantially mitigates geometric deformation artifacts at low bitrates and achieves significant BD-rate reductions of 14.4%, 15.7%, and 15.1% against VTM-12.1 on Kodak, CLIC, and Tecnick datasets respectively.

Conclusion: DCIC-sgp's deeply-conditioned approach with self-generated priors effectively disentangles information streams and establishes highly competitive performance in learned image compression.

Abstract: Learned image compression (LIC) has shown great promise for achieving high
rate-distortion performance. However, current LIC methods are often limited in
their capability to model the complex correlation structures inherent in
natural images, particularly the entanglement of invariant global structures
with transient local textures within a single monolithic representation. This
limitation precipitates severe geometric deformation at low bitrates. To
address this, we introduce a framework predicated on functional decomposition,
which we term Deeply-Conditioned Image Compression via self-generated priors
(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior
to encapsulate the image's structural backbone. This prior is subsequently
utilized not as mere side-information, but to holistically modulate the entire
compression pipeline. This deep conditioning, most critically of the analysis
transform, liberates it to dedicate its representational capacity to the
residual, high-entropy details. This hierarchical, dependency-driven approach
achieves an effective disentanglement of information streams. Our extensive
experiments validate this assertion; visual analysis demonstrates that our
method substantially mitigates the geometric deformation artifacts that plague
conventional codecs at low bitrates. Quantitatively, our framework establishes
highly competitive performance, achieving significant BD-rate reductions of
14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,
and Tecnick datasets.

</details>


### [55] [Rethinking Visual Intelligence: Insights from Video Pretraining](https://arxiv.org/abs/2510.24448)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: Video Diffusion Models (VDMs) pretrained on spatiotemporal data show better data efficiency and compositional understanding than LLMs in visual tasks, suggesting video pretraining provides useful inductive biases for visual foundation models.


<details>
  <summary>Details</summary>
Motivation: LLMs have succeeded in language tasks but struggle with compositional understanding and sample efficiency in the visual domain. The paper investigates whether Video Diffusion Models can bridge this gap.

Method: Compare pretrained LLMs and VDMs equipped with lightweight adapters on various visual benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata.

Result: VDMs demonstrate higher data efficiency than LLMs across all benchmarks, showing better performance in visual tasks with less training data.

Conclusion: Video pretraining provides strong inductive biases for structure and dynamics that support progress toward effective visual foundation models.

Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining
enables systems to adapt rapidly to new problems with little supervision in the
language domain. This success, however, has not translated as effectively to
the visual domain, where models, including LLMs, continue to struggle with
compositional understanding, sample efficiency, and general-purpose
problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
direction for bridging this gap. Pretraining on spatiotemporal data endows
these models with strong inductive biases for structure and dynamics, which we
hypothesize can support broad task adaptability. To test this, we design a
controlled evaluation in which both a pretrained LLM and a pretrained VDM are
equipped with lightweight adapters and presented with tasks in their natural
modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
route planning, and cellular automata, VDMs demonstrate higher data efficiency
than their language counterparts. Taken together, our results indicate that
video pretraining offers inductive biases that support progress toward visual
foundation models.

</details>


### [56] [A Critical Study towards the Detection of Parkinsons Disease using ML Technologies](https://arxiv.org/abs/2510.24456)
*Vivek Chetia,Abdul Taher Khan,Rahish Gogoi,David Kapsian Khual,Purnendu Bikash,Sajal Saha*

Main category: cs.CV

TL;DR: Deep learning approach for detecting and segmenting three tea leaf diseases (Red Rust, Helopeltis, Red Spider Mite) using object detection models and custom damage area calculation.


<details>
  <summary>Details</summary>
Motivation: To automatically classify tea leaf diseases caused by pests and pathogens, and quantify the damaged area on leaves to help with disease management.

Method: Evaluated SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for object detection, and Mask R-CNN for instance segmentation with custom method to calculate damaged leaf portions.

Result: Faster R-CNN ResNet50 V1 achieved better performance with 25% mAP compared to SSD MobileNet V2's 20.9% mAP. Both models showed low precision and recall values on IOU 0.50:0.95 range.

Conclusion: Faster R-CNN ResNet50 V1 outperforms SSD MobileNet V2 for tea leaf disease detection, and Mask R-CNN with custom method enables damaged area quantification, though overall performance needs improvement.

Abstract: The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.

</details>


### [57] [Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras](https://arxiv.org/abs/2510.24464)
*Charles Javerliat,Pierre Raimbaud,Guillaume Lavou√©*

Main category: cs.CV

TL;DR: Kineo is a calibration-free pipeline for markerless motion capture from unsynchronized, uncalibrated RGB cameras that simultaneously calibrates cameras and reconstructs 3D keypoints at metric scale with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing calibration-free motion capture methods suffer from high computational cost and reduced accuracy, limiting accessibility for non-experts and in-the-wild captures.

Method: Leverages 2D keypoints from off-the-shelf detectors with confidence-driven spatio-temporal sampling and graph-based global optimization for simultaneous camera calibration and 3D reconstruction, including Brown-Conrady distortion coefficients.

Result: Substantial improvements over prior methods: reduces camera translation error by 83-85%, camera angular error by 86-92%, and world mean-per-joint error by 83-91%. Processes multi-view sequences faster than their duration in some cases.

Conclusion: Kineo provides an efficient, accurate calibration-free solution for markerless motion capture that outperforms existing methods and enables practical adoption in real-world scenarios.

Abstract: Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.

</details>


### [58] [Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling](https://arxiv.org/abs/2510.24474)
*Kyungmin Lee,Sihyun Yu,Jinwoo Shin*

Main category: cs.CV

TL;DR: Decoupled MeanFlow converts pretrained flow models into flow map models without architectural changes, enabling high-quality image generation in 1-4 steps with 100x faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing denoising generative models require many sampling steps due to discretization error, and flow map training typically needs architectural modifications that limit compatibility with pretrained models.

Method: Decoupled MeanFlow conditions the final blocks of diffusion transformers on the subsequent timestep, converting flow models to flow maps without architectural changes. Combined with enhanced training techniques.

Result: Achieves 1-step FID of 2.16 and 2.12 on ImageNet 256x256 and 512x512 respectively, surpassing prior art. With 4 steps, achieves FID of 1.51 and 1.68, nearly matching flow model performance.

Conclusion: Training flow models first and then converting them to flow maps is more efficient than training flow maps from scratch, enabling high-quality generation with dramatically faster inference.

Abstract: Denoising generative models, such as diffusion and flow-based models, produce
high-quality samples but require many denoising steps due to discretization
error. Flow maps, which estimate the average velocity between timesteps,
mitigate this error and enable faster sampling. However, their training
typically demands architectural changes that limit compatibility with
pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding
strategy that converts flow models into flow map models without architectural
modifications. Our method conditions the final blocks of diffusion transformers
on the subsequent timestep, allowing pretrained flow models to be directly
repurposed as flow maps. Combined with enhanced training techniques, this
design enables high-quality generation in as few as 1 to 4 steps. Notably, we
find that training flow models and subsequently converting them is more
efficient and effective than training flow maps from scratch. On ImageNet
256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,
respectively, surpassing prior art by a large margin. Furthermore, we achieve
FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the
performance of flow models while delivering over 100x faster inference.

</details>


### [59] [Fast and accurate neural reflectance transformation imaging through knowledge distillation](https://arxiv.org/abs/2510.24486)
*Tinsae G. Dulecha,Leonardo Righetto,Ruggero Pintus,Enrico Gobbetti,Andrea Giachetti*

Main category: cs.CV

TL;DR: NeuralRTI uses neural autoencoders for superior reflectance field modeling but is computationally expensive. DisK-NeuralRTI applies knowledge distillation to reduce computational costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Traditional RTI methods like PTM and HSH are compact but struggle with complex reflectance fields, while NeuralRTI provides superior quality but is computationally expensive for interactive relighting on limited hardware.

Method: Proposes DisK-NeuralRTI using knowledge distillation to transfer knowledge from a large teacher network to a smaller student network, reducing computational costs while maintaining rendering quality.

Result: The method successfully reduces computational requirements of NeuralRTI while preserving the quality of reflectance field modeling and interactive relighting capabilities.

Conclusion: Knowledge distillation enables efficient NeuralRTI implementation that maintains high-quality results while being computationally feasible for interactive use on limited hardware.

Abstract: Reflectance Transformation Imaging (RTI) is very popular for its ability to
visually analyze surfaces by enhancing surface details through interactive
relighting, starting from only a few tens of photographs taken with a fixed
camera and variable illumination. Traditional methods like Polynomial Texture
Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle
to accurately capture complex reflectance fields using few per-pixel
coefficients and fixed bases, leading to artifacts, especially in highly
reflective or shadowed areas. The NeuralRTI approach, which exploits a neural
autoencoder to learn a compact function that better approximates the local
reflectance as a function of light directions, has been shown to produce
superior quality at comparable storage cost. However, as it performs
interactive relighting with custom decoder networks with many parameters, the
rendering step is computationally expensive and not feasible at full resolution
for large images on limited hardware. Earlier attempts to reduce costs by
directly training smaller networks have failed to produce valid results. For
this reason, we propose to reduce its computational cost through a novel
solution based on Knowledge Distillation (DisK-NeuralRTI). ...

</details>


### [60] [Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs](https://arxiv.org/abs/2510.24514)
*Huanyu Zhang,Wenshan Wu,Chengzu Li,Ning Shang,Yan Xia,Yangyu Huang,Yifan Zhang,Li Dong,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.CV

TL;DR: Latent Sketchpad equips MLLMs with an internal visual scratchpad for generative visual thinking, enabling them to interleave textual reasoning with visual latent generation without compromising reasoning performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs excel at visual understanding but struggle with complex scenarios requiring visual planning and imagination, unlike humans who use sketching for visual thinking and idea development.

Method: Integrates visual generation directly into MLLMs' native autoregressive reasoning process using two components: Context-Aware Vision Head for autoregressive visual representation production and pretrained Sketch Decoder for rendering interpretable sketch images.

Result: Delivers comparable or superior reasoning performance to backbone MLLMs on MazePlanning dataset, generalizes across different MLLMs (Gemma3, Qwen2.5-VL), and enables visual thinking alongside textual reasoning.

Conclusion: The framework extends MLLMs' reasoning capabilities to visual thinking, opening new opportunities for richer human-computer interaction and broader applications.

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.

</details>


### [61] [OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents](https://arxiv.org/abs/2510.24563)
*Hongrui Jia,Jitong Liao,Xi Zhang,Haiyang Xu,Tianbao Xie,Chaoya Jiang,Ming Yan,Si Liu,Wei Ye,Fei Huang*

Main category: cs.CV

TL;DR: OSWorld-MCP is the first comprehensive benchmark for evaluating multimodal agents' tool invocation, GUI operation, and decision-making abilities in real-world computer environments, addressing the gap in fair assessment of tool usage capabilities.


<details>
  <summary>Details</summary>
Motivation: Past evaluations focused mainly on GUI interaction skills while overlooking tool invocation abilities enabled by Model Context Protocol (MCP), creating unfair comparisons between agents with integrated tools and those evaluated only on GUI interaction.

Method: Developed OSWorld-MCP benchmark with automated code-generation pipeline to create tools, combined with curated existing tools, resulting in 158 high-quality tools covering 7 common applications, all manually validated for functionality, applicability, and versatility.

Result: MCP tools significantly improved task success rates (e.g., from 8.3% to 20.4% for OpenAI o3, from 40.1% to 43.3% for Claude 4 Sonnet), but even the strongest models had low tool invocation rates (only 36.3%), indicating room for improvement.

Conclusion: OSWorld-MCP sets a new standard for evaluating multimodal agents in complex, tool-assisted environments by explicitly measuring MCP tool usage skills, providing deeper understanding of agent capabilities and highlighting the benchmark's challenging nature.

Abstract: With advances in decision-making and reasoning capabilities, multimodal
agents show strong potential in computer application scenarios. Past
evaluations have mainly assessed GUI interaction skills, while tool invocation
abilities, such as those enabled by the Model Context Protocol (MCP), have been
largely overlooked. Comparing agents with integrated tool invocation to those
evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,
the first comprehensive and fair benchmark for assessing computer-use agents'
tool invocation, GUI operation, and decision-making abilities in a real-world
environment. We design a novel automated code-generation pipeline to create
tools and combine them with a curated selection from existing tools. Rigorous
manual validation yields 158 high-quality tools (covering 7 common
applications), each verified for correct functionality, practical
applicability, and versatility. Extensive evaluations of state-of-the-art
multimodal agents on OSWorld-MCP show that MCP tools generally improve task
success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%
to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of
assessing tool invocation capabilities. However, even the strongest models have
relatively low tool invocation rates, Only 36.3%, indicating room for
improvement and highlighting the benchmark's challenge. By explicitly measuring
MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents
and sets a new standard for evaluating performance in complex, tool-assisted
environments. Our code, environment, and data are publicly available at
https://osworld-mcp.github.io.

</details>


### [62] [Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT](https://arxiv.org/abs/2510.24579)
*Xu Jiang,Huiying Pan,Ligen Shi,Jianing Sun,Wenfeng Xu,Xing Zhao*

Main category: cs.CV

TL;DR: Deep learning method using Gaussian RBF and KAN networks to correct scatter artifacts in CBCT images by modeling rotational symmetry of scatter distribution.


<details>
  <summary>Details</summary>
Motivation: CBCT suffers from scatter artifacts that cause CT value bias and reduced tissue contrast, degrading diagnostic accuracy.

Method: Uses Gaussian Radial Basis Functions to model point scatter function and embeds it into Kolmogorov-Arnold Networks layers to learn high-dimensional scatter features with physical prior knowledge.

Result: Effectively corrects scatter artifacts in reconstructed images and outperforms current methods in quantitative metrics, validated through synthetic and real-scan experiments.

Conclusion: The proposed method successfully combines physical scatter characteristics with KAN's complex function mapping to improve scatter representation and artifact correction in CBCT imaging.

Abstract: Cone-beam CT (CBCT) employs a flat-panel detector to achieve
three-dimensional imaging with high spatial resolution. However, CBCT is
susceptible to scatter during data acquisition, which introduces CT value bias
and reduced tissue contrast in the reconstructed images, ultimately degrading
diagnostic accuracy. To address this issue, we propose a deep learning-based
scatter artifact correction method inspired by physical prior knowledge.
Leveraging the fact that the observed point scatter probability density
distribution exhibits rotational symmetry in the projection domain. The method
uses Gaussian Radial Basis Functions (RBF) to model the point scatter function
and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides
efficient nonlinear mapping capabilities for learning high-dimensional scatter
features. By incorporating the physical characteristics of the scattered photon
distribution together with the complex function mapping capacity of KAN, the
model improves its ability to accurately represent scatter. The effectiveness
of the method is validated through both synthetic and real-scan experiments.
Experimental results show that the model can effectively correct the scatter
artifacts in the reconstructed images and is superior to the current methods in
terms of quantitative metrics.

</details>


### [63] [A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries](https://arxiv.org/abs/2510.24640)
*Xin Zhang,Yuqi Song,Fei Zuo*

Main category: cs.CV

TL;DR: A dual-branch CNN for face forgery detection that combines spatial (RGB) and frequency domain features with adaptive fusion and a unified loss function, achieving strong performance across multiple forgery types.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of generative AI enables creation of highly realistic forged facial images, posing threats to AI security, digital media integrity, and public trust. There is urgent need for robust detection methods against malicious uses like misinformation and identity fraud.

Method: Dual-branch CNN with RGB branch for semantic information and frequency branch for high-frequency artifacts. Uses channel attention module for adaptive feature fusion and FSC Loss (focal loss, supervised contrastive loss, frequency center margin loss) to enhance class separability.

Result: Evaluated on DiFF benchmark with four forgery types (text-to-image, image-to-image, face swap, face edit). Achieved strong performance across all categories and outperformed average human accuracy.

Conclusion: The model demonstrates effectiveness and potential contribution to safeguarding AI ecosystems against visual forgery attacks through its dual-domain approach and unified loss strategy.

Abstract: The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.

</details>


### [64] [Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653)
*Veronica Thai,Rui Li,Meng Ling,Shuning Jiang,Jeremy Wolfe,Raghu Machiraju,Yan Hu,Zaibo Li,Anil Parwani,Jian Chen*

Main category: cs.CV

TL;DR: PathoGaze1.0 is a comprehensive behavioral dataset capturing pathologists' visual search and decision-making during cancer diagnosis from whole-slide images, including eye-tracking, mouse interactions, and diagnostic decisions.


<details>
  <summary>Details</summary>
Motivation: Pathologists' diagnostic accuracy averages around 70% with poor inter-observer consistency, and there's a lack of behavioral data to explain diagnostic errors and inconsistencies in whole-slide image interpretation.

Method: Collected 18.69 hours of eye-tracking, mouse interaction, stimulus tracking, viewport navigation, and diagnostic decision data from 19 pathologists interpreting 397 whole-slide images using the PTAH application-grounded testbed for ecological validity.

Result: Recorded 171,909 fixations, 263,320 saccades, and 1,867,362 mouse interaction events, creating a comprehensive behavioral dataset of pathologists' diagnostic workflow.

Conclusion: The PathoGaze1.0 dataset provides valuable behavioral data that can help explain diagnostic errors and improve training for both pathologists and AI systems, with preregistered experiments and publicly available data.

Abstract: Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.

</details>


### [65] [Group Relative Attention Guidance for Image Editing](https://arxiv.org/abs/2510.24657)
*Xuanpu Zhang,Xuesong Niu,Ruidong Chen,Dan Song,Jianhao Zeng,Penghui Du,Haoxiang Cao,Kai Wu,An-an Liu*

Main category: cs.CV

TL;DR: Proposes Group Relative Attention Guidance (GRAG) for fine-grained control over image editing intensity in Diffusion-in-Transformer models by reweighting attention delta values.


<details>
  <summary>Details</summary>
Motivation: Existing Diffusion-in-Transformer editing methods lack effective control over editing degree, limiting customization capabilities.

Method: Analyzes MM-Attention mechanism in DiT models, identifies bias vectors as inherent editing behavior, and proposes GRAG to reweight delta values between tokens and biases to modulate editing focus.

Result: GRAG enables continuous and fine-grained control over editing intensity without tuning, integrates with existing frameworks in 4 lines of code, and outperforms Classifier-Free Guidance in smoothness and precision.

Conclusion: GRAG provides an effective solution for controlling editing intensity in Diffusion-in-Transformer models, enhancing editing quality with minimal implementation effort.

Abstract: Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.

</details>


### [66] [SAGE: Structure-Aware Generative Video Transitions between Diverse Clips](https://arxiv.org/abs/2510.24667)
*Mia Kan,Yilin Liu,Niloy Mitra*

Main category: cs.CV

TL;DR: SAGE is a zero-shot method for video transitions that combines structural guidance with generative synthesis to create smooth, semantically consistent transitions between diverse clips with large temporal gaps.


<details>
  <summary>Details</summary>
Motivation: Existing video transition methods struggle with bridging diverse clips involving large temporal gaps or significant semantic differences, creating artifacts and breaking temporal coherence.

Method: SAGE uses structure-aware generative approach with line maps and motion flow for structural guidance, combined with generative synthesis, without requiring fine-tuning.

Result: Extensive experiments show SAGE outperforms both classical and generative baselines (FILM, TVG, DiffMorpher, VACE, GI) on quantitative metrics and user studies.

Conclusion: SAGE enables smooth, semantically consistent transitions between diverse video clips without fine-tuning, bridging the gap for content-aware and visually coherent transitions.

Abstract: Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.

</details>


### [67] [MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection](https://arxiv.org/abs/2510.24688)
*Yun Zhang,Zhaoliang Zheng,Johnson Liu,Zhiyu Huang,Zewei Zhou,Zonglin Meng,Tianhui Cai,Jiaqi Ma*

Main category: cs.CV

TL;DR: MIC-BEV is a Transformer-based BEV perception framework for infrastructure-based multi-camera 3D object detection that handles variable cameras and sensor degradation, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing camera-based detection models underperform in infrastructure scenarios due to multi-view setups, diverse camera configurations, degraded inputs, and varying road layouts.

Method: Uses Transformer-based BEV framework with graph-enhanced fusion module that integrates multi-view features using geometric relationships between cameras and BEV cells, plus visual cues.

Result: Achieves state-of-the-art performance on both synthetic M2I dataset and real-world RoScenes dataset, with strong robustness under extreme weather and sensor degradation.

Conclusion: MIC-BEV demonstrates strong potential for real-world deployment in intelligent transportation systems with infrastructure-based perception.

Abstract: Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.

</details>


### [68] [Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?](https://arxiv.org/abs/2510.24709)
*Yihao Li,Saeed Salehi,Lyle Ungar,Konrad P. Kording*

Main category: cs.CV

TL;DR: Vision Transformers naturally develop object binding capabilities during self-supervised pretraining, allowing them to determine whether image patches belong to the same object, which actively guides attention and serves the learning objective.


<details>
  <summary>Details</summary>
Motivation: To investigate whether object binding - the ability to group features into coherent object representations - naturally emerges in pre-trained Vision Transformers, rather than being explicitly imposed through architectural constraints like Slot Attention.

Method: Used a similarity probe to decode 'IsSameObject' (whether two patches belong to the same object) from patch embeddings across ViT layers, comparing self-supervised models (DINO, MAE, CLIP) with ImageNet-supervised models.

Result: Self-supervised ViTs achieved over 90% accuracy in detecting same-object patches, while supervised models showed markedly weaker binding capabilities. IsSameObject is encoded in a low-dimensional subspace and actively guides attention.

Conclusion: Object binding emerges naturally in self-supervised ViTs as an ability acquired through specific pretraining objectives, challenging the view that ViTs lack object binding and demonstrating how symbolic knowledge of object composition emerges in connectionist systems.

Abstract: Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.

</details>


### [69] [Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance](https://arxiv.org/abs/2510.24711)
*Yujie Wei,Shiwei Zhang,Hangjie Yuan,Yujin Han,Zhekai Chen,Jiayu Wang,Difan Zou,Xihui Liu,Yingya Zhang,Yu Liu,Hongming Shan*

Main category: cs.CV

TL;DR: ProMoE is a novel Mixture-of-Experts framework for Diffusion Transformers that uses a two-step router with explicit routing guidance to address the challenges of applying MoE to visual tokens, achieving state-of-the-art performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Existing attempts to apply MoE to Diffusion Transformers have shown limited gains due to fundamental differences between language and visual tokens. Visual tokens exhibit spatial redundancy and functional heterogeneity, which hinders expert specialization in vision MoE applications.

Method: ProMoE features a two-step router with explicit routing guidance: 1) conditional routing that partitions image tokens into conditional and unconditional sets based on functional roles, and 2) prototypical routing with learnable prototypes that refines assignments of conditional image tokens based on semantic content. It also includes a routing contrastive loss to enhance intra-expert coherence and inter-expert diversity.

Result: Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives, showing significant improvements over previous MoE approaches for Diffusion Transformers.

Conclusion: The proposed ProMoE framework successfully addresses the challenges of applying MoE to visual tokens through its two-step routing mechanism and explicit semantic guidance, establishing that such guidance is crucial for effective vision MoE systems.

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.

</details>


### [70] [Uniform Discrete Diffusion with Metric Path for Video Generation](https://arxiv.org/abs/2510.24717)
*Haoge Deng,Ting Pan,Fan Zhang,Yang Liu,Zhuoyan Luo,Yufeng Cui,Wenxuan Wang,Chunhua Shen,Shiguang Shan,Zhaoxiang Zhang,Xinlong Wang*

Main category: cs.CV

TL;DR: URSA is a discrete generative modeling framework for scalable video generation that bridges the performance gap with continuous approaches through iterative global refinement of discrete tokens.


<details>
  <summary>Details</summary>
Motivation: Discrete approaches for video generation lag behind continuous-space methods due to error accumulation and long-context inconsistency issues.

Method: URSA formulates video generation as iterative global refinement of discrete spatiotemporal tokens, using Linearized Metric Path and Resolution-dependent Timestep Shifting mechanisms, with asynchronous temporal fine-tuning for unified task handling.

Result: URSA outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods on challenging video and image generation benchmarks.

Conclusion: URSA successfully bridges the gap between discrete and continuous approaches for scalable video generation, enabling efficient high-resolution synthesis and long-duration generation with fewer inference steps.

Abstract: Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA

</details>


### [71] [Generative View Stitching](https://arxiv.org/abs/2510.24718)
*Chonghyuk Song,Michal Stary,Boyuan Chen,George Kopanas,Vincent Sitzmann*

Main category: cs.CV

TL;DR: Generative View Stitching (GVS) enables stable, collision-free camera-guided video generation by sampling entire sequences in parallel and using diffusion stitching with Omni Guidance for temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Autoregressive video diffusion models cannot incorporate future conditioning, leading to collisions with generated scenes when following predefined camera trajectories, causing model collapse.

Method: Proposes GVS sampling algorithm that extends diffusion stitching from robot planning to video generation, compatible with any Diffusion Forcing-trained model. Introduces Omni Guidance for past/future conditioning and loop-closing mechanism.

Result: GVS achieves stable, collision-free, frame-to-frame consistent video generation that closes loops for various predefined camera paths, including impossible geometries like the Impossible Staircase.

Conclusion: The method successfully addresses limitations of autoregressive models in camera-guided generation, enabling faithful scene generation along complex camera trajectories with long-range coherence.

Abstract: Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [72] [MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images](https://arxiv.org/abs/2510.24136)
*Ovi Sarkar,Md Shafiuzzaman,Md. Faysal Ahamed,Golam Mahmud,Muhammad E. H. Chowdhury*

Main category: eess.IV

TL;DR: Proposed MSRANetV2, a CNN architecture with ResNet50V2 backbone enhanced with residual attention and SE blocks, achieving state-of-the-art performance in colorectal cancer tissue classification on two public datasets.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer is a leading cause of cancer mortality, and conventional diagnostic methods like colonoscopy are subjective, time-consuming, and variable. Deep learning can enhance diagnostic precision and efficiency in digital pathology.

Method: Developed MSRANetV2 using ResNet50V2 backbone with residual attention mechanisms and squeeze-and-excitation blocks. Uses channel alignment and upsampling to fuse multi-scale features. Evaluated with five-fold stratified cross-validation on CRC-VAL-HE-7K and NCT-CRC-HE-100K datasets. Incorporated Grad-CAM for interpretability.

Result: Achieved outstanding performance: on 7K dataset - Precision: 0.9884¬±0.0151, Recall: 0.9900¬±0.0151, F1: 0.9900¬±0.0145, AUC: 0.9999¬±0.00006, Accuracy: 0.9905¬±0.0025; on 100K dataset - Precision: 0.9904¬±0.0091, Recall: 0.9900¬±0.0071, F1: 0.9900¬±0.0071, AUC: 0.9997¬±0.00016, Accuracy: 0.9902¬±0.0006.

Conclusion: MSRANetV2 is a reliable, interpretable, and high-performing model for colorectal cancer tissue classification, validated through comprehensive evaluation and visualization techniques.

Abstract: Colorectal cancer (CRC) is a leading worldwide cause of cancer-related
mortality, and the role of prompt precise detection is of paramount interest in
improving patient outcomes. Conventional diagnostic methods such as colonoscopy
and histological examination routinely exhibit subjectivity, are extremely
time-consuming, and are susceptible to variation. Through the development of
digital pathology, deep learning algorithms have become a powerful approach in
enhancing diagnostic precision and efficiency. In our work, we proposed a
convolutional neural network architecture named MSRANetV2, specially optimized
for the classification of colorectal tissue images. The model employs a
ResNet50V2 backbone, extended with residual attention mechanisms and
squeeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained
spatial features. With channel alignment and upsampling operations, MSRANetV2
effectively fuses multi-scale representations, thereby enhancing the robustness
of the classification. We evaluated our model on a five-fold stratified
cross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and
NCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,
recall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900
plus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and
0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were
0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,
0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM
visualizations were incorporated to enhance model interpretability by
highlighting tissue areas that are medically relevant. These findings validate
that MSRANetV2 is a reliable, interpretable, and high-performing architectural
model for classifying CRC tissues.

</details>
